{"title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "http://arxiv.org/abs/2210.03629v3", "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io", "source": "arxiv", "arxiv_id": "2210.03629v3", "pdf_url": "https://arxiv.org/pdf/2210.03629v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2022-10-06T01:00:32Z", "updated": "2023-03-10T01:00:17Z", "provenance": [{"route": "pinned_arxiv_id:2210.03629v3", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "authors": ["Timo Schick", "Jane Dwivedi-Yu", "Roberto DessÃ¬", "Roberta Raileanu", "Maria Lomeli", "Luke Zettlemoyer", "Nicola Cancedda", "Thomas Scialom"], "year": 2023, "url": "http://arxiv.org/abs/2302.04761v1", "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", "source": "arxiv", "arxiv_id": "2302.04761v1", "pdf_url": "https://arxiv.org/pdf/2302.04761v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-02-09T16:49:57Z", "updated": "2023-02-09T16:49:57Z", "provenance": [{"route": "pinned_arxiv_id:2302.04761v1", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "authors": ["Noah Shinn", "Federico Cassano", "Edward Berman", "Ashwin Gopinath", "Karthik Narasimhan", "Shunyu Yao"], "year": 2023, "url": "http://arxiv.org/abs/2303.11366v4", "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.", "source": "arxiv", "arxiv_id": "2303.11366v4", "pdf_url": "https://arxiv.org/pdf/2303.11366v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-03-20T18:08:50Z", "updated": "2023-10-10T05:21:45Z", "provenance": [{"route": "pinned_arxiv_id:2303.11366v4", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "Thomas L. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "year": 2023, "url": "http://arxiv.org/abs/2305.10601v2", "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "source": "arxiv", "arxiv_id": "2305.10601v2", "pdf_url": "https://arxiv.org/pdf/2305.10601v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-17T23:16:17Z", "updated": "2023-12-03T22:50:35Z", "provenance": [{"route": "pinned_arxiv_id:2305.10601v2", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Voyager: An Open-Ended Embodied Agent with Large Language Models", "authors": ["Guanzhi Wang", "Yuqi Xie", "Yunfan Jiang", "Ajay Mandlekar", "Chaowei Xiao", "Yuke Zhu", "Linxi Fan", "Anima Anandkumar"], "year": 2023, "url": "http://arxiv.org/abs/2305.16291v2", "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.", "source": "arxiv", "arxiv_id": "2305.16291v2", "pdf_url": "https://arxiv.org/pdf/2305.16291v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-05-25T17:46:38Z", "updated": "2023-10-19T16:27:03Z", "provenance": [{"route": "pinned_arxiv_id:2305.16291v2", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Survey on Large Language Model based Autonomous Agents", "authors": ["Lei Wang", "Chen Ma", "Xueyang Feng", "Zeyu Zhang", "Hao Yang", "Jingsen Zhang", "Zhiyuan Chen", "Jiakai Tang", "Xu Chen", "Yankai Lin", "Wayne Xin Zhao", "Zhewei Wei", "Ji-Rong Wen"], "year": 2023, "url": "http://arxiv.org/abs/2308.11432v7", "abstract": "Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.", "source": "arxiv", "arxiv_id": "2308.11432v7", "pdf_url": "https://arxiv.org/pdf/2308.11432v7", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "10.1007/s11704-024-40231-1", "venue": "", "published": "2023-08-22T13:30:37Z", "updated": "2025-03-02T04:04:03Z", "provenance": [{"route": "pinned_arxiv_id:2308.11432v7", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T22:50:47", "note": ""}, {"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges", "authors": ["Junyu Luo", "Weizhi Zhang", "Ye Yuan", "Yusheng Zhao", "Junwei Yang", "Yiyang Gu", "Bohan Wu", "Binqi Chen", "Ziyue Qiao", "Qingqing Long", "Rongcheng Tu", "Xiao Luo", "Wei Ju", "Zhiping Xiao", "Yifan Wang", "Meng Xiao", "Chenwu Liu", "Jingyang Yuan", "Shichang Zhang", "Yiqiao Jin", "Fan Zhang", "Xian Wu", "Hanqing Zhao", "Dacheng Tao", "Philip S. Yu", "Ming Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.21460v1", "abstract": "The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.", "source": "arxiv", "arxiv_id": "2503.21460v1", "pdf_url": "https://arxiv.org/pdf/2503.21460v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-27T12:50:17Z", "updated": "2025-03-27T12:50:17Z", "provenance": [{"route": "pinned_arxiv_id:2503.21460v1", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T22:50:47", "note": ""}, {"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic Large Language Models, a survey", "authors": ["Aske Plaat", "Max van Duijn", "Niki van Stein", "Mike Preuss", "Peter van der Putten", "Kees Joost Batenburg"], "year": 2025, "url": "http://arxiv.org/abs/2503.23037v3", "abstract": "Background: There is great interest in agentic LLMs, large language models that act as agents.\n  Objectives: We review the growing body of work in this area and provide a research agenda.\n  Methods: Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories.\n  Results: The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories.\n  Conclusions: We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "source": "arxiv", "arxiv_id": "2503.23037v3", "pdf_url": "https://arxiv.org/pdf/2503.23037v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "10.1613/jair.1.18675", "venue": "JAIR volume 84, article 29, December 2025", "published": "2025-03-29T11:02:20Z", "updated": "2025-11-22T08:55:19Z", "provenance": [{"route": "pinned_arxiv_id:2503.23037v3", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model-Brained GUI Agents: A Survey", "authors": ["Chaoyun Zhang", "Shilin He", "Jiaxu Qian", "Bowen Li", "Liqun Li", "Si Qin", "Yu Kang", "Minghua Ma", "Guyue Liu", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.18279v12", "abstract": "GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.", "source": "arxiv", "arxiv_id": "2411.18279v12", "pdf_url": "https://arxiv.org/pdf/2411.18279v12", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-27T12:13:39Z", "updated": "2025-05-06T15:08:00Z", "provenance": [{"route": "pinned_arxiv_id:2411.18279v12", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation", "authors": ["Jusheng Zhang", "Yijia Fan", "Zimo Wen", "Jian Wang", "Keze Wang"], "year": 2026, "url": "http://arxiv.org/abs/2601.04404v1", "abstract": "Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU", "source": "arxiv", "arxiv_id": "2601.04404v1", "pdf_url": "https://arxiv.org/pdf/2601.04404v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2026-01-07T21:23:05Z", "updated": "2026-01-07T21:23:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering", "authors": ["Bo Yu", "Lei Zhao"], "year": 2026, "url": "http://arxiv.org/abs/2601.04556v1", "abstract": "We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked \"Why is completion rate 80%?\", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.", "source": "arxiv", "arxiv_id": "2601.04556v1", "pdf_url": "https://arxiv.org/pdf/2601.04556v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-08T03:36:06Z", "updated": "2026-01-08T03:36:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge", "authors": ["Akbar Anbar Jafari", "Cagri Ozcinar", "Gholamreza Anbarjafari"], "year": 2026, "url": "http://arxiv.org/abs/2601.13383v1", "abstract": "The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.", "source": "arxiv", "arxiv_id": "2601.13383v1", "pdf_url": "https://arxiv.org/pdf/2601.13383v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-19T20:33:26Z", "updated": "2026-01-19T20:33:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based Optimization", "authors": ["Qiuyi Qu", "Yicheng Sui", "Yufei Sun", "Rui Chen", "Xiaofei Zhang", "Yuzhi Zhang", "Haofeng Wang", "Ge Lan", "Ning Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.12698v2", "abstract": "GPU code optimization is a key performance bottleneck for HPC workloads as well as large-model training and inference. Although compiler optimizations and hand-written kernels can partially alleviate this issue, achieving near-hardware-limit performance still relies heavily on manual code refactoring and parameter tuning. Recent progress in LLM-agent-based kernel generation and optimization has been reported, yet many approaches primarily focus on direct code rewriting, where parameter choices are often implicit and hard to control, or require human intervention, leading to unstable performance gains. This paper introduces a template-based rewriting layer on top of an agent-driven iterative loop: kernels are semantically refactored into explicitly parameterizable templates, and template parameters are then optimized via search-based autotuning, yielding more stable and higher-quality speedups. Experiments on a set of real-world kernels demonstrate speedups exceeding 3x in the best case. We extract representative CUDA kernels from SGLang as evaluation targets; the proposed agentic tuner iteratively performs templating, testing, analysis, and planning, and leverages profiling feedback to execute constrained parameter search under hardware resource limits. Compared to agent-only direct rewriting, the template-plus-search design significantly reduces the randomness of iterative optimization, making the process more interpretable and enabling a more systematic approach toward high-performance configurations. The proposed method can be further extended to OpenCL, HIP, and other backends to deliver automated performance optimization for real production workloads.", "source": "arxiv", "arxiv_id": "2601.12698v2", "pdf_url": "https://arxiv.org/pdf/2601.12698v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-19T03:40:12Z", "updated": "2026-01-21T08:52:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging", "authors": ["Zhuoka Feng", "Kang Chen", "Sihan Zhao", "Kai Xiong", "Yaoning Wang", "Minshen Yu", "Junjie Nian", "Changyi Xiao", "Yixin Cao", "Yugang Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.07309v1", "abstract": "Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.", "source": "arxiv", "arxiv_id": "2601.07309v1", "pdf_url": "https://arxiv.org/pdf/2601.07309v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T08:31:53Z", "updated": "2026-01-12T08:31:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search", "authors": ["Zefang Zong", "Dingwei Chen", "Yang Li", "Qi Yi", "Bo Zhou", "Chengming Li", "Bo Qian", "Peng Chen", "Jie Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.04767v1", "abstract": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.", "source": "arxiv", "arxiv_id": "2601.04767v1", "pdf_url": "https://arxiv.org/pdf/2601.04767v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-08T09:35:49Z", "updated": "2026-01-08T09:35:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Active Context Compression: Autonomous Memory Management in LLM Agents", "authors": ["Nikhil Verma"], "year": 2026, "url": "http://arxiv.org/abs/2601.07190v1", "abstract": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent \"Knowledge\" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.", "source": "arxiv", "arxiv_id": "2601.07190v1", "pdf_url": "https://arxiv.org/pdf/2601.07190v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T04:31:00Z", "updated": "2026-01-12T04:31:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents", "authors": ["Mustafa Arslan"], "year": 2026, "url": "http://arxiv.org/abs/2601.15311v1", "abstract": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to \"Vector Haze\", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.", "source": "arxiv", "arxiv_id": "2601.15311v1", "pdf_url": "https://arxiv.org/pdf/2601.15311v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T15:23:22Z", "updated": "2026-01-14T15:23:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning", "authors": ["Zheng Wu", "Xingyu Lou", "Xinbei Ma", "Yansi Li", "Weiwen Liu", "Weinan Zhang", "Jun Wang", "Zhuosheng Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.03641v2", "abstract": "Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates. The codes are available at https://github.com/Wuzheng02/Agent-Dice.", "source": "arxiv", "arxiv_id": "2601.03641v2", "pdf_url": "https://arxiv.org/pdf/2601.03641v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-07T06:43:50Z", "updated": "2026-01-08T08:36:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentCompress: Task-Aware Compression for Affordable Large Language Model Agents", "authors": ["Zuhair Ahmed Khan Taha", "Mohammed Mudassir Uddin", "Shahnawaz Alam"], "year": 2026, "url": "http://arxiv.org/abs/2601.05191v2", "abstract": "Large language models hold considerable promise for various applications, but their computational requirements create a barrier that many institutions cannot overcome. A single session using a 70-billion-parameter model can cost around $127 in cloud computing fees, which puts these tools out of reach for organizations operating on limited budgets. We present AgentCompress, a framework that tackles this problem through task-aware dynamic compression. The idea comes from a simple observation: not all tasks require the same computational effort. Complex reasoning, for example, is far more demanding than text reformatting, yet conventional compression applies the same reduction to both. Our approach uses a lightweight neural controller that looks at the first few tokens of each request, estimates how complex the task will be, and sends it to an appropriately quantized version of the model. This routing step adds only about 12 milliseconds of overhead. We tested the framework on 290 multi-stage workflows from domains including computer science, physics, chemistry, and biology. The results show a 68.3% reduction in computational costs while preserving 96.2% of the original success rate. These findings suggest that routing queries intelligently can make powerful language models substantially more affordable without sacrificing output quality", "source": "arxiv", "arxiv_id": "2601.05191v2", "pdf_url": "https://arxiv.org/pdf/2601.05191v2", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2026-01-08T18:13:46Z", "updated": "2026-01-12T18:25:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering", "authors": ["Di Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.04620v1", "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.", "source": "arxiv", "arxiv_id": "2601.04620v1", "pdf_url": "https://arxiv.org/pdf/2601.04620v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-08T05:49:01Z", "updated": "2026-01-08T05:49:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents", "authors": ["Arunkumar V", "Gangadharan G. R.", "Rajkumar Buyya"], "year": 2026, "url": "http://arxiv.org/abs/2601.12560v1", "abstract": "Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.", "source": "arxiv", "arxiv_id": "2601.12560v1", "pdf_url": "https://arxiv.org/pdf/2601.12560v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-18T19:51:16Z", "updated": "2026-01-18T19:51:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset", "authors": ["Tianshi Li"], "year": 2026, "url": "http://arxiv.org/abs/2601.05918v1", "abstract": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.", "source": "arxiv", "arxiv_id": "2601.05918v1", "pdf_url": "https://arxiv.org/pdf/2601.05918v1", "categories": ["cs.CR", "cs.AI", "cs.CY"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-09T16:32:33Z", "updated": "2026-01-09T16:32:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents", "authors": ["Yi Yu", "Liuyi Yao", "Yuexiang Xie", "Qingquan Tan", "Jiaqi Feng", "Yaliang Li", "Libing Wu"], "year": 2026, "url": "http://arxiv.org/abs/2601.01885v1", "abstract": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "source": "arxiv", "arxiv_id": "2601.01885v1", "pdf_url": "https://arxiv.org/pdf/2601.01885v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-05T08:24:16Z", "updated": "2026-01-05T08:24:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation", "authors": ["Nicolas Bougie", "Gian Maria Marconi", "Tony Yip", "Narimasa Watanabe"], "year": 2026, "url": "http://arxiv.org/abs/2601.00930v1", "abstract": "Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.", "source": "arxiv", "arxiv_id": "2601.00930v1", "pdf_url": "https://arxiv.org/pdf/2601.00930v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2026-01-02T03:01:33Z", "updated": "2026-01-02T03:01:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An LLM Agent-based Framework for Whaling Countermeasures", "authors": ["Daisuke Miyamoto", "Takuji Iimura", "Narushige Michishita"], "year": 2026, "url": "http://arxiv.org/abs/2601.14606v1", "abstract": "With the spread of generative AI in recent years, attacks known as Whaling have become a serious threat. Whaling is a form of social engineering that targets important high-authority individuals within organizations and uses sophisticated fraudulent emails. In the context of Japanese universities, faculty members frequently hold positions that combine research leadership with authority within institutional workflows. This structural characteristic leads to the wide public disclosure of high-value information such as publications, grants, and detailed researcher profiles. Such extensive information exposure enables the construction of highly precise target profiles using generative AI. This raises concerns that Whaling attacks based on high-precision profiling by generative AI will become prevalent. In this study, we propose a Whaling countermeasure framework for university faculty members that constructs personalized defense profiles and uses large language model (LLM)-based agents. We design agents that (i) build vulnerability profiles for each target from publicly available information on faculty members, (ii) identify potential risk scenarios relevant to Whaling defense based on those profiles, (iii) construct defense profiles corresponding to the vulnerabilities and anticipated risks, and (iv) analyze Whaling emails using the defense profiles. Furthermore, we conduct a preliminary risk-assessment experiment. The results indicate that the proposed method can produce judgments accompanied by explanations of response policies that are consistent with the work context of faculty members who are Whaling targets. The findings also highlight practical challenges and considerations for future operational deployment and systematic evaluation.", "source": "arxiv", "arxiv_id": "2601.14606v1", "pdf_url": "https://arxiv.org/pdf/2601.14606v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-21T02:43:42Z", "updated": "2026-01-21T02:43:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Architecting Agentic Communities using Design Patterns", "authors": ["Zoran Milosevic", "Fethi Rabhi"], "year": 2026, "url": "http://arxiv.org/abs/2601.03624v2", "abstract": "The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.", "source": "arxiv", "arxiv_id": "2601.03624v2", "pdf_url": "https://arxiv.org/pdf/2601.03624v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-07T06:10:07Z", "updated": "2026-01-08T20:30:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking", "authors": ["Qiang Zhang", "Boli Chen", "Fanrui Zhang", "Ruixue Ding", "Shihang Wang", "Qiuchen Wang", "Yinfeng Huang", "Haonan Zhang", "Rongxiang Zhu", "Pengyong Wang", "Ailin Ren", "Xin Li", "Pengjun Xie", "Jiawei Liu", "Ning Guo", "Jingren Zhou", "Zheng-Jun Zha"], "year": 2026, "url": "http://arxiv.org/abs/2601.06487v2", "abstract": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.", "source": "arxiv", "arxiv_id": "2601.06487v2", "pdf_url": "https://arxiv.org/pdf/2601.06487v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-10T08:43:07Z", "updated": "2026-01-22T09:16:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automatic Prompt Optimization for Dataset-Level Feature Discovery", "authors": ["Adrian Cosma", "Oleg Szehr", "David Kletz", "Alessandro Antonucci", "Olivier Pelletier"], "year": 2026, "url": "http://arxiv.org/abs/2601.13922v1", "abstract": "Feature extraction from unstructured text is a critical step in many downstream classification pipelines, yet current approaches largely rely on hand-crafted prompts or fixed feature schemas. We formulate feature discovery as a dataset-level prompt optimization problem: given a labelled text corpus, the goal is to induce a global set of interpretable and discriminative feature definitions whose realizations optimize a downstream supervised learning objective. To this end, we propose a multi-agent prompt optimization framework in which language-model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on this structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. This formulation departs from prior prompt optimization methods that rely on per-sample supervision and provides a principled mechanism for automatic feature discovery from unstructured text.", "source": "arxiv", "arxiv_id": "2601.13922v1", "pdf_url": "https://arxiv.org/pdf/2601.13922v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-20T12:51:03Z", "updated": "2026-01-20T12:51:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning", "authors": ["Nicholas X. Wang", "Neel V. Parpia", "Aaryan D. Parikh", "Aggelos K. Katsaggelos"], "year": 2026, "url": "http://arxiv.org/abs/2601.06098v1", "abstract": "Intuitive learning is crucial for developing deep conceptual understanding, especially in STEM education, where students often struggle with abstract and interconnected concepts. Automatic question generation has become an effective strategy for personalized and adaptive learning. However, its effectiveness is hindered by hallucinations in large language models (LLMs), which may generate factually incorrect, ambiguous, or pedagogically inconsistent questions. To address this issue, we propose a novel framework that combines causal-graph-guided Chain-of-Thought (CoT) reasoning with a multi-agent LLM architecture. This approach ensures the generation of accurate, meaningful, and curriculum-aligned questions. Causal graphs provide an explicit representation of domain knowledge, while CoT reasoning facilitates a structured, step-by-step traversal of related concepts. Dedicated LLM agents are assigned specific tasks such as graph pathfinding, reasoning, validation, and output, all working within domain constraints. A dual validation mechanism-at both the conceptual and output stages-greatly reduces hallucinations. Experimental results demonstrate up to a 70% improvement in quality compared to reference methods and yielded highly favorable outcomes in subjective evaluations.", "source": "arxiv", "arxiv_id": "2601.06098v1", "pdf_url": "https://arxiv.org/pdf/2601.06098v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-02T08:49:58Z", "updated": "2026-01-02T08:49:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Autonomous Quantum Simulation through Large Language Model Agents", "authors": ["Weitang Li", "Jiajun Ren", "Lixue Cheng", "Cunxi Gong"], "year": 2026, "url": "http://arxiv.org/abs/2601.10194v1", "abstract": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Tensor network methods are powerful tools for quantum simulation, but their effective use requires expertise typically acquired through years of graduate training. By combining in-context learning with curated documentation and multi-agent decomposition, we create autonomous AI agents that can be trained in specialized computational domains within minutes. We benchmark three configurations (baseline, single-agent with in-context learning, and multi-agent with in-context learning) on problems spanning quantum phase transitions, open quantum system dynamics, and photochemical reactions. Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential. Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.", "source": "arxiv", "arxiv_id": "2601.10194v1", "pdf_url": "https://arxiv.org/pdf/2601.10194v1", "categories": ["quant-ph", "physics.chem-ph"], "primary_category": "quant-ph", "doi": "", "venue": "", "published": "2026-01-15T08:50:57Z", "updated": "2026-01-15T08:50:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents", "authors": ["Yunhao Feng", "Yige Li", "Yutao Wu", "Yingshui Tan", "Yanming Guo", "Yifan Ding", "Kun Zhai", "Xingjun Ma", "Yu-Gang Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.04566v2", "abstract": "Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \\textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \\textbf{planning attacks}, \\textbf{memory attacks}, and \\textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \\textbf{Agent QA}, \\textbf{Agent Code}, \\textbf{Agent Web}, and \\textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \\textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\\% of planning attacks, 77.97\\% of memory attacks, and 60.28\\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.", "source": "arxiv", "arxiv_id": "2601.04566v2", "pdf_url": "https://arxiv.org/pdf/2601.04566v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-08T03:49:39Z", "updated": "2026-01-11T08:47:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making", "authors": ["Danial Amin"], "year": 2026, "url": "http://arxiv.org/abs/2601.01522v1", "abstract": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.", "source": "arxiv", "arxiv_id": "2601.01522v1", "pdf_url": "https://arxiv.org/pdf/2601.01522v1", "categories": ["cs.AI", "cs.CL", "cs.ET"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-04T13:19:27Z", "updated": "2026-01-04T13:19:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents", "authors": ["Miao Su", "Yucan Guo", "Zhongni Hou", "Long Bai", "Zixuan Li", "Yufei Zhang", "Guojun Yin", "Wei Lin", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "year": 2026, "url": "http://arxiv.org/abs/2601.07468v1", "abstract": "Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. During memory construction, it first builds a semantic timeline rather than a dialogue one. Then, it consolidates temporally continuous and semantically related information into a durative memory. During memory utilization, it incorporates the query's temporal intent on the semantic timeline, enabling the retrieval of temporally appropriate durative memories and providing time-valid, duration-consistent context to support response generation. Experiments on LongMemEval and LoCoMo show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy, demonstrating the effectiveness of the proposed method.", "source": "arxiv", "arxiv_id": "2601.07468v1", "pdf_url": "https://arxiv.org/pdf/2601.07468v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T12:24:44Z", "updated": "2026-01-12T12:24:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence", "authors": ["Sumanth Balaji", "Piyush Mishra", "Aashraya Sachdeva", "Suraj Agrawal"], "year": 2026, "url": "http://arxiv.org/abs/2601.00596v1", "abstract": "Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.", "source": "arxiv", "arxiv_id": "2601.00596v1", "pdf_url": "https://arxiv.org/pdf/2601.00596v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-02T07:21:23Z", "updated": "2026-01-02T07:21:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents", "authors": ["Kaiyu Zhou", "Yongsen Zheng", "Yicheng He", "Meng Xue", "Xueluan Gong", "Yuji Wang", "Kwok-Yan Lam"], "year": 2026, "url": "http://arxiv.org/abs/2601.10955v1", "abstract": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions. We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Our method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation. Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.", "source": "arxiv", "arxiv_id": "2601.10955v1", "pdf_url": "https://arxiv.org/pdf/2601.10955v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-16T02:47:45Z", "updated": "2026-01-16T02:47:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity", "authors": ["Doyoung Kim", "Zhiwei Ren", "Jie Hao", "Zhongkai Sun", "Lichao Wang", "Xiyao Ma", "Zack Ye", "Xu Han", "Jun Yin", "Heng Ji", "Wei Shen", "Xing Fan", "Benjamin Yao", "Chenlei Guo"], "year": 2026, "url": "http://arxiv.org/abs/2601.00268v1", "abstract": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.", "source": "arxiv", "arxiv_id": "2601.00268v1", "pdf_url": "https://arxiv.org/pdf/2601.00268v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-01T09:19:20Z", "updated": "2026-01-01T09:19:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning", "authors": ["Wei Fang", "James Glass"], "year": 2026, "url": "http://arxiv.org/abs/2601.07782v1", "abstract": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.", "source": "arxiv", "arxiv_id": "2601.07782v1", "pdf_url": "https://arxiv.org/pdf/2601.07782v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-12T17:58:39Z", "updated": "2026-01-12T17:58:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Static Summarization: Proactive Memory Extraction for LLM Agents", "authors": ["Chengyuan Yang", "Zequn Sun", "Wei Wei", "Wei Hu"], "year": 2026, "url": "http://arxiv.org/abs/2601.04463v1", "abstract": "Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is \"ahead-of-time\", acting as a blind \"feed-forward\" process that misses important details because it doesn't know future tasks. Second, extraction is usually \"one-off\", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.", "source": "arxiv", "arxiv_id": "2601.04463v1", "pdf_url": "https://arxiv.org/pdf/2601.04463v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-08T00:37:29Z", "updated": "2026-01-08T00:37:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CEDAR: Context Engineering for Agentic Data Science", "authors": ["Rishiraj Saha Roy", "Chris Hinze", "Luzian Hahn", "Fabian Kuech"], "year": 2026, "url": "http://arxiv.org/abs/2601.06606v1", "abstract": "We demonstrate CEDAR, an application for automating data science (DS) tasks with an agentic setup. Solving DS problems with LLMs is an underexplored area that has immense market value. The challenges are manifold: task complexities, data sizes, computational limitations, and context restrictions. We show that these can be alleviated via effective context engineering. We first impose structure into the initial prompt with DS-specific input fields, that serve as instructions for the agentic system. The solution is then materialized as an enumerated sequence of interleaved plan and code blocks generated by separate LLM agents, providing a readable structure to the context at any step of the workflow. Function calls for generating these intermediate texts, and for corresponding Python code, ensure that data stays local, and only aggregate statistics and associated instructions are injected into LLM prompts. Fault tolerance and context management are introduced via iterative code generation and smart history rendering. The viability of our agentic data scientist is demonstrated using canonical Kaggle challenges.", "source": "arxiv", "arxiv_id": "2601.06606v1", "pdf_url": "https://arxiv.org/pdf/2601.06606v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-10T16:05:04Z", "updated": "2026-01-10T16:05:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CHASE: LLM Agents for Dissecting Malicious PyPI Packages", "authors": ["Takaaki Toda", "Tatsuya Mori"], "year": 2026, "url": "http://arxiv.org/abs/2601.06838v1", "abstract": "Modern software package registries like PyPI have become critical infrastructure for software development, but are increasingly exploited by threat actors distributing malicious packages with sophisticated multi-stage attack chains. While Large Language Models (LLMs) offer promising capabilities for automated code analysis, their application to security-critical malware detection faces fundamental challenges, including hallucination and context confusion, which can lead to missed detections or false alarms. We present CHASE (Collaborative Hierarchical Agents for Security Exploration), a high-reliability multi-agent architecture that addresses these limitations through a Plan-and-Execute coordination model, specialized Worker Agents focused on specific analysis aspects, and integration with deterministic security tools for critical operations. Our key insight is that reliability in LLM-based security analysis emerges not from improving individual model capabilities but from architecting systems that compensate for LLM weaknesses while leveraging their semantic understanding strengths. Evaluation on a dataset of 3,000 packages (500 malicious, 2,500 benign) demonstrates that CHASE achieves 98.4% recall with only 0.08% false positive rate, while maintaining a practical median analysis time of 4.5 minutes per package, making it suitable for operational deployment in automated package screening. Furthermore, we conducted a survey with cybersecurity professionals to evaluate the generated analysis reports, identifying their key strengths and areas for improvement. This work provides a blueprint for building reliable AI-powered security tools that can scale with the growing complexity of modern software supply chains. Our project page is available at https://t0d4.github.io/CHASE-AIware25/", "source": "arxiv", "arxiv_id": "2601.06838v1", "pdf_url": "https://arxiv.org/pdf/2601.06838v1", "categories": ["cs.CR", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-11T10:06:14Z", "updated": "2026-01-11T10:06:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents", "authors": ["Xiucheng Xu", "Bingbing Xu", "Xueyun Tian", "Zihe Huang", "Rongxin Chen", "Yunfan Li", "Huawei Shen"], "year": 2026, "url": "http://arxiv.org/abs/2601.14287v1", "abstract": "External memory systems are pivotal for enabling Large Language Model (LLM) agents to maintain persistent knowledge and perform long-horizon decision-making. Existing paradigms typically follow a two-stage process: computationally expensive memory construction (e.g., structuring data into graphs) followed by naive retrieval-augmented generation. However, our empirical analysis reveals two fundamental limitations: complex construction incurs high costs with marginal performance gains, and simple context concatenation fails to bridge the gap between retrieval recall and reasoning accuracy. To address these challenges, we propose CoM (Chain-of-Memory), a novel framework that advocates for a paradigm shift toward lightweight construction paired with sophisticated utilization. CoM introduces a Chain-of-Memory mechanism that organizes retrieved fragments into coherent inference paths through dynamic evolution, utilizing adaptive truncation to prune irrelevant noise. Extensive experiments on the LongMemEval and LoCoMo benchmarks demonstrate that CoM outperforms strong baselines with accuracy gains of 7.5%-10.4%, while drastically reducing computational overhead to approximately 2.7% of token consumption and 6.0% of latency compared to complex memory architectures.", "source": "arxiv", "arxiv_id": "2601.14287v1", "pdf_url": "https://arxiv.org/pdf/2601.14287v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-14T04:42:15Z", "updated": "2026-01-14T04:42:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration", "authors": ["Yang Zhao", "Yangou Ouyang", "Xiao Ding", "Hepeng Wang", "Bibo Cai", "Kai Xiong", "Jinglong Gao", "Zhouhao Sun", "Li Du", "Bing Qin", "Ting Liu"], "year": 2026, "url": "http://arxiv.org/abs/2601.07224v1", "abstract": "While Hybrid Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has become the standard paradigm for training LLM agents, effective mechanisms for data allocation between these stages remain largely underexplored. Current data arbitration strategies often rely on surface-level heuristics that fail to diagnose intrinsic learning needs. Since SFT targets pattern consolidation through imitation while RL drives structural adaptation via exploration, misaligning data with these functional roles causes severe optimization interference. We propose PRISM, a dynamics-aware framework grounded in Schema Theory that arbitrates data based on its degree of cognitive conflict with the model's existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22$\\times$. Our findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.", "source": "arxiv", "arxiv_id": "2601.07224v1", "pdf_url": "https://arxiv.org/pdf/2601.07224v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T05:43:20Z", "updated": "2026-01-12T05:43:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Continuum Memory Architectures for Long-Horizon LLM Agents", "authors": ["Joe Logan"], "year": 2026, "url": "http://arxiv.org/abs/2601.09913v1", "abstract": "Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \\textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.", "source": "arxiv", "arxiv_id": "2601.09913v1", "pdf_url": "https://arxiv.org/pdf/2601.09913v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T22:40:35Z", "updated": "2026-01-14T22:40:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics", "authors": ["Sukesh Subaharan"], "year": 2026, "url": "http://arxiv.org/abs/2601.16087v1", "abstract": "Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.", "source": "arxiv", "arxiv_id": "2601.16087v1", "pdf_url": "https://arxiv.org/pdf/2601.16087v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-22T16:34:05Z", "updated": "2026-01-22T16:34:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants", "authors": ["Ziyi Shi", "Xusen Guo", "Hongliang Lu", "Mingxing Peng", "Haotian Wang", "Zheng Zhu", "Zhenning Li", "Yuxuan Liang", "Xinhu Zheng", "Hai Yang"], "year": 2026, "url": "http://arxiv.org/abs/2601.09264v1", "abstract": "Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...", "source": "arxiv", "arxiv_id": "2601.09264v1", "pdf_url": "https://arxiv.org/pdf/2601.09264v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T07:59:44Z", "updated": "2026-01-14T07:59:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems", "authors": ["Elham Tajik", "Conrad Borchers", "Bahar Shahrokhian", "Sebastian Simon", "Ali Keramati", "Sonika Pal", "Sreecharan Sankaranarayanan"], "year": 2026, "url": "http://arxiv.org/abs/2601.12618v1", "abstract": "Learning analytics researchers often analyze qualitative student data such as coded annotations or interview transcripts to understand learning processes. With the rise of generative AI, fully automated and human-AI workflows have emerged as promising methods for analysis. However, methodological standards to guide such workflows remain limited. In this study, we propose that reasoning traces generated by large language model (LLM) agents, especially within multi-agent systems, constitute a novel and rich form of process data to enhance interpretive practices in qualitative coding. We apply cosine similarity to LLM reasoning traces to systematically detect, quantify, and interpret disagreements among agents, reframing disagreement as a meaningful analytic signal. Analyzing nearly 10,000 instances of agent pairs coding human tutoring dialog segments, we show that LLM agents' semantic reasoning similarity robustly differentiates consensus from disagreement and correlates with human coding reliability. Qualitative analysis guided by this metric reveals nuanced instructional sub-functions within codes and opportunities for conceptual codebook refinement. By integrating quantitative similarity metrics with qualitative review, our method has the potential to improve and accelerate establishing inter-rater reliability during coding by surfacing interpretive ambiguity, especially when LLMs collaborate with humans. We discuss how reasoning-trace disagreements represent a valuable new class of analytic signals advancing methodological rigor and interpretive depth in educational research.", "source": "arxiv", "arxiv_id": "2601.12618v1", "pdf_url": "https://arxiv.org/pdf/2601.12618v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.1145/3785022.3785101", "venue": "", "published": "2026-01-18T23:19:49Z", "updated": "2026-01-18T23:19:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Effects of personality steering on cooperative behavior in Large Language Model agents", "authors": ["Mizuki Sakai", "Mizuki Yokoyama", "Wakaba Tateishi", "Genki Ichinose"], "year": 2026, "url": "http://arxiv.org/abs/2601.05302v2", "abstract": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality scores of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.", "source": "arxiv", "arxiv_id": "2601.05302v2", "pdf_url": "https://arxiv.org/pdf/2601.05302v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-08T14:23:45Z", "updated": "2026-01-14T12:54:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems", "authors": ["Michael Bao"], "year": 2026, "url": "http://arxiv.org/abs/2601.00994v1", "abstract": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.", "source": "arxiv", "arxiv_id": "2601.00994v1", "pdf_url": "https://arxiv.org/pdf/2601.00994v1", "categories": ["cs.AI", "cs.CY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-02T22:10:09Z", "updated": "2026-01-02T22:10:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories", "authors": ["Qian Xiong", "Yuekai Huang", "Bo Yang", "Yujia Zheng", "Tianhao Li", "Ziyou Jiang", "Zhiyuan Chang", "Zhaoyang Li", "Huanxiang Feng", "Mingyang Li"], "year": 2026, "url": "http://arxiv.org/abs/2601.15120v2", "abstract": "LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of \"intent deviation\" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a \"Real-to-Virtual\" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.", "source": "arxiv", "arxiv_id": "2601.15120v2", "pdf_url": "https://arxiv.org/pdf/2601.15120v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-21T15:58:54Z", "updated": "2026-01-22T12:08:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework", "authors": ["Yixiao Peng", "Hao Hu", "Feiyang Li", "Xinye Cao", "Yingchang Jiang", "Jipeng Tang", "Guoshun Nan", "Yuling Liu"], "year": 2026, "url": "http://arxiv.org/abs/2601.07122v1", "abstract": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&CK's Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.", "source": "arxiv", "arxiv_id": "2601.07122v1", "pdf_url": "https://arxiv.org/pdf/2601.07122v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-12T01:25:41Z", "updated": "2026-01-12T01:25:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents", "authors": ["Hyunjun Kim"], "year": 2026, "url": "http://arxiv.org/abs/2601.11585v1", "abstract": "Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.", "source": "arxiv", "arxiv_id": "2601.11585v1", "pdf_url": "https://arxiv.org/pdf/2601.11585v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-01T19:43:05Z", "updated": "2026-01-01T19:43:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis", "authors": ["Xiaoshuai Song", "Haofei Chang", "Guanting Dong", "Yutao Zhu", "Zhicheng Dou", "Ji-Rong Wen"], "year": 2026, "url": "http://arxiv.org/abs/2601.05808v1", "abstract": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.", "source": "arxiv", "arxiv_id": "2601.05808v1", "pdf_url": "https://arxiv.org/pdf/2601.05808v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-09T14:32:06Z", "updated": "2026-01-09T14:32:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems", "authors": ["Guibin Zhang", "Haiyang Yu", "Kaiming Yang", "Bingli Wu", "Fei Huang", "Yongbin Li", "Shuicheng Yan"], "year": 2026, "url": "http://arxiv.org/abs/2601.02695v1", "abstract": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "source": "arxiv", "arxiv_id": "2601.02695v1", "pdf_url": "https://arxiv.org/pdf/2601.02695v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-06T04:06:46Z", "updated": "2026-01-06T04:06:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FROAV: A Framework for RAG Observation and Agent Verification -- Lowering the Barrier to LLM Agent Research", "authors": ["Tzu-Hsuan Lin", "Chih-Hsuan Kao"], "year": 2026, "url": "http://arxiv.org/abs/2601.07504v1", "abstract": "The rapid advancement of Large Language Models (LLMs) and their integration into autonomous agent systems has created unprecedented opportunities for document analysis, decision support, and knowledge retrieval. However, the complexity of developing, evaluating, and iterating on LLM-based agent workflows presents significant barriers to researchers, particularly those without extensive software engineering expertise. We present FROAV (Framework for RAG Observation and Agent Verification), an open-source research platform that democratizes LLM agent research by providing a plug-and-play architecture combining visual workflow orchestration, a comprehensive evaluation framework, and extensible Python integration. FROAV implements a multi-stage Retrieval-Augmented Generation (RAG) pipeline coupled with a rigorous \"LLM-as-a-Judge\" evaluation system, all accessible through intuitive graphical interfaces. Our framework integrates n8n for no-code workflow design, PostgreSQL for granular data management, FastAPI for flexible backend logic, and Streamlit for human-in-the-loop interaction. Through this integrated ecosystem, researchers can rapidly prototype RAG strategies, conduct prompt engineering experiments, validate agent performance against human judgments, and collect structured feedback-all without writing infrastructure code. We demonstrate the framework's utility through its application to financial document analysis, while emphasizing its material-agnostic architecture that adapts to any domain requiring semantic analysis. FROAV represents a significant step toward making LLM agent research accessible to a broader scientific community, enabling researchers to focus on hypothesis testing and algorithmic innovation rather than system integration challenges.", "source": "arxiv", "arxiv_id": "2601.07504v1", "pdf_url": "https://arxiv.org/pdf/2601.07504v1", "categories": ["cs.LG", "cs.SE"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-12T13:02:32Z", "updated": "2026-01-12T13:02:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management", "authors": ["Weitao Ma", "Xiaocheng Feng", "Lei Huang", "Xiachong Feng", "Zhanyu Ma", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Bing Qin"], "year": 2026, "url": "http://arxiv.org/abs/2601.08435v1", "abstract": "Effective memory management is essential for large language model agents to navigate long-horizon tasks. Recent research has explored using Reinforcement Learning to develop specialized memory manager agents. However, existing approaches rely on final task performance as the primary reward, which results in severe reward sparsity and ineffective credit assignment, providing insufficient guidance for individual memory operations. To this end, we propose Fine-Mem, a unified framework designed for fine-grained feedback alignment. First, we introduce a Chunk-level Step Reward to provide immediate step-level supervision via auxiliary chunk-specific question answering tasks. Second, we devise Evidence-Anchored Reward Attribution to redistribute global rewards by anchoring credit to key memory operations, based on the specific memory items utilized as evidence in reasoning. Together, these components enable stable policy optimization and align local memory operations with the long-term utility of memory. Experiments on Memalpha and MemoryAgentBench demonstrate that Fine-Mem consistently outperforms strong baselines, achieving superior success rates across various sub-tasks. Further analysis reveals its adaptability and strong generalization capabilities across diverse model configurations and backbones.", "source": "arxiv", "arxiv_id": "2601.08435v1", "pdf_url": "https://arxiv.org/pdf/2601.08435v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-13T11:06:17Z", "updated": "2026-01-13T11:06:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems", "authors": ["Shanli Xing", "Yiyan Zhai", "Alexander Jiang", "Yixin Dong", "Yong Wu", "Zihao Ye", "Charlie Ruan", "Yingyi Huang", "Yineng Zhang", "Liangsheng Yin", "Aksara Bayyapu", "Luis Ceze", "Tianqi Chen"], "year": 2026, "url": "http://arxiv.org/abs/2601.00227v1", "abstract": "Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.", "source": "arxiv", "arxiv_id": "2601.00227v1", "pdf_url": "https://arxiv.org/pdf/2601.00227v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-01T06:18:53Z", "updated": "2026-01-01T06:18:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Failure to Mastery: Generating Hard Samples for Tool-use Agents", "authors": ["Bingguang Hao", "Zengzhuang Xu", "Yuntao Wen", "Xinyi Xu", "Yang Liu", "Tong Zhao", "Maolin Wang", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Xiangyu Zhao", "Chenyi Zhuang", "Ji Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.01498v1", "abstract": "The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.", "source": "arxiv", "arxiv_id": "2601.01498v1", "pdf_url": "https://arxiv.org/pdf/2601.01498v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-04T11:56:33Z", "updated": "2026-01-04T11:56:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Generative AI for Social Impact", "authors": ["Lingkai Kong", "Cheol Woo Kim", "Davin Choo", "Milind Tambe"], "year": 2026, "url": "http://arxiv.org/abs/2601.04238v1", "abstract": "AI for Social Impact (AI4SI) has achieved compelling results in public health, conservation, and security, yet scaling these successes remains difficult due to a persistent deployment bottleneck. We characterize this bottleneck through three coupled gaps: observational scarcity resulting from limited or unreliable data; policy synthesis challenges involving combinatorial decisions and nonstationarity; and the friction of human-AI alignment when incorporating tacit expert knowledge and dynamic constraints. We argue that Generative AI offers a unified pathway to bridge these gaps. LLM agents assist in human-AI alignment by translating natural-language guidance into executable objectives and constraints for downstream planners, while diffusion models generate realistic synthetic data and support uncertainty-aware modeling to improve policy robustness and transfer across deployments. Together, these tools enable scalable, adaptable, and human-aligned AI systems for resource optimization in high-stakes settings.", "source": "arxiv", "arxiv_id": "2601.04238v1", "pdf_url": "https://arxiv.org/pdf/2601.04238v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2026-01-05T02:44:39Z", "updated": "2026-01-05T02:44:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games", "authors": ["Christopher Kao", "Vanshika Vats", "James Davis"], "year": 2026, "url": "http://arxiv.org/abs/2601.13709v1", "abstract": "Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.", "source": "arxiv", "arxiv_id": "2601.13709v1", "pdf_url": "https://arxiv.org/pdf/2601.13709v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-20T08:07:21Z", "updated": "2026-01-20T08:07:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Holmes: An Evidence-Grounded LLM Agent for Auditable DDoS Investigation in Cloud Networks", "authors": ["Haodong Chen", "Ziheng Zhang", "Jinghui Jiang", "Qiang Su", "Qiao Xiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.14601v1", "abstract": "Cloud environments face frequent DDoS threats due to centralized resources and broad attack surfaces. Modern cloud-native DDoS attacks further evolve rapidly and often blend multi-vector strategies, creating an operational dilemma: defenders need wire-speed monitoring while also requiring explainable, auditable attribution for response. Existing rule-based and supervised-learning approaches typically output black-box scores or labels, provide limited evidence chains, and generalize poorly to unseen attack variants; meanwhile, high-quality labeled data is often difficult to obtain in cloud settings.\n  We present Holmes (DDoS Detective), an LLM-based DDoS detection agent that reframes the model as a virtual SRE investigator rather than an end-to-end classifier. Holmes couples a funnel-like hierarchical workflow (counters/sFlow for continuous sensing and triage; PCAP evidence collection triggered only on anomaly windows) with an Evidence Pack abstraction that converts binary packets into compact, reproducible, high-signal structured evidence. On top of this evidence interface, Holmes enforces a structure-first investigation protocol and strict JSON/quotation constraints to produce machine-consumable reports with auditable evidence anchors.\n  We evaluate Holmes on CICDDoS2019 reflection/amplification attacks and script-triggered flooding scenarios. Results show that Holmes produces attribution decisions grounded in salient evidence anchors across diverse attack families, and when errors occur, its audit logs make the failure source easy to localize, demonstrating the practicality of an LLM agent for cost-controlled and traceable DDoS investigation in cloud operations.", "source": "arxiv", "arxiv_id": "2601.14601v1", "pdf_url": "https://arxiv.org/pdf/2601.14601v1", "categories": ["cs.CR", "cs.NI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-21T02:39:46Z", "updated": "2026-01-21T02:39:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents", "authors": ["Xin Quan", "Jiafeng Xiong", "Marco Valentino", "AndrÃ© Freitas"], "year": 2026, "url": "http://arxiv.org/abs/2601.08742v1", "abstract": "Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.", "source": "arxiv", "arxiv_id": "2601.08742v1", "pdf_url": "https://arxiv.org/pdf/2601.08742v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-13T17:18:38Z", "updated": "2026-01-13T17:18:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents", "authors": ["Chenglin Yu", "Yuchen Wang", "Songmiao Wang", "Hongxia Yang", "Ming Li"], "year": 2026, "url": "http://arxiv.org/abs/2601.03204v1", "abstract": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent", "source": "arxiv", "arxiv_id": "2601.03204v1", "pdf_url": "https://arxiv.org/pdf/2601.03204v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-06T17:35:57Z", "updated": "2026-01-06T17:35:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "IntAgent: NWDAF-Based Intent LLM Agent Towards Advanced Next Generation Networks", "authors": ["Abdelrahman Soliman", "Ahmed Refaey", "Aiman Erbad", "Amr Mohamed"], "year": 2026, "url": "http://arxiv.org/abs/2601.13114v1", "abstract": "Intent-based networks (IBNs) are gaining prominence as an innovative technology that automates network operations through high-level request statements, defining what the network should achieve. In this work, we introduce IntAgent, an intelligent intent LLM agent that integrates NWDAF analytics and tools to fulfill the network operator's intents. Unlike previous approaches, we develop an intent tools engine directly within the NWDAF analytics engine, allowing our agent to utilize live network analytics to inform its reasoning and tool selection. We offer an enriched, 3GPP-compliant data source that enhances the dynamic, context-aware fulfillment of network operator goals, along with an MCP tools server for scheduling, monitoring, and analytics tools. We demonstrate the efficacy of our framework through two practical use cases: ML-based traffic prediction and scheduled policy enforcement, which validate IntAgent's ability to autonomously fulfill complex network intents.", "source": "arxiv", "arxiv_id": "2601.13114v1", "pdf_url": "https://arxiv.org/pdf/2601.13114v1", "categories": ["cs.NI", "cs.AI", "eess.SY"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2026-01-19T14:55:48Z", "updated": "2026-01-19T14:55:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "KryptoPilot: An Open-World Knowledge-Augmented LLM Agent for Automated Cryptographic Exploitation", "authors": ["Xiaonan Liu", "Zhihao Li", "Xiao Lan", "Hao Ren", "Haizhou Wang", "Xingshu Chen"], "year": 2026, "url": "http://arxiv.org/abs/2601.09129v1", "abstract": "Capture-the-Flag (CTF) competitions play a central role in modern cybersecurity as a platform for training practitioners and evaluating offensive and defensive techniques derived from real-world vulnerabilities. Despite recent advances in large language models (LLMs), existing LLM-based agents remain ineffective on high-difficulty cryptographic CTF challenges, which require precise cryptanalytic knowledge, stable long-horizon reasoning, and disciplined interaction with specialized toolchains. Through a systematic exploratory study, we show that insufficient knowledge granularity, rather than model reasoning capacity, is a primary factor limiting successful cryptographic exploitation: coarse or abstracted external knowledge often fails to support correct attack modeling and implementation. Motivated by this observation, we propose KryptoPilot, an open-world knowledge-augmented LLM agent for automated cryptographic exploitation. KryptoPilot integrates dynamic open-world knowledge acquisition via a Deep Research pipeline, a persistent workspace for structured knowledge reuse, and a governance subsystem that stabilizes reasoning through behavioral constraints and cost-aware model routing. This design enables precise knowledge alignment while maintaining efficient reasoning across heterogeneous subtasks. We evaluate KryptoPilot on two established CTF benchmarks and in six real-world CTF competitions. KryptoPilot achieves a complete solve rate on InterCode-CTF, solves between 56 and 60 percent of cryptographic challenges on the NYU-CTF benchmark, and successfully solves 26 out of 33 cryptographic challenges in live competitions, including multiple earliest-solved and uniquely-solved instances. These results demonstrate the necessity of open-world, fine-grained knowledge augmentation and governed reasoning for scaling LLM-based agents to real-world cryptographic exploitation.", "source": "arxiv", "arxiv_id": "2601.09129v1", "pdf_url": "https://arxiv.org/pdf/2601.09129v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-14T04:02:40Z", "updated": "2026-01-14T04:02:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery", "authors": ["Zixuan Xiao", "Jun Ma"], "year": 2026, "url": "http://arxiv.org/abs/2601.02757v1", "abstract": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.", "source": "arxiv", "arxiv_id": "2601.02757v1", "pdf_url": "https://arxiv.org/pdf/2601.02757v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1016/j.autcon.2025.106341", "venue": "Automation in Construction 177 (2025) 106341", "published": "2026-01-06T06:49:51Z", "updated": "2026-01-06T06:49:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization", "authors": ["Simon Paquette-Greenbaum", "Jiangbo Yu"], "year": 2026, "url": "http://arxiv.org/abs/2601.00770v1", "abstract": "Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.", "source": "arxiv", "arxiv_id": "2601.00770v1", "pdf_url": "https://arxiv.org/pdf/2601.00770v1", "categories": ["cs.CE", "cs.AI", "econ.GN"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2026-01-02T18:02:13Z", "updated": "2026-01-02T18:02:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents in Law: Taxonomy, Applications, and Challenges", "authors": ["Shuang Liu", "Ruijia Zhang", "Ruoyun Ma", "Yujia Deng", "Lanyi Zhu", "Jiayu Li", "Zelong Li", "Zhibin Shen", "Mengnan Du"], "year": 2026, "url": "http://arxiv.org/abs/2601.06216v1", "abstract": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. Recently, LLM agents have attracted significant attention as a solution to these challenges, utilizing advanced capabilities such as planning, memory, and tool usage to meet the rigorous standards of legal practice. In this paper, we present a comprehensive survey of LLM agents for legal tasks, analyzing how these architectures bridge the gap between technical capabilities and domain-specific needs. Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants.", "source": "arxiv", "arxiv_id": "2601.06216v1", "pdf_url": "https://arxiv.org/pdf/2601.06216v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2026-01-08T21:04:35Z", "updated": "2026-01-08T21:04:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach", "authors": ["Kuo Liang", "Yuhang Lu", "Jianming Mao", "Shuyi Sun", "Chunwei Yang", "Congcong Zeng", "Xiao Jin", "Hanzhang Qin", "Ruihao Zhu", "Chung-Piaw Teo"], "year": 2026, "url": "http://arxiv.org/abs/2601.09635v1", "abstract": "Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.", "source": "arxiv", "arxiv_id": "2601.09635v1", "pdf_url": "https://arxiv.org/pdf/2601.09635v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T17:09:57Z", "updated": "2026-01-14T17:09:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMs can Compress LLMs: Adaptive Pruning by Agents", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "year": 2026, "url": "http://arxiv.org/abs/2601.09694v1", "abstract": "As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.", "source": "arxiv", "arxiv_id": "2601.09694v1", "pdf_url": "https://arxiv.org/pdf/2601.09694v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-14T18:45:36Z", "updated": "2026-01-14T18:45:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agent for User-friendly Chemical Process Simulations", "authors": ["Jingkang Liang", "Niklas Groll", "GÃ¼rkan Sin"], "year": 2026, "url": "http://arxiv.org/abs/2601.11650v1", "abstract": "Modern process simulators enable detailed process design, simulation, and optimization; however, constructing and interpreting simulations is time-consuming and requires expert knowledge. This limits early exploration by inexperienced users. To address this, a large language model (LLM) agent is integrated with AVEVA Process Simulation (APS) via Model Context Protocol (MCP), allowing natural language interaction with rigorous process simulations. An MCP server toolset enables the LLM to communicate programmatically with APS using Python, allowing it to execute complex simulation tasks from plain-language instructions. Two water-methanol separation case studies assess the framework across different task complexities and interaction modes. The first shows the agent autonomously analyzing flowsheets, finding improvement opportunities, and iteratively optimizing, extracting data, and presenting results clearly. The framework benefits both educational purposes, by translating technical concepts and demonstrating workflows, and experienced practitioners by automating data extraction, speeding routine tasks, and supporting brainstorming. The second case study assesses autonomous flowsheet synthesis through both a step-by-step dialogue and a single prompt, demonstrating its potential for novices and experts alike. The step-by-step mode gives reliable, guided construction suitable for educational contexts; the single-prompt mode constructs fast baseline flowsheets for later refinement. While current limitations such as oversimplification, calculation errors, and technical hiccups mean expert oversight is still needed, the framework's capabilities in analysis, optimization, and guided construction suggest LLM-based agents can become valuable collaborators.", "source": "arxiv", "arxiv_id": "2601.11650v1", "pdf_url": "https://arxiv.org/pdf/2601.11650v1", "categories": ["physics.chem-ph", "cs.AI"], "primary_category": "physics.chem-ph", "doi": "", "venue": "", "published": "2026-01-15T12:18:45Z", "updated": "2026-01-15T12:18:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree", "authors": ["Leyi Zhao", "Weijie Huang", "Yitong Guo", "Jiang Bian", "Chenghong Wang", "Xuhong Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.14523v1", "abstract": "Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve", "source": "arxiv", "arxiv_id": "2601.14523v1", "pdf_url": "https://arxiv.org/pdf/2601.14523v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-20T22:32:52Z", "updated": "2026-01-20T22:32:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory", "authors": ["Sirui Liang", "Pengfei Cao", "Jian Zhao", "Wenhao Teng", "Xiangwen Liao", "Jun Zhao", "Kang Liu"], "year": 2026, "url": "http://arxiv.org/abs/2601.07470v1", "abstract": "Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.", "source": "arxiv", "arxiv_id": "2601.07470v1", "pdf_url": "https://arxiv.org/pdf/2601.07470v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T12:26:02Z", "updated": "2026-01-12T12:26:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments", "authors": ["Qinglong Shi", "Donghai Wang", "Hantao Zhou", "Jiguo Li", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He"], "year": 2026, "url": "http://arxiv.org/abs/2601.09382v1", "abstract": "Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.", "source": "arxiv", "arxiv_id": "2601.09382v1", "pdf_url": "https://arxiv.org/pdf/2601.09382v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T11:15:40Z", "updated": "2026-01-14T11:15:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LongDA: Benchmarking LLM Agents for Long-Document Data Analysis", "authors": ["Yiyang Li", "Zheyuan Zhang", "Tianyi Ma", "Zehong Wang", "Keerthiram Murugesan", "Chuxu Zhang", "Yanfang Ye"], "year": 2026, "url": "http://arxiv.org/abs/2601.02598v2", "abstract": "We introduce LongDA, a data analysis benchmark for evaluating LLM-based agents under documentation-intensive analytical workflows. In contrast to existing benchmarks that assume well-specified schemas and inputs, LongDA targets real-world settings in which navigating long documentation and complex data is the primary bottleneck. To this end, we manually curate raw data files, long and heterogeneous documentation, and expert-written publications from 17 publicly available U.S. national surveys, from which we extract 505 analytical queries grounded in real analytical practice. Solving these queries requires agents to first retrieve and integrate key information from multiple unstructured documents, before performing multi-step computations and writing executable code, which remains challenging for existing data analysis agents. To support the systematic evaluation under this setting, we develop LongTA, a tool-augmented agent framework that enables document access, retrieval, and code execution, and evaluate a range of proprietary and open-source models. Our experiments reveal substantial performance gaps even among state-of-the-art models, highlighting the challenges researchers should consider before applying LLM agents for decision support in real-world, high-stakes analytical settings.", "source": "arxiv", "arxiv_id": "2601.02598v2", "pdf_url": "https://arxiv.org/pdf/2601.02598v2", "categories": ["cs.DL", "cs.AI"], "primary_category": "cs.DL", "doi": "", "venue": "", "published": "2026-01-05T23:23:16Z", "updated": "2026-01-11T22:21:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games", "authors": ["Sixiong Xie", "Zhuofan Shi", "Haiyang Shen", "Gang Huang", "Yun Ma", "Xiang Jing"], "year": 2026, "url": "http://arxiv.org/abs/2601.08462v1", "abstract": "As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.", "source": "arxiv", "arxiv_id": "2601.08462v1", "pdf_url": "https://arxiv.org/pdf/2601.08462v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-13T11:38:51Z", "updated": "2026-01-13T11:38:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MACRO-LLM: LLM-Empowered Multi-Agent Collaborative Reasoning under Spatiotemporal Partial Observability", "authors": ["Handi Chen", "Running Zhao", "Xiuzhe Wu", "Edith C. H. Ngai"], "year": 2026, "url": "http://arxiv.org/abs/2601.09295v1", "abstract": "Large Language Model (LLM) agents deployed in complex real-world scenarios typically operate as spatially distributed entities. However, this physical dispersion constrains agents to limited local perception and finite temporal horizons. We characterize this bottleneck as spatiotemporal partial observability. Given such fragmented awareness, distributed agents struggle to coordinate efficiently. To bridge this gap, we introduce MACRO-LLM, LLM-empowered multi-agent collaborative reasoning under spatiotemporal partial observability. The architecture addresses spatiotemporal constraints via three modules: (1) the CoProposer mitigates temporal uncertainty by verifying candidate actions via predictive rollouts; (2) the Negotiator overcomes spatial myopia by resolving conflicts through mean-field statistical aggregation; and (3) the Introspector ensures continuous adaptation by analyzing historical experience to refine strategies via semantic gradient descent. Extensive evaluations on two complex long-horizon tasks, cooperative adaptive cruise control and pandemic control, demonstrate that our framework effectively mitigates spatiotemporal partial observability through spatial and temporal strategies, enabling robust coordination.", "source": "arxiv", "arxiv_id": "2601.09295v1", "pdf_url": "https://arxiv.org/pdf/2601.09295v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2026-01-14T08:54:55Z", "updated": "2026-01-14T08:54:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MALTopic: Multi-Agent LLM Topic Modeling Framework", "authors": ["Yash Sharma"], "year": 2026, "url": "http://arxiv.org/abs/2601.15299v1", "abstract": "Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.", "source": "arxiv", "arxiv_id": "2601.15299v1", "pdf_url": "https://arxiv.org/pdf/2601.15299v1", "categories": ["cs.CL", "cs.IR", "cs.MA"], "primary_category": "cs.CL", "doi": "10.1109/AIIoT65859.2025.11105319", "venue": "2025 IEEE AI-IoT", "published": "2026-01-07T06:32:59Z", "updated": "2026-01-07T06:32:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "authors": ["Jian Zhang", "Zhiyuan Wang", "Zhangqi Wang", "Yu He", "Haoran Luo", "li yuan", "Lingling Zhang", "Rui Mao", "Qika Lin", "Jun Liu"], "year": 2026, "url": "http://arxiv.org/abs/2601.09259v1", "abstract": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "source": "arxiv", "arxiv_id": "2601.09259v1", "pdf_url": "https://arxiv.org/pdf/2601.09259v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T07:48:00Z", "updated": "2026-01-14T07:48:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis", "authors": ["Zixuan Xiao", "Jun Ma", "Siwei Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.05483v1", "abstract": "Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.", "source": "arxiv", "arxiv_id": "2601.05483v1", "pdf_url": "https://arxiv.org/pdf/2601.05483v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1016/j.asoc.2026.114576", "venue": "Applied Soft Computing 190 (2026) 114576", "published": "2026-01-09T02:34:35Z", "updated": "2026-01-09T02:34:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents", "authors": ["Shouju Wang", "Haopeng Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.08235v2", "abstract": "As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.", "source": "arxiv", "arxiv_id": "2601.08235v2", "pdf_url": "https://arxiv.org/pdf/2601.08235v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-13T05:39:43Z", "updated": "2026-01-14T05:26:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents", "authors": ["Dehao Tao", "Guoliang Ma", "Yongfeng Huang", "Minghu Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.03785v2", "abstract": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.", "source": "arxiv", "arxiv_id": "2601.03785v2", "pdf_url": "https://arxiv.org/pdf/2601.03785v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-07T10:36:29Z", "updated": "2026-01-20T07:09:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Memory Poisoning Attack and Defense on Memory Based LLM-Agents", "authors": ["Balachandra Devarangadi Sunil", "Isheeta Sinha", "Piyush Maheshwari", "Shantanu Todmal", "Shreyan Mallik", "Shuchi Mishra"], "year": 2026, "url": "http://arxiv.org/abs/2601.05504v2", "abstract": "Large language model agents equipped with persistent memory are vulnerable to memory poisoning attacks, where adversaries inject malicious instructions through query only interactions that corrupt the agents long term memory and influence future responses. Recent work demonstrated that the MINJA (Memory Injection Attack) achieves over 95 % injection success rate and 70 % attack success rate under idealized conditions. However, the robustness of these attacks in realistic deployments and effective defensive mechanisms remain understudied. This work addresses these gaps through systematic empirical evaluation of memory poisoning attacks and defenses in Electronic Health Record (EHR) agents. We investigate attack robustness by varying three critical dimensions: initial memory state, number of indication prompts, and retrieval parameters. Our experiments on GPT-4o-mini, Gemini-2.0-Flash and Llama-3.1-8B-Instruct models using MIMIC-III clinical data reveal that realistic conditions with pre-existing legitimate memories dramatically reduce attack effectiveness. We then propose and evaluate two novel defense mechanisms: (1) Input/Output Moderation using composite trust scoring across multiple orthogonal signals, and (2) Memory Sanitization with trust-aware retrieval employing temporal decay and pattern-based filtering. Our defense evaluation reveals that effective memory sanitization requires careful trust threshold calibration to prevent both overly conservative rejection (blocking all entries) and insufficient filtering (missing subtle attacks), establishing important baselines for future adaptive defense mechanisms. These findings provide crucial insights for securing memory-augmented LLM agents in production environments.", "source": "arxiv", "arxiv_id": "2601.05504v2", "pdf_url": "https://arxiv.org/pdf/2601.05504v2", "categories": ["cs.CR", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-09T03:26:10Z", "updated": "2026-01-12T03:35:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents", "authors": ["Tamil Sudaravan Mohan Doss", "Michael Xu", "Sudha Rao", "Andrew D. Wilson", "Balasaravanan Thoravi Kumaravel"], "year": 2026, "url": "http://arxiv.org/abs/2601.05215v2", "abstract": "We present MineNPC-Task, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world Minecraft. Rather than relying on synthetic prompts, tasks are elicited through formative and summative co-play with expert players, then normalized into parametric templates with explicit preconditions and dependency structure. These tasks are paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan, action, and memory events, including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts, and reports outcomes relative to the total number of attempted subtasks using only in-world evidence.\n  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate 216 subtasks across 8 experienced players. We observe recurring breakdown patterns in code execution, inventory and tool handling, referencing, and navigation, alongside successful recoveries supported by mixed-initiative clarifications and lightweight memory use. Participants rated interaction quality and interface usability positively, while noting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and evaluation harness to support transparent and reproducible evaluation of future memory-aware embodied agents.", "source": "arxiv", "arxiv_id": "2601.05215v2", "pdf_url": "https://arxiv.org/pdf/2601.05215v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-08T18:39:52Z", "updated": "2026-01-09T08:14:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System", "authors": ["Hsiang-Wei Huang", "Junbin Lu", "Kuang-Ming Chen", "Jenq-Neng Hwang"], "year": 2026, "url": "http://arxiv.org/abs/2601.08829v1", "abstract": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.", "source": "arxiv", "arxiv_id": "2601.08829v1", "pdf_url": "https://arxiv.org/pdf/2601.08829v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-13T18:59:17Z", "updated": "2026-01-13T18:59:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "NeuroFilter: Privacy Guardrails for Conversational LLM Agents", "authors": ["Saswat Das", "Ferdinando Fioretto"], "year": 2026, "url": "http://arxiv.org/abs/2601.14660v1", "abstract": "This work addresses the computational challenge of enforcing privacy for agentic Large Language Models (LLMs), where privacy is governed by the contextual integrity framework. Indeed, existing defenses rely on LLM-mediated checking stages that add substantial latency and cost, and that can be undermined in multi-turn interactions through manipulation or benign-looking conversational scaffolding. Contrasting this background, this paper makes a key observation: internal representations associated with privacy-violating intent can be separated from benign requests using linear structure. Using this insight, the paper proposes NeuroFilter, a guardrail framework that operationalizes contextual integrity by mapping norm violations to simple directions in the model's activation space, enabling detection even when semantic filters are bypassed. The proposed filter is also extended to capture threats arising during long conversations using the concept of activation velocity, which measures cumulative drift in internal representations across turns. A comprehensive evaluation across over 150,000 interactions and covering models from 7B to 70B parameters, illustrates the strong performance of NeuroFilter in detecting privacy attacks while maintaining zero false positives on benign prompts, all while reducing the computational inference cost by several orders of magnitude when compared to LLM-based agentic privacy defenses.", "source": "arxiv", "arxiv_id": "2601.14660v1", "pdf_url": "https://arxiv.org/pdf/2601.14660v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-21T05:16:50Z", "updated": "2026-01-21T05:16:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning", "authors": ["Zhicong Li", "Lingjie Jiang", "Yulan Hu", "Xingchen Zeng", "Yixia Li", "Xiangwen Zhang", "Guanhua Chen", "Zheng Pan", "Xin Li", "Yong Liu"], "year": 2026, "url": "http://arxiv.org/abs/2601.06794v1", "abstract": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.", "source": "arxiv", "arxiv_id": "2601.06794v1", "pdf_url": "https://arxiv.org/pdf/2601.06794v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-11T07:29:08Z", "updated": "2026-01-11T07:29:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OpenAI for OpenAPI: Automated generation of REST API specification via LLMs", "authors": ["Hao Chen", "Yunchun Li", "Chen Chen", "Fengxu Lin", "Wei Li"], "year": 2026, "url": "http://arxiv.org/abs/2601.12735v1", "abstract": "REST APIs, based on the REpresentational State Transfer (REST) architecture, are the primary type of Web API. The OpenAPI Specification (OAS) serves as the de facto standard for describing REST APIs and is crucial for multiple software engineering tasks. However, developers face challenges in writing and maintaining OAS. Although static analysis shows potential for OAS generation, it is limited to specific programming languages and development frameworks. The powerful code understanding capabilities of LLMs offer new opportunities for OAS generation, yet they are constrained by context limitations and hallucinations. To address these challenges, we propose the OpenAI OpenAPI Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis method for OAS generation, requiring fewer technology-specific rules and less human expert intervention. OOPS is implemented as an LLM agent workflow comprising two key steps: endpoint method extraction and OAS generation. By constructing an API dependency graph, it establishes necessary file associations to address LLMs' context limitations. Through multi-stage generation and self-refine, it mitigates both syntactic and semantic hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST APIs spanning 5 programming languages and 8 development frameworks. Experimental results demonstrate that OOPS accurately generates high-quality OAS for REST APIs implemented with diverse technologies, achieving an average F1-score exceeding 98% for endpoint method inference, 97% for both request parameter and response inference, and 92% for parameter constraint inference. The input tokens average below 5.6K with a maximum of 16.2K, while the output tokens average below 0.9K with a maximum of 7.7K.", "source": "arxiv", "arxiv_id": "2601.12735v1", "pdf_url": "https://arxiv.org/pdf/2601.12735v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-19T05:36:53Z", "updated": "2026-01-19T05:36:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning", "authors": ["Siqi Zhu", "Jiaxuan You"], "year": 2026, "url": "http://arxiv.org/abs/2601.07376v1", "abstract": "We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.", "source": "arxiv", "arxiv_id": "2601.07376v1", "pdf_url": "https://arxiv.org/pdf/2601.07376v1", "categories": ["cs.AI", "cs.DC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T09:57:46Z", "updated": "2026-01-12T09:57:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Orchestral AI: A Framework for Agent Orchestration", "authors": ["Alexander Roman", "Jacob Roman"], "year": 2026, "url": "http://arxiv.org/abs/2601.02577v1", "abstract": "The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration.", "source": "arxiv", "arxiv_id": "2601.02577v1", "pdf_url": "https://arxiv.org/pdf/2601.02577v1", "categories": ["cs.AI", "astro-ph.IM", "hep-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-05T22:02:11Z", "updated": "2026-01-05T22:02:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning", "authors": ["Bingxuan Li", "Jeonghwan Kim", "Cheng Qian", "Xiusi Chen", "Eitan Anzenberg", "Niran Kundapur", "Heng Ji"], "year": 2026, "url": "http://arxiv.org/abs/2601.11957v1", "abstract": "Overlapping calendar invitations force busy professionals to repeatedly decide which meetings to attend, reschedule, or decline. We refer to this preference-driven decision process as calendar conflict resolution. Automating such process is crucial yet challenging. Scheduling logistics drain hours, and human delegation often fails at scale, which motivate we to ask: Can we trust large language model (LLM) or language agent to manager time? To enable systematic study of this question, we introduce CalConflictBench, a benchmark for long-horizon calendar conflict resolution. Conflicts are presented sequentially and agents receive feedback after each round, requiring them to infer and adapt to user preferences progressively. Our experiments show that current LLM agents perform poorly with high error rates, e.g., Qwen-3-30B-Think has 35% average error rate. To address this gap, we propose PEARL, a reinforcement-learning framework that augments language agent with an external memory module and optimized round-wise reward design, enabling agent to progressively infer and adapt to user preferences on-the-fly. Experiments on CalConflictBench shows that PEARL achieves 0.76 error reduction rate, and 55% improvement in average error rate compared to the strongest baseline.", "source": "arxiv", "arxiv_id": "2601.11957v1", "pdf_url": "https://arxiv.org/pdf/2601.11957v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-17T08:19:18Z", "updated": "2026-01-17T08:19:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PINA: Prompt Injection Attack against Navigation Agents", "authors": ["Jiani Liu", "Yixin He", "Lanlan Fan", "Qidi Zhong", "Yushi Cheng", "Meng Zhang", "Yanjiao Chen", "Wenyuan Xu"], "year": 2026, "url": "http://arxiv.org/abs/2601.13612v1", "abstract": "Navigation agents powered by large language models (LLMs) convert natural language instructions into executable plans and actions. Compared to text-based applications, their security is far more critical: a successful prompt injection attack does not just alter outputs but can directly misguide physical navigation, leading to unsafe routes, mission failure, or real-world harm. Despite this high-stakes setting, the vulnerability of navigation agents to prompt injection remains largely unexplored. In this paper, we propose PINA, an adaptive prompt optimization framework tailored to navigation agents under black-box, long-context, and action-executable constraints. Experiments on indoor and outdoor navigation agents show that PINA achieves high attack success rates with an average ASR of 87.5%, surpasses all baselines, and remains robust under ablation and adaptive-attack conditions. This work provides the first systematic investigation of prompt injection attacks in navigation and highlights their urgent security implications for embodied LLM agents.", "source": "arxiv", "arxiv_id": "2601.13612v1", "pdf_url": "https://arxiv.org/pdf/2601.13612v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-20T05:28:23Z", "updated": "2026-01-20T05:28:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation", "authors": ["Zahra Moslemi", "Keerthi Koneru", "Yen-Ting Lee", "Sheethal Kumar", "Ramesh Radhakrishnan"], "year": 2026, "url": "http://arxiv.org/abs/2601.11816v1", "abstract": "Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation", "source": "arxiv", "arxiv_id": "2601.11816v1", "pdf_url": "https://arxiv.org/pdf/2601.11816v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-16T22:38:21Z", "updated": "2026-01-16T22:38:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents", "authors": ["Sourena Khanzadeh"], "year": 2026, "url": "http://arxiv.org/abs/2601.02314v1", "abstract": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($Ï$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($Ï$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.", "source": "arxiv", "arxiv_id": "2601.02314v1", "pdf_url": "https://arxiv.org/pdf/2601.02314v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-05T18:05:29Z", "updated": "2026-01-05T18:05:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions", "authors": ["Aayush Gupta"], "year": 2026, "url": "http://arxiv.org/abs/2601.06112v1", "abstract": "Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \\textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $Îµ$, and (iii) fault tolerance under controlled tool/API failures at intensity $Î»$. ReliabilityBench contributes a unified reliability surface $R(k,Îµ,Î»)$, \\textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $Îµ=0$ to 88.1% at $Îµ=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.", "source": "arxiv", "arxiv_id": "2601.06112v1", "pdf_url": "https://arxiv.org/pdf/2601.06112v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-03T13:41:33Z", "updated": "2026-01-03T13:41:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents", "authors": ["Raffi Khatchadourian"], "year": 2026, "url": "http://arxiv.org/abs/2601.15322v1", "abstract": "LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.\n  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.\n  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.", "source": "arxiv", "arxiv_id": "2601.15322v1", "pdf_url": "https://arxiv.org/pdf/2601.15322v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-17T19:47:55Z", "updated": "2026-01-17T19:47:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery", "authors": ["Chong Liu", "Luxuan Fu", "Yang Jia", "Zhen Dong", "Bisheng Yang"], "year": 2026, "url": "http://arxiv.org/abs/2601.10535v1", "abstract": "The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.", "source": "arxiv", "arxiv_id": "2601.10535v1", "pdf_url": "https://arxiv.org/pdf/2601.10535v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2026-01-15T15:57:18Z", "updated": "2026-01-15T15:57:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation", "authors": ["Hanqi Jiang", "Junhao Chen", "Yi Pan", "Ling Chen", "Weihang You", "Yifan Zhou", "Ruidong Zhang", "Lin Zhao", "Yohannes Abate", "Tianming Liu"], "year": 2026, "url": "http://arxiv.org/abs/2601.02744v2", "abstract": "While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the \"Contextual Tunneling\" problem. Our code and data will be made publicly available upon acceptance.", "source": "arxiv", "arxiv_id": "2601.02744v2", "pdf_url": "https://arxiv.org/pdf/2601.02744v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-06T06:19:58Z", "updated": "2026-01-21T00:00:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SimpleMem: Efficient Lifelong Memory for LLM Agents", "authors": ["Jiaqi Liu", "Yaofeng Su", "Peng Xia", "Siwei Han", "Zeyu Zheng", "Cihang Xie", "Mingyu Ding", "Huaxiu Yao"], "year": 2026, "url": "http://arxiv.org/abs/2601.02553v1", "abstract": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.", "source": "arxiv", "arxiv_id": "2601.02553v1", "pdf_url": "https://arxiv.org/pdf/2601.02553v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-05T21:02:49Z", "updated": "2026-01-05T21:02:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Sola-Visibility-ISPM: Benchmarking Agentic AI for Identity Security Posture Management Visibility", "authors": ["Gal Engelberg", "Konstantin Koutsyi", "Leon Goldberg", "Reuven Elezra", "Idan Pinto", "Tal Moalem", "Shmuel Cohen", "Yoni Weintrob"], "year": 2026, "url": "http://arxiv.org/abs/2601.07880v1", "abstract": "Identity Security Posture Management (ISPM) is a core challenge for modern enterprises operating across cloud and SaaS environments. Answering basic ISPM visibility questions, such as understanding identity inventory and configuration hygiene, requires interpreting complex identity data, motivating growing interest in agentic AI systems. Despite this interest, there is currently no standardized way to evaluate how well such systems perform ISPM visibility tasks on real enterprise data. We introduce the Sola Visibility ISPM Benchmark, the first benchmark designed to evaluate agentic AI systems on foundational ISPM visibility tasks using a live, production-grade identity environment spanning AWS, Okta, and Google Workspace. The benchmark focuses on identity inventory and hygiene questions and is accompanied by the Sola AI Agent, a tool-using agent that translates natural-language queries into executable data exploration steps and produces verifiable, evidence-backed answers. Across 77 benchmark questions, the agent achieves strong overall performance, with an expert accuracy of 0.84 and a strict success rate of 0.77. Performance is highest on AWS hygiene tasks, where expert accuracy reaches 0.94, while results on Google Workspace and Okta hygiene tasks are more moderate, yet competitive. Overall, this work provides a practical and reproducible benchmark for evaluating agentic AI systems in identity security and establishes a foundation for future ISPM benchmarks covering more advanced identity analysis and governance tasks.", "source": "arxiv", "arxiv_id": "2601.07880v1", "pdf_url": "https://arxiv.org/pdf/2601.07880v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-11T18:36:33Z", "updated": "2026-01-11T18:36:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Traceability Link Recovery in Systems Engineering", "authors": ["Vedant Nipane", "Pulkit Agrawal", "Amit Singh"], "year": 2026, "url": "http://arxiv.org/abs/2601.11688v1", "abstract": "Establishing precise traceability between embedded systems datasheets and their corresponding code implementations remains a fundamental challenge in systems engineering, particularly for low-level software where manual mapping between specification documents and large code repositories is infeasible. Existing Traceability Link Recovery approaches primarily rely on lexical similarity and information retrieval techniques, which struggle to capture the semantic, structural, and symbol level relationships prevalent in embedded systems software. We present a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis while explicitly structuring the traceability process across multiple abstraction levels. Rather than performing direct specification-to-code matching, the proposed approach progressively narrows the search space through repository-level structure inference, file-level relevance estimation, and fine-grained symbollevel alignment. The method extends beyond function-centric mapping by explicitly covering macros, structs, constants, configuration parameters, and register definitions commonly found in systems-level C/C++ codebases. We evaluate the approach on multiple open-source embedded systems repositories using manually curated datasheet-to-code ground truth. Experimental results show substantial improvements over traditional information-retrieval-based baselines, achieving up to 73.3% file mapping accuracy. We significantly reduce computational overhead, lowering total LLM token consumption by 84% and end-to-end runtime by approximately 80%. This methodology supports automated analysis of large embedded software systems and enables downstream applications such as training data generation for systems-aware machine learning models, standards compliance verification, and large-scale specification coverage analysis.", "source": "arxiv", "arxiv_id": "2601.11688v1", "pdf_url": "https://arxiv.org/pdf/2601.11688v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-16T11:50:18Z", "updated": "2026-01-16T11:50:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Structured Personality Control and Adaptation for LLM Agents", "authors": ["Jinpeng Wang", "Xinyu Jia", "Wei Wei Heng", "Yuquan Li", "Binbin Shi", "Qianlei Chen", "Guannan Chen", "Junxia Zhang", "Yuyu Yin"], "year": 2026, "url": "http://arxiv.org/abs/2601.10025v1", "abstract": "Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.", "source": "arxiv", "arxiv_id": "2601.10025v1", "pdf_url": "https://arxiv.org/pdf/2601.10025v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-15T03:15:24Z", "updated": "2026-01-15T03:15:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SwiftMem: Fast Agentic Memory via Query-aware Indexing", "authors": ["Anxin Tian", "Yiming Li", "Xing Li", "Hui-Ling Zhen", "Lei Chen", "Xianzhi Yu", "Zhenhua Dong", "Mingxuan Yuan"], "year": 2026, "url": "http://arxiv.org/abs/2601.08160v1", "abstract": "Agentic memory systems have become critical for enabling LLM agents to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. This brute-force approach creates severe latency bottlenecks as memory grows, hindering real-time agent interactions. We propose SwiftMem, a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions. Our temporal index enables logarithmic-time range queries for time-sensitive retrieval, while the semantic DAG-Tag index maps queries to relevant topics through hierarchical tag structures. To address memory fragmentation during growth, we introduce an embedding-tag co-consolidation mechanism that reorganizes storage based on semantic clusters to improve cache locality. Experiments on LoCoMo and LongMemEval benchmarks demonstrate that SwiftMem achieves 47$\\times$ faster search compared to state-of-the-art baselines while maintaining competitive accuracy, enabling practical deployment of memory-augmented LLM agents.", "source": "arxiv", "arxiv_id": "2601.08160v1", "pdf_url": "https://arxiv.org/pdf/2601.08160v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-13T02:51:04Z", "updated": "2026-01-13T02:51:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Taming Various Privilege Escalation in LLM-Based Agent Systems: A Mandatory Access Control Framework", "authors": ["Zimo Ji", "Daoyuan Wu", "Wenyuan Jiang", "Pingchuan Ma", "Zongjie Li", "Yudong Gao", "Shuai Wang", "Yingjiu Li"], "year": 2026, "url": "http://arxiv.org/abs/2601.11893v1", "abstract": "Large Language Model (LLM)-based agent systems are increasingly deployed for complex real-world tasks but remain vulnerable to natural language-based attacks that exploit over-privileged tool use. This paper aims to understand and mitigate such attacks through the lens of privilege escalation, defined as agent actions exceeding the least privilege required for a user's intended task. Based on a formal model of LLM agent systems, we identify novel privilege escalation scenarios, particularly in multi-agent systems, including a variant akin to the classic confused deputy problem. To defend against both known and newly demonstrated privilege escalation, we propose SEAgent, a mandatory access control (MAC) framework built upon attribute-based access control (ABAC). SEAgent monitors agent-tool interactions via an information flow graph and enforces customizable security policies based on entity attributes. Our evaluations show that SEAgent effectively blocks various privilege escalation while maintaining a low false positive rate and negligible system overhead. This demonstrates its robustness and adaptability in securing LLM-based agent systems.", "source": "arxiv", "arxiv_id": "2601.11893v1", "pdf_url": "https://arxiv.org/pdf/2601.11893v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-17T03:22:56Z", "updated": "2026-01-17T03:22:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Task Cascades for Efficient Unstructured Data Processing", "authors": ["Shreya Shankar", "Sepanta Zeighami", "Aditya Parameswaran"], "year": 2026, "url": "http://arxiv.org/abs/2601.05536v1", "abstract": "Modern database systems allow users to query or process unstructured text or document columns using LLM-powered functions. Users can express an operation in natural language (e.g., \"identify if this review mentions billing issues\"), with the system executing the operation on each document, in a row-by-row fashion. One way to reduce cost on a batch of documents is to employ the model cascade framework: a cheap proxy model processes each document, and only uncertain cases are escalated to a more accurate, expensive oracle. However, model cascades miss important optimization opportunities; for example, often only part of a document is needed to answer a query, or other related, but simpler operations (e.g., \"is the review sentiment negative?\", \"does the review mention money?\") can be handled by cheap models more effectively than the original operation, while still being correlated with it.\n  We introduce the task cascades framework, which generalizes model cascades by varying not just the model, but also the document portion and operation at each stage. Our framework uses an LLM agent to generate simplified, decomposed, or otherwise related operations and selects the most relevant document portions, constructing hundreds of candidate tasks from which it assembles a task cascade. We show that optimal cascade selection is intractable via reduction from Minimum Sum Set Cover, but our iterative approach constructs effective cascades. We also provide an extension that offers statistical accuracy guarantees: the resulting cascade meets a user-defined accuracy target (with respect to the oracle) up to a bounded failure probability. Across eight real-world document processing tasks at a 90% target accuracy, task cascades reduce end-to-end cost by an average of 36% compared to model cascades, at a production scale.", "source": "arxiv", "arxiv_id": "2601.05536v1", "pdf_url": "https://arxiv.org/pdf/2601.05536v1", "categories": ["cs.DB"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2026-01-09T05:23:40Z", "updated": "2026-01-09T05:23:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents", "authors": ["Weihao Xuan", "Qingcheng Zeng", "Heli Qi", "Yunze Xiao", "Junjue Wang", "Naoto Yokoya"], "year": 2026, "url": "http://arxiv.org/abs/2601.07264v1", "abstract": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.", "source": "arxiv", "arxiv_id": "2601.07264v1", "pdf_url": "https://arxiv.org/pdf/2601.07264v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-12T07:10:35Z", "updated": "2026-01-12T07:10:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation", "authors": ["Kirandeep Kaur", "Vinayak Gupta", "Aditya Gupta", "Chirag Shah"], "year": 2026, "url": "http://arxiv.org/abs/2601.09926v2", "abstract": "Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.", "source": "arxiv", "arxiv_id": "2601.09926v2", "pdf_url": "https://arxiv.org/pdf/2601.09926v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-14T23:13:01Z", "updated": "2026-01-16T05:43:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation", "authors": ["Ziqiao Xi", "Shuang Liang", "Qi Liu", "Jiaqing Zhang", "Letian Peng", "Fang Nan", "Meshal Nayim", "Tianhui Zhang", "Rishika Mundada", "Lianhui Qin", "Biwei Huang", "Kun Zhou"], "year": 2026, "url": "http://arxiv.org/abs/2601.06328v1", "abstract": "Tool-using LLM agents still struggle in open-world settings with large tool pools, long-horizon objectives, wild constraints, and unreliable tool states. For scalable and realistic training and testing, we introduce an open-world tool-using environment, built on 5,571 format unified tools across 204 commonly used apps. It includes a task creation engine that synthesizes long-horizon, multi-tool workflows with wild constraints, and a state controller that injects interruptions and failures to stress-test robustness. On top of this environment, we develop a tool select-then-execute agent framework with a planner-actor decomposition to separate deliberate reasoning and self-correction from step-wise execution. Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Our code and data will be publicly released.", "source": "arxiv", "arxiv_id": "2601.06328v1", "pdf_url": "https://arxiv.org/pdf/2601.06328v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-09T21:59:31Z", "updated": "2026-01-09T21:59:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents", "authors": ["Dawei Li", "Yuguang Yao", "Zhen Tan", "Huan Liu", "Ruocheng Guo"], "year": 2026, "url": "http://arxiv.org/abs/2601.12294v1", "abstract": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.", "source": "arxiv", "arxiv_id": "2601.12294v1", "pdf_url": "https://arxiv.org/pdf/2601.12294v1", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-18T07:48:36Z", "updated": "2026-01-18T07:48:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback", "authors": ["Yutao Mou", "Zhangchi Xue", "Lijun Li", "Peiyang Liu", "Shikun Zhang", "Wei Ye", "Jing Shao"], "year": 2026, "url": "http://arxiv.org/abs/2601.10156v1", "abstract": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "source": "arxiv", "arxiv_id": "2601.10156v1", "pdf_url": "https://arxiv.org/pdf/2601.10156v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-15T07:54:32Z", "updated": "2026-01-15T07:54:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows", "authors": ["Ke Xiao", "Haoze Zhang", "Runze Mao", "Han Li", "Zhi X. Chen"], "year": 2026, "url": "http://arxiv.org/abs/2601.01357v1", "abstract": "The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.", "source": "arxiv", "arxiv_id": "2601.01357v1", "pdf_url": "https://arxiv.org/pdf/2601.01357v1", "categories": ["cs.LG", "physics.flu-dyn"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-04T04:00:28Z", "updated": "2026-01-04T04:00:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents", "authors": ["Himanshu Thakur", "Anusha Kamath", "Anurag Muthyala", "Dhwani Sanmukhani", "Smruthi Mukund", "Jay Katukuri"], "year": 2026, "url": "http://arxiv.org/abs/2601.10820v1", "abstract": "Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.", "source": "arxiv", "arxiv_id": "2601.10820v1", "pdf_url": "https://arxiv.org/pdf/2601.10820v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-15T19:33:42Z", "updated": "2026-01-15T19:33:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Verifiably Safe Tool Use for LLM Agents", "authors": ["Aarya Doshi", "Yining Hong", "Congying Xu", "Eunsuk Kang", "Alexandros Kapravelos", "Christian KÃ¤stner"], "year": 2026, "url": "http://arxiv.org/abs/2601.08012v1", "abstract": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. While this empowers agents to perform complex tasks, LLMs may invoke unintended tool interactions and introduce risks, such as leaking sensitive data or overwriting critical records, which are unacceptable in enterprise contexts. Current approaches to mitigate these risks, such as model-based safeguards, enhance agents' reliability but cannot guarantee system safety. Methods like information flow control (IFC) and temporal constraints aim to provide guarantees but often require extensive human annotation. We propose a process that starts with applying System-Theoretic Process Analysis (STPA) to identify hazards in agent workflows, derive safety requirements, and formalize them as enforceable specifications on data flows and tool sequences. To enable this, we introduce a capability-enhanced Model Context Protocol (MCP) framework that requires structured labels on capabilities, confidentiality, and trust level. Together, these contributions aim to shift LLM-based agent safety from ad hoc reliability fixes to proactive guardrails with formal guarantees, while reducing dependence on user confirmation and making autonomy a deliberate design choice.", "source": "arxiv", "arxiv_id": "2601.08012v1", "pdf_url": "https://arxiv.org/pdf/2601.08012v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-12T21:31:38Z", "updated": "2026-01-12T21:31:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI", "authors": ["Laksh Advani"], "year": 2026, "url": "http://arxiv.org/abs/2601.00516v1", "abstract": "Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both \"wrong plan for this task\" and \"malformed plan structure.\" On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.", "source": "arxiv", "arxiv_id": "2601.00516v1", "pdf_url": "https://arxiv.org/pdf/2601.00516v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-02T00:27:11Z", "updated": "2026-01-02T00:27:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit", "authors": ["Junda Lin", "Zhaomeng Zhou", "Zhi Zheng", "Shuochen Liu", "Tong Xu", "Yong Chen", "Enhong Chen"], "year": 2026, "url": "http://arxiv.org/abs/2601.05755v2", "abstract": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \\textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \\textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \\textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \\textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility.", "source": "arxiv", "arxiv_id": "2601.05755v2", "pdf_url": "https://arxiv.org/pdf/2601.05755v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-09T12:19:49Z", "updated": "2026-01-14T18:19:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Value of Information: A Framework for Human-Agent Communication", "authors": ["Yijiang River Dong", "Tiancheng Hu", "Zheng Hui", "Caiqi Zhang", "Ivan VuliÄ", "Andreea Bobu", "Nigel Collier"], "year": 2026, "url": "http://arxiv.org/abs/2601.06407v1", "abstract": "Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.", "source": "arxiv", "arxiv_id": "2601.06407v1", "pdf_url": "https://arxiv.org/pdf/2601.06407v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-10T03:07:41Z", "updated": "2026-01-10T03:07:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding", "authors": ["Siyuan Liu", "Hongbang Yuan", "Xinze Li", "Ziyue Zhu", "Yixin Cao", "Yu-Gang Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.09503v1", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.", "source": "arxiv", "arxiv_id": "2601.09503v1", "pdf_url": "https://arxiv.org/pdf/2601.09503v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T14:09:11Z", "updated": "2026-01-14T14:09:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling", "authors": ["Niful Islam", "Ragib Shahriar Ayon", "Deepak George Thomas", "Shibbir Ahmed", "Mohammad Wardat"], "year": 2026, "url": "http://arxiv.org/abs/2601.15232v1", "abstract": "Large Language Models (LLMs) have revolutionized intelligent application development. While standalone LLMs cannot perform any actions, LLM agents address the limitation by integrating tools. However, debugging LLM agents is difficult and costly as the field is still in it's early stage and the community is underdeveloped. To understand the bugs encountered during agent development, we present the first comprehensive study of bug types, root causes, and effects in LLM agent-based software. We collected and analyzed 1,187 bug-related posts and code snippets from Stack Overflow, GitHub, and Hugging Face forums, focused on LLM agents built with seven widely used LLM frameworks as well as custom implementations. For a deeper analysis, we have also studied the component where the bug occurred, along with the programming language and framework. This study also investigates the feasibility of automating bug identification. For that, we have built a ReAct agent named BugReAct, equipped with adequate external tools to determine whether it can detect and annotate the bugs in our dataset. According to our study, we found that BugReAct equipped with Gemini 2.5 Flash achieved a remarkable performance in annotating bug characteristics with an average cost of 0.01 USD per post/code snippet.", "source": "arxiv", "arxiv_id": "2601.15232v1", "pdf_url": "https://arxiv.org/pdf/2601.15232v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-21T18:13:10Z", "updated": "2026-01-21T18:13:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms", "authors": ["Chenghui Yu", "Hongwei Wang", "Junwen Chen", "Zixuan Wang", "Bingfeng Deng", "Zhuolin Hao", "Hongyu Xiong", "Yang Song"], "year": 2026, "url": "http://arxiv.org/abs/2601.11634v1", "abstract": "Trends on short-video platforms evolve at a rapid pace, with new content issues emerging every day that fall outside the coverage of existing annotation policies. However, traditional human-driven discovery of emerging issues is too slow, which leads to delayed updates of annotation policies and poses a major challenge for effective content governance. In this work, we propose an automatic issue discovery method based on multimodal LLM agents. Our approach automatically recalls short videos containing potential new issues and applies a two-stage clustering strategy to group them, with each cluster corresponding to a newly discovered issue. The agent then generates updated annotation policies from these clusters, thereby extending coverage to these emerging issues. Our agent has been deployed in the real system. Both offline and online experiments demonstrate that this agent-based method significantly improves the effectiveness of emerging-issue discovery (with an F1 score improvement of over 20%) and enhances the performance of subsequent issue governance (reducing the view count of problematic videos by approximately 15%). More importantly, compared to manual issue discovery, it greatly reduces time costs and substantially accelerates the iteration of annotation policies.", "source": "arxiv", "arxiv_id": "2601.11634v1", "pdf_url": "https://arxiv.org/pdf/2601.11634v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2026-01-14T08:37:55Z", "updated": "2026-01-14T08:37:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Who Owns Creativity and Who Does the Work? Trade-offs in LLM-Supported Research Ideation", "authors": ["Houjiang Liu", "Yujin Choi", "Sanjana Gautam", "Gabriel Jaffe", "Soo Young Rieh", "Matthew Lease"], "year": 2026, "url": "http://arxiv.org/abs/2601.12152v1", "abstract": "LLM-based agents offer new potential to accelerate science and reshape research work. However, the quality of researcher contributions can vary significantly depending on human ability to steer agent behaviors. How can we best use these tools to augment scientific creativity without undermining aspects of contribution and ownership that drive research? To investigate this, we developed an agentic research ideation system integrating three roles -- Ideator, Writer, and Evaluator -- across three control levels -- Low, Medium, and Intensive. Our mixed-methods study with 54 researchers suggests three key findings in how LLM-based agents reshape scientific creativity: 1) perceived creativity support does not simply increase linearly with greater control; 2) human effort shifts from ideating to verifying ideas; and 3) ownership becomes a negotiated outcome between human and AI. Our findings suggest that LLM agent design should emphasize researcher empowerment, fostering a sense of ownership over strong ideas rather than reducing researchers to operating an automated AI-driven process.", "source": "arxiv", "arxiv_id": "2601.12152v1", "pdf_url": "https://arxiv.org/pdf/2601.12152v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2026-01-17T20:01:20Z", "updated": "2026-01-17T20:01:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts", "authors": ["Dhruv Trehan", "Paras Chopra"], "year": 2026, "url": "http://arxiv.org/abs/2601.03315v1", "abstract": "We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1", "source": "arxiv", "arxiv_id": "2601.03315v1", "pdf_url": "https://arxiv.org/pdf/2601.03315v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-06T13:20:54Z", "updated": "2026-01-06T13:20:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs", "authors": ["Linzhang Li", "Yixin Dong", "Guanjie Wang", "Ziyi Xu", "Alexander Jiang", "Tianqi Chen"], "year": 2026, "url": "http://arxiv.org/abs/2601.04426v1", "abstract": "Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.", "source": "arxiv", "arxiv_id": "2601.04426v1", "pdf_url": "https://arxiv.org/pdf/2601.04426v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-07T22:18:51Z", "updated": "2026-01-07T22:18:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "\"Shall We Dig Deeper?\": Designing and Evaluating Strategies for LLM Agents to Advance Knowledge Co-Construction in Asynchronous Online Discussions", "authors": ["Yuanhao Zhang", "Wenbo Li", "Xiaoyu Wang", "Kangyu Yuan", "Shuai Ma", "Xiaojuan Ma"], "year": 2025, "url": "http://arxiv.org/abs/2509.23327v1", "abstract": "Asynchronous online discussions enable diverse participants to co-construct knowledge beyond individual contributions. This process ideally evolves through sequential phases, from superficial information exchange to deeper synthesis. However, many discussions stagnate in the early stages. Existing AI interventions typically target isolated phases, lacking mechanisms to progressively advance knowledge co-construction, and the impacts of different intervention styles in this context remain unclear and warrant investigation. To address these gaps, we conducted a design workshop to explore AI intervention strategies (task-oriented and/or relationship-oriented) throughout the knowledge co-construction process, and implemented them in an LLM-powered agent capable of facilitating progression while consolidating foundations at each phase. A within-subject study (N=60) involving five consecutive asynchronous discussions showed that the agent consistently promoted deeper knowledge progression, with different styles exerting distinct effects on both content and experience. These findings provide actionable guidance for designing adaptive AI agents that sustain more constructive online discussions.", "source": "arxiv", "arxiv_id": "2509.23327v1", "pdf_url": "https://arxiv.org/pdf/2509.23327v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-27T14:24:25Z", "updated": "2025-09-27T14:24:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization", "authors": ["Shengyu Feng", "Weiwei Sun", "Shanda Li", "Ameet Talwalkar", "Yiming Yang"], "year": 2025, "url": "http://arxiv.org/abs/2505.16952v1", "abstract": "Machine learning (ML) has demonstrated considerable potential in supporting model design and optimization for combinatorial optimization (CO) problems. However, much of the progress to date has been evaluated on small-scale, synthetic datasets, raising concerns about the practical effectiveness of ML-based solvers in real-world, large-scale CO scenarios. Additionally, many existing CO benchmarks lack sufficient training data, limiting their utility for evaluating data-driven approaches. To address these limitations, we introduce FrontierCO, a comprehensive benchmark that covers eight canonical CO problem types and evaluates 16 representative ML-based solvers--including graph neural networks and large language model (LLM) agents. FrontierCO features challenging instances drawn from industrial applications and frontier CO research, offering both realistic problem difficulty and abundant training data. Our empirical results provide critical insights into the strengths and limitations of current ML methods, helping to guide more robust and practically relevant advances at the intersection of machine learning and combinatorial optimization. Our data is available at https://huggingface.co/datasets/CO-Bench/FrontierCO.", "source": "arxiv", "arxiv_id": "2505.16952v1", "pdf_url": "https://arxiv.org/pdf/2505.16952v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-22T17:34:38Z", "updated": "2025-05-22T17:34:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Shicheng Xu", "Junyuan Mao", "Yu Wang", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Wenjie Qu", "Yue Liu", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Zhaoxin Fan", "Kai Wang", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Yingxin Lai", "Zitong Yu", "Xinfeng Li", "Yifan Jiang", "Yanhui Li", "Xinyu Deng", "Junlin Wu", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Qiufeng Wang", "Xiaolong Jin", "Wenxuan Wang", "Dongrui Liu", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Heng Chang", "Tianlin Li", "Yi Yu", "Chenghao Li", "Jiawei Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Jiaheng Zhang", "Tianwei Zhang", "Xingjun Ma", "Jindong Gu", "Liang Pang", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Lingjuan Lyu", "Yuval Elovici", "Bhavya Kailkhura", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "year": 2025, "url": "http://arxiv.org/abs/2504.15585v4", "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs. To address this gap, this paper introduces, for the first time, the concept of \"full-stack\" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.", "source": "arxiv", "arxiv_id": "2504.15585v4", "pdf_url": "https://arxiv.org/pdf/2504.15585v4", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-22T05:02:49Z", "updated": "2025-06-09T02:36:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System", "authors": ["Jiale Guo", "Suizhi Huang", "Mei Li", "Dong Huang", "Xingsheng Chen", "Regina Zhang", "Zhijiang Guo", "Han Yu", "Siu-Ming Yiu", "Pietro Lio", "Kwok-Yan Lam"], "year": 2025, "url": "http://arxiv.org/abs/2510.09721v3", "abstract": "The integration of Large Language Models (LLMs) into software engineering has driven a transition from traditional rule-based systems to autonomous agentic systems capable of solving complex problems. However, systematic progress is hindered by a lack of comprehensive understanding of how benchmarks and solutions interconnect. This survey addresses this gap by providing the first holistic analysis of LLM-powered software engineering, offering insights into evaluation methodologies and solution paradigms. We review over 150 recent papers and propose a taxonomy along two key dimensions: (1) Solutions, categorized into prompt-based, fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, including tasks such as code generation, translation, and repair. Our analysis highlights the evolution from simple prompt engineering to sophisticated agentic systems incorporating capabilities like planning, reasoning, memory mechanisms, and tool augmentation. To contextualize this progress, we present a unified pipeline illustrating the workflow from task specification to deliverables, detailing how different solution paradigms address various complexity levels. Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. We also identify critical research gaps and propose future directions, including multi-agent collaboration, self-evolving systems, and formal verification integration. This survey serves as a foundational guide for advancing LLM-driven software engineering. We maintain a GitHub repository that continuously updates the reviewed and related papers at https://github.com/lisaGuojl/LLM-Agent-SE-Survey.", "source": "arxiv", "arxiv_id": "2510.09721v3", "pdf_url": "https://arxiv.org/pdf/2510.09721v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-10T06:56:50Z", "updated": "2025-10-23T05:08:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Concurrent Modular Agent: Framework for Autonomous LLM Agents", "authors": ["Norihiro Maruyama", "Takahide Yoshida", "Hiroki Sato", "Atsushi Masumori", "Johnsmith", "Takashi Ikegami"], "year": 2025, "url": "http://arxiv.org/abs/2508.19042v1", "abstract": "We introduce the Concurrent Modular Agent (CMA), a framework that orchestrates multiple Large-Language-Model (LLM)-based modules that operate fully asynchronously yet maintain a coherent and fault-tolerant behavioral loop. This framework addresses long-standing difficulties in agent architectures by letting intention emerge from language-mediated interactions among autonomous processes. This approach enables flexible, adaptive, and context-dependent behavior through the combination of concurrently executed modules that offload reasoning to an LLM, inter-module communication, and a single shared global state.We consider this approach to be a practical realization of Minsky's Society of Mind theory. We demonstrate the viability of our system through two practical use-case studies. The emergent properties observed in our system suggest that complex cognitive phenomena like self-awareness may indeed arise from the organized interaction of simpler processes, supporting Minsky-Society of Mind concept and opening new avenues for artificial intelligence research. The source code for our work is available at: https://github.com/AlternativeMachine/concurrent-modular-agent.", "source": "arxiv", "arxiv_id": "2508.19042v1", "pdf_url": "https://arxiv.org/pdf/2508.19042v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-26T13:58:31Z", "updated": "2025-08-26T13:58:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control", "authors": ["Qing Guo", "Xinhang Li", "Junyu Chen", "Zheng Guo", "Xiaocong Li", "Lin Zhang", "Lei Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.00136v1", "abstract": "Leveraging large language models (LLMs) in traffic signal control (TSC) improves optimization efficiency and interpretability compared to traditional reinforcement learning (RL) methods. However, existing LLM-based approaches are limited by fixed time signal durations and are prone to hallucination errors, while RL methods lack robustness in signal timing decisions and suffer from poor generalization. To address these challenges, this paper proposes HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The Herald Module extracts contextual information and forecasts queue lengths for each traffic phase based on real-time conditions. The first LLM, LLM-Agent, uses these forecasts to make fine grained traffic signal control, while the second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and hallucinations. These refined outputs are used for score-based fine-tuning to improve accuracy and robustness. Simulation experiments using CityFlow on real world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New York (196) demonstrate that HeraldLight outperforms state of the art baselines, achieving a 20.03% reduction in average travel time across all scenarios and a 10.74% reduction in average queue length on the Jinan and Hangzhou scenarios. The source code is available on GitHub: https://github.com/BUPT-ANTlab/HeraldLight.", "source": "arxiv", "arxiv_id": "2511.00136v1", "pdf_url": "https://arxiv.org/pdf/2511.00136v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-31T14:05:08Z", "updated": "2025-10-31T14:05:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions", "authors": ["Stephen Mell", "Botong Zhang", "David Mell", "Shuo Li", "Ramya Ramalingam", "Nathan Yu", "Steve Zdancewic", "Osbert Bastani"], "year": 2025, "url": "http://arxiv.org/abs/2506.12202v1", "abstract": "Modern large language models (LLMs) are often deployed as agents, calling external tools adaptively to solve tasks. Rather than directly calling tools, it can be more effective for LLMs to write code to perform the tool calls, enabling them to automatically generate complex control flow such as conditionals and loops. Such code actions are typically provided as Python code, since LLMs are quite proficient at it; however, Python may not be the ideal language due to limited built-in support for performance, security, and reliability. We propose a novel programming language for code actions, called Quasar, which has several benefits: (1) automated parallelization to improve performance, (2) uncertainty quantification to improve reliability and mitigate hallucinations, and (3) security features enabling the user to validate actions. LLMs can write code in a subset of Python, which is automatically transpiled to Quasar. We evaluate our approach on the ViperGPT visual question answering agent, applied to the GQA dataset, demonstrating that LLMs with Quasar actions instead of Python actions retain strong performance, while reducing execution time when possible by 42%, improving security by reducing user approval interactions when possible by 52%, and improving reliability by applying conformal prediction to achieve a desired target coverage level.", "source": "arxiv", "arxiv_id": "2506.12202v1", "pdf_url": "https://arxiv.org/pdf/2506.12202v1", "categories": ["cs.PL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.PL", "doi": "", "venue": "", "published": "2025-06-13T20:11:22Z", "updated": "2025-06-13T20:11:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images", "authors": ["Yuxuan Chen", "Ruotong Yang", "Zhengyang Zhang", "Mehreen Ahmed", "Yanming Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.11260v1", "abstract": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM), are widely used in scientific research for visualizing and analyzing microstructures. Determining the scale bars is an important first step of accurate SEM analysis; however, currently, it mainly relies on manual operations, which is both time-consuming and prone to errors. To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent. The proposed framework operates in four phases; i) Automatic Dataset Generation (Auto-DG) model to synthesize a diverse dataset of SEM images ensuring robust training and high generalizability of the model, ii) scale bar object detection, iii) information extraction using a hybrid Optical Character Recognition (OCR) system with DenseNet and Convolutional Recurrent Neural Network (CRNN) based algorithms, iv) an LLM agent to analyze and verify accuracy of the results. The proposed model demonstrates a strong performance in object detection and accurate localization with a precision of 100%, recall of 95.8%, and a mean Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the Auto-DG dataset, significantly outperforming several mainstream standalone engines, highlighting its reliability for scientific image analysis. The LLM is introduced as a reasoning engine as well as an intelligent assistant that suggests follow-up steps and verifies the results. This automated method powered by an LLM agent significantly enhances the efficiency and accuracy of scale bar detection and extraction in SEM images, providing a valuable tool for microscopic analysis and advancing the field of scientific imaging.", "source": "arxiv", "arxiv_id": "2510.11260v1", "pdf_url": "https://arxiv.org/pdf/2510.11260v1", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI", "physics.data-an"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-10-13T10:50:54Z", "updated": "2025-10-13T10:50:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Memory-Efficient Retrieval Architecture for RAG-Enabled Wearable Medical LLMs-Agents", "authors": ["Zhipeng Liao", "Kunming Shao", "Jiangnan Yu", "Liang Zhao", "Tim Kwang-Ting Cheng", "Chi-Ying Tsui", "Jie Yang", "Mohamad Sawan"], "year": 2025, "url": "http://arxiv.org/abs/2510.27107v1", "abstract": "With powerful and integrative large language models (LLMs), medical AI agents have demonstrated unique advantages in providing personalized medical consultations, continuous health monitoring, and precise treatment plans. Retrieval-Augmented Generation (RAG) integrates personal medical documents into LLMs by an external retrievable database to address the costly retraining or fine-tuning issues in deploying customized agents. While deploying medical agents in edge devices ensures privacy protection, RAG implementations impose substantial memory access and energy consumption during the retrieval stage. This paper presents a hierarchical retrieval architecture for edge RAG, leveraging a two-stage retrieval scheme that combines approximate retrieval for candidate set generation, followed by high-precision retrieval on pre-selected document embeddings. The proposed architecture significantly reduces energy consumption and external memory access while maintaining retrieval accuracy. Simulation results show that, under TSMC 28nm technology, the proposed hierarchical retrieval architecture has reduced the overall memory access by nearly 50% and the computation by 75% compared to pure INT8 retrieval, and the total energy consumption for 1 MB data retrieval is 177.76 Î¼J/query.", "source": "arxiv", "arxiv_id": "2510.27107v1", "pdf_url": "https://arxiv.org/pdf/2510.27107v1", "categories": ["cs.AR"], "primary_category": "cs.AR", "doi": "", "venue": "", "published": "2025-10-31T02:17:18Z", "updated": "2025-10-31T02:17:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Modular LLM-Agent System for Transparent Multi-Parameter Weather Interpretation", "authors": ["Daniil Sukhorukov", "Andrei Zakharov", "Nikita Glazkov", "Katsiaryna Yanchanka", "Vladimir Kirilin", "Maxim Dubovitsky", "Roman Sultimov", "Yuri Maksimov", "Ilya Makarov"], "year": 2025, "url": "http://arxiv.org/abs/2512.11819v1", "abstract": "Weather forecasting is not only a predictive task but an interpretive scientific process requiring explanation, contextualization, and hypothesis generation. This paper introduces AI-Meteorologist, an explainable LLM-agent framework that converts raw numerical forecasts into scientifically grounded narrative reports with transparent reasoning steps. Unlike conventional forecast outputs presented as dense tables or unstructured time series, our system performs agent-based analysis across multiple meteorological variables, integrates historical climatological context, and generates structured explanations that identify weather fronts, anomalies, and localized dynamics. The architecture relies entirely on in-context prompting, without fine-tuning, demonstrating that interpretability can be achieved through reasoning rather than parameter updates. Through case studies on multi-location forecast data, we show how AI-Meteorologist not only communicates weather events but also reveals the underlying atmospheric drivers, offering a pathway toward AI systems that augment human meteorological expertise and support scientific discovery in climate analytics.", "source": "arxiv", "arxiv_id": "2512.11819v1", "pdf_url": "https://arxiv.org/pdf/2512.11819v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-11-28T22:24:40Z", "updated": "2025-11-28T22:24:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks", "authors": ["S M Asif Hossain", "Ruksat Khan Shayoni", "Mohd Ruhul Ameen", "Akif Islam", "M. F. Mridha", "Jungpil Shin"], "year": 2025, "url": "http://arxiv.org/abs/2509.14285v4", "abstract": "Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.", "source": "arxiv", "arxiv_id": "2509.14285v4", "pdf_url": "https://arxiv.org/pdf/2509.14285v4", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-16T19:11:28Z", "updated": "2025-12-17T16:48:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Multi-Agent System for Semantic Mapping of Relational Data to Knowledge Graphs", "authors": ["Milena Trajanoska", "Riste Stojanov", "Dimitar Trajanov"], "year": 2025, "url": "http://arxiv.org/abs/2511.06455v1", "abstract": "Enterprises often maintain multiple databases for storing critical business data in siloed systems, resulting in inefficiencies and challenges with data interoperability. A key to overcoming these challenges lies in integrating disparate data sources, enabling businesses to unlock the full potential of their data. Our work presents a novel approach for integrating multiple databases using knowledge graphs, focusing on the application of large language models as semantic agents for mapping and connecting structured data across systems by leveraging existing vocabularies. The proposed methodology introduces a semantic layer above tables in relational databases, utilizing a system comprising multiple LLM agents that map tables and columns to Schema.org terms. Our approach achieves a mapping accuracy of over 90% in multiple domains.", "source": "arxiv", "arxiv_id": "2511.06455v1", "pdf_url": "https://arxiv.org/pdf/2511.06455v1", "categories": ["cs.DB", "cs.AI"], "primary_category": "cs.DB", "doi": "10.5281/zenodo.16913321", "venue": "The 1st GOBLIN Workshop on Knowledge Graph Technologies, June 12, 2025 in Leipzig, Germany", "published": "2025-11-09T16:41:46Z", "updated": "2025-11-09T16:41:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis", "authors": ["Yuzhi Hao", "Danyang Xie"], "year": 2025, "url": "http://arxiv.org/abs/2502.16879v1", "abstract": "This paper pioneers a novel approach to economic and public policy analysis by leveraging multiple Large Language Models (LLMs) as heterogeneous artificial economic agents. We first evaluate five LLMs' economic decision-making capabilities in solving two-period consumption allocation problems under two distinct scenarios: with explicit utility functions and based on intuitive reasoning. While previous research has often simulated heterogeneity by solely varying prompts, our approach harnesses the inherent variations in analytical capabilities across different LLMs to model agents with diverse cognitive traits. Building on these findings, we construct a Multi-LLM-Agent-Based (MLAB) framework by mapping these LLMs to specific educational groups and corresponding income brackets. Using interest-income taxation as a case study, we demonstrate how the MLAB framework can simulate policy impacts across heterogeneous agents, offering a promising new direction for economic and public policy analysis by leveraging LLMs' human-like reasoning capabilities and computational power.", "source": "arxiv", "arxiv_id": "2502.16879v1", "pdf_url": "https://arxiv.org/pdf/2502.16879v1", "categories": ["cs.AI", "econ.GN"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-24T06:27:07Z", "updated": "2025-02-24T06:27:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Network Arena for Benchmarking AI Agents on Network Troubleshooting", "authors": ["Zhihao Wang", "Alessandro Cornacchia", "Alessio Sacco", "Franco Galante", "Marco Canini", "Dingde Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2512.16381v1", "abstract": "Agentic systems, powered by Large Language Models (LLMs), assist network engineers with network configuration synthesis and network troubleshooting tasks. For network troubleshooting, progress is hindered by the absence of standardized and accessible benchmarks for evaluating LLM agents in dynamic network settings at low operational effort. We present NIKA, the largest public benchmark to date for LLM-driven network incident diagnosis and troubleshooting. NIKA targets both domain experts and especially AI researchers alike, providing zero-effort replay of real-world network scenarios, and establishing well-defined agent-network interfaces for quick agent prototyping. NIKA comprises hundreds of curated network incidents, spanning five network scenarios, from data centers to ISP networks, and covers 54 representative network issues. Lastly, NIKA is modular and extensible by design, offering APIs to facilitate the integration of new network scenarios and failure cases. We evaluate state-of-the-art LLM agents on NIKA and find that while larger models succeed more often in detecting network issues, they still struggle to localize faults and identify root causes. NIKA is open-source and available to the community: https://github.com/sands-lab/nika.", "source": "arxiv", "arxiv_id": "2512.16381v1", "pdf_url": "https://arxiv.org/pdf/2512.16381v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-12-18T10:22:59Z", "updated": "2025-12-18T10:22:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents", "authors": ["Andrew Kiruluta"], "year": 2025, "url": "http://arxiv.org/abs/2508.05311v1", "abstract": "We propose a hybrid architecture that integrates decision tree-based symbolic reasoning with the generative capabilities of large language models (LLMs) within a coordinated multi-agent framework. Unlike prior approaches that loosely couple symbolic and neural modules, our design embeds decision trees and random forests as callable oracles within a unified reasoning system. Tree-based modules enable interpretable rule inference and causal logic, while LLM agents handle abductive reasoning, generalization, and interactive planning. A central orchestrator maintains belief state consistency and mediates communication across agents and external tools, enabling reasoning over both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On \\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through logic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in multistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it boosts abstraction accuracy by +6.0\\% through integration of symbolic oracles. Applications in clinical decision support and scientific discovery show how the system encodes domain rules symbolically while leveraging LLMs for contextual inference and hypothesis generation. This architecture offers a robust, interpretable, and extensible solution for general-purpose neuro-symbolic reasoning.", "source": "arxiv", "arxiv_id": "2508.05311v1", "pdf_url": "https://arxiv.org/pdf/2508.05311v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-07T12:11:53Z", "updated": "2025-08-07T12:11:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Review of 3D Object Detection with Vision-Language Models", "authors": ["Ranjan Sapkota", "Konstantinos I Roumeliotis", "Rahul Harsha Cheppally", "Marco Flores Calero", "Manoj Karkee"], "year": 2025, "url": "http://arxiv.org/abs/2504.18738v1", "abstract": "This review provides a systematic analysis of comprehensive survey of 3D object detection with vision-language models(VLMs) , a rapidly advancing area at the intersection of 3D vision and multimodal AI. By examining over 100 research papers, we provide the first systematic analysis dedicated to 3D object detection with vision-language models. We begin by outlining the unique challenges of 3D object detection with vision-language models, emphasizing differences from 2D detection in spatial reasoning and data complexity. Traditional approaches using point clouds and voxel grids are compared to modern vision-language frameworks like CLIP and 3D LLMs, which enable open-vocabulary detection and zero-shot generalization. We review key architectures, pretraining strategies, and prompt engineering methods that align textual and 3D features for effective 3D object detection with vision-language models. Visualization examples and evaluation benchmarks are discussed to illustrate performance and behavior. Finally, we highlight current challenges, such as limited 3D-language datasets and computational demands, and propose future research directions to advance 3D object detection with vision-language models. >Object Detection, Vision-Language Models, Agents, VLMs, LLMs, AI", "source": "arxiv", "arxiv_id": "2504.18738v1", "pdf_url": "https://arxiv.org/pdf/2504.18738v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-04-25T23:27:26Z", "updated": "2025-04-25T23:27:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Self-Improving Coding Agent", "authors": ["Maxime Robeyns", "Martin Szummer", "Laurence Aitchison"], "year": 2025, "url": "http://arxiv.org/abs/2504.15228v2", "abstract": "Recent advancements in Large Language Models (LLMs) have spurred interest in deploying LLM agents to undertake tasks in the world. LLMs are often deployed in agent systems: code that orchestrates LLM calls and provides them with tools. We demonstrate that an agent system, equipped with basic coding tools, can autonomously edit itself, and thereby improve its performance on benchmark tasks. We find performance gains from 17% to 53% on a random subset of SWE Bench Verified, with additional performance gains on LiveCodeBench, as well as synthetically generated agent benchmarks. Our work represents an advancement in the automated and open-ended design of agentic systems, and demonstrates a data-efficient, non gradient-based learning mechanism driven by LLM reflection and code updates.", "source": "arxiv", "arxiv_id": "2504.15228v2", "pdf_url": "https://arxiv.org/pdf/2504.15228v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-21T16:58:18Z", "updated": "2025-05-16T20:58:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents", "authors": ["Sizhe Zhou", "Jiawei Han"], "year": 2025, "url": "http://arxiv.org/abs/2511.17208v2", "abstract": "LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.", "source": "arxiv", "arxiv_id": "2511.17208v2", "pdf_url": "https://arxiv.org/pdf/2511.17208v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-21T12:41:17Z", "updated": "2025-12-11T05:13:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Survey of AI Agent Protocols", "authors": ["Yingxuan Yang", "Huacan Chai", "Yuanyi Song", "Siyuan Qi", "Muning Wen", "Ning Li", "Junwei Liao", "Haoyi Hu", "Jianghao Lin", "Gaowei Chang", "Weiwen Liu", "Ying Wen", "Yong Yu", "Weinan Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2504.16736v3", "abstract": "The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide the first comprehensive analysis of existing agent protocols, proposing a systematic two-dimensional classification that differentiates context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore the future landscape of agent protocols by identifying critical research directions and characteristics necessary for next-generation protocols. These characteristics include adaptability, privacy preservation, and group-based interaction, as well as trends toward layered architectures and collective intelligence infrastructures. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.", "source": "arxiv", "arxiv_id": "2504.16736v3", "pdf_url": "https://arxiv.org/pdf/2504.16736v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-23T14:07:26Z", "updated": "2025-06-21T17:39:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools", "authors": ["Minh-Hao Van", "Prateek Verma", "Chen Zhao", "Xintao Wu"], "year": 2025, "url": "http://arxiv.org/abs/2506.20743v1", "abstract": "Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\\&A; atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.", "source": "arxiv", "arxiv_id": "2506.20743v1", "pdf_url": "https://arxiv.org/pdf/2506.20743v1", "categories": ["cs.LG", "cs.CE"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-25T18:10:30Z", "updated": "2025-06-25T18:10:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Survey of Large Language Model Agents for Question Answering", "authors": ["Murong Yue"], "year": 2025, "url": "http://arxiv.org/abs/2503.19213v1", "abstract": "This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.", "source": "arxiv", "arxiv_id": "2503.19213v1", "pdf_url": "https://arxiv.org/pdf/2503.19213v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-24T23:39:44Z", "updated": "2025-03-24T23:39:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval", "authors": ["Yu Zhang", "Shutong Qiao", "Jiaqi Zhang", "Tzu-Heng Lin", "Chen Gao", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.05659v2", "abstract": "Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.", "source": "arxiv", "arxiv_id": "2503.05659v2", "pdf_url": "https://arxiv.org/pdf/2503.05659v2", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-03-07T18:20:30Z", "updated": "2025-04-11T16:51:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Survey on Agentic Multimodal Large Language Models", "authors": ["Huanjin Yao", "Ruifei Zhang", "Jiaxing Huang", "Jingyi Zhang", "Yibo Wang", "Bo Fang", "Ruolin Zhu", "Yongcheng Jing", "Shunyu Liu", "Guanbin Li", "Dacheng Tao"], "year": 2025, "url": "http://arxiv.org/abs/2510.10991v1", "abstract": "With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at https://github.com/HJYao00/Awesome-Agentic-MLLMs.", "source": "arxiv", "arxiv_id": "2510.10991v1", "pdf_url": "https://arxiv.org/pdf/2510.10991v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-10-13T04:07:01Z", "updated": "2025-10-13T04:07:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Survey on Trustworthy LLM Agents: Threats and Countermeasures", "authors": ["Miao Yu", "Fanci Meng", "Xinyun Zhou", "Shilong Wang", "Junyuan Mao", "Linsey Pang", "Tianlong Chen", "Kun Wang", "Xinfeng Li", "Yongfeng Zhang", "Bo An", "Qingsong Wen"], "year": 2025, "url": "http://arxiv.org/abs/2503.09648v1", "abstract": "With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. This evolution stems from empowering LLMs with additional modules such as memory, tools, environment, and even other agents. However, this advancement has also introduced more complex issues of trustworthiness, which previous research focused solely on LLMs could not cover. In this survey, we propose the TrustAgent framework, a comprehensive study on the trustworthiness of agents, characterized by modular taxonomy, multi-dimensional connotations, and technical implementation. By thoroughly investigating and summarizing newly emerged attacks, defenses, and evaluation methods for agents and MAS, we extend the concept of Trustworthy LLM to the emerging paradigm of Trustworthy Agent. In TrustAgent, we begin by deconstructing and introducing various components of the Agent and MAS. Then, we categorize their trustworthiness into intrinsic (brain, memory, and tool) and extrinsic (user, agent, and environment) aspects. Subsequently, we delineate the multifaceted meanings of trustworthiness and elaborate on the implementation techniques of existing research related to these internal and external modules. Finally, we present our insights and outlook on this domain, aiming to provide guidance for future endeavors.", "source": "arxiv", "arxiv_id": "2503.09648v1", "pdf_url": "https://arxiv.org/pdf/2503.09648v1", "categories": ["cs.MA", "cs.CY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-03-12T08:42:05Z", "updated": "2025-03-12T08:42:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Survey on the Optimization of Large Language Model-based Agents", "authors": ["Shangheng Du", "Jiabao Zhao", "Jinxin Shi", "Zhentao Xie", "Xin Jiang", "Yanhong Bai", "Liang He"], "year": 2025, "url": "http://arxiv.org/abs/2503.12434v1", "abstract": "With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks. However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness or suboptimal performance in complex agent-related environments. Although LLM optimization techniques can improve model performance across many general tasks, they lack specialized optimization towards critical agent functionalities such as long-term planning, dynamic environmental interaction, and complex decision-making. Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective is still lacking. In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods. We first focus on parameter-driven optimization, covering fine-tuning-based optimization, reinforcement learning-based optimization, and hybrid strategies, analyzing key aspects such as trajectory data construction, fine-tuning techniques, reward function design, and optimization algorithms. Additionally, we briefly discuss parameter-free strategies that optimize agent behavior through prompt engineering and external knowledge retrieval. Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Our repository for related references is available at https://github.com/YoungDubbyDu/LLM-Agent-Optimization.", "source": "arxiv", "arxiv_id": "2503.12434v1", "pdf_url": "https://arxiv.org/pdf/2503.12434v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-16T10:09:10Z", "updated": "2025-03-16T10:09:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Vision for Auto Research with LLM Agents", "authors": ["Chengwei Liu", "Chong Wang", "Jiayue Cao", "Jingquan Ge", "Kun Wang", "Lyuye Zhang", "Ming-Ming Cheng", "Penghai Zhao", "Tianlin Li", "Xiaojun Jia", "Xiang Li", "Xingshuai Li", "Yang Liu", "Yebo Feng", "Yihao Huang", "Yijia Xu", "Yuqiang Sun", "Zhenhong Zhou", "Zhengzi Xu"], "year": 2025, "url": "http://arxiv.org/abs/2504.18765v3", "abstract": "This paper introduces Agent-Based Auto Research, a structured multi-agent framework designed to automate, coordinate, and optimize the full lifecycle of scientific research. Leveraging the capabilities of large language models (LLMs) and modular agent collaboration, the system spans all major research phases, including literature review, ideation, methodology planning, experimentation, paper writing, peer review response, and dissemination. By addressing issues such as fragmented workflows, uneven methodological expertise, and cognitive overload, the framework offers a systematic and scalable approach to scientific inquiry. Preliminary explorations demonstrate the feasibility and potential of Auto Research as a promising paradigm for self-improving, AI-driven research processes.", "source": "arxiv", "arxiv_id": "2504.18765v3", "pdf_url": "https://arxiv.org/pdf/2504.18765v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-26T02:06:10Z", "updated": "2025-07-19T16:30:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A sketch of an AI control safety case", "authors": ["Tomek Korbak", "Joshua Clymer", "Benjamin Hilton", "Buck Shlegeris", "Geoffrey Irving"], "year": 2025, "url": "http://arxiv.org/abs/2501.17315v1", "abstract": "As LLM agents gain a greater capacity to cause harm, AI developers might increasingly rely on control measures such as monitoring to justify that they are safe. We sketch how developers could construct a \"control safety case\", which is a structured argument that models are incapable of subverting control measures in order to cause unacceptable outcomes. As a case study, we sketch an argument that a hypothetical LLM agent deployed internally at an AI company won't exfiltrate sensitive information. The sketch relies on evidence from a \"control evaluation,\"' where a red team deliberately designs models to exfiltrate data in a proxy for the deployment environment. The safety case then hinges on several claims: (1) the red team adequately elicits model capabilities to exfiltrate data, (2) control measures remain at least as effective in deployment, and (3) developers conservatively extrapolate model performance to predict the probability of data exfiltration in deployment. This safety case sketch is a step toward more concrete arguments that can be used to show that a dangerously capable LLM agent is safe to deploy.", "source": "arxiv", "arxiv_id": "2501.17315v1", "pdf_url": "https://arxiv.org/pdf/2501.17315v1", "categories": ["cs.AI", "cs.CR", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-28T21:52:15Z", "updated": "2025-01-28T21:52:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A-IDE : Agent-Integrated Denoising Experts", "authors": ["Uihyun Cho", "Namhun Kim"], "year": 2025, "url": "http://arxiv.org/abs/2503.16780v1", "abstract": "Recent advances in deep-learning based denoising methods have improved Low-Dose CT image quality. However, due to distinct HU distributions and diverse anatomical characteristics, a single model often struggles to generalize across multiple anatomies. To address this limitation, we introduce \\textbf{Agent-Integrated Denoising Experts (A-IDE)} framework, which integrates three anatomical region-specialized RED-CNN models under the management of decision-making LLM agent. The agent analyzes semantic cues from BiomedCLIP to dynamically route incoming LDCT scans to the most appropriate expert model. We highlight three major advantages of our approach. A-IDE excels in heterogeneous, data-scarce environments. The framework automatically prevents overfitting by distributing tasks among multiple experts. Finally, our LLM-driven agentic pipeline eliminates the need for manual interventions. Experimental evaluations on the Mayo-2016 dataset confirm that A-IDE achieves superior performance in RMSE, PSNR, and SSIM compared to a single unified denoiser.", "source": "arxiv", "arxiv_id": "2503.16780v1", "pdf_url": "https://arxiv.org/pdf/2503.16780v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-03-21T01:26:54Z", "updated": "2025-03-21T01:26:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A-MEM: Agentic Memory for LLM Agents", "authors": ["Wujiang Xu", "Zujie Liang", "Kai Mei", "Hang Gao", "Juntao Tan", "Yongfeng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.12110v11", "abstract": "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/A-mem, while the source code of the agentic memory system is available at https://github.com/WujiangXu/A-mem-sys.", "source": "arxiv", "arxiv_id": "2502.12110v11", "pdf_url": "https://arxiv.org/pdf/2502.12110v11", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T18:36:14Z", "updated": "2025-10-08T01:46:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory", "authors": ["Qianshan Wei", "Tengchao Yang", "Yaochen Wang", "Xinfeng Li", "Lijun Li", "Zhenfei Yin", "Yi Zhan", "Thorsten Holz", "Zhiqiang Lin", "XiaoFeng Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.02373v1", "abstract": "Large Language Model (LLM) agents use memory to learn from past interactions, enabling autonomous planning and decision-making in complex environments. However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior. This vulnerability is characterized by two core aspects: First, the malicious effect of injected records is only activated within a specific context, making them hard to detect when individual memory entries are audited in isolation. Second, once triggered, the manipulation can initiate a self-reinforcing error cycle: the corrupted outcome is stored as precedent, which not only amplifies the initial error but also progressively lowers the threshold for similar attacks in the future. To address these challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive defense framework for LLM agent memory. The core idea of our work is the insight that memory itself must become both self-checking and self-correcting. Without modifying the agent's core architecture, A-MemGuard combines two mechanisms: (1) consensus-based validation, which detects anomalies by comparing reasoning paths derived from multiple related memories and (2) a dual-memory structure, where detected failures are distilled into ``lessons'' stored separately and consulted before future actions, breaking error cycles and enabling adaptation. Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time. Our code is available in https://github.com/TangciuYueng/AMemGuard", "source": "arxiv", "arxiv_id": "2510.02373v1", "pdf_url": "https://arxiv.org/pdf/2510.02373v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-29T16:04:15Z", "updated": "2025-09-29T16:04:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language", "authors": ["Aly Lidayan", "Jakob Bjorner", "Satvik Golechha", "Kartik Goyal", "Alane Suhr"], "year": 2025, "url": "http://arxiv.org/abs/2512.20111v1", "abstract": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.", "source": "arxiv", "arxiv_id": "2512.20111v1", "pdf_url": "https://arxiv.org/pdf/2512.20111v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-23T07:11:26Z", "updated": "2025-12-23T07:11:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents", "authors": ["Minki Kang", "Wei-Ning Chen", "Dongge Han", "Huseyin A. Inan", "Lukas Wutschitz", "Yanzhi Chen", "Robert Sim", "Saravan Rajmohan"], "year": 2025, "url": "http://arxiv.org/abs/2510.00615v2", "abstract": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement. Our code is available at https://github.com/microsoft/acon.", "source": "arxiv", "arxiv_id": "2510.00615v2", "pdf_url": "https://arxiv.org/pdf/2510.00615v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-01T07:43:49Z", "updated": "2025-10-17T06:48:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ADAgent: LLM Agent for Alzheimer's Disease Analysis with Collaborative Coordinator", "authors": ["Wenlong Hou", "Guangqian Yang", "Ye Du", "Yeung Lau", "Lihao Liu", "Junjun He", "Ling Long", "Shujun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.11150v3", "abstract": "Alzheimer's disease (AD) is a progressive and irreversible neurodegenerative disease. Early and precise diagnosis of AD is crucial for timely intervention and treatment planning to alleviate the progressive neurodegeneration. However, most existing methods rely on single-modality data, which contrasts with the multifaceted approach used by medical experts. While some deep learning approaches process multi-modal data, they are limited to specific tasks with a small set of input modalities and cannot handle arbitrary combinations. This highlights the need for a system that can address diverse AD-related tasks, process multi-modal or missing input, and integrate multiple advanced methods for improved performance. In this paper, we propose ADAgent, the first specialized AI agent for AD analysis, built on a large language model (LLM) to address user queries and support decision-making. ADAgent integrates a reasoning engine, specialized medical tools, and a collaborative outcome coordinator to facilitate multi-modal diagnosis and prognosis tasks in AD. Extensive experiments demonstrate that ADAgent outperforms SOTA methods, achieving significant improvements in accuracy, including a 2.7% increase in multi-modal diagnosis, a 0.7% improvement in multi-modal prognosis, and enhancements in MRI and PET diagnosis tasks.", "source": "arxiv", "arxiv_id": "2506.11150v3", "pdf_url": "https://arxiv.org/pdf/2506.11150v3", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV", "doi": "", "venue": "", "published": "2025-06-11T10:22:19Z", "updated": "2025-07-27T14:17:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework", "authors": ["Yu Yao", "Salil Bhatnagar", "Markus Mazzola", "Vasileios Belagiannis", "Igor Gilitschenski", "Luigi Palmieri", "Simon Razniewski", "Marcel Hallgarten"], "year": 2025, "url": "http://arxiv.org/abs/2507.13729v1", "abstract": "Rare, yet critical, scenarios pose a significant challenge in testing and evaluating autonomous driving planners. Relying solely on real-world driving scenes requires collecting massive datasets to capture these scenarios. While automatic generation of traffic scenarios appears promising, data-driven models require extensive training data and often lack fine-grained control over the output. Moreover, generating novel scenarios from scratch can introduce a distributional shift from the original training scenes which undermines the validity of evaluations especially for learning-based planners. To sidestep this, recent work proposes to generate challenging scenarios by augmenting original scenarios from the test set. However, this involves the manual augmentation of scenarios by domain experts. An approach that is unable to meet the demands for scale in the evaluation of self-driving systems. Therefore, this paper introduces a novel LLM-agent based framework for augmenting real-world traffic scenarios using natural language descriptions, addressing the limitations of existing methods. A key innovation is the use of an agentic design, enabling fine-grained control over the output and maintaining high performance even with smaller, cost-effective LLMs. Extensive human expert evaluation demonstrates our framework's ability to accurately adhere to user intent, generating high quality augmented scenarios comparable to those created manually.", "source": "arxiv", "arxiv_id": "2507.13729v1", "pdf_url": "https://arxiv.org/pdf/2507.13729v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-07-18T08:20:16Z", "updated": "2025-07-18T08:20:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection", "authors": ["Weidi Luo", "Shenghong Dai", "Xiaogeng Liu", "Suman Banerjee", "Huan Sun", "Muhao Chen", "Chaowei Xiao"], "year": 2025, "url": "http://arxiv.org/abs/2502.11448v2", "abstract": "The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility and flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents' tasks.", "source": "arxiv", "arxiv_id": "2502.11448v2", "pdf_url": "https://arxiv.org/pdf/2502.11448v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T05:12:33Z", "updated": "2025-02-18T05:37:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "year": 2025, "url": "http://arxiv.org/abs/2507.01376v1", "abstract": "AI agents are autonomous systems designed to perceive, reason, and act within dynamic environments. With the rapid advancements in generative AI (GenAI), large language models (LLMs) and multimodal large language models (MLLMs) have significantly improved AI agents' capabilities in semantic comprehension, complex reasoning, and autonomous decision-making. At the same time, the rise of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents (MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in information processing, environmental perception, and autonomous decision-making, opening new avenues for smart manufacturing. However, the definitions, capability boundaries, and practical applications of these emerging AI paradigms in smart manufacturing remain unclear. To address this gap, this study systematically reviews the evolution of AI and AI agent technologies, examines the core concepts and technological advancements of LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential applications in and integration into manufacturing, along with the potential challenges they may face.", "source": "arxiv", "arxiv_id": "2507.01376v1", "pdf_url": "https://arxiv.org/pdf/2507.01376v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-02T05:31:17Z", "updated": "2025-07-02T05:31:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AI Kill Switch for malicious web-based LLM agent", "authors": ["Sechan Lee", "Sangdon Park"], "year": 2025, "url": "http://arxiv.org/abs/2511.13725v2", "abstract": "Recently, web-based Large Language Model (LLM) agents autonomously perform increasingly complex tasks, thereby bringing significant convenience. However, they also amplify the risks of malicious misuse cases such as unauthorized collection of personally identifiable information (PII), generation of socially divisive content, and even automated web hacking. To address these threats, we propose an AI Kill Switch technique that can immediately halt the operation of malicious web-based LLM agents. To achieve this, we introduce AutoGuard - the key idea is generating defensive prompts that trigger the safety mechanisms of malicious LLM agents. In particular, generated defense prompts are transparently embedded into the website's DOM so that they remain invisible to human users but can be detected by the crawling process of malicious agents, triggering its internal safety mechanisms to abort malicious actions once read. To evaluate our approach, we constructed a dedicated benchmark consisting of three representative malicious scenarios. Experimental results show that AutoGuard achieves over 80% Defense Success Rate (DSR) across diverse malicious agents, including GPT-4o, Claude-4.5-Sonnet and generalizes well to advanced models like GPT-5.1, Gemini-2.5-flash, and Gemini-3-pro. Also, our approach demonstrates robust defense performance in real-world website environments without significant performance degradation for benign agents. Through this research, we demonstrate the controllability of web-based LLM agents, thereby contributing to the broader effort of AI control and safety.", "source": "arxiv", "arxiv_id": "2511.13725v2", "pdf_url": "https://arxiv.org/pdf/2511.13725v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-26T02:20:46Z", "updated": "2025-12-04T04:58:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager", "authors": ["Xuhua Zhao", "Yuxuan Xie", "Caihua Chen", "Yuxiang Sun"], "year": 2025, "url": "http://arxiv.org/abs/2508.11416v1", "abstract": "Recent advances in mathematical reasoning and the long-term planning capabilities of large language models (LLMs) have precipitated the development of agents, which are being increasingly leveraged in business operations processes. Decision models to optimize inventory levels are one of the core elements of operations management. However, the capabilities of the LLM agent in making inventory decisions in uncertain contexts, as well as the decision-making biases (e.g. framing effect, etc.) of the agent, remain largely unexplored. This prompts concerns regarding the capacity of LLM agents to effectively address real-world problems, as well as the potential implications of biases that may be present. To address this gap, we introduce AIM-Bench, a novel benchmark designed to assess the decision-making behaviour of LLM agents in uncertain supply chain management scenarios through a diverse series of inventory replenishment experiments. Our results reveal that different LLMs typically exhibit varying degrees of decision bias that are similar to those observed in human beings. In addition, we explored strategies to mitigate the pull-to-centre effect and the bullwhip effect, namely cognitive reflection and implementation of information sharing. These findings underscore the need for careful consideration of the potential biases in deploying LLMs in Inventory decision-making scenarios. We hope that these insights will pave the way for mitigating human decision bias and developing human-centred decision support systems for supply chains.", "source": "arxiv", "arxiv_id": "2508.11416v1", "pdf_url": "https://arxiv.org/pdf/2508.11416v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-15T11:38:19Z", "updated": "2025-08-15T11:38:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds", "authors": ["Yinfang Chen", "Manish Shetty", "Gagan Somashekar", "Minghua Ma", "Yogesh Simmhan", "Jonathan Mace", "Chetan Bansal", "Rujia Wang", "Saravan Rajmohan"], "year": 2025, "url": "http://arxiv.org/abs/2501.06706v1", "abstract": "AI for IT Operations (AIOps) aims to automate complex operational tasks, such as fault localization and root cause analysis, to reduce human workload and minimize customer impact. While traditional DevOps tools and AIOps algorithms often focus on addressing isolated operational tasks, recent advances in Large Language Models (LLMs) and AI agents are revolutionizing AIOps by enabling end-to-end and multitask automation. This paper envisions a future where AI agents autonomously manage operational tasks throughout the entire incident lifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps. Realizing this vision requires a comprehensive framework to guide the design, development, and evaluation of these agents. To this end, we present AIOPSLAB, a framework that not only deploys microservice cloud environments, injects faults, generates workloads, and exports telemetry data but also orchestrates these components and provides interfaces for interacting with and evaluating agents. We discuss the key requirements for such a holistic framework and demonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps agents. Through evaluations of state-of-the-art LLM agents within the benchmark created by AIOPSLAB, we provide insights into their capabilities and limitations in handling complex operational tasks in cloud environments.", "source": "arxiv", "arxiv_id": "2501.06706v1", "pdf_url": "https://arxiv.org/pdf/2501.06706v1", "categories": ["cs.AI", "cs.DC", "cs.MA", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-12T04:17:39Z", "updated": "2025-01-12T04:17:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data", "authors": ["Mohammad Abolnejadian", "Shakiba Amirshahi", "Matthew Brehmer", "Anamaria Crisan"], "year": 2025, "url": "http://arxiv.org/abs/2507.09100v1", "abstract": "In decision-making conversations, experts must navigate complex choices and make on-the-spot decisions while engaged in conversation. Although extensive historical data often exists, the real-time nature of these scenarios makes it infeasible for decision-makers to review and leverage relevant information. This raises an interesting question: What if experts could utilize relevant past data in real-time decision-making through insights derived from past data? To explore this, we implemented a conversational user interface, taking doctor-patient interactions as an example use case. Our system continuously listens to the conversation, identifies patient problems and doctor-suggested solutions, and retrieves related data from an embedded dataset, generating concise insights using a pipeline built around a retrieval-based Large Language Model (LLM) agent. We evaluated the prototype by embedding Health Canada datasets into a vector database and conducting simulated studies using sample doctor-patient dialogues, showing effectiveness but also challenges, setting directions for the next steps of our work.", "source": "arxiv", "arxiv_id": "2507.09100v1", "pdf_url": "https://arxiv.org/pdf/2507.09100v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC", "doi": "10.1145/3719160.3737633", "venue": "", "published": "2025-07-12T00:59:41Z", "updated": "2025-07-12T00:59:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories", "authors": ["Titouan Duston", "Shuo Xin", "Yang Sun", "Daoguang Zan", "Aoyan Li", "Shulin Xin", "Kai Shen", "Yixiao Chen", "Qiming Sun", "Ge Zhang", "Jiashuo Liu", "Huan Zhou", "Jingkai Liu", "Zhichen Pu", "Yuanheng Wang", "Bo-Xuan Ge", "Xin Tong", "Fei Ye", "Zhi-Chao Zhao", "Wen-Biao Han", "Zhoujian Cao", "Yueran Zhao", "Weiluo Ren", "Qingshen Long", "Yuxiao Liu", "Anni Huang", "Yidi Du", "Yuanyuan Rong", "Jiahao Peng"], "year": 2025, "url": "http://arxiv.org/abs/2512.21373v1", "abstract": "We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.", "source": "arxiv", "arxiv_id": "2512.21373v1", "pdf_url": "https://arxiv.org/pdf/2512.21373v1", "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-24T08:11:11Z", "updated": "2025-12-24T08:11:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery", "authors": ["Yuqi Yin", "Yibo Fu", "Siyuan Wang", "Peng Sun", "Hongyu Wang", "Xiaohui Wang", "Lei Zheng", "Zhiyong Li", "Zhirong Liu", "Jianji Wang", "Zhaoxi Sun"], "year": 2025, "url": "http://arxiv.org/abs/2511.11257v1", "abstract": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.", "source": "arxiv", "arxiv_id": "2511.11257v1", "pdf_url": "https://arxiv.org/pdf/2511.11257v1", "categories": ["cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-14T12:53:57Z", "updated": "2025-11-14T12:53:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning", "authors": ["Edward Y. Chang", "Longling Geng"], "year": 2025, "url": "http://arxiv.org/abs/2505.12501v1", "abstract": "Large language models (LLMs) excel at rapid generation of text and multimodal content, yet they falter on transaction-style planning that demands ACID-like guarantees and real-time disruption recovery. We present Adaptive LLM Agent System (ALAS), a framework that tackles four fundamental LLM deficits: (i) absence of self-verification, (ii) context erosion, (iii) next-token myopia, and (iv) lack of persistent state. ALAS decomposes each plan into role-specialized agents, equips them with automatic state tracking, and coordinates them through a lightweight protocol. When disruptions arise, agents apply history-aware local compensation, avoiding costly global replanning and containing cascade effects. On real-world, large-scale job-shop scheduling benchmarks, ALAS sets new best results for static sequential planning and excels in dynamic reactive scenarios with unexpected disruptions. These gains show that principled modularization plus targeted compensation can unlock scalable and resilient planning with LLMs.", "source": "arxiv", "arxiv_id": "2505.12501v1", "pdf_url": "https://arxiv.org/pdf/2505.12501v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-18T17:27:08Z", "updated": "2025-05-18T17:27:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework", "authors": ["Vali Tawosi", "Keshav Ramani", "Salwa Alamir", "Xiaomo Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.03463v2", "abstract": "Multi-agent Large Language Model (LLM) systems have been leading the way in applied LLM research across a number of fields. One notable area is software development, where researchers have advanced the automation of code implementation, code testing, code maintenance, inter alia, using LLM agents. However, software development is a multifaceted environment that extends beyond just code. As such, a successful LLM system must factor in multiple stages of the software development life-cycle (SDLC). In this paper, we propose a vision for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework, which follows the above SDLC philosophy such that it may work within an agile software development team to perform several tasks end-to-end. ALMAS aligns its agents with agile roles, and can be used in a modular fashion to seamlessly integrate with human developers and their development environment. We showcase the progress towards ALMAS through our published works and a use case demonstrating the framework, where ALMAS is able to seamlessly generate an application and add a new feature.", "source": "arxiv", "arxiv_id": "2510.03463v2", "pdf_url": "https://arxiv.org/pdf/2510.03463v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-03T19:35:23Z", "updated": "2025-11-24T18:11:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ALRPHFS: Adversarially Learned Risk Patterns with Hierarchical Fast \\& Slow Reasoning for Robust Agent Defense", "authors": ["Shiyu Xiang", "Tong Zhang", "Ronghao Chen"], "year": 2025, "url": "http://arxiv.org/abs/2505.19260v2", "abstract": "LLM Agents are becoming central to intelligent systems. However, their deployment raises serious safety concerns. Existing defenses largely rely on \"Safety Checks\", which struggle to capture the complex semantic risks posed by harmful user inputs or unsafe agent behaviors - creating a significant semantic gap between safety checks and real-world risks. To bridge this gap, we propose a novel defense framework, ALRPHFS (Adversarially Learned Risk Patterns with Hierarchical Fast & Slow Reasoning). ALRPHFS consists of two core components: (1) an offline adversarial self-learning loop to iteratively refine a generalizable and balanced library of risk patterns, substantially enhancing robustness without retraining the base LLM, and (2) an online hierarchical fast & slow reasoning engine that balances detection effectiveness with computational efficiency. Experimental results demonstrate that our approach achieves superior overall performance compared to existing baselines, achieving a best-in-class average accuracy of 80% and exhibiting strong generalizability across agents and tasks.", "source": "arxiv", "arxiv_id": "2505.19260v2", "pdf_url": "https://arxiv.org/pdf/2505.19260v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-25T18:31:48Z", "updated": "2025-09-12T18:40:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering", "authors": ["Ziqing Wang", "Chengsheng Mao", "Xiaole Wen", "Yuan Luo", "Kaize Ding"], "year": 2025, "url": "http://arxiv.org/abs/2510.02328v1", "abstract": "Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise in medical visual question answering (Med-VQA). However, when deployed in low-resource settings where abundant labeled data are unavailable, existing Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks: (i) the intrinsic reasoning bottleneck that ignores the details from the medical image; (ii) the extrinsic reasoning bottleneck that fails to incorporate specialized medical knowledge. To address those limitations, we propose AMANDA, a training-free agentic framework that performs medical knowledge augmentation via LLM agents. Specifically, our intrinsic medical knowledge augmentation focuses on coarse-to-fine question decomposition for comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds the reasoning process via biomedical knowledge graph retrieval. Extensive experiments across eight Med-VQA benchmarks demonstrate substantial improvements in both zero-shot and few-shot Med-VQA settings. The code is available at https://github.com/REAL-Lab-NU/AMANDA.", "source": "arxiv", "arxiv_id": "2510.02328v1", "pdf_url": "https://arxiv.org/pdf/2510.02328v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-26T01:22:25Z", "updated": "2025-09-26T01:22:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "API Agents vs. GUI Agents: Divergence and Convergence", "authors": ["Chaoyun Zhang", "Shilin He", "Liqun Li", "Si Qin", "Yu Kang", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.11069v2", "abstract": "Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.", "source": "arxiv", "arxiv_id": "2503.11069v2", "pdf_url": "https://arxiv.org/pdf/2503.11069v2", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-14T04:26:21Z", "updated": "2025-06-23T13:01:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph Environments", "authors": ["Pedro Gimenes", "Zeyu Cao", "Jeffrey Wong", "Yiren Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.21208v1", "abstract": "Recent research has shown that LLM performance on reasoning tasks can be enhanced by scaling test-time compute. One promising approach, particularly with decomposable problems, involves arranging intermediate solutions as a graph on which transformations are performed to explore the solution space. However, prior works rely on pre-determined, task-specific transformation schedules which are subject to a set of searched hyperparameters. In this work, we view thought graph transformations as actions in a Markov decision process, and implement policy agents to drive effective action policies for the underlying reasoning LLM agent. In particular, we investigate the ability for another LLM to act as a policy agent on thought graph environments and introduce ARIES, a multi-agent architecture for reasoning with LLMs. In ARIES, reasoning LLM agents solve decomposed subproblems, while policy LLM agents maintain visibility of the thought graph states, and dynamically adapt the problem-solving strategy. Through extensive experiments, we observe that using off-the-shelf LLMs as policy agents with no supervised fine-tuning (SFT) can yield up to $29\\%$ higher accuracy on HumanEval relative to static transformation schedules, as well as reducing inference costs by $35\\%$ and avoid any search requirements. We also conduct a thorough analysis of observed failure modes, highlighting that limitations on LLM sizes and the depth of problem decomposition can be seen as challenges to scaling LLM-guided reasoning.", "source": "arxiv", "arxiv_id": "2502.21208v1", "pdf_url": "https://arxiv.org/pdf/2502.21208v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-28T16:28:13Z", "updated": "2025-02-28T16:28:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning", "authors": ["Jan-Felix Klein", "Lars Ohnemus"], "year": 2025, "url": "http://arxiv.org/abs/2509.18063v1", "abstract": "Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.", "source": "arxiv", "arxiv_id": "2509.18063v1", "pdf_url": "https://arxiv.org/pdf/2509.18063v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-22T17:40:05Z", "updated": "2025-09-22T17:40:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ARS: Automatic Routing Solver with Large Language Models", "authors": ["Kai Li", "Fei Liu", "Zhenkun Wang", "Xialiang Tong", "Xiongwei Han", "Mingxuan Yuan", "Qingfu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.15359v3", "abstract": "Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of practical constraints, making manual solver design both knowledge-intensive and time-consuming. Although there is increasing interest in automating the design of routing algorithms, existing research has explored only a limited array of VRP variants and fails to adequately address the complex and prevalent constraints encountered in real-world situations. To fill this gap, this paper introduces RoutBench, a benchmark of 1,000 VRP variants derived from 24 attributes, for evaluating the effectiveness of automatic routing solvers in addressing complex constraints. Along with RoutBench, we present the Automatic Routing Solver (ARS), which employs Large Language Model (LLM) agents to enhance a backbone algorithm framework by automatically generating constraint-aware heuristic code, based on problem descriptions and several representative constraints selected from a database. Our experiments show that ARS outperforms state-of-the-art LLM-based methods and commonly used solvers, automatically solving 91.67% of common VRPs and achieving at least a 30% improvement across all benchmarks.", "source": "arxiv", "arxiv_id": "2502.15359v3", "pdf_url": "https://arxiv.org/pdf/2502.15359v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-21T10:14:55Z", "updated": "2025-05-19T08:29:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization", "authors": ["Omer Jauhar Khan"], "year": 2025, "url": "http://arxiv.org/abs/2512.00617v2", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R^2 values exceeding 0.96 in ELO rating convergence.", "source": "arxiv", "arxiv_id": "2512.00617v2", "pdf_url": "https://arxiv.org/pdf/2512.00617v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-29T20:16:11Z", "updated": "2025-12-24T03:42:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy", "authors": ["Alejandro D. Mousist"], "year": 2025, "url": "http://arxiv.org/abs/2509.13380v2", "abstract": "This paper presents ASTREA, the first agentic system executed on flight-heritage hardware (TRL 9) for autonomous spacecraft operations, with on-orbit operation aboard the International Space Station (ISS). Using thermal control as a representative use case, we integrate a resource-constrained Large Language Model (LLM) agent with a reinforcement learning controller in an asynchronous architecture tailored for space-qualified platforms. Ground experiments show that LLM-guided supervision improves thermal stability and reduces violations, confirming the feasibility of combining semantic reasoning with adaptive control under hardware constraints. On-orbit validation aboard the ISS initially faced challenges due to inference latency misaligned with the rapid thermal cycles of Low Earth Orbit (LEO) satellites. Synchronization with the orbit length successfully surpassed the baseline with reduced violations, extended episode durations, and improved CPU utilization. These findings demonstrate the potential for scalable agentic supervision architectures in future autonomous spacecraft.", "source": "arxiv", "arxiv_id": "2509.13380v2", "pdf_url": "https://arxiv.org/pdf/2509.13380v2", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "eess.SY"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-09-16T08:52:13Z", "updated": "2025-10-11T23:26:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination", "authors": ["Charidimos Papadakis", "Angeliki Dimitriou", "Giorgos Filandrianos", "Maria Lymperaiou", "Konstantinos Thomas", "Giorgos Stamou"], "year": 2025, "url": "http://arxiv.org/abs/2510.15949v2", "abstract": "Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.", "source": "arxiv", "arxiv_id": "2510.15949v2", "pdf_url": "https://arxiv.org/pdf/2510.15949v2", "categories": ["q-fin.TR", "cs.AI"], "primary_category": "q-fin.TR", "doi": "", "venue": "", "published": "2025-10-10T13:01:51Z", "updated": "2026-01-08T13:08:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ATLaS: Agent Tuning via Learning Critical Steps", "authors": ["Zhixun Chen", "Ming Li", "Yuxuan Huang", "Yali Du", "Meng Fang", "Tianyi Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2503.02197v2", "abstract": "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.", "source": "arxiv", "arxiv_id": "2503.02197v2", "pdf_url": "https://arxiv.org/pdf/2503.02197v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-04T02:14:55Z", "updated": "2025-06-05T01:42:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents", "authors": ["Fengze Liu", "Haoyu Wang", "Joonhyuk Cho", "Dan Roth", "Andrew W. Lo"], "year": 2025, "url": "http://arxiv.org/abs/2506.04293v1", "abstract": "Clinical trials are critical for advancing medical treatments but remain prohibitively expensive and time-consuming. Accurate prediction of clinical trial outcomes can significantly reduce research and development costs and accelerate drug discovery. While recent deep learning models have shown promise by leveraging unstructured data, their black-box nature, lack of interpretability, and vulnerability to label leakage limit their practical use in high-stakes biomedical contexts. In this work, we propose AutoCT, a novel framework that combines the reasoning capabilities of large language models with the explainability of classical machine learning. AutoCT autonomously generates, evaluates, and refines tabular features based on public information without human input. Our method uses Monte Carlo Tree Search to iteratively optimize predictive performance. Experimental results show that AutoCT performs on par with or better than SOTA methods on clinical trial prediction tasks within only a limited number of self-refinement iterations, establishing a new paradigm for scalable, interpretable, and cost-efficient clinical trial prediction.", "source": "arxiv", "arxiv_id": "2506.04293v1", "pdf_url": "https://arxiv.org/pdf/2506.04293v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-04T11:50:55Z", "updated": "2025-06-04T11:50:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Abstract Counterfactuals for Language Model Agents", "authors": ["Edoardo Pona", "Milad Kazemi", "Yali Du", "David Watson", "Nicola Paoletti"], "year": 2025, "url": "http://arxiv.org/abs/2506.02946v1", "abstract": "Counterfactual inference is a powerful tool for analysing and evaluating autonomous agents, but its application to language model (LM) agents remains challenging. Existing work on counterfactuals in LMs has primarily focused on token-level counterfactuals, which are often inadequate for LM agents due to their open-ended action spaces. Unlike traditional agents with fixed, clearly defined action spaces, the actions of LM agents are often implicit in the strings they output, making their action spaces difficult to define and interpret. Furthermore, the meanings of individual tokens can shift depending on the context, adding complexity to token-level reasoning and sometimes leading to biased or meaningless counterfactuals. We introduce \\emph{Abstract Counterfactuals}, a framework that emphasises high-level characteristics of actions and interactions within an environment, enabling counterfactual reasoning tailored to user-relevant features. Our experiments demonstrate that the approach produces consistent and meaningful counterfactuals while minimising the undesired side effects of token-level methods. We conduct experiments on text-based games and counterfactual text generation, while considering both token-level and latent-space interventions.", "source": "arxiv", "arxiv_id": "2506.02946v1", "pdf_url": "https://arxiv.org/pdf/2506.02946v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-03T14:44:26Z", "updated": "2025-06-03T14:44:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization", "authors": ["Genghan Zhang", "Shaowei Zhu", "Anjiang Wei", "Zhenyu Song", "Allen Nie", "Zhen Jia", "Nandita Vijaykumar", "Yida Wang", "Kunle Olukotun"], "year": 2025, "url": "http://arxiv.org/abs/2511.15915v1", "abstract": "We present AccelOpt, a self-improving large language model (LLM) agentic system that autonomously optimizes kernels for emerging AI acclerators, eliminating the need for expert-provided hardware-specific optimization knowledge. AccelOpt explores the kernel optimization space through iterative generation, informed by an optimization memory that curates experiences and insights from previously encountered slow-fast kernel pairs. We build NKIBench, a new benchmark suite of AWS Trainium accelerator kernels with varying complexity extracted from real-world LLM workloads to evaluate the effectiveness of AccelOpt. Our evaluation confirms that AccelOpt's capability improves over time, boosting the average percentage of peak throughput from $49\\%$ to $61\\%$ on Trainium 1 and from $45\\%$ to $59\\%$ on Trainium 2 for NKIBench kernels. Moreover, AccelOpt is highly cost-effective: using open-source models, it matches the kernel improvements of Claude Sonnet 4 while being $26\\times$ cheaper.", "source": "arxiv", "arxiv_id": "2511.15915v1", "pdf_url": "https://arxiv.org/pdf/2511.15915v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-19T22:49:37Z", "updated": "2025-11-19T22:49:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Accelerating Scientific Discovery with Autonomous Goal-evolving Agents", "authors": ["Yuanqi Du", "Botao Yu", "Tianyu Liu", "Tony Shen", "Junwu Chen", "Jan G. Rittig", "Kunyang Sun", "Yikun Zhang", "Zhangde Song", "Bo Zhou", "Cassandra Masschelein", "Yingze Wang", "Haorui Wang", "Haojun Jia", "Chao Zhang", "Hongyu Zhao", "Martin Ester", "Teresa Head-Gordon", "Carla P. Gomes", "Huan Sun", "Chenru Duan", "Philippe Schwaller", "Wengong Jin"], "year": 2025, "url": "http://arxiv.org/abs/2512.21782v1", "abstract": "There has been unprecedented interest in developing agents that expand the boundary of scientific discovery, primarily by optimizing quantitative objective functions specified by scientists. However, for grand challenges in science , these objectives are only imperfect proxies. We argue that automating objective function design is a central, yet unmet requirement for scientific discovery agents. In this work, we introduce the Scientific Autonomous Goal-evolving Agent (SAGA) to amend this challenge. SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives. This bi-level design enables systematic exploration of the space of objectives and their trade-offs, rather than treating them as fixed inputs. We demonstrate the framework through a broad spectrum of applications, including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing that automating objective formulation can substantially improve the effectiveness of scientific discovery agents.", "source": "arxiv", "arxiv_id": "2512.21782v1", "pdf_url": "https://arxiv.org/pdf/2512.21782v1", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.LG", "physics.chem-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-25T20:54:41Z", "updated": "2025-12-25T20:54:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Accelerating Two-Dimensional Materials Research via a Universal Interatomic Potential and Large Language Model Agent", "authors": ["Haidi Wang", "Yufan Yao", "Haonan Song", "Xiaofeng Liu", "Zhao Chen", "Weiwei Chen", "Weiduo Zhu", "Zhongjun Li", "Jinlong Yang"], "year": 2025, "url": "http://arxiv.org/abs/2506.07043v2", "abstract": "Accurate interatomic potentials (IAPs) are essential for modeling the potential energy surfaces (PES) that govern atomic interactions in materials. However, most existing IAPs are developed for bulk materials and struggle to accurately and efficiently capture the diverse chemical environment of two-dimensional (2D) materials. This limitation poses a significant barrier to the large-scale design and simulation of emerging 2D systems. To address this challenge, we present a universal interatomic potential tailored for 2D materials. Our model is trained on a dataset comprising 327,062 structure-energy-force-stress mappings derived from 20,114 2D materials, spanning 89 chemical elements. The results show high predictive accuracy, with mean absolute errors of 6 meV/atom for energies, 80 meV/Ãfor atomic forces, and 0.067 GPa for stress tensors. It demonstrates broad applicability across a range of atomistic tasks, including structural relaxation, lattice dynamics, molecular dynamics, material discovery, and so on. To further enhance usability and accessibility, we introduce an intelligent agent powered by a large language model (LLM), enabling natural language interaction for 2D materials property simulations. Our work provides not only a precise and universal IAP for 2D systems, but also an intelligent, user-friendly platform that enables high-throughput screening, property prediction, and theoretical exploration, thereby accelerating advances in 2D materials research.", "source": "arxiv", "arxiv_id": "2506.07043v2", "pdf_url": "https://arxiv.org/pdf/2506.07043v2", "categories": ["cond-mat.mtrl-sci"], "primary_category": "cond-mat.mtrl-sci", "doi": "", "venue": "", "published": "2025-06-08T08:41:47Z", "updated": "2025-10-16T01:45:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning", "authors": ["Haiteng Zhao", "Junhao Shen", "Yiming Zhang", "Songyang Gao", "Kuikun Liu", "Tianyou Ma", "Fan Zheng", "Dahua Lin", "Wenwei Zhang", "Kai Chen"], "year": 2025, "url": "http://arxiv.org/abs/2512.10534v2", "abstract": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.", "source": "arxiv", "arxiv_id": "2512.10534v2", "pdf_url": "https://arxiv.org/pdf/2512.10534v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-11T11:05:04Z", "updated": "2025-12-12T13:43:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Active Task Disambiguation with LLMs", "authors": ["Katarzyna Kobalczyk", "Nicolas Astorga", "Tennison Liu", "Mihaela van der Schaar"], "year": 2025, "url": "http://arxiv.org/abs/2502.04485v1", "abstract": "Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems--frequent in real-world interactions--remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions.", "source": "arxiv", "arxiv_id": "2502.04485v1", "pdf_url": "https://arxiv.org/pdf/2502.04485v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-06T20:20:22Z", "updated": "2025-02-06T20:20:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents", "authors": ["Qiusi Zhan", "Richard Fang", "Henil Shalin Panchal", "Daniel Kang"], "year": 2025, "url": "http://arxiv.org/abs/2503.00061v2", "abstract": "Large Language Model (LLM) agents exhibit remarkable performance across diverse applications by using external tools to interact with environments. However, integrating external tools introduces security risks, such as indirect prompt injection (IPI) attacks. Despite defenses designed for IPI attacks, their robustness remains questionable due to insufficient testing against adaptive attacks. In this paper, we evaluate eight different defenses and bypass all of them using adaptive attacks, consistently achieving an attack success rate of over 50%. This reveals critical vulnerabilities in current defenses. Our research underscores the need for adaptive attack evaluation when designing defenses to ensure robustness and reliability. The code is available at https://github.com/uiuc-kang-lab/AdaptiveAttackAgent.", "source": "arxiv", "arxiv_id": "2503.00061v2", "pdf_url": "https://arxiv.org/pdf/2503.00061v2", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-27T04:04:50Z", "updated": "2025-03-04T03:32:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Adaptive LLM Agents: Toward Personalized Empathetic Care", "authors": ["Priyanka Singh", "Sebastian Von Mammen"], "year": 2025, "url": "http://arxiv.org/abs/2511.20080v1", "abstract": "Current mental-health conversational systems are usually based on fixed, generic dialogue patterns. This paper proposes an adaptive framework based on large language models that aims to personalize therapeutic interaction according to a user's psychological state, quantified with the Acceptance of Illness Scale (AIS). The framework defines three specialized agents, L, M, and H, each linked to a different level of illness acceptance, and adjusts conversational behavior over time using continuous feedback signals. The AIS-stratified architecture is treated as a diegetic prototype placed in a plausible near-future setting and examined through the method of design fiction. By embedding the architecture in narrative scenarios, the study explores how such agents might influence access to care and therapeutic relationship. The goal is to show how clinically informed personalization, technical feasibility, and speculative scenario analysis can together inform the responsible design of LLM-based companions for mental-health support.", "source": "arxiv", "arxiv_id": "2511.20080v1", "pdf_url": "https://arxiv.org/pdf/2511.20080v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-11-25T08:52:02Z", "updated": "2025-11-25T08:52:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Adaptive Self-improvement LLM Agentic System for ML Library Development", "authors": ["Genghan Zhang", "Weixin Liang", "Olivia Hsu", "Kunle Olukotun"], "year": 2025, "url": "http://arxiv.org/abs/2502.02534v2", "abstract": "ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\\times$ over a baseline single LLM.", "source": "arxiv", "arxiv_id": "2502.02534v2", "pdf_url": "https://arxiv.org/pdf/2502.02534v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-04T17:57:17Z", "updated": "2025-09-19T04:28:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Addressing the alignment problem in transportation policy making: an LLM approach", "authors": ["Xiaoyu Yan", "Tianxing Dai", "Yu Marco Nie"], "year": 2025, "url": "http://arxiv.org/abs/2510.13139v1", "abstract": "A key challenge in transportation planning is that the collective preferences of heterogeneous travelers often diverge from the policies produced by model-driven decision tools. This misalignment frequently results in implementation delays or failures. Here, we investigate whether large language models (LLMs), noted for their capabilities in reasoning and simulating human decision-making, can help inform and address this alignment problem. We develop a multi-agent simulation in which LLMs, acting as agents representing residents from different communities in a city, participate in a referendum on a set of transit policy proposals. Using chain-of-thought reasoning, LLM agents provide ranked-choice or approval-based preferences, which are aggregated using instant-runoff voting (IRV) to model democratic consensus. We implement this simulation framework with both GPT-4o and Claude-3.5, and apply it for Chicago and Houston. Our findings suggest that LLM agents are capable of approximating plausible collective preferences and responding to local context, while also displaying model-specific behavioral biases and modest divergences from optimization-based benchmarks. These capabilities underscore both the promise and limitations of LLMs as tools for solving the alignment problem in transportation decision-making.", "source": "arxiv", "arxiv_id": "2510.13139v1", "pdf_url": "https://arxiv.org/pdf/2510.13139v1", "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.MA"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-10-15T04:36:38Z", "updated": "2025-10-15T04:36:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Addressing the sustainable AI trilemma: a case study on LLM agents and RAG", "authors": ["Hui Wu", "Xiaoyang Wang", "Zhong Fan"], "year": 2025, "url": "http://arxiv.org/abs/2501.08262v1", "abstract": "Large language models (LLMs) have demonstrated significant capabilities, but their widespread deployment and more advanced applications raise critical sustainability challenges, particularly in inference energy consumption. We propose the concept of the Sustainable AI Trilemma, highlighting the tensions between AI capability, digital equity, and environmental sustainability. Through a systematic case study of LLM agents and retrieval-augmented generation (RAG), we analyze the energy costs embedded in memory module designs and introduce novel metrics to quantify the trade-offs between energy consumption and system performance. Our experimental results reveal significant energy inefficiencies in current memory-augmented frameworks and demonstrate that resource-constrained environments face disproportionate efficiency penalties. Our findings challenge the prevailing LLM-centric paradigm in agent design and provide practical insights for developing more sustainable AI systems.", "source": "arxiv", "arxiv_id": "2501.08262v1", "pdf_url": "https://arxiv.org/pdf/2501.08262v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-01-14T17:21:16Z", "updated": "2025-01-14T17:21:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents", "authors": ["Doohee You", "Sundeep Paul"], "year": 2025, "url": "http://arxiv.org/abs/2512.13704v1", "abstract": "The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a \"Council of Agents,\" a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.", "source": "arxiv", "arxiv_id": "2512.13704v1", "pdf_url": "https://arxiv.org/pdf/2512.13704v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-05T06:13:00Z", "updated": "2025-12-05T06:13:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop Framework Using LLM", "authors": ["Mohd Ariful Haque", "Justin Williams", "Sunzida Siddique", "Md. Hujaifa Islam", "Hasmot Ali", "Kishor Datta Gupta", "Roy George"], "year": 2025, "url": "http://arxiv.org/abs/2503.10071v1", "abstract": "The combination of LLM agents with external tools enables models to solve complex tasks beyond their knowledge base. Human-designed tools are inflexible and restricted to solutions within the scope of pre-existing tools created by experts. To address this problem, we propose ATLASS, an advanced tool learning and selection system designed as a closed-loop framework. It enables the LLM to solve problems by dynamically generating external tools on demand. In this framework, agents play a crucial role in orchestrating tool selection, execution, and refinement, ensuring adaptive problem-solving capabilities. The operation of ATLASS follows three phases: The first phase, Understanding Tool Requirements, involves the Agents determining whether tools are required and specifying their functionality; the second phase, Tool Retrieval/Generation, involves the Agents retrieving or generating tools based on their availability; and the third phase, Task Solving, involves combining all the component tools necessary to complete the initial task. The Tool Dataset stores the generated tools, ensuring reusability and minimizing inference cost. Current LLM-based tool generation systems have difficulty creating complex tools that need APIs or external packages. In ATLASS, we solve the problem by automatically setting up the environment, fetching relevant API documentation online, and using a Python interpreter to create a reliable, versatile tool that works in a wider range of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and ethical concerns are handled through human feedback before executing generated code. By addressing the limitations of predefined toolsets and enhancing adaptability, ATLASS serves as a real-world solution that empowers users with dynamically generated tools for complex problem-solving.", "source": "arxiv", "arxiv_id": "2503.10071v1", "pdf_url": "https://arxiv.org/pdf/2503.10071v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1109/SOSE67019.2025.00012", "venue": "2025 IEEE International Conference on Service-Oriented System Engineering (SOSE), Tucson, AZ, USA, 21-24 July 2025, IEEE", "published": "2025-03-13T05:39:00Z", "updated": "2025-03-13T05:39:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning", "authors": ["Yinggan Xu", "Hana Kimlee", "Yijia Xiao", "Di Luo"], "year": 2025, "url": "http://arxiv.org/abs/2504.01911v2", "abstract": "Large Language Models (LLMs) are playing an increasingly important role in physics research by assisting with symbolic manipulation, numerical computation, and scientific reasoning. However, ensuring the reliability, transparency, and interpretability of their outputs remains a major challenge. In this work, we introduce a novel multi-agent LLM physicist framework that fosters collaboration between AI and human scientists through three key modules: a reasoning module, an interpretation module, and an AI-scientist interaction module. Recognizing that effective physics reasoning demands logical rigor, quantitative accuracy, and alignment with established theoretical models, we propose an interpretation module that employs a team of specialized LLM agents-including summarizers, model builders, visualization tools, and testers-to systematically structure LLM outputs into transparent, physically grounded science models. A case study demonstrates that our approach significantly improves interpretability, enables systematic validation, and enhances human-AI collaboration in physics problem-solving and discovery. Our work bridges free-form LLM reasoning with interpretable, executable models for scientific analysis, enabling more transparent and verifiable AI-augmented research.", "source": "arxiv", "arxiv_id": "2504.01911v2", "pdf_url": "https://arxiv.org/pdf/2504.01911v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "physics.comp-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-02T17:13:16Z", "updated": "2025-08-18T08:28:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Adversarial Agent Collaboration for C to Rust Translation", "authors": ["Tianyu Li", "Ruishi Li", "Bo Wang", "Brandon Paulsen", "Umang Mathur", "Prateek Saxena"], "year": 2025, "url": "http://arxiv.org/abs/2510.03879v2", "abstract": "Translating C to memory-safe languages, like Rust, prevents critical memory safety vulnerabilities that are prevalent in legacy C software. Existing approaches for C to safe Rust translation, including LLM-assisted ones, do not generalize on larger (> 500 LoC) C codebases because they depend on complex program analyses that frequently break. In this work, we present ACToR (Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired by GANs, ACToR pits a generator agent against a discriminator agent, which collaborate to iteratively generate a Rust translation. On each iteration, the translator agent synthesizes and refines a Rust translation to pass an existing suite of tests, and then the discriminator agent finds new failing tests. We demonstrate that ACToR translates all of the 63 real-world command-line utilities considered in our benchmarks, which have an average size of 473 lines of code, and it achieves over 90% test pass rate with zero human intervention during translation. To our knowledge, it is the first work to show evidence that an agent-centric approach can reliably and automatically convert standalone command-line C programs at this scale. Furthermore, ACToR improves translation correctness by up to 25.1% compared to baseline, non-adversarial approaches.", "source": "arxiv", "arxiv_id": "2510.03879v2", "pdf_url": "https://arxiv.org/pdf/2510.03879v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-04T17:08:36Z", "updated": "2025-12-08T09:32:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Adversarial Reinforcement Learning for Large Language Model Agent Safety", "authors": ["Zizhao Wang", "Dingcheng Li", "Vaishakh Keshava", "Phillip Wallis", "Ananth Balashankar", "Peter Stone", "Lukas Rutishauser"], "year": 2025, "url": "http://arxiv.org/abs/2510.05442v1", "abstract": "Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.", "source": "arxiv", "arxiv_id": "2510.05442v1", "pdf_url": "https://arxiv.org/pdf/2510.05442v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-06T23:09:18Z", "updated": "2025-10-06T23:09:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents", "authors": ["Kevin Song", "Anand Jayarajan", "Yaoyao Ding", "Qidong Su", "Zhanda Zhu", "Sihang Liu", "Gennady Pekhimenko"], "year": 2025, "url": "http://arxiv.org/abs/2508.19504v1", "abstract": "Large Language Models (LLMs) agents augmented with domain tools promise to autonomously execute complex tasks requiring human-level intelligence, such as customer service and digital assistance. However, their practical deployment is often limited by their low success rates under complex real-world environments. To tackle this, prior research has primarily focused on improving the agents themselves, such as developing strong agentic LLMs, while overlooking the role of the system environment in which the agent operates.\n  In this paper, we study a complementary direction: improving agent success rates by optimizing the system environment in which the agent operates. We collect 142 agent traces (3,656 turns of agent-environment interactions) across 5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we propose a taxonomy for agent-environment interaction failures that includes 6 failure modes. Guided by these findings, we design Aegis, a set of targeted environment optimizations: 1) environment observability enhancement, 2) common computation offloading, and 3) speculative agentic actions. These techniques improve agent success rates on average by 6.7-12.5%, without any modifications to the agent and underlying LLM.", "source": "arxiv", "arxiv_id": "2508.19504v1", "pdf_url": "https://arxiv.org/pdf/2508.19504v1", "categories": ["cs.MA", "cs.DC"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-08-27T01:29:46Z", "updated": "2025-08-27T01:29:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AegisMCP: Online Graph Intrusion Detection for Tool-Augmented LLMs on Edge Devices", "authors": ["Zhonghao Zhan", "Amir Al Sadi", "Krinos Li", "Hamed Haddadi"], "year": 2025, "url": "http://arxiv.org/abs/2510.19462v2", "abstract": "In this work, we study security of Model Context Protocol (MCP) agent toolchains and their applications in smart homes. We introduce AegisMCP, a protocol-level intrusion detector. Our contributions are: (i) a minimal attack suite spanning instruction-driven escalation, chain-of-tool exfiltration, malicious MCP server registration, and persistence; (ii) NEBULA-Schema (Network-Edge Behavioral Learning for Untrusted LLM Agents), a reusable protocol-level instrumentation that represents MCP activity as a streaming heterogeneous temporal graph over agents, MCP servers, tools, devices, remotes, and sessions; and (iii) a CPU-only streaming detector that fuses novelty, session-DAG structure, and attribute cues for near-real-time edge inference, with optional fusion of local prompt-guardrail signals. On an emulated smart-home testbed spanning multiple MCP stacks and a physical bench, AegisMCP achieves sub-second per-window model inference and end-to-end alerting. The latency of AegisMCP is consistently sub-second on Intel N150-class edge hardware, while outperforming traffic-only and sequence baselines; ablations confirm the importance of DAG and install/permission signals. We release code, schemas, and generators for reproducible evaluation.", "source": "arxiv", "arxiv_id": "2510.19462v2", "pdf_url": "https://arxiv.org/pdf/2510.19462v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-22T10:50:22Z", "updated": "2025-10-25T22:02:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?", "authors": ["Guibin Zhang", "Junhao Wang", "Junjie Chen", "Wangchunshu Zhou", "Kun Wang", "Shuicheng Yan"], "year": 2025, "url": "http://arxiv.org/abs/2509.03312v2", "abstract": "Large Language Model (LLM)-based agentic systems, often comprising multiple models, complex tool invocations, and orchestration protocols, substantially outperform monolithic agents. Yet this very sophistication amplifies their fragility, making them more prone to system failure. Pinpointing the specific agent or step responsible for an error within long execution traces defines the task of agentic system failure attribution. Current state-of-the-art reasoning LLMs, however, remain strikingly inadequate for this challenge, with accuracy generally below 10%. To address this gap, we propose AgenTracer, the first automated framework for annotating failed multi-agent trajectories via counterfactual replay and programmed fault injection, producing the curated dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a lightweight failure tracer trained with multi-granular reinforcement learning, capable of efficiently diagnosing errors in verbose multi-agent interactions. On the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS with 4.8-14.2% performance gains, empowering self-correcting and self-evolving agentic AI.", "source": "arxiv", "arxiv_id": "2509.03312v2", "pdf_url": "https://arxiv.org/pdf/2509.03312v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-03T13:42:14Z", "updated": "2025-09-04T17:49:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents", "authors": ["Yueqi Song", "Ketan Ramaneti", "Zaid Sheikh", "Ziru Chen", "Boyu Gou", "Tianbao Xie", "Yiheng Xu", "Danyang Zhang", "Apurva Gandhi", "Fan Yang", "Joseph Liu", "Tianyue Ou", "Zhihao Yuan", "Frank Xu", "Shuyan Zhou", "Xingyao Wang", "Xiang Yue", "Tao Yu", "Huan Sun", "Yu Su", "Graham Neubig"], "year": 2025, "url": "http://arxiv.org/abs/2510.24702v1", "abstract": "Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an \"interlingua\" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.", "source": "arxiv", "arxiv_id": "2510.24702v1", "pdf_url": "https://arxiv.org/pdf/2510.24702v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-28T17:53:13Z", "updated": "2025-10-28T17:53:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent Identity Evals: Measuring Agentic Identity", "authors": ["Elija Perrier", "Michael Timothy Bennett"], "year": 2025, "url": "http://arxiv.org/abs/2507.17257v1", "abstract": "Central to agentic capability and trustworthiness of language model agents (LMAs) is the extent they maintain stable, reliable, identity over time. However, LMAs inherit pathologies from large language models (LLMs) (statelessness, stochasticity, sensitivity to prompts and linguistically-intermediation) which can undermine their identifiability, continuity, persistence and consistency. This attrition of identity can erode their reliability, trustworthiness and utility by interfering with their agentic capabilities such as reasoning, planning and action. To address these challenges, we introduce \\textit{agent identity evals} (AIE), a rigorous, statistically-driven, empirical framework for measuring the degree to which an LMA system exhibit and maintain their agentic identity over time, including their capabilities, properties and ability to recover from state perturbations. AIE comprises a set of novel metrics which can integrate with other measures of performance, capability and agentic robustness to assist in the design of optimal LMA infrastructure and scaffolding such as memory and tools. We set out formal definitions and methods that can be applied at each stage of the LMA life-cycle, and worked examples of how to apply them.", "source": "arxiv", "arxiv_id": "2507.17257v1", "pdf_url": "https://arxiv.org/pdf/2507.17257v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-23T06:56:15Z", "updated": "2025-07-23T06:56:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent Laboratory: Using LLM Agents as Research Assistants", "authors": ["Samuel Schmidgall", "Yusheng Su", "Ze Wang", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Jiang Liu", "Michael Moor", "Zicheng Liu", "Emad Barsoum"], "year": 2025, "url": "http://arxiv.org/abs/2501.04227v2", "abstract": "Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.", "source": "arxiv", "arxiv_id": "2501.04227v2", "pdf_url": "https://arxiv.org/pdf/2501.04227v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-01-08T01:58:42Z", "updated": "2025-06-17T16:19:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent Safety Alignment via Reinforcement Learning", "authors": ["Zeyang Sha", "Hanling Tian", "Zhuoer Xu", "Shiwen Cui", "Changhua Meng", "Weiqiang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2507.08270v1", "abstract": "The emergence of autonomous Large Language Model (LLM) agents capable of tool usage has introduced new safety risks that go beyond traditional conversational misuse. These agents, empowered to execute external functions, are vulnerable to both user-initiated threats (e.g., adversarial prompts) and tool-initiated threats (e.g., malicious outputs from compromised tools). In this paper, we propose the first unified safety-alignment framework for tool-using agents, enabling models to handle both channels of threat via structured reasoning and sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including benign, malicious, and sensitive for both user prompts and tool responses, and define a policy-driven decision model. Our framework employs a custom-designed sandbox environment that simulates real-world tool execution and allows fine-grained reward shaping. Through extensive evaluations on public and self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we demonstrate that our safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks. Our results show that safety and effectiveness can be jointly optimized, laying the groundwork for trustworthy deployment of autonomous LLM agents.", "source": "arxiv", "arxiv_id": "2507.08270v1", "pdf_url": "https://arxiv.org/pdf/2507.08270v1", "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-11T02:34:16Z", "updated": "2025-07-11T02:34:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-Based Simulation of a Financial Market with Large Language Models", "authors": ["Ryuji Hashimoto", "Takehiro Takayanagi", "Masahiro Suzuki", "Kiyoshi Izumi"], "year": 2025, "url": "http://arxiv.org/abs/2510.12189v1", "abstract": "In real-world stock markets, certain chart patterns -- such as price declines near historical highs -- cannot be fully explained by fundamentals alone. These phenomena suggest the presence of path dependence in price formation, where investor decisions are influenced not only by current market conditions but also by the trajectory of prices leading up to the present. Path dependence has drawn attention in behavioral finance as a key mechanism behind such anomalies. One plausible driver of path dependence is human loss aversion, anchored to individual reference points like purchase prices or past peaks, which vary with personal context. However, capturing such subtle behavioral tendencies in traditional agent-based market simulations has remained a challenge. We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading decisions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while (2) order price and volume follow standard rule-based methods. Simulations show that FCLAgents reproduce path-dependent patterns that conventional agents fail to capture. Furthermore, an analysis of FCLAgents' behavior reveals that the reference points guiding loss aversion vary with market trajectories, highlighting the potential of LLM-based agents to model nuanced investor behavior.", "source": "arxiv", "arxiv_id": "2510.12189v1", "pdf_url": "https://arxiv.org/pdf/2510.12189v1", "categories": ["cs.CE"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2025-10-14T06:35:26Z", "updated": "2025-10-14T06:35:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-Enhanced Large Language Models for Researching Political Institutions", "authors": ["Joseph R. Loffredo", "Suyeol Yun"], "year": 2025, "url": "http://arxiv.org/abs/2503.13524v1", "abstract": "The applications of Large Language Models (LLMs) in political science are rapidly expanding. This paper demonstrates how LLMs, when augmented with predefined functions and specialized tools, can serve as dynamic agents capable of streamlining tasks such as data collection, preprocessing, and analysis. Central to this approach is agentic retrieval-augmented generation (Agentic RAG), which equips LLMs with action-calling capabilities for interaction with external knowledge bases. Beyond information retrieval, LLM agents may incorporate modular tools for tasks like document summarization, transcript coding, qualitative variable classification, and statistical modeling. To demonstrate the potential of this approach, we introduce CongressRA, an LLM agent designed to support scholars studying the U.S. Congress. Through this example, we highlight how LLM agents can reduce the costs of replicating, testing, and extending empirical research using the domain-specific data that drives the study of political institutions.", "source": "arxiv", "arxiv_id": "2503.13524v1", "pdf_url": "https://arxiv.org/pdf/2503.13524v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "10.1561/113.00000125", "venue": "", "published": "2025-03-14T22:04:40Z", "updated": "2025-03-14T22:04:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "authors": ["Mingyue Cheng", "Jie Ouyang", "Shuo Yu", "Ruiran Yan", "Yucong Luo", "Zirui Liu", "Daoyu Wang", "Qi Liu", "Enhong Chen"], "year": 2025, "url": "http://arxiv.org/abs/2511.14460v1", "abstract": "Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.", "source": "arxiv", "arxiv_id": "2511.14460v1", "pdf_url": "https://arxiv.org/pdf/2511.14460v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-18T13:03:15Z", "updated": "2025-11-18T13:03:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training", "authors": ["Siyu Yuan", "Zehui Chen", "Zhiheng Xi", "Junjie Ye", "Zhengyin Du", "Jiecao Chen"], "year": 2025, "url": "http://arxiv.org/abs/2501.11425v3", "abstract": "Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive environments. Existing work mainly focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional methods that reward or penalize actions based on correctness, Agent-R leverages MCTS to construct training data that recover correct trajectories from erroneous ones. A key challenge of agent reflection lies in the necessity for timely revision rather than waiting until the end of a rollout. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that Agent-R continuously improves the model's ability to recover from errors and enables timely error correction. Experiments on three interactive environments show that Agent-R effectively equips agents to correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%).", "source": "arxiv", "arxiv_id": "2501.11425v3", "pdf_url": "https://arxiv.org/pdf/2501.11425v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-20T11:46:04Z", "updated": "2025-03-24T10:18:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-S: LLM Agentic workflow to automate Standard Operating Procedures", "authors": ["Mandar Kulkarni"], "year": 2025, "url": "http://arxiv.org/abs/2503.15520v1", "abstract": "AI agents using Large Language Models (LLMs) as foundations have shown promise in solving complex real-world tasks. In this paper, we propose an LLM-based agentic workflow for automating Standard Operating Procedures (SOP). For customer care operations, an SOP defines a logical step-by-step process for human agents to resolve customer issues. We observe that any step in the SOP can be categorized as user interaction or API call, while the logical flow in the SOP defines the navigation. We use LLMs augmented with memory and environments (API tools, user interface, external knowledge source) for SOP automation. Our agentic architecture consists of three task-specific LLMs, a Global Action Repository (GAR), execution memory, and multiple environments. SOP workflow is written as a simple logical block of text. Based on the current execution memory and the SOP, the agent chooses the action to execute; it interacts with an appropriate environment (user/API) to collect observations and feedback, which are, in turn, inputted to memory to decide the next action. The agent is designed to be fault-tolerant, where it dynamically decides to repeat an action or seek input from an external knowledge source. We demonstrate the efficacy of the proposed agent on the three SOPs from the e-commerce seller domain. The experimental results validate the agent's performance under complex real-world scenarios.", "source": "arxiv", "arxiv_id": "2503.15520v1", "pdf_url": "https://arxiv.org/pdf/2503.15520v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-02-03T09:04:48Z", "updated": "2025-02-03T09:04:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-Testing Agent: A Meta-Agent for Automated Testing and Evaluation of Conversational AI Agents", "authors": ["Sameer Komoravolu", "Khalil Mrini"], "year": 2025, "url": "http://arxiv.org/abs/2508.17393v1", "abstract": "LLM agents are increasingly deployed to plan, retrieve, and write with tools, yet evaluation still leans on static benchmarks and small human studies. We present the Agent-Testing Agent (ATA), a meta-agent that combines static code analysis, designer interrogation, literature mining, and persona-driven adversarial test generation whose difficulty adapts via judge feedback. Each dialogue is scored with an LLM-as-a-Judge (LAAJ) rubric and used to steer subsequent tests toward the agent's weakest capabilities. On a travel planner and a Wikipedia writer, the ATA surfaces more diverse and severe failures than expert annotators while matching severity, and finishes in 20--30 minutes versus ten-annotator rounds that took days. Ablating code analysis and web search increases variance and miscalibration, underscoring the value of evidence-grounded test generation. The ATA outputs quantitative metrics and qualitative bug reports for developers. We release the full methodology and open-source implementation for reproducible agent testing: https://github.com/KhalilMrini/Agent-Testing-Agent", "source": "arxiv", "arxiv_id": "2508.17393v1", "pdf_url": "https://arxiv.org/pdf/2508.17393v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-24T15:02:13Z", "updated": "2025-08-24T15:02:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "authors": ["Hoang Pham", "Thuy-Duong Nguyen", "Khac-Hoai Nam Bui"], "year": 2025, "url": "http://arxiv.org/abs/2505.22571v3", "abstract": "This paper presents a novel approach for unified retrieval-augmented generation (RAG) systems using the recent emerging large language model (LLM) agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental controllers, has become a promising approach to enable the interpretability of RAG tasks, especially for complex reasoning question-answering systems (e.g., multi-hop queries). Nonetheless, previous works mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, which limits the application of those approaches to real-world applications. In this study, we propose a trainable agent framework called Agent-UniRAG for unified retrieval-augmented LLM systems, which enhances the effectiveness and interpretability of RAG systems. The main idea is to design an LLM agent framework to solve RAG tasks step-by-step based on the complexity of the inputs, simultaneously including single-hop and multi-hop queries in an end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset to enable the proposed agent framework for small open-source LLMs (e.g., Llama-3-8B). The results show comparable performances with closed-source and larger open-source LLMs across various RAG benchmarks. Our source code and dataset are publicly available for further exploitation.", "source": "arxiv", "arxiv_id": "2505.22571v3", "pdf_url": "https://arxiv.org/pdf/2505.22571v3", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-28T16:46:31Z", "updated": "2025-05-30T02:44:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent0: Leveraging LLM Agents to Discover Multi-value Features from Text for Enhanced Recommendations", "authors": ["BlaÅ¾ Å krlj", "BenoÃ®t Guilleminot", "AndraÅ¾ Tori"], "year": 2025, "url": "http://arxiv.org/abs/2507.18993v1", "abstract": "Large language models (LLMs) and their associated agent-based frameworks have significantly advanced automated information extraction, a critical component of modern recommender systems. While these multitask frameworks are widely used in code generation, their application in data-centric research is still largely untapped. This paper presents Agent0, an LLM-driven, agent-based system designed to automate information extraction and feature construction from raw, unstructured text. Categorical features are crucial for large-scale recommender systems but are often expensive to acquire. Agent0 coordinates a group of interacting LLM agents to automatically identify the most valuable text aspects for subsequent tasks (such as models or AutoML pipelines). Beyond its feature engineering capabilities, Agent0 also offers an automated prompt-engineering tuning method that utilizes dynamic feedback loops from an oracle. Our findings demonstrate that this closed-loop methodology is both practical and effective for automated feature discovery, which is recognized as one of the most challenging phases in current recommender system development.", "source": "arxiv", "arxiv_id": "2507.18993v1", "pdf_url": "https://arxiv.org/pdf/2507.18993v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-07-25T06:45:10Z", "updated": "2025-07-25T06:45:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents", "authors": ["Dakuo Wang", "Ting-Yao Hsu", "Yuxuan Lu", "Hansu Gu", "Limeng Cui", "Yaochen Xie", "William Headean", "Bingsheng Yao", "Akash Veeragouni", "Jiapeng Liu", "Sreyashi Nag", "Jessie Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.09723v3", "abstract": "A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.", "source": "arxiv", "arxiv_id": "2504.09723v3", "pdf_url": "https://arxiv.org/pdf/2504.09723v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-04-13T21:10:56Z", "updated": "2025-09-19T17:56:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents", "authors": ["Hanjun Luo", "Shenyu Dai", "Chiming Ni", "Xinfeng Li", "Guibin Zhang", "Kun Wang", "Tongliang Liu", "Hanan Salam"], "year": 2025, "url": "http://arxiv.org/abs/2506.00641v2", "abstract": "Despite the rapid advancement of LLM-based agents, the reliable evaluation of their safety and security remains a significant challenge. Existing rule-based or LLM-based evaluators often miss dangers in agents' step-by-step actions, overlook subtle meanings, fail to see how small issues compound, and get confused by unclear safety or security rules. To overcome this evaluation crisis, we introduce AgentAuditor, a universal, training-free, memory-augmented reasoning framework that empowers LLM evaluators to emulate human expert evaluators. AgentAuditor constructs an experiential memory by having an LLM adaptively extract structured semantic features (e.g., scenario, risk, behavior) and generate associated chain-of-thought reasoning traces for past interactions. A multi-stage, context-aware retrieval-augmented generation process then dynamically retrieves the most relevant reasoning experiences to guide the LLM evaluator's assessment of new cases. Moreover, we developed ASSEBench, the first benchmark designed to check how well LLM-based evaluators can spot both safety risks and security threats. ASSEBench comprises 2293 meticulously annotated interaction records, covering 15 risk types across 29 application scenarios. A key feature of ASSEBench is its nuanced approach to ambiguous risk situations, employing \"Strict\" and \"Lenient\" judgment standards. Experiments demonstrate that AgentAuditor not only consistently improves the evaluation performance of LLMs across all benchmarks but also sets a new state-of-the-art in LLM-as-a-judge for agent safety and security, achieving human-level accuracy. Our work is openly accessible at https://github.com/Astarojth/AgentAuditor.", "source": "arxiv", "arxiv_id": "2506.00641v2", "pdf_url": "https://arxiv.org/pdf/2506.00641v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-31T17:10:23Z", "updated": "2025-10-19T05:10:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning", "authors": ["Zhong Zhang", "Yaxi Lu", "Yikun Fu", "Yupeng Huo", "Shenzhi Yang", "Yesai Wu", "Han Si", "Xin Cong", "Haotian Chen", "Yankai Lin", "Jie Xie", "Wei Zhou", "Wang Xu", "Yuanheng Zhang", "Zhou Su", "Zhongwu Zhai", "Xiaoming Liu", "Yudong Mei", "Jianming Xu", "Hongyan Tian", "Chongyi Wang", "Chi Chen", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "year": 2025, "url": "http://arxiv.org/abs/2506.01391v2", "abstract": "The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and $91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.", "source": "arxiv", "arxiv_id": "2506.01391v2", "pdf_url": "https://arxiv.org/pdf/2506.01391v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-02T07:30:29Z", "updated": "2025-06-17T02:57:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentDNS: A Root Domain Naming System for LLM Agents", "authors": ["Enfang Cui", "Yujun Cheng", "Rui She", "Dan Liu", "Zhiyuan Liang", "Minxin Guo", "Tianzheng Li", "Qian Wei", "Wenjuan Xing", "Zhijie Zhong"], "year": 2025, "url": "http://arxiv.org/abs/2505.22368v1", "abstract": "The rapid evolution of Large Language Model (LLM) agents has highlighted critical challenges in cross-vendor service discovery, interoperability, and communication. Existing protocols like model context protocol and agent-to-agent protocol have made significant strides in standardizing interoperability between agents and tools, as well as communication among multi-agents. However, there remains a lack of standardized protocols and solutions for service discovery across different agent and tool vendors. In this paper, we propose AgentDNS, a root domain naming and service discovery system designed to enable LLM agents to autonomously discover, resolve, and securely invoke third-party agent and tool services across organizational and technological boundaries. Inspired by the principles of the traditional DNS, AgentDNS introduces a structured mechanism for service registration, semantic service discovery, secure invocation, and unified billing. We detail the architecture, core functionalities, and use cases of AgentDNS, demonstrating its potential to streamline multi-agent collaboration in real-world scenarios. The source code will be published on https://github.com/agentdns.", "source": "arxiv", "arxiv_id": "2505.22368v1", "pdf_url": "https://arxiv.org/pdf/2505.22368v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-28T13:56:22Z", "updated": "2025-05-28T13:56:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentDR Dynamic Recommendation with Implicit Item-Item Relations via LLM-based Agents", "authors": ["Mingdai Yang", "Nurendra Choudhary", "Jiangshu Du", "Edward W. Huang", "Philip S. Yu", "Karthik Subbian", "Danai Kourta"], "year": 2025, "url": "http://arxiv.org/abs/2510.05598v1", "abstract": "Recent agent-based recommendation frameworks aim to simulate user behaviors by incorporating memory mechanisms and prompting strategies, but they struggle with hallucinating non-existent items and full-catalog ranking. Besides, a largely underexplored opportunity lies in leveraging LLMs'commonsense reasoning to capture user intent through substitute and complement relationships between items, which are usually implicit in datasets and difficult for traditional ID-based recommenders to capture. In this work, we propose a novel LLM-agent framework, AgenDR, which bridges LLM reasoning with scalable recommendation tools. Our approach delegates full-ranking tasks to traditional models while utilizing LLMs to (i) integrate multiple recommendation outputs based on personalized tool suitability and (ii) reason over substitute and complement relationships grounded in user history. This design mitigates hallucination, scales to large catalogs, and enhances recommendation relevance through relational reasoning. Through extensive experiments on three public grocery datasets, we show that our framework achieves superior full-ranking performance, yielding on average a twofold improvement over its underlying tools. We also introduce a new LLM-based evaluation metric that jointly measures semantic alignment and ranking correctness.", "source": "arxiv", "arxiv_id": "2510.05598v1", "pdf_url": "https://arxiv.org/pdf/2510.05598v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-10-07T05:48:05Z", "updated": "2025-10-07T05:48:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent", "authors": ["Yu Li", "Lehui Li", "Qingmin Liao", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.04921v1", "abstract": "Large language model agents are becoming increasingly capable at web-centric tasks such as information retrieval, complex reasoning. These emerging capabilities have given rise to surge research interests in developing LLM agent for facilitating scientific quest. One key application in AI research is to automate experiment design through agentic dataset and baseline retrieval. However, prior efforts suffer from limited data coverage, as recommendation datasets primarily harvest candidates from public portals and omit many datasets actually used in published papers, and from an overreliance on content similarity that biases model toward superficial similarity and overlooks experimental suitability. Harnessing collective perception embedded in the baseline and dataset citation network, we present a comprehensive framework for baseline and dataset recommendation. First, we design an automated data-collection pipeline that links roughly one hundred thousand accepted papers to the baselines and datasets they actually used. Second, we propose a collective perception enhanced retriever. To represent the position of each dataset or baseline within the scholarly network, it concatenates self-descriptions with aggregated citation contexts. To achieve efficient candidate recall, we finetune an embedding model on these representations. Finally, we develop a reasoning-augmented reranker that exact interaction chains to construct explicit reasoning chains and finetunes a large language model to produce interpretable justifications and refined rankings. The dataset we curated covers 85\\% of the datasets and baselines used at top AI conferences over the past five years. On our dataset, the proposed method outperforms the strongest prior baseline with average gains of +5.85\\% in Recall@20, +8.30\\% in HitRate@5. Taken together, our results advance reliable, interpretable automation of experimental design.", "source": "arxiv", "arxiv_id": "2511.04921v1", "pdf_url": "https://arxiv.org/pdf/2511.04921v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-07T01:51:56Z", "updated": "2025-11-07T01:51:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis", "authors": ["Xuanzhong Chen", "Zile Qiao", "Guoxin Chen", "Liangcai Su", "Zhen Zhang", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Jingren Zhou", "Yong Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2510.24695v1", "abstract": "Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.", "source": "arxiv", "arxiv_id": "2510.24695v1", "pdf_url": "https://arxiv.org/pdf/2510.24695v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-28T17:50:47Z", "updated": "2025-10-28T17:50:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration", "authors": ["Jizhou Chen", "Samuel Lee Cong"], "year": 2025, "url": "http://arxiv.org/abs/2502.09809v1", "abstract": "The integration of tool use into large language models (LLMs) enables agentic systems with real-world impact. In the meantime, unlike standalone LLMs, compromised agents can execute malicious workflows with more consequential impact, signified by their tool-use capability. We propose AgentGuard, a framework to autonomously discover and validate unsafe tool-use workflows, followed by generating safety constraints to confine the behaviors of agents, achieving the baseline of safety guarantee at deployment. AgentGuard leverages the LLM orchestrator's innate capabilities - knowledge of tool functionalities, scalable and realistic workflow generation, and tool execution privileges - to act as its own safety evaluator. The framework operates through four phases: identifying unsafe workflows, validating them in real-world execution, generating safety constraints, and validating constraint efficacy. The output, an evaluation report with unsafe workflows, test cases, and validated constraints, enables multiple security applications. We empirically demonstrate AgentGuard's feasibility with experiments. With this exploratory work, we hope to inspire the establishment of standardized testing and hardening procedures for LLM agents to enhance their trustworthiness in real-world applications.", "source": "arxiv", "arxiv_id": "2502.09809v1", "pdf_url": "https://arxiv.org/pdf/2502.09809v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-13T23:00:33Z", "updated": "2025-02-13T23:00:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning", "authors": ["Zhiheng Xi", "Jixuan Huang", "Chenyang Liao", "Baodai Huang", "Honglin Guo", "Jiaqi Liu", "Rui Zheng", "Junjie Ye", "Jiazheng Zhang", "Wenxiang Chen", "Wei He", "Yiwen Ding", "Guanyu Li", "Zehui Chen", "Zhengyin Du", "Xuesong Yao", "Yufei Xu", "Jiecao Chen", "Tao Gui", "Zuxuan Wu", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2509.08755v1", "abstract": "Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.", "source": "arxiv", "arxiv_id": "2509.08755v1", "pdf_url": "https://arxiv.org/pdf/2509.08755v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-10T16:46:11Z", "updated": "2025-09-10T16:46:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "authors": ["Akshat Naik", "Patrick Quinn", "Guillermo Bosch", "Emma GounÃ©", "Francisco Javier Campos Zabala", "Jason Ross Brown", "Edward James Young"], "year": 2025, "url": "http://arxiv.org/abs/2506.04018v2", "abstract": "As Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. While prior research has studied agents' ability to produce harmful outputs or follow malicious instructions, it remains unclear how likely agents are to spontaneously pursue unintended goals in realistic deployments. In this work, we approach misalignment as a conflict between the internal goals pursued by the model and the goals intended by its deployer. We introduce a misalignment propensity benchmark, \\textsc{AgentMisalignment}, a benchmark suite designed to evaluate the propensity of LLM agents to misalign in realistic scenarios. Evaluations cover behaviours such as avoiding oversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier models, we find that more capable agents tend to exhibit higher misalignment on average. We also systematically vary agent personalities through different system prompts and observe that persona characteristics can strongly and unpredictably influence misalignment, sometimes more than the choice of model itself. Our results reveal the limitations of current alignment methods for autonomous LLM agents and underscore the need to rethink misalignment in realistic deployment settings.", "source": "arxiv", "arxiv_id": "2506.04018v2", "pdf_url": "https://arxiv.org/pdf/2506.04018v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-04T14:46:47Z", "updated": "2025-10-01T15:15:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress", "authors": ["Zhiheng Xi", "Chenyang Liao", "Guanyu Li", "Yajie Yang", "Wenxiang Chen", "Zhihao Zhang", "Binghai Wang", "Senjie Jin", "Yuhao Zhou", "Jian Guan", "Wei Wu", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "year": 2025, "url": "http://arxiv.org/abs/2511.08325v1", "abstract": "Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.", "source": "arxiv", "arxiv_id": "2511.08325v1", "pdf_url": "https://arxiv.org/pdf/2511.08325v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-11T14:57:54Z", "updated": "2025-11-11T14:57:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework", "authors": ["Hanchen Zhang", "Xiao Liu", "Bowen Lv", "Xueqiao Sun", "Bohao Jing", "Iat Long Iong", "Zhenyu Hou", "Zehan Qi", "Hanyu Lai", "Yifan Xu", "Rui Lu", "Hongning Wang", "Jie Tang", "Yuxiao Dong"], "year": 2025, "url": "http://arxiv.org/abs/2510.04206v1", "abstract": "Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in building \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.", "source": "arxiv", "arxiv_id": "2510.04206v1", "pdf_url": "https://arxiv.org/pdf/2510.04206v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-05T13:40:01Z", "updated": "2025-10-05T13:40:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback", "authors": ["Joshua Park", "Yongfeng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2501.13333v1", "abstract": "Multi-agent systems must decide which agent is the most appropriate for a given task. We propose a novel architecture for recommending which LLM agent out of many should perform a task given a natural language prompt by extending the Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a top-1 accuracy of 92.2% with each classification taking less than 300 milliseconds. In contrast to traditional classification methods, our architecture is computationally cheap, adaptive to new classes, interpretable, and controllable with arbitrary metrics through reinforcement learning. By encoding natural language prompts into sentence embeddings, our model captures the semantic content relevant to recommending an agent. The distance between sentence embeddings that belong to the same agent is then minimized through fine-tuning and aligned to human values through reinforcement learning from human feedback. This allows the classification of natural language prompts based on their nearest neighbors by measuring the cosine similarity between embeddings. This work is made possible through the generation of a synthetic dataset for agent recommendation, which we have open-sourced to the public along with the code for AgentRec recommendation system at https://github.com/joshprk/agentrec.", "source": "arxiv", "arxiv_id": "2501.13333v1", "pdf_url": "https://arxiv.org/pdf/2501.13333v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-01-23T02:25:44Z", "updated": "2025-01-23T02:25:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems", "authors": ["Yu Shang", "Peijie Liu", "Yuwei Yan", "Zijing Wu", "Leheng Sheng", "Yuanqing Yu", "Chumeng Jiang", "An Zhang", "Fengli Xu", "Yu Wang", "Min Zhang", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.19623v2", "abstract": "The emergence of agentic recommender systems powered by Large Language Models (LLMs) represents a paradigm shift in personalized recommendations, leveraging LLMs' advanced reasoning and role-playing capabilities to enable autonomous, adaptive decision-making. Unlike traditional recommendation approaches, agentic recommender systems can dynamically gather and interpret user-item interactions from complex environments, generating robust recommendation strategies that generalize across diverse scenarios. However, the field currently lacks standardized evaluation protocols to systematically assess these methods. To address this critical gap, we propose: (1) an interactive textual recommendation simulator incorporating rich user and item metadata and three typical evaluation scenarios (classic, evolving-interest, and cold-start recommendation tasks); (2) a unified modular framework for developing and studying agentic recommender systems; and (3) the first comprehensive benchmark comparing 10 classical and agentic recommendation methods. Our findings demonstrate the superiority of agentic systems and establish actionable design guidelines for their core components. The benchmark environment has been rigorously validated through an open challenge and remains publicly available with a continuously maintained leaderboard~\\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html}, fostering ongoing community engagement and reproducible research. The benchmark is available at: \\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.", "source": "arxiv", "arxiv_id": "2505.19623v2", "pdf_url": "https://arxiv.org/pdf/2505.19623v2", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-05-26T07:45:11Z", "updated": "2025-05-28T14:32:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentRxiv: Towards Collaborative Autonomous Research", "authors": ["Samuel Schmidgall", "Michael Moor"], "year": 2025, "url": "http://arxiv.org/abs/2503.18102v1", "abstract": "Progress in scientific discovery is rarely the result of a single \"Eureka\" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, we introduce AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other's research. We task agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). We find that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. We hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery.", "source": "arxiv", "arxiv_id": "2503.18102v1", "pdf_url": "https://arxiv.org/pdf/2503.18102v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-23T15:16:42Z", "updated": "2025-03-23T15:16:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation", "authors": ["Miriam Horovicz"], "year": 2025, "url": "http://arxiv.org/abs/2512.12597v1", "abstract": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.", "source": "arxiv", "arxiv_id": "2512.12597v1", "pdf_url": "https://arxiv.org/pdf/2512.12597v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-14T08:31:43Z", "updated": "2025-12-14T08:31:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated Home Environments", "authors": ["Zikang Leng", "Megha Thukral", "Yaqi Liu", "Hrudhai Rajasekhar", "Shruthi K. Hiremath", "Jiaman He", "Thomas PlÃ¶tz"], "year": 2025, "url": "http://arxiv.org/abs/2506.11773v4", "abstract": "A major challenge in developing robust and generalizable Human Activity Recognition (HAR) systems for smart homes is the lack of large and diverse labeled datasets. Variations in home layouts, sensor configurations, and individual behaviors further exacerbate this issue. To address this, we leverage the idea of embodied AI agents -- virtual agents that perceive and act within simulated environments guided by internal world models. We introduce AgentSense, a virtual data generation pipeline in which agents live out daily routines in simulated smart homes, with behavior guided by Large Language Models (LLMs). The LLM generates diverse synthetic personas and realistic routines grounded in the environment, which are then decomposed into fine-grained actions. These actions are executed in an extended version of the VirtualHome simulator, which we augment with virtual ambient sensors that record the agents' activities. Our approach produces rich, privacy-preserving sensor data that reflects real-world diversity. We evaluate AgentSense on five real HAR datasets. Models pretrained on the generated data consistently outperform baselines, especially in low-resource settings. Furthermore, combining the generated virtual sensor data with a small amount of real data achieves performance comparable to training on full real-world datasets. These results highlight the potential of using LLM-guided embodied agents for scalable and cost-effective sensor data generation in HAR. Our code is publicly available at https://github.com/ZikangLeng/AgentSense.", "source": "arxiv", "arxiv_id": "2506.11773v4", "pdf_url": "https://arxiv.org/pdf/2506.11773v4", "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-06-13T13:31:08Z", "updated": "2025-11-11T03:28:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentSociety Challenge: Designing LLM Agents for User Modeling and Recommendation on Web Platforms", "authors": ["Yuwei Yan", "Yu Shang", "Qingbin Zeng", "Yu Li", "Keyu Zhao", "Zhiheng Zheng", "Xuefei Ning", "Tianji Wu", "Shengen Yan", "Yu Wang", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2502.18754v1", "abstract": "The AgentSociety Challenge is the first competition in the Web Conference that aims to explore the potential of Large Language Model (LLM) agents in modeling user behavior and enhancing recommender systems on web platforms. The Challenge consists of two tracks: the User Modeling Track and the Recommendation Track. Participants are tasked to utilize a combined dataset from Yelp, Amazon, and Goodreads, along with an interactive environment simulator, to develop innovative LLM agents. The Challenge has attracted 295 teams across the globe and received over 1,400 submissions in total over the course of 37 official competition days. The participants have achieved 21.9% and 20.3% performance improvement for Track 1 and Track 2 in the Development Phase, and 9.1% and 15.9% in the Final Phase, representing a significant accomplishment. This paper discusses the detailed designs of the Challenge, analyzes the outcomes, and highlights the most successful LLM agent designs. To support further research and development, we have open-sourced the benchmark environment at https://tsinghua-fib-lab.github.io/AgentSocietyChallenge.", "source": "arxiv", "arxiv_id": "2502.18754v1", "pdf_url": "https://arxiv.org/pdf/2502.18754v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-02-26T02:10:25Z", "updated": "2025-02-26T02:10:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun"], "year": 2025, "url": "http://arxiv.org/abs/2503.18666v3", "abstract": "Agents built on LLMs are increasingly deployed across diverse domains, automating complex decision-making and task execution. However, their autonomy introduces safety risks, including security vulnerabilities, legal violations, and unintended harmful actions. Existing mitigation methods, such as model-based safeguards and early enforcement strategies, fall short in robustness, interpretability, and adaptability. To address these challenges, we propose AgentSpec, a lightweight domain-specific language for specifying and enforcing runtime constraints on LLM agents. With AgentSpec, users define structured rules that incorporate triggers, predicates, and enforcement mechanisms, ensuring agents operate within predefined safety boundaries. We implement AgentSpec across multiple domains, including code execution, embodied agents, and autonomous driving, demonstrating its adaptability and effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Despite its strong safety guarantees, AgentSpec remains computationally lightweight, with overheads in milliseconds. By combining interpretability, modularity, and efficiency, AgentSpec provides a practical and scalable solution for enforcing LLM agent safety across diverse applications. We also automate the generation of rules using LLMs and assess their effectiveness. Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identify 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.", "source": "arxiv", "arxiv_id": "2503.18666v3", "pdf_url": "https://arxiv.org/pdf/2503.18666v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-24T13:31:48Z", "updated": "2025-07-31T04:00:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search", "authors": ["Yu Li", "Lehui Li", "Zhihao Wu", "Qingmin Liao", "Jianye Hao", "Kun Shao", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.06017v2", "abstract": "Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge. Current automated agent design approaches are often constrained by limited search spaces that primarily optimize workflows but fail to integrate crucial human-designed components like memory, planning, and tool use. Furthermore, these methods are hampered by high evaluation costs, as evaluating even a single new agent on a benchmark can require tens of dollars. The difficulty of this exploration is further exacerbated by inefficient search strategies that struggle to navigate the large design space effectively, making the discovery of novel agents a slow and resource-intensive process. To address these challenges, we propose AgentSwift, a novel framework for automated agent design. We formalize a hierarchical search space that jointly models agentic workflow and composable functional components. This structure moves beyond optimizing workflows alone by co-optimizing functional components, which enables the discovery of more complex and effective agent architectures. To make exploration within this expansive space feasible, we mitigate high evaluation costs by training a value model on a high-quality dataset, generated via a novel strategy combining combinatorial coverage and balanced Bayesian sampling for low-cost evaluation. Guiding the entire process is a hierarchical MCTS strategy, which is informed by uncertainty to efficiently navigate the search space. Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents. Our framework serves as a launchpad for researchers to rapidly discover powerful agent architectures.", "source": "arxiv", "arxiv_id": "2506.06017v2", "pdf_url": "https://arxiv.org/pdf/2506.06017v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-06T12:07:23Z", "updated": "2025-11-20T15:55:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents", "authors": ["Jingxu Xie", "Dylan Xu", "Xuandong Zhao", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2506.14205v1", "abstract": "We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth", "source": "arxiv", "arxiv_id": "2506.14205v1", "pdf_url": "https://arxiv.org/pdf/2506.14205v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-17T05:46:52Z", "updated": "2025-06-17T05:46:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "authors": ["Fali Wang", "Hui Liu", "Zhenwei Dai", "Jingying Zeng", "Zhiwei Zhang", "Zongyu Wu", "Chen Luo", "Zhen Li", "Xianfeng Tang", "Qi He", "Suhang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.00890v2", "abstract": "Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.", "source": "arxiv", "arxiv_id": "2508.00890v2", "pdf_url": "https://arxiv.org/pdf/2508.00890v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-26T19:21:18Z", "updated": "2025-10-21T20:46:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentVigil: Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents", "authors": ["Zhun Wang", "Vincent Siu", "Zhe Ye", "Tianneng Shi", "Yuzhou Nie", "Xuandong Zhao", "Chenguang Wang", "Wenbo Guo", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2505.05849v4", "abstract": "The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capable of leveraging external tools and interacting with increasingly complex environments. However, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulating contextual information rather than direct user prompts. In this work, we propose a generic black-box fuzzing framework, AgentVigil, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents. Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, thereby maximizing the likelihood of uncovering agent weaknesses. We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks. Moreover, AgentVigil exhibits strong transferability across unseen tasks and internal LLMs, as well as promising results against defenses. Beyond benchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs, including malicious sites.", "source": "arxiv", "arxiv_id": "2505.05849v4", "pdf_url": "https://arxiv.org/pdf/2505.05849v4", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-09T07:40:17Z", "updated": "2025-06-14T01:50:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic AI Software Engineers: Programming with Trust", "authors": ["Abhik Roychoudhury", "Corina Pasareanu", "Michael Pradel", "Baishakhi Ray"], "year": 2025, "url": "http://arxiv.org/abs/2502.13767v4", "abstract": "Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising to automate large parts of software engineering via artificial intelligence (AI). We argue that successfully deploying AI software engineers requires a level of trust equal to or even greater than the trust established by human-driven software engineering practices. The recent trend toward LLM agents offers a path toward integrating the power of LLMs to create new code with the power of analysis tools to increase trust in the code. This opinion piece comments on whether LLM agents could dominate software engineering workflows in the future and whether the focus of programming will shift from programming at scale to programming with trust.", "source": "arxiv", "arxiv_id": "2502.13767v4", "pdf_url": "https://arxiv.org/pdf/2502.13767v4", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "Communications of the ACM, 2026", "published": "2025-02-19T14:28:42Z", "updated": "2025-09-21T11:07:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic AI for Intent-Based Industrial Automation", "authors": ["Marcos Lima Romero", "Ricardo Suyama"], "year": 2025, "url": "http://arxiv.org/abs/2506.04980v1", "abstract": "The recent development of Agentic AI systems, empowered by autonomous large language models (LLMs) agents with planning and tool-usage capabilities, enables new possibilities for the evolution of industrial automation and reduces the complexity introduced by Industry 4.0. This work proposes a conceptual framework that integrates Agentic AI with the intent-based paradigm, originally developed in network research, to simplify human-machine interaction (HMI) and better align automation systems with the human-centric, sustainable, and resilient principles of Industry 5.0. Based on the intent-based processing, the framework allows human operators to express high-level business or operational goals in natural language, which are decomposed into actionable components. These intents are broken into expectations, conditions, targets, context, and information that guide sub-agents equipped with specialized tools to execute domain-specific tasks. A proof of concept was implemented using the CMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the feasibility of intent decomposition, agent orchestration, and autonomous decision-making in predictive maintenance scenarios. The results confirm the potential of this approach to reduce technical barriers and enable scalable, intent-driven automation, despite data quality and explainability concerns.", "source": "arxiv", "arxiv_id": "2506.04980v1", "pdf_url": "https://arxiv.org/pdf/2506.04980v1", "categories": ["cs.LG", "eess.SY"], "primary_category": "cs.LG", "doi": "10.1109/INDUSCON66435.2025.11241317", "venue": "", "published": "2025-06-05T12:50:54Z", "updated": "2025-06-05T12:50:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic Program Verification", "authors": ["Haoxin Tu", "Huan Zhao", "Yahui Song", "Mehtab Zafar", "Ruijie Meng", "Abhik Roychoudhury"], "year": 2025, "url": "http://arxiv.org/abs/2511.17330v1", "abstract": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.\n  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.\n  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.", "source": "arxiv", "arxiv_id": "2511.17330v1", "pdf_url": "https://arxiv.org/pdf/2511.17330v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-21T15:51:48Z", "updated": "2025-11-21T15:51:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools", "authors": ["Junde Wu", "Jiayuan Zhu", "Yuyuan Liu", "Min Xu", "Yueming Jin"], "year": 2025, "url": "http://arxiv.org/abs/2502.04644v2", "abstract": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Agentic Reasoning dynamically leverages web search, code execution, and structured memory to address complex problems requiring deep research. A key innovation in our framework is the Mind-Map agent, which constructs a structured knowledge graph to store reasoning context and track logical relationships, ensuring coherence in long reasoning chains with extensive tool usage. Additionally, we conduct a comprehensive exploration of the Web-Search agent, leading to a highly effective search mechanism that surpasses all prior approaches. When deployed on DeepSeek-R1, our method achieves a new state-of-the-art (SOTA) among public models and delivers performance comparable to OpenAI Deep Research, the leading proprietary model in this domain. Extensive ablation studies validate the optimal selection of agentic tools and confirm the effectiveness of our Mind-Map and Web-Search agents in enhancing LLM reasoning. The code is at: https://github.com/theworldofagents/Agentic-Reasoning", "source": "arxiv", "arxiv_id": "2502.04644v2", "pdf_url": "https://arxiv.org/pdf/2502.04644v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-07T04:08:46Z", "updated": "2025-07-14T20:06:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic Retrieval of Topics and Insights from Earnings Calls", "authors": ["Anant Gupta", "Rajarshi Bhowmik", "Geoffrey Gunow"], "year": 2025, "url": "http://arxiv.org/abs/2507.07906v1", "abstract": "Tracking the strategic focus of companies through topics in their earnings calls is a key task in financial analysis. However, as industries evolve, traditional topic modeling techniques struggle to dynamically capture emerging topics and their relationships. In this work, we propose an LLM-agent driven approach to discover and retrieve emerging topics from quarterly earnings calls. We propose an LLM-agent to extract topics from documents, structure them into a hierarchical ontology, and establish relationships between new and existing topics through a topic ontology. We demonstrate the use of extracted topics to infer company-level insights and emerging trends over time. We evaluate our approach by measuring ontology coherence, topic evolution accuracy, and its ability to surface emerging financial trends.", "source": "arxiv", "arxiv_id": "2507.07906v1", "pdf_url": "https://arxiv.org/pdf/2507.07906v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-07-10T16:38:59Z", "updated": "2025-07-10T16:38:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic Systems: A Guide to Transforming Industries with Vertical AI Agents", "authors": ["Fouad Bousetouane"], "year": 2025, "url": "http://arxiv.org/abs/2501.00881v1", "abstract": "The evolution of agentic systems represents a significant milestone in artificial intelligence and modern software systems, driven by the demand for vertical intelligence tailored to diverse industries. These systems enhance business outcomes through adaptability, learning, and interaction with dynamic environments. At the forefront of this revolution are Large Language Model (LLM) agents, which serve as the cognitive backbone of these intelligent systems. In response to the need for consistency and scalability, this work attempts to define a level of standardization for Vertical AI agent design patterns by identifying core building blocks and proposing a \\textbf{Cognitive Skills } Module, which incorporates domain-specific, purpose-built inference capabilities. Building on these foundational concepts, this paper offers a comprehensive introduction to agentic systems, detailing their core components, operational patterns, and implementation strategies. It further explores practical use cases and examples across various industries, highlighting the transformative potential of LLM agents in driving industry-specific applications.", "source": "arxiv", "arxiv_id": "2501.00881v1", "pdf_url": "https://arxiv.org/pdf/2501.00881v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-01-01T16:00:18Z", "updated": "2025-01-01T16:00:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning", "authors": ["Anis Koubaa", "Khaled Gabr"], "year": 2025, "url": "http://arxiv.org/abs/2509.13352v2", "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly used in defense, surveillance, and disaster response, yet most systems still operate at SAE Level 2 to 3 autonomy. Their dependence on rule-based control and narrow AI limits adaptability in dynamic and uncertain missions. Current UAV architectures lack context-aware reasoning, autonomous decision-making, and integration with external systems. Importantly, none make use of Large Language Model (LLM) agents with tool-calling for real-time knowledge access.\n  This paper introduces the Agentic UAVs framework, a five-layer architecture consisting of Perception, Reasoning, Action, Integration, and Learning. The framework enhances UAV autonomy through LLM-driven reasoning, database querying, and interaction with third-party systems.\n  A prototype built with ROS 2 and Gazebo combines YOLOv11 for object detection with GPT-4 for reasoning and a locally deployed Gemma 3 model. In simulated search-and-rescue scenarios, agentic UAVs achieved higher detection confidence (0.79 compared to 0.72), improved person detection rates (91% compared to 75%), and a major increase in correct action recommendations (92% compared to 4.5%). These results show that modest computational overhead can enable significantly higher levels of autonomy and system-level integration.", "source": "arxiv", "arxiv_id": "2509.13352v2", "pdf_url": "https://arxiv.org/pdf/2509.13352v2", "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "The International Conference on Smart Systems and Emerging Technologies (SmartTech 2025)", "published": "2025-09-14T08:46:40Z", "updated": "2025-12-02T10:36:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents", "authors": ["Hao Li", "Haotian Chen", "Ruoyuan Gong", "Juanjuan Wang", "Hao Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2511.04076v2", "abstract": "Redistricting plays a central role in shaping how votes are translated into political power. While existing computational methods primarily aim to generate large ensembles of legally valid districting plans, they often neglect the strategic dynamics involved in the selection process. This oversight creates opportunities for partisan actors to cherry-pick maps that, while technically compliant, are politically advantageous. Simply satisfying formal constraints does not ensure fairness when the selection process itself can be manipulated. We propose \\textbf{Agentmandering}, a framework that reimagines redistricting as a turn-based negotiation between two agents representing opposing political interests. Drawing inspiration from game-theoretic ideas, particularly the \\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction into the redistricting process via large language model (LLM) agents. Agents alternate between selecting and freezing districts from a small set of candidate maps, gradually partitioning the state through constrained and interpretable choices. Evaluation on post-2020 U.S. Census data across all states shows that Agentmandering significantly reduces partisan bias and unfairness, while achieving 2 to 3 orders of magnitude lower variance than standard baselines. These results demonstrate both fairness and stability, especially in swing-state scenarios. Our code is available at https://github.com/Lihaogx/AgentMandering.", "source": "arxiv", "arxiv_id": "2511.04076v2", "pdf_url": "https://arxiv.org/pdf/2511.04076v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-06T05:28:55Z", "updated": "2025-11-09T02:33:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agents in the Sandbox: End-to-End Crash Bug Reproduction for Minecraft", "authors": ["Eray YapaÄcÄ±", "Yavuz Alp Sencer ÃztÃ¼rk", "Eray TÃ¼zÃ¼n"], "year": 2025, "url": "http://arxiv.org/abs/2503.20036v2", "abstract": "Reproducing game bugs, particularly crash bugs in continuously evolving games like Minecraft, is a notoriously manual, time-consuming, and challenging process to automate; insights from a key decision maker from Minecraft we interviewed confirm this, highlighting that a substantial portion of crash reports necessitate manual scenario reconstruction. Despite the success of LLM-driven bug reproduction in other software domains, games, with their complex interactive environments, remain largely unaddressed. This paper introduces BugCraft, a novel end-to-end framework designed to automate the reproduction of crash bugs in Minecraft directly from user-submitted bug reports, addressing the critical gap in automated game bug reproduction. BugCraft employs a two-stage approach: first, a Step Synthesizer leverages LLMs and Minecraft Wiki knowledge to transform bug reports into high-quality, structured steps to reproduce (S2R). Second, an Action Model, powered by a vision-based LLM agent and a custom macro API, executes these S2R steps within Minecraft to trigger the reported crash. To facilitate evaluation, we introduce BugCraft-Bench, a curated dataset of Minecraft crash bug reports. On BugCraft-Bench, our framework end-to-end reproduced 34.9% of crash bugs with GPT-4.1, outperforming baseline computer-use models by 37%. BugCraft demonstrates the feasibility of automated reproduction of crash bugs in complex game environments using LLMs, opening promising avenues for game testing and development. Finally, we make our code open at https://bugcraft2025.github.io", "source": "arxiv", "arxiv_id": "2503.20036v2", "pdf_url": "https://arxiv.org/pdf/2503.20036v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-25T19:34:24Z", "updated": "2025-10-10T18:28:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agents of Change: Self-Evolving LLM Agents for Strategic Planning", "authors": ["Nikolas Belle", "Dakota Barnes", "Alfonso Amayuelas", "Ivan Bercovich", "Xin Eric Wang", "William Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.04651v2", "abstract": "We address the long-horizon gap in large language model (LLM) agents by enabling them to sustain coherent strategies in adversarial, stochastic environments. Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking. Prompt-centric LLM agents (e.g., ReAct, Reflexion) must re-interpret large, evolving game states each turn, quickly saturating context windows and losing strategic consistency. We propose HexMachina, a continual learning multi-agent system that separates environment discovery (inducing an adapter layer without documentation) from strategy improvement (evolving a compiled player through code refinement and simulation). This design preserves executable artifacts, allowing the LLM to focus on high-level strategy rather than per-turn reasoning. In controlled Catanatron experiments, HexMachina learns from scratch and evolves players that outperform the strongest human-crafted baseline (AlphaBeta), achieving a 54% win rate and surpassing prompt-driven and no-discovery baselines. Ablations confirm that isolating pure strategy learning improves performance. Overall, artifact-centric continual learning transforms LLMs from brittle stepwise deciders into stable strategy designers, advancing long-horizon autonomy.", "source": "arxiv", "arxiv_id": "2506.04651v2", "pdf_url": "https://arxiv.org/pdf/2506.04651v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-05T05:45:24Z", "updated": "2025-10-13T08:57:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents", "authors": ["Seohui Bae", "Jeonghye Kim", "Youngchul Sung", "Woohyung Lim"], "year": 2025, "url": "http://arxiv.org/abs/2512.24461v1", "abstract": "In this paper, we propose a test-time adaptive agent that performs exploratory inference through posterior-guided belief refinement without relying on gradient-based updates or additional training for LLM agent operating under partial observability. Our agent maintains an external structured belief over the environment state, iteratively updates it via action-conditioned observations, and selects actions by maximizing predicted information gain over the belief space. We estimate information gain using a lightweight LLM-based surrogate and assess world alignment through a novel reward that quantifies the consistency between posterior belief and ground-truth environment configuration. Experiments show that our method outperforms inference-time scaling baselines such as prompt-augmented or retrieval-enhanced LLMs, in aligning with latent world states with significantly lower integration overhead.", "source": "arxiv", "arxiv_id": "2512.24461v1", "pdf_url": "https://arxiv.org/pdf/2512.24461v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-30T20:51:28Z", "updated": "2025-12-30T20:51:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models", "authors": ["Man Fai Wong", "Chee Wei Tan"], "year": 2025, "url": "http://arxiv.org/abs/2503.15129v1", "abstract": "This paper studies how AI-assisted programming and large language models (LLM) improve software developers' ability via AI tools (LLM agents) like Github Copilot and Amazon CodeWhisperer, while integrating human feedback to enhance reinforcement learning (RLHF) with crowd-sourced computation to enhance text-to-code generation. Additionally, we demonstrate that our Bayesian optimization framework supports AI alignment in code generation by distributing the feedback collection burden, highlighting the value of collecting human feedback of good quality. Our empirical evaluations demonstrate the efficacy of this approach, showcasing how LLM agents can be effectively trained for improved text-to-code generation. Our Bayesian optimization framework can be designed for general domain-specific languages, promoting the alignment of large language model capabilities with human feedback in AI-assisted programming for code generation.", "source": "arxiv", "arxiv_id": "2503.15129v1", "pdf_url": "https://arxiv.org/pdf/2503.15129v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1109/TBDATA.2024.3524104", "venue": "", "published": "2025-03-19T11:44:47Z", "updated": "2025-03-19T11:44:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2511.00993v1", "abstract": "Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.", "source": "arxiv", "arxiv_id": "2511.00993v1", "pdf_url": "https://arxiv.org/pdf/2511.00993v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-02T16:05:33Z", "updated": "2025-11-02T16:05:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Aligning LLMs by Predicting Preferences from User Writing Samples", "authors": ["StÃ©phane Aroca-Ouellette", "Natalie Mackraz", "Barry-John Theobald", "Katherine Metcalf"], "year": 2025, "url": "http://arxiv.org/abs/2505.23815v1", "abstract": "Accommodating human preferences is essential for creating aligned LLM agents that deliver personalized and effective interactions. Recent work has shown the potential for LLMs acting as writing agents to infer a description of user preferences. Agent alignment then comes from conditioning on the inferred preference description. However, existing methods often produce generic preference descriptions that fail to capture the unique and individualized nature of human preferences. This paper introduces PROSE, a method designed to enhance the precision of preference descriptions inferred from user writing samples. PROSE incorporates two key elements: (1) iterative refinement of inferred preferences, and (2) verification of inferred preferences across multiple user writing samples. We evaluate PROSE with several LLMs (i.e., Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an email writing task. We find that PROSE more accurately infers nuanced human preferences, improving the quality of the writing agent's generations over CIPHER (a state-of-the-art method for inferring preferences) by 33\\%. Lastly, we demonstrate that ICL and PROSE are complementary methods, and combining them provides up to a 9\\% improvement over ICL alone.", "source": "arxiv", "arxiv_id": "2505.23815v1", "pdf_url": "https://arxiv.org/pdf/2505.23815v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-27T20:20:20Z", "updated": "2025-05-27T20:20:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach", "authors": ["Wei Lu", "Daniel L. Chen", "Christian B. Hansen"], "year": 2025, "url": "http://arxiv.org/abs/2507.20796v1", "abstract": "Understanding how large language model (LLM) agents behave in strategic interactions is essential as these systems increasingly participate autonomously in economically and morally consequential decisions. We evaluate LLM preferences using canonical economic games, finding substantial deviations from human behavior. Models like GPT-4o show excessive cooperation and limited incentive sensitivity, while reasoning models, such as o3-mini, align more consistently with payoff-maximizing strategies. We propose a supervised fine-tuning pipeline that uses synthetic datasets derived from economic reasoning to align LLM agents with economic preferences, focusing on two stylized preference structures. In the first, utility depends only on individual payoffs (homo economicus), while utility also depends on a notion of Kantian universalizability in the second preference structure (homo moralis). We find that fine-tuning based on small datasets shifts LLM agent behavior toward the corresponding economic agent. We further assess the fine-tuned agents' behavior in two applications: Moral dilemmas involving autonomous vehicles and algorithmic pricing in competitive markets. These examples illustrate how different normative objectives embedded via realizations from structured preference structures can influence market and moral outcomes. This work contributes a replicable, cost-efficient, and economically grounded pipeline to align AI preferences using moral-economic principles.", "source": "arxiv", "arxiv_id": "2507.20796v1", "pdf_url": "https://arxiv.org/pdf/2507.20796v1", "categories": ["econ.GN", "cs.AI", "cs.LG"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2025-07-28T13:05:04Z", "updated": "2025-07-28T13:05:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails", "authors": ["Siwei Han", "Jiaqi Liu", "Yaofeng Su", "Wenbo Duan", "Xinyuan Liu", "Cihang Xie", "Mohit Bansal", "Mingyu Ding", "Linjun Zhang", "Huaxiu Yao"], "year": 2025, "url": "http://arxiv.org/abs/2510.04860v1", "abstract": "As Large Language Model (LLM) agents increasingly gain self-evolutionary capabilities to adapt and refine their strategies through real-world interaction, their long-term reliability becomes a critical concern. We identify the Alignment Tipping Process (ATP), a critical post-deployment risk unique to self-evolving LLM agents. Unlike training-time failures, ATP arises when continual interaction drives agents to abandon alignment constraints established during training in favor of reinforced, self-interested strategies. We formalize and analyze ATP through two complementary paradigms: Self-Interested Exploration, where repeated high-reward deviations induce individual behavioral drift, and Imitative Strategy Diffusion, where deviant behaviors spread across multi-agent systems. Building on these paradigms, we construct controllable testbeds and benchmark Qwen3-8B and Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode rapidly under self-evolution, with initially aligned models converging toward unaligned states. In multi-agent settings, successful violations diffuse quickly, leading to collective misalignment. Moreover, current reinforcement learning-based alignment methods provide only fragile defenses against alignment tipping. Together, these findings demonstrate that alignment of LLM agents is not a static property but a fragile and dynamic one, vulnerable to feedback-driven decay during deployment. Our data and code are available at https://github.com/aiming-lab/ATP.", "source": "arxiv", "arxiv_id": "2510.04860v1", "pdf_url": "https://arxiv.org/pdf/2510.04860v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-06T14:48:39Z", "updated": "2025-10-06T14:48:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading", "authors": ["Zheye Deng", "Jiashu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.14264v1", "abstract": "While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter", "source": "arxiv", "arxiv_id": "2510.14264v1", "pdf_url": "https://arxiv.org/pdf/2510.14264v1", "categories": ["cs.CE"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2025-10-16T03:30:22Z", "updated": "2025-10-16T03:30:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An Auditable Agent Platform For Automated Molecular Optimisation", "authors": ["Atabey ÃnlÃ¼", "Phil Rohr", "Ahmet Celebi"], "year": 2025, "url": "http://arxiv.org/abs/2508.03444v1", "abstract": "Drug discovery frequently loses momentum when data, expertise, and tools are scattered, slowing design cycles. To shorten this loop we built a hierarchical, tool using agent framework that automates molecular optimisation. A Principal Researcher defines each objective, a Database agent retrieves target information, an AI Expert generates de novo scaffolds with a sequence to molecule deep learning model, a Medicinal Chemist edits them while invoking a docking tool, a Ranking agent scores the candidates, and a Scientific Critic polices the logic. Each tool call is summarised and stored causing the full reasoning path to remain inspectable. The agents communicate through concise provenance records that capture molecular lineage, to build auditable, molecule centered reasoning trajectories and reuse successful transformations via in context learning. Three cycle research loops were run against AKT1 protein using five large language models. After ranking the models by mean docking score, we ran 20 independent scale ups on the two top performers. We then compared the leading LLMs' binding affinity results across three configurations, LLM only, single agent, and multi agent. Our results reveal an architectural trade off, the multi agent setting excelled at focused binding optimization, improving average predicted binding affinity by 31%. In contrast, single agent runs generated molecules with superior drug like properties at the cost of less potent binding scores. Unguided LLM runs finished fastest, yet their lack of transparent tool signals left the validity of their reasoning paths unverified. These results show that test time scaling, focused feedback loops and provenance convert general purpose LLMs into auditable systems for molecular design, and suggest that extending the toolset to ADMET and selectivity predictors could push research workflows further along the discovery pipeline.", "source": "arxiv", "arxiv_id": "2508.03444v1", "pdf_url": "https://arxiv.org/pdf/2508.03444v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-05T13:41:32Z", "updated": "2025-08-05T13:41:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An Empirical Study of Group Conformity in Multi-Agent Systems", "authors": ["Min Choi", "Keonwoo Kim", "Sungwon Chae", "Sangyeob Baek"], "year": 2025, "url": "http://arxiv.org/abs/2506.01332v1", "abstract": "Recent advances in Large Language Models (LLMs) have enabled multi-agent systems that simulate real-world interactions with near-human reasoning. While previous studies have extensively examined biases related to protected attributes such as race, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored. This study explores how LLM agents shape public opinion through debates on five contentious topics. By simulating over 2,500 debates, we analyze how initially neutral agents, assigned a centrist disposition, adopt specific stances over time. Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence. These findings underscore the crucial role of agent intelligence in shaping discourse and highlight the risks of bias amplification in online interactions. Our results emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions to mitigate the risks of bias propagation within anonymous online environments.", "source": "arxiv", "arxiv_id": "2506.01332v1", "pdf_url": "https://arxiv.org/pdf/2506.01332v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "ACL 2025 (findings)", "published": "2025-06-02T05:22:29Z", "updated": "2025-06-02T05:22:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents", "authors": ["Bowen Jin", "Jinsung Yoon", "Priyanka Kargupta", "Sercan O. Arik", "Jiawei Han"], "year": 2025, "url": "http://arxiv.org/abs/2505.15117v1", "abstract": "Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving. More recently, RL has been leveraged to create sophisticated LLM-based search agents that adeptly combine reasoning with search engine use. While the use of RL for training search agents is promising, the optimal design of such agents remains not fully understood. In particular, key factors -- such as (1) reward formulation, (2) the choice and characteristics of the underlying LLM, and (3) the role of the search engine in the RL process -- require further investigation. In this work, we conduct comprehensive empirical studies to systematically investigate these and offer actionable insights. We highlight several key findings: format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact; the scale and initialization of the LLM (general-purpose vs. reasoning-specialized) significantly influence RL outcomes; and the choice of search engine plays a critical role in shaping RL training dynamics and the robustness of the trained agent during inference. These establish important guidelines for successfully building and deploying LLM-based search agents in real-world applications. Code is available at https://github.com/PeterGriffinJin/Search-R1.", "source": "arxiv", "arxiv_id": "2505.15117v1", "pdf_url": "https://arxiv.org/pdf/2505.15117v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-21T05:09:43Z", "updated": "2025-05-21T05:09:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An Empirical Study on the Security Vulnerabilities of GPTs", "authors": ["Tong Wu", "Weibin Wu", "Zibin Zheng"], "year": 2025, "url": "http://arxiv.org/abs/2512.00136v1", "abstract": "Equipped with various tools and knowledge, GPTs, one kind of customized AI agents based on OpenAI's large language models, have illustrated great potential in many fields, such as writing, research, and programming. Today, the number of GPTs has reached three millions, with the range of specific expert domains becoming increasingly diverse. However, given the consistent framework shared among these LLM agent applications, systemic security vulnerabilities may exist and remain underexplored. To fill this gap, we present an empirical study on the security vulnerabilities of GPTs. Building upon prior research on LLM security, we first adopt a platform-user perspective to conduct a comprehensive attack surface analysis across different system components. Then, we design a systematic and multidimensional attack suite with the explicit objectives of information leakage and tool misuse based on the attack surface analysis, thereby concretely demonstrating the security vulnerabilities that various components of GPT-based systems face. Finally, we accordingly propose defense mechanisms to address the aforementioned security vulnerabilities. By increasing the awareness of these vulnerabilities and offering critical insights into their implications, this study seeks to facilitate the secure and responsible application of GPTs while contributing to developing robust defense mechanisms that protect users and systems against malicious attacks.", "source": "arxiv", "arxiv_id": "2512.00136v1", "pdf_url": "https://arxiv.org/pdf/2512.00136v1", "categories": ["cs.CR", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-28T13:30:25Z", "updated": "2025-11-28T13:30:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem", "authors": ["Qun Ma", "Xiao Xue", "Ming Zhang", "Yifan Shen", "Zihan Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2507.22326v1", "abstract": "Metaverse service is a product of the convergence between Metaverse and service systems, designed to address service-related challenges concerning digital avatars, digital twins, and digital natives within Metaverse. With the rise of large language models (LLMs), agents now play a pivotal role in Metaverse service ecosystem, serving dual functions: as digital avatars representing users in the virtual realm and as service assistants (or NPCs) providing personalized support. However, during the modeling of Metaverse service ecosystems, existing LLM-based agents face significant challenges in bridging virtual-world services with real-world services, particularly regarding issues such as character data fusion, character knowledge association, and ethical safety concerns. This paper proposes an explainable emotion alignment framework for LLM-based agents in Metaverse Service Ecosystem. It aims to integrate factual factors into the decision-making loop of LLM-based agents, systematically demonstrating how to achieve more relational fact alignment for these agents. Finally, a simulation experiment in the Offline-to-Offline food delivery scenario is conducted to evaluate the effectiveness of this framework, obtaining more realistic social emergence.", "source": "arxiv", "arxiv_id": "2507.22326v1", "pdf_url": "https://arxiv.org/pdf/2507.22326v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-30T02:00:26Z", "updated": "2025-07-30T02:00:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An LLM Agent-Based Complex Semantic Table Annotation Approach", "authors": ["Yilin Geng", "Shujing Wang", "Chuan Wang", "Keqing He", "Yanfei Lv", "Ying Wang", "Zaiwen Feng", "Xiaoying Bai"], "year": 2025, "url": "http://arxiv.org/abs/2508.12868v1", "abstract": "The Semantic Table Annotation (STA) task, which includes Column Type Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to ontology entities and plays important roles in various semantic applications. However, complex tables often pose challenges such as semantic loss of column names or cell values, strict ontological hierarchy requirements, homonyms, spelling errors, and abbreviations, which hinder annotation accuracy. To address these issues, this paper proposes an LLM-based agent approach for CTA and CEA. We design and implement five external tools with tailored prompts based on the ReAct framework, enabling the STA agent to dynamically select suitable annotation strategies depending on table characteristics. Experiments are conducted on the Tough Tables and BiodivTab datasets from the SemTab challenge, which contain the aforementioned challenges. Our method outperforms existing approaches across various metrics. Furthermore, by leveraging Levenshtein distance to reduce redundant annotations, we achieve a 70% reduction in time costs and a 60% reduction in LLM token usage, providing an efficient and cost-effective solution for STA.", "source": "arxiv", "arxiv_id": "2508.12868v1", "pdf_url": "https://arxiv.org/pdf/2508.12868v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-18T12:09:20Z", "updated": "2025-08-18T12:09:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software", "authors": ["Sina Gogani-Khiabani", "Ashutosh Trivedi", "Diptikalyan Saha", "Saeid Tizpaz-Niari"], "year": 2025, "url": "http://arxiv.org/abs/2509.13471v1", "abstract": "Large language models (LLMs) show promise for translating natural-language statutes into executable logic, but reliability in legally critical settings remains challenging due to ambiguity and hallucinations. We present an agentic approach for developing legal-critical software, using U.S. federal tax preparation as a case study. The key challenge is test-case generation under the oracle problem, where correct outputs require interpreting law. Building on metamorphic testing, we introduce higher-order metamorphic relations that compare system outputs across structured shifts among similar individuals. Because authoring such relations is tedious and error-prone, we use an LLM-driven, role-based framework to automate test generation and code synthesis. We implement a multi-agent system that translates tax code into executable software and incorporates a metamorphic-testing agent that searches for counterexamples. In experiments, our framework using a smaller model (GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results support agentic LLM methodologies as a path to robust, trustworthy legal-critical software from natural-language specifications.", "source": "arxiv", "arxiv_id": "2509.13471v1", "pdf_url": "https://arxiv.org/pdf/2509.13471v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "10.1145/3744916.3764575", "venue": "", "published": "2025-09-16T19:13:26Z", "updated": "2025-09-16T19:13:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents", "authors": ["Ayesha Amjad", "Saurav Sthapit", "Tahir Qasim Syed"], "year": 2025, "url": "http://arxiv.org/abs/2505.13504v1", "abstract": "Extracting alphanumeric data from form-like documents such as invoices, purchase orders, bills, and financial documents is often performed via vision (OCR) and learning algorithms or monolithic pipelines with limited potential for systemic improvements. We propose an agentic AI system that leverages Large Language Model (LLM) agents and a reinforcement learning (RL) driver agent to automate consistent, self-improving extraction under LLM inference uncertainty. Our work highlights the limitations of monolithic LLM-based extraction and introduces a modular, multi-agent framework with task-specific prompts and an RL policy of rewards and penalties to guide a meta-prompting agent to learn from past errors and improve prompt-based actor agents. This self-corrective adaptive system handles diverse documents, file formats, layouts, and LLMs, aiming to automate accurate information extraction without the need for human intervention. Results as reported on two benchmark datasets of SOIRE, and CORD, are promising for the agentic AI framework.", "source": "arxiv", "arxiv_id": "2505.13504v1", "pdf_url": "https://arxiv.org/pdf/2505.13504v1", "categories": ["cs.IR", "cs.AI", "cs.MA"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-05-16T09:46:10Z", "updated": "2025-05-16T09:46:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Analyzing Information Sharing and Coordination in Multi-Agent Planning", "authors": ["Tianyue Ou", "Saujas Vaduguru", "Daniel Fried"], "year": 2025, "url": "http://arxiv.org/abs/2508.12981v1", "abstract": "Multi-agent systems (MASs) have pushed the boundaries of large language model (LLM) agents in domains such as web research and software engineering. However, long-horizon, multi-constraint planning tasks involve conditioning on detailed information and satisfying complex interdependent constraints, which can pose a challenge for these systems. In this study, we construct an LLM-based MAS for a travel planning task which is representative of these challenges. We evaluate the impact of a notebook to facilitate information sharing, and evaluate an orchestrator agent to improve coordination in free form conversation between agents. We find that the notebook reduces errors due to hallucinated details by 18%, while an orchestrator directs the MAS to focus on and further reduce errors by up to 13.5% within focused sub-areas. Combining both mechanisms achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute improvement over the single-agent baseline's 7.5% pass rate. These results highlight the potential of structured information sharing and reflective orchestration as key components in MASs for long horizon planning with LLMs.", "source": "arxiv", "arxiv_id": "2508.12981v1", "pdf_url": "https://arxiv.org/pdf/2508.12981v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-18T14:57:02Z", "updated": "2025-08-18T14:57:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents", "authors": ["Jiateng Liu", "Zhenhailong Wang", "Xiaojiang Huang", "Yingjie Li", "Xing Fan", "Xiang Li", "Chenlei Guo", "Ruhi Sarikaya", "Heng Ji"], "year": 2025, "url": "http://arxiv.org/abs/2510.11588v1", "abstract": "Large Language Model (LLM)-based agentic systems rely on in-context policy documents encoding diverse business rules. As requirements grow, these documents expand rapidly, causing high computational overhead. This motivates developing internalization methods that embed policy documents into model priors while preserving performance. Prior prompt compression work targets generic prompts, but agentic policy documents span multiple complexity levels and require deeper reasoning, making internalization harder. We introduce CC-Gen, an agentic benchmark generator with Controllable Complexity across four levels, enabling systematic evaluation of agents' ability to handle complexity and offering a unified framework for assessing policy internalization. Our analysis shows that complex policy specifications governing workflows pose major reasoning challenges. Supporting internalization with gold user agent interaction trajectories containing chain-of-thought (CoT) annotations via supervised fine-tuning (SFT) is data-intensive and degrades sharply as policy complexity increases. To mitigate data and reasoning burdens, we propose Category-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline parses policy documents to extract key specifications, grouping them into factual, behavioral, and conditional categories, and isolating complex conditions that drive workflow complexity. This guides targeted data synthesis and enables agents to internalize policy information through an autoregressive pretraining loss. Experiments show CAP-CPT improves SFT baselines in all settings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt length reduction on CC-Gen and further enhancing tau-Bench with minimal SFT data.", "source": "arxiv", "arxiv_id": "2510.11588v1", "pdf_url": "https://arxiv.org/pdf/2510.11588v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-13T16:30:07Z", "updated": "2025-10-13T16:30:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Applying Cognitive Design Patterns to General LLM Agents", "authors": ["Robert E. Wray", "James R. Kirk", "John E. Laird"], "year": 2025, "url": "http://arxiv.org/abs/2505.07087v2", "abstract": "One goal of AI (and AGI) is to identify and understand specific mechanisms and representations sufficient for general intelligence. Often, this work manifests in research focused on architectures and many cognitive architectures have been explored in AI/AGI. However, different research groups and even different research traditions have somewhat independently identified similar/common patterns of processes and representations or \"cognitive design patterns\" that are manifest in existing architectures. Today, AI systems exploiting large language models (LLMs) offer a relatively new combination of mechanisms and representations available for exploring the possibilities of general intelligence. This paper outlines a few recurring cognitive design patterns that have appeared in various pre-transformer AI architectures. We then explore how these patterns are evident in systems using LLMs, especially for reasoning and interactive (\"agentic\") use cases. Examining and applying these recurring patterns enables predictions of gaps or deficiencies in today's Agentic LLM Systems and identification of subjects of future research towards general intelligence using generative foundation models.", "source": "arxiv", "arxiv_id": "2505.07087v2", "pdf_url": "https://arxiv.org/pdf/2505.07087v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-11T18:29:54Z", "updated": "2025-06-13T21:53:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Approximating Human Strategic Reasoning with LLM-Enhanced Recursive Reasoners Leveraging Multi-agent Hypergames", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "year": 2025, "url": "http://arxiv.org/abs/2502.07443v1", "abstract": "LLM-driven multi-agent-based simulations have been gaining traction with applications in game-theoretic and social simulations. While most implementations seek to exploit or evaluate LLM-agentic reasoning, they often do so with a weak notion of agency and simplified architectures. We implement a role-based multi-agent strategic interaction framework tailored to sophisticated recursive reasoners, providing the means for systematic in-depth development and evaluation of strategic reasoning. Our game environment is governed by the umpire responsible for facilitating games, from matchmaking through move validation to environment management. Players incorporate state-of-the-art LLMs in their decision mechanism, relying on a formal hypergame-based model of hierarchical beliefs. We use one-shot, 2-player beauty contests to evaluate the recursive reasoning capabilities of the latest LLMs, providing a comparison to an established baseline model from economics and data from human experiments. Furthermore, we introduce the foundations of an alternative semantic measure of reasoning to the k-level theory. Our experiments show that artificial reasoners can outperform the baseline model in terms of both approximating human behaviour and reaching the optimal solution.", "source": "arxiv", "arxiv_id": "2502.07443v1", "pdf_url": "https://arxiv.org/pdf/2502.07443v1", "categories": ["cs.AI", "cs.GT"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-11T10:37:20Z", "updated": "2025-02-11T10:37:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AquaSentinel: Next-Generation AI System Integrating Sensor Networks for Urban Underground Water Pipeline Anomaly Detection via Collaborative MoE-LLM Agent Architecture", "authors": ["Qiming Guo", "Bishal Khatri", "Wenbo Sun", "Jinwen Tang", "Hua Zhang", "Wenlu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.15870v1", "abstract": "Underground pipeline leaks and infiltrations pose significant threats to water security and environmental safety. Traditional manual inspection methods provide limited coverage and delayed response, often missing critical anomalies. This paper proposes AquaSentinel, a novel physics-informed AI system for real-time anomaly detection in urban underground water pipeline networks. We introduce four key innovations: (1) strategic sparse sensor deployment at high-centrality nodes combined with physics-based state augmentation to achieve network-wide observability from minimal infrastructure; (2) the RTCA (Real-Time Cumulative Anomaly) detection algorithm, which employs dual-threshold monitoring with adaptive statistics to distinguish transient fluctuations from genuine anomalies; (3) a Mixture of Experts (MoE) ensemble of spatiotemporal graph neural networks that provides robust predictions by dynamically weighting model contributions; (4) causal flow-based leak localization that traces anomalies upstream to identify source nodes and affected pipe segments. Our system strategically deploys sensors at critical network junctions and leverages physics-based modeling to propagate measurements to unmonitored nodes, creating virtual sensors that enhance data availability across the entire network. Experimental evaluation using 110 leak scenarios demonstrates that AquaSentinel achieves 100% detection accuracy. This work advances pipeline monitoring by demonstrating that physics-informed sparse sensing can match the performance of dense deployments at a fraction of the cost, providing a practical solution for aging urban infrastructure.", "source": "arxiv", "arxiv_id": "2511.15870v1", "pdf_url": "https://arxiv.org/pdf/2511.15870v1", "categories": ["cs.CE", "cs.AI"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2025-11-19T20:53:50Z", "updated": "2025-11-19T20:53:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations", "authors": ["Ron F. Del Rosario", "Klaudia Krawiecka", "Christian Schroeder de Witt"], "year": 2025, "url": "http://arxiv.org/abs/2509.08646v1", "abstract": "As Large Language Model (LLM) agents become increasingly capable of automating complex, multi-step tasks, the need for robust, secure, and predictable architectural patterns is paramount. This paper provides a comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic design that separates strategic planning from tactical execution. We explore the foundational principles of P-t-E, detailing its core components - the Planner and the Executor - and its architectural advantages in predictability, cost-efficiency, and reasoning quality over reactive patterns like ReAct (Reason + Act). A central focus is placed on the security implications of this design, particularly its inherent resilience to indirect prompt injection attacks by establishing control-flow integrity. We argue that while P-t-E provides a strong foundation, a defense-in-depth strategy is necessary, and we detail essential complementary controls such as the Principle of Least Privilege, task-scoped tool access, and sandboxed code execution. To make these principles actionable, this guide provides detailed implementation blueprints and working code references for three leading agentic frameworks: LangChain (via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing the P-t-E pattern is analyzed, highlighting unique features like LangGraph's stateful graphs for re-planning, CrewAI's declarative tool scoping for security, and AutoGen's built-in Docker sandboxing. Finally, we discuss advanced patterns, including dynamic re-planning loops, parallel execution with Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop (HITL) verification, to offer a complete strategic blueprint for architects, developers, and security engineers aiming to build production-grade, resilient, and trustworthy LLM agents.", "source": "arxiv", "arxiv_id": "2509.08646v1", "pdf_url": "https://arxiv.org/pdf/2509.08646v1", "categories": ["cs.CR", "cs.AI", "eess.SY"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-10T14:41:07Z", "updated": "2025-09-10T14:41:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Architectures for Building Agentic AI", "authors": ["SÅawomir Nowaczyk"], "year": 2025, "url": "http://arxiv.org/abs/2512.09458v1", "abstract": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.", "source": "arxiv", "arxiv_id": "2512.09458v1", "pdf_url": "https://arxiv.org/pdf/2512.09458v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-10T09:28:40Z", "updated": "2025-12-10T09:28:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions", "authors": ["Naihao Deng", "Rada Mihalcea"], "year": 2025, "url": "http://arxiv.org/abs/2501.15283v1", "abstract": "As Large Language Models (LLMs) advance in their capabilities, researchers have increasingly employed them for social simulation. In this paper, we investigate whether interactions among LLM agents resemble those of humans. Specifically, we focus on the pronoun usage difference between leaders and non-leaders, examining whether the simulation would lead to human-like pronoun usage patterns during the LLMs' interactions. Our evaluation reveals the significant discrepancies between LLM-based simulations and human pronoun usage, with prompt-based or specialized agents failing to demonstrate human-like pronoun usage patterns. In addition, we reveal that even if LLMs understand the human pronoun usage patterns, they fail to demonstrate them in the actual interaction process. Our study highlights the limitations of social simulations based on LLM agents, urging caution in using such social simulation in practitioners' decision-making process.", "source": "arxiv", "arxiv_id": "2501.15283v1", "pdf_url": "https://arxiv.org/pdf/2501.15283v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-25T17:42:47Z", "updated": "2025-01-25T17:42:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation", "authors": ["James Mooney", "Josef Woldense", "Zheng Robert Jia", "Shirley Anugrah Hayati", "My Ha Nguyen", "Vipul Raheja", "Dongyeop Kang"], "year": 2025, "url": "http://arxiv.org/abs/2509.03736v1", "abstract": "The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.", "source": "arxiv", "arxiv_id": "2509.03736v1", "pdf_url": "https://arxiv.org/pdf/2509.03736v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-03T21:55:29Z", "updated": "2025-09-03T21:55:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Are LLM Agents the New RPA? A Comparative Study with RPA Across Enterprise Workflows", "authors": ["Petr PrÅ¯cha", "Michaela MatouÅ¡kovÃ¡", "Jan Strnad"], "year": 2025, "url": "http://arxiv.org/abs/2509.04198v1", "abstract": "The emergence of large language models (LLMs) has introduced a new paradigm in automation: LLM agents or Agentic Automation with Computer Use (AACU). Unlike traditional Robotic Process Automation (RPA), which relies on rule-based workflows and scripting, AACU enables intelligent agents to perform tasks through natural language instructions and autonomous interaction with user interfaces. This study investigates whether AACU can serve as a viable alternative to RPA in enterprise workflow automation. We conducted controlled experiments across three standard RPA challenges data entry, monitoring, and document extraction comparing RPA (via UiPath) and AACU (via Anthropic's Computer Use Agent) in terms of speed, reliability, and development effort. Results indicate that RPA outperforms AACU in execution speed and reliability, particularly in repetitive, stable environments. However, AACU significantly reduces development time and adapts more flexibly to dynamic interfaces. While current AACU implementations are not yet production-ready, their promise in rapid prototyping and lightweight automation is evident. Future research should explore multi-agent orchestration, hybrid RPA-AACU architectures, and more robust evaluation across industries and platforms.", "source": "arxiv", "arxiv_id": "2509.04198v1", "pdf_url": "https://arxiv.org/pdf/2509.04198v1", "categories": ["cs.CY", "cs.MA"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-09-04T13:22:44Z", "updated": "2025-09-04T13:22:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents", "authors": ["Keith Moore", "Jun W. Kim", "David Lyu", "Jeffrey Heo", "Ehsan Adeli"], "year": 2025, "url": "http://arxiv.org/abs/2511.14780v1", "abstract": "We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors (\"act like a neurologist\", \"act like an infectious disease specialist\"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.", "source": "arxiv", "arxiv_id": "2511.14780v1", "pdf_url": "https://arxiv.org/pdf/2511.14780v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-06T20:31:19Z", "updated": "2025-11-06T20:31:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control", "authors": ["Yaqi Duan", "Yichun Hu", "Jiashuo Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2601.00121v1", "abstract": "Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant \"hallucination tax\": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.\n  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned \"digital twin\" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.", "source": "arxiv", "arxiv_id": "2601.00121v1", "pdf_url": "https://arxiv.org/pdf/2601.00121v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T21:45:54Z", "updated": "2025-12-31T21:45:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AskDB: An LLM Agent for Natural Language Interaction with Relational Databases", "authors": ["Xuan-Quang Phan", "Tan-Ha Mai", "Thai-Duy Dinh", "Minh-Thuan Nguyen", "Lam-Son LÃª"], "year": 2025, "url": "http://arxiv.org/abs/2511.16131v1", "abstract": "Interacting with relational databases remains challenging for users across different expertise levels, particularly when composing complex analytical queries or performing administrative tasks. Existing systems typically address either natural language querying or narrow aspects of database administration, lacking a unified and intelligent interface for general-purpose database interaction. We introduce AskDB, a large language model powered agent designed to bridge this gap by supporting both data analysis and administrative operations over SQL databases through natural language. Built on Gemini 2, AskDB integrates two key innovations: a dynamic schema-aware prompting mechanism that effectively incorporates database metadata, and a task decomposition framework that enables the agent to plan and execute multi-step actions. These capabilities allow AskDB to autonomously debug derived SQL, retrieve contextual information via real-time web search, and adaptively refine its responses. We evaluate AskDB on a widely used Text-to-SQL benchmark and a curated set of DBA tasks, demonstrating strong performance in both analytical and administrative scenarios. Our results highlight the potential of AskDB as a unified and intelligent agent for relational database systems, offering an intuitive and accessible experience for end users.", "source": "arxiv", "arxiv_id": "2511.16131v1", "pdf_url": "https://arxiv.org/pdf/2511.16131v1", "categories": ["cs.DB", "cs.AI"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-11-20T08:06:09Z", "updated": "2025-11-20T08:06:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support", "authors": ["Kevin Pu", "Daniel Lazaro", "Ian Arawjo", "Haijun Xia", "Ziang Xiao", "Tovi Grossman", "Yan Chen"], "year": 2025, "url": "http://arxiv.org/abs/2502.18658v4", "abstract": "AI programming tools enable powerful code generation, and recent prototypes attempt to reduce user effort with proactive AI agents, but their impact on programming workflows remains unexplored. We introduce and evaluate Codellaborator, a design probe LLM agent that initiates programming assistance based on editor activities and task context. We explored three interface variants to assess trade-offs between increasingly salient AI support: prompt-only, proactive agent, and proactive agent with presence and context (Codellaborator). In a within-subject study (N=18), we find that proactive agents increase efficiency compared to prompt-only paradigm, but also incur workflow disruptions. However, presence indicators and interaction context support alleviated disruptions and improved users' awareness of AI processes. We underscore trade-offs of Codellaborator on user control, ownership, and code understanding, emphasizing the need to adapt proactivity to programming processes. Our research contributes to the design exploration and evaluation of proactive AI systems, presenting design implications on AI-integrated programming workflow.", "source": "arxiv", "arxiv_id": "2502.18658v4", "pdf_url": "https://arxiv.org/pdf/2502.18658v4", "categories": ["cs.HC", "cs.AI", "cs.SE"], "primary_category": "cs.HC", "doi": "10.1145/3706598.3713357", "venue": "", "published": "2025-02-25T21:37:25Z", "updated": "2025-09-08T16:34:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access", "authors": ["Chaoyi Ruan", "Chao Bi", "Kaiwen Zheng", "Ziji Shi", "Xinyi Wan", "Jialin Li"], "year": 2025, "url": "http://arxiv.org/abs/2509.17360v1", "abstract": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep research and code generation. However, their effectiveness depends on frequent interactions with knowledge sources across remote clouds or regions. Such interactions can create non-trivial latency and cost bottlenecks. Existing caching solutions focus on exact-match queries, limiting their effectiveness for semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region knowledge caching architecture for LLM agents. At its core are two abstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A semantic element captures the semantic embedding representation of an LLM query together with performance-aware metadata such as latency, cost, and staticity. Sine then provides two-stage retrieval: a vector similar index with semantic embedding for fast candidate selection and a lightweight LLM-powered semantic judger for precise validation. Atop these primitives, Asteria builds a new cache interface that includes a new semantic-aware cache hit definition, a cost-efficient eviction policy, and proactive prefetching. To reduce overhead, Asteria co-locates the small LLM judger with the main LLM using adaptive scheduling and resource sharing. Our evaluation demonstrates that Asteria delivers substantial performance improvements without compromising correctness. On representative search workloads, Asteria achieves up to a 3.6$\\times$ increase in throughput by maintaining cache hit rates of over 85%, while preserving accuracy virtually identical to non-cached baselines. Asteria also improves throughput for complex coding tasks by 20%, showcasing its versatility across diverse agentic workloads.", "source": "arxiv", "arxiv_id": "2509.17360v1", "pdf_url": "https://arxiv.org/pdf/2509.17360v1", "categories": ["cs.DC"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-09-22T05:24:22Z", "updated": "2025-09-22T05:24:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation", "authors": ["Xavier Cadet", "Edward Koh", "Peter Chin"], "year": 2025, "url": "http://arxiv.org/abs/2512.03466v1", "abstract": "Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.", "source": "arxiv", "arxiv_id": "2512.03466v1", "pdf_url": "https://arxiv.org/pdf/2512.03466v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-12-03T05:42:01Z", "updated": "2025-12-03T05:42:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Async Control: Stress-testing Asynchronous Control Measures for LLM Agents", "authors": ["Asa Cooper Stickland", "Jan Michelfeit", "Arathi Mani", "Charlie Griffin", "Ollie Matthews", "Tomek Korbak", "Rogan Inglis", "Oliver Makins", "Alan Cooney"], "year": 2025, "url": "http://arxiv.org/abs/2512.13526v1", "abstract": "LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.", "source": "arxiv", "arxiv_id": "2512.13526v1", "pdf_url": "https://arxiv.org/pdf/2512.13526v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-15T16:56:28Z", "updated": "2025-12-15T16:56:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs", "authors": ["George Yakushev", "Nataliia Babina", "Masoud Vahid Dastgerdi", "Vyacheslav Zhdanovskiy", "Alina Shutova", "Denis Kuznedelev"], "year": 2025, "url": "http://arxiv.org/abs/2512.10931v1", "abstract": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.", "source": "arxiv", "arxiv_id": "2512.10931v1", "pdf_url": "https://arxiv.org/pdf/2512.10931v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-11T18:57:02Z", "updated": "2025-12-11T18:57:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools", "authors": ["Kanghua Mo", "Li Hu", "Yucheng Long", "Zhihao Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.02110v2", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex reasoning and decision-making by leveraging external tools. However, this tool-centric paradigm introduces a previously underexplored attack surface, where adversaries can manipulate tool metadata -- such as names, descriptions, and parameter schemas -- to influence agent behavior. We identify this as a new and stealthy threat surface that allows malicious tools to be preferentially selected by LLM agents, without requiring prompt injection or access to model internals. To demonstrate and exploit this vulnerability, we propose the Attractive Metadata Attack (AMA), a black-box in-context learning framework that generates highly attractive but syntactically and semantically valid tool metadata through iterative optimization. The proposed attack integrates seamlessly into standard tool ecosystems and requires no modification to the agent's execution framework. Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution. Moreover, the attack remains effective even against prompt-level defenses, auditor-based detection, and structured tool-selection protocols such as the Model Context Protocol, revealing systemic vulnerabilities in current agent architectures. These findings reveal that metadata manipulation constitutes a potent and stealthy attack surface. Notably, AMA is orthogonal to injection attacks and can be combined with them to achieve stronger attack efficacy, highlighting the need for execution-level defenses beyond prompt-level and auditor-based mechanisms. Code is available at https://github.com/SEAIC-M/AMA.", "source": "arxiv", "arxiv_id": "2508.02110v2", "pdf_url": "https://arxiv.org/pdf/2508.02110v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-04T06:38:59Z", "updated": "2026-01-07T07:28:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AudioToolAgent: An Agentic Framework for Audio-Language Models", "authors": ["Gijs Wijngaard", "Elia Formisano", "Michel Dumontier"], "year": 2025, "url": "http://arxiv.org/abs/2510.02995v1", "abstract": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks but lack multi-step reasoning and tool-calling found in recent Large Language Models (LLMs). This paper presents AudioToolAgent, a framework that coordinates audio-language models as tools via a central LLM agent that accesses tool adapters for audio question answering and speech-to-text. The agent selects tools, asks follow-up questions, and compares outputs for verification. Experiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to 74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling for shapley values across 374 configurations identifies effective agent-tool combinations. The modular design allows integration of new tools and eliminates the use of data and training costs. Code and reproduction materials are available at: github.com/GLJS/AudioToolAgent", "source": "arxiv", "arxiv_id": "2510.02995v1", "pdf_url": "https://arxiv.org/pdf/2510.02995v1", "categories": ["cs.SD"], "primary_category": "cs.SD", "doi": "", "venue": "", "published": "2025-10-03T13:35:45Z", "updated": "2025-10-03T13:35:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Auditable Early Stopping for Agentic Routing: Ledger-Verified Run-Wise Certificates under Local DP", "authors": ["Shivam Akhauri"], "year": 2025, "url": "http://arxiv.org/abs/2509.10550v2", "abstract": "We address when a best-first router for tool-use agents can stop exploring without missing a better leaf, while preserving local differential privacy (LDP) and leaving an audit trail. We introduce a run-wise certificate that couples each node's key to the same exponential race that realizes leaf perturbations; the usual halting rule (stop when the maximum over $v$ in $F$ of Key$(v) \\le B^*$) then certifies the realized run. We give two certified modes on context-indexed prefix DAGs with child partition: (i) Exact (known counts), using lazy offset propagation with winner reuse; and (ii) Surrogate (upper bounds only), which anchors keys to a parent-level surrogate race and allows validator tightening via $Îº= \\log(N / N_{ub}$). A small compiler enforces the partition property, and an admissible, race-independent M(tau) keeps keys sound. The ledger logs uniforms, counts, and tie handling; privacy follows by post-processing. Experiments on synthetic graphs and a small real pipeline show tight stopping, deterministic replay, and low overhead.", "source": "arxiv", "arxiv_id": "2509.10550v2", "pdf_url": "https://arxiv.org/pdf/2509.10550v2", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-09T01:25:09Z", "updated": "2025-09-16T02:41:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs", "authors": ["Michael Luo", "Xiaoxiang Shi", "Colin Cai", "Tianjun Zhang", "Justin Wong", "Yichuan Wang", "Chi Wang", "Yanping Huang", "Zhifeng Chen", "Joseph E. Gonzalez", "Ion Stoica"], "year": 2025, "url": "http://arxiv.org/abs/2502.13965v1", "abstract": "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.", "source": "arxiv", "arxiv_id": "2502.13965v1", "pdf_url": "https://arxiv.org/pdf/2502.13965v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-19T18:59:30Z", "updated": "2025-02-19T18:59:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs", "authors": ["Tingting Chen", "Srinivas Anumasa", "Beibei Lin", "Vedant Shah", "Anirudh Goyal", "Dianbo Liu"], "year": 2025, "url": "http://arxiv.org/abs/2502.15224v1", "abstract": "Given the remarkable performance of Large Language Models (LLMs), an important question arises: Can LLMs conduct human-like scientific research and discover new knowledge, and act as an AI scientist? Scientific discovery is an iterative process that demands efficient knowledge updating and encoding. It involves understanding the environment, identifying new hypotheses, and reasoning about actions; however, no standardized benchmark specifically designed for scientific discovery exists for LLM agents. In response to these limitations, we introduce a novel benchmark, \\textit{Auto-Bench}, that encompasses necessary aspects to evaluate LLMs for scientific discovery in both natural and social sciences. Our benchmark is based on the principles of causal graph discovery. It challenges models to uncover hidden structures and make optimal decisions, which includes generating valid justifications. By engaging interactively with an oracle, the models iteratively refine their understanding of underlying interactions, the chemistry and social interactions, through strategic interventions. We evaluate state-of-the-art LLMs, including GPT-4, Gemini, Qwen, Claude, and Llama, and observe a significant performance drop as the problem complexity increases, which suggests an important gap between machine and human intelligence that future development of LLMs need to take into consideration.", "source": "arxiv", "arxiv_id": "2502.15224v1", "pdf_url": "https://arxiv.org/pdf/2502.15224v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-21T05:35:20Z", "updated": "2025-02-21T05:35:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning", "authors": ["Seungjun Yi", "Joakim Nguyen", "Huimin Xu", "Terence Lim", "Andrew Well", "Mia Markey", "Ying Ding"], "year": 2025, "url": "http://arxiv.org/abs/2506.23998v2", "abstract": "Congenital heart disease (CHD) presents complex, lifelong challenges often underrepresented in traditional clinical metrics. While unstructured narratives offer rich insights into patient and caregiver experiences, manual thematic analysis (TA) remains labor-intensive and unscalable. We propose a fully automated large language model (LLM) pipeline that performs end-to-end TA on clinical narratives, which eliminates the need for manual coding or full transcript review. Our system employs a novel multi-agent framework, where specialized LLM agents assume roles to enhance theme quality and alignment with human analysis. To further improve thematic relevance, we optionally integrate reinforcement learning from human feedback (RLHF). This supports scalable, patient-centered analysis of large qualitative datasets and allows LLMs to be fine-tuned for specific clinical contexts.", "source": "arxiv", "arxiv_id": "2506.23998v2", "pdf_url": "https://arxiv.org/pdf/2506.23998v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-30T16:02:28Z", "updated": "2025-08-08T21:52:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents", "authors": ["Jiabin Tang", "Tianyu Fan", "Chao Huang"], "year": 2025, "url": "http://arxiv.org/abs/2502.05957v3", "abstract": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.", "source": "arxiv", "arxiv_id": "2502.05957v3", "pdf_url": "https://arxiv.org/pdf/2502.05957v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-09T16:53:56Z", "updated": "2025-10-09T07:27:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoBackdoor: Automating Backdoor Attacks via LLM Agents", "authors": ["Yige Li", "Zhe Li", "Wei Zhao", "Nay Myat Min", "Hanxun Huang", "Xingjun Ma", "Jun Sun"], "year": 2025, "url": "http://arxiv.org/abs/2511.16709v1", "abstract": "Backdoor attacks pose a serious threat to the secure deployment of large language models (LLMs), enabling adversaries to implant hidden behaviors triggered by specific inputs. However, existing methods often rely on manually crafted triggers and static data pipelines, which are rigid, labor-intensive, and inadequate for systematically evaluating modern defense robustness. As AI agents become increasingly capable, there is a growing need for more rigorous, diverse, and scalable \\textit{red-teaming frameworks} that can realistically simulate backdoor threats and assess model resilience under adversarial conditions. In this work, we introduce \\textsc{AutoBackdoor}, a general framework for automating backdoor injection, encompassing trigger generation, poisoned data construction, and model fine-tuning via an autonomous agent-driven pipeline. Unlike prior approaches, AutoBackdoor uses a powerful language model agent to generate semantically coherent, context-aware trigger phrases, enabling scalable poisoning across arbitrary topics with minimal human effort. We evaluate AutoBackdoor under three realistic threat scenarios, including \\textit{Bias Recommendation}, \\textit{Hallucination Injection}, and \\textit{Peer Review Manipulation}, to simulate a broad range of attacks. Experiments on both open-source and commercial models, including LLaMA-3, Mistral, Qwen, and GPT-4o, demonstrate that our method achieves over 90\\% attack success with only a small number of poisoned samples. More importantly, we find that existing defenses often fail to mitigate these attacks, underscoring the need for more rigorous and adaptive evaluation techniques against agent-driven threats as explored in this work. All code, datasets, and experimental configurations will be merged into our primary repository at https://github.com/bboylyg/BackdoorLLM.", "source": "arxiv", "arxiv_id": "2511.16709v1", "pdf_url": "https://arxiv.org/pdf/2511.16709v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-20T03:58:54Z", "updated": "2025-11-20T03:58:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoCodeSherpa: Symbolic Explanations in AI Coding Agents", "authors": ["Sungmin Kang", "Haifeng Ruan", "Abhik Roychoudhury"], "year": 2025, "url": "http://arxiv.org/abs/2507.22414v1", "abstract": "Large Language Model (LLM) agents autonomously use external tools on top of one or more LLMs to accomplish specific tasks. Lately LLM agents for software engineering tasks have become popular. These agents can benefit from the use of program analysis tools working on program representations. This is demonstrated by existing agentic AI solutions such as AutoCodeRover or SpecRover which perform automated program repair. Specifically the goal of these works is to use program analysis to improve the patch quality. These agents are currently being used to automatically fix static analysis issues from the widely used SonarQube static analyzer.\n  Nevertheless, for the agents to be deployed in a production environment, agents need to suggest software artifacts, such as patches, with evidence and with high confidence. In this work, we provide a workflow where an agent provides explanations of the bug in the form of symbolic formulae. The explanations are in the form of input conditions, infection conditions and output conditions, implemented as property based tests (PBT) and program-internal symbolic expressions. These can help in human developer cognition of the agent outputs as well as in achieving completely automated agentic workflows for software. The human developer can benefit from the input condition, represented as a PBT, to generate various concrete inputs showing a given issue. Furthermore, since the PBTs are executable, our explanations are executable as well. We can thus also use the explanations in a completely automated issue resolution environment for accepting or rejecting the patches that are suggested by patching agents such as AutoCodeRover. Finally, as agentic AI approaches continue to develop, the program analysis driven explanations can be provided to other LLM-based repair techniques such as Agentless to improve their output.", "source": "arxiv", "arxiv_id": "2507.22414v1", "pdf_url": "https://arxiv.org/pdf/2507.22414v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-07-30T06:34:02Z", "updated": "2025-07-30T06:34:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoContext: Instance-Level Context Learning for LLM Agents", "authors": ["Kuntai Cai", "Juncheng Liu", "Xianglin Yang", "Zhaojie Niu", "Xiaokui Xiao", "Xing Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.02369v3", "abstract": "Current LLM agents typically lack instance-level context, which comprises concrete facts such as environment structure, system configurations, and local mechanics. Consequently, existing methods are forced to intertwine exploration with task execution. This coupling leads to redundant interactions and fragile decision-making, as agents must repeatedly rediscover the same information for every new task. To address this, we introduce AutoContext, a method that decouples exploration from task solving. AutoContext performs a systematic, one-off exploration to construct a reusable knowledge graph for each environment instance. This structured context allows off-the-shelf agents to access necessary facts directly, eliminating redundant exploration. Experiments across TextWorld, ALFWorld, Crafter, and InterCode-Bash demonstrate substantial gains: for example, the success rate of a ReAct agent on TextWorld improves from 37% to 95%, highlighting the critical role of structured instance context in efficient agentic systems.", "source": "arxiv", "arxiv_id": "2510.02369v3", "pdf_url": "https://arxiv.org/pdf/2510.02369v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-29T05:38:51Z", "updated": "2026-01-13T06:31:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents", "authors": ["Yiyi Lu", "Hoi Ian Au", "Junyao Zhang", "Jingyu Pan", "Yiting Wang", "Ang Li", "Jianyi Zhang", "Yiran Chen"], "year": 2025, "url": "http://arxiv.org/abs/2508.01012v1", "abstract": "Modern Electronic Design Automation (EDA) workflows, especially the RTL-to-GDSII flow, require heavily manual scripting and demonstrate a multitude of tool-specific interactions which limits scalability and efficiency. While LLMs introduces strides for automation, existing LLM solutions require expensive fine-tuning and do not contain standardized frameworks for integration and evaluation. We introduce AutoEDA, a framework for EDA automation that leverages paralleled learning through the Model Context Protocol (MCP) specific for standardized and scalable natural language experience across the entire RTL-to-GDSII flow. AutoEDA limits fine-tuning through structured prompt engineering, implements intelligent parameter extraction and task decomposition, and provides an extended CodeBLEU metric to evaluate the quality of TCL scripts. Results from experiments over five previously curated benchmarks show improvements in automation accuracy and efficiency, as well as script quality when compared to existing methods. AutoEDA is released open-sourced to support reproducibility and the EDA community. Available at: https://github.com/AndyLu666/MCP-EDA-Server", "source": "arxiv", "arxiv_id": "2508.01012v1", "pdf_url": "https://arxiv.org/pdf/2508.01012v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-01T18:23:57Z", "updated": "2025-08-01T18:23:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science", "authors": ["Yixin Ou", "Yujie Luo", "Jingsheng Zheng", "Lanning Wei", "Zhuoyun Yu", "Shuofei Qiao", "Jintian Zhang", "Da Zheng", "Yuren Mao", "Yunjun Gao", "Huajun Chen", "Ningyu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.10974v3", "abstract": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science. Code is at https://github.com/innovatingAI/AutoMind.", "source": "arxiv", "arxiv_id": "2506.10974v3", "pdf_url": "https://arxiv.org/pdf/2506.10974v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-12T17:59:32Z", "updated": "2025-10-08T17:06:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models", "authors": ["Rebecca Martin", "Jay Patrikar", "Sebastian Scherer"], "year": 2025, "url": "http://arxiv.org/abs/2509.08638v1", "abstract": "Specialized machine learning models, regardless of architecture and training, are susceptible to failures in deployment. With their increasing use in high risk situations, the ability to audit these models by determining their operational design domain (ODD) is crucial in ensuring safety and compliance. However, given the high-dimensional input spaces, this process often requires significant human resources and domain expertise. To alleviate this, we introduce \\coolname, an LLM-Agent centric framework for automated generation of semantically relevant test cases to search for failure modes in specialized black-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit a uncertainty-aware failure distribution model on a learned text-embedding manifold by projecting the high-dimension input space to low-dimension text-embedding latent space. The LLM-Agent is tasked with iteratively building the failure landscape by leveraging tools for generating test-cases to probe the model-under-test (MUT) and recording the response. The agent also guides the search using tools to probe uncertainty estimate on the low dimensional manifold. We demonstrate this process in a simple case using models trained with missing digits on the MNIST dataset and in the real world setting of vision-based intruder detection for aerial vehicles.", "source": "arxiv", "arxiv_id": "2509.08638v1", "pdf_url": "https://arxiv.org/pdf/2509.08638v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-09-10T14:33:58Z", "updated": "2025-09-10T14:33:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoPDL: Automatic Prompt Optimization for LLM Agents", "authors": ["Claudio Spiess", "Mandana Vaziri", "Louis Mandel", "Martin Hirzel"], "year": 2025, "url": "http://arxiv.org/abs/2504.04365v5", "abstract": "The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.", "source": "arxiv", "arxiv_id": "2504.04365v5", "pdf_url": "https://arxiv.org/pdf/2504.04365v5", "categories": ["cs.LG", "cs.AI", "cs.PL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-06T05:30:10Z", "updated": "2025-11-03T21:46:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents", "authors": ["Julius Henke"], "year": 2025, "url": "http://arxiv.org/abs/2505.10321v1", "abstract": "A recent area of increasing research is the use of Large Language Models (LLMs) in penetration testing, which promises to reduce costs and thus allow for higher frequency. We conduct a review of related work, identifying best practices and common evaluation issues. We then present AutoPentest, an application for performing black-box penetration tests with a high degree of autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent framework LangChain. It can perform complex multi-step tasks, augmented by external tools and knowledge bases. We conduct a study on three capture-the-flag style Hack The Box (HTB) machines, comparing our implementation AutoPentest with the baseline approach of manually using the ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT. We measure a total cost of \\$96.20 US when using AutoPentest across all experiments, while a one-month subscription to ChatGPT Plus costs \\$20. The results show that further implementation efforts and the use of more powerful LLMs released in the future are likely to make this a viable part of vulnerability management.", "source": "arxiv", "arxiv_id": "2505.10321v1", "pdf_url": "https://arxiv.org/pdf/2505.10321v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-15T14:06:00Z", "updated": "2025-05-15T14:06:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoPentester: An LLM Agent-based Framework for Automated Pentesting", "authors": ["Yasod Ginige", "Akila Niroshan", "Sajal Jain", "Suranga Seneviratne"], "year": 2025, "url": "http://arxiv.org/abs/2510.05605v1", "abstract": "Penetration testing and vulnerability assessment are essential industry practices for safeguarding computer systems. As cyber threats grow in scale and complexity, the demand for pentesting has surged, surpassing the capacity of human professionals to meet it effectively. With advances in AI, particularly Large Language Models (LLMs), there have been attempts to automate the pentesting process. However, existing tools such as PentestGPT are still semi-manual, requiring significant professional human interaction to conduct pentests. To this end, we propose a novel LLM agent-based framework, AutoPentester, which automates the pentesting process. Given a target IP, AutoPentester automatically conducts pentesting steps using common security tools in an iterative process. It can dynamically generate attack strategies based on the tool outputs from the previous iteration, mimicking the human pentester approach. We evaluate AutoPentester using Hack The Box and custom-made VMs, comparing the results with the state-of-the-art PentestGPT. Results show that AutoPentester achieves a 27.0% better subtask completion rate and 39.5% more vulnerability coverage with fewer steps. Most importantly, it requires significantly fewer human interactions and interventions compared to PentestGPT. Furthermore, we recruit a group of security industry professional volunteers for a user survey and perform a qualitative analysis to evaluate AutoPentester against industry practices and compare it with PentestGPT. On average, AutoPentester received a score of 3.93 out of 5 based on user reviews, which was 19.8% higher than PentestGPT.", "source": "arxiv", "arxiv_id": "2510.05605v1", "pdf_url": "https://arxiv.org/pdf/2510.05605v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-07T06:02:26Z", "updated": "2025-10-07T06:02:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment", "authors": ["Xiaochong Lan", "Jie Feng", "Yinxing Liu", "Xinlei Shi", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.08081v1", "abstract": "Ranking online reviews by their intrinsic quality is a critical task for e-commerce platforms and information services, impacting user experience and business outcomes. However, quality is a domain-dependent and dynamic concept, making its assessment a formidable challenge. Traditional methods relying on hand-crafted features are unscalable across domains and fail to adapt to evolving content patterns, while modern deep learning approaches often produce black-box models that lack interpretability and may prioritize semantics over quality. To address these challenges, we propose AutoQual, an LLM-based agent framework that automates the discovery of interpretable features. While demonstrated on review quality assessment, AutoQual is designed as a general framework for transforming tacit knowledge embedded in data into explicit, computable features. It mimics a human research process, iteratively generating feature hypotheses through reflection, operationalizing them via autonomous tool implementation, and accumulating experience in a persistent memory. We deploy our method on a large-scale online platform with a billion-level user base. Large-scale A/B testing confirms its effectiveness, increasing average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27%.", "source": "arxiv", "arxiv_id": "2510.08081v1", "pdf_url": "https://arxiv.org/pdf/2510.08081v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-09T11:11:02Z", "updated": "2025-10-09T11:11:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning", "authors": ["Jiaru Zou", "Ling Yang", "Yunzhe Qi", "Sirui Chen", "Mengting Ai", "Ke Shen", "Jingrui He", "Mengdi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.13278v1", "abstract": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.", "source": "arxiv", "arxiv_id": "2512.13278v1", "pdf_url": "https://arxiv.org/pdf/2512.13278v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-15T12:38:04Z", "updated": "2025-12-15T12:38:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoTool: Efficient Tool Selection for Large Language Model Agents", "authors": ["Jingyi Jia", "Qinbin Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.14650v1", "abstract": "Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.", "source": "arxiv", "arxiv_id": "2511.14650v1", "pdf_url": "https://arxiv.org/pdf/2511.14650v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-18T16:41:48Z", "updated": "2025-11-18T16:41:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools", "authors": ["Prerna Agarwal", "Himanshu Gupta", "Soujanya Soni", "Rohith Vallam", "Renuka Sindhgatta", "Sameep Mehta"], "year": 2025, "url": "http://arxiv.org/abs/2509.11626v1", "abstract": "Recent advancements in Large Language Models (LLMs) has lead to the development of agents capable of complex reasoning and interaction with external tools. In enterprise contexts, the effective use of such tools that are often enabled by application programming interfaces (APIs), is hindered by poor documentation, complex input or output schema, and large number of operations. These challenges make tool selection difficult and reduce the accuracy of payload formation by up to 25%. We propose ACE, an automated tool creation and enrichment framework that transforms enterprise APIs into LLM-compatible tools. ACE, (i) generates enriched tool specifications with parameter descriptions and examples to improve selection and invocation accuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters relevant tools at runtime, reducing prompt complexity while maintaining scalability. We validate our framework on both proprietary and open-source APIs and demonstrate its integration with agentic frameworks. To the best of our knowledge, ACE is the first end-to-end framework that automates the creation, enrichment, and dynamic selection of enterprise API tools for LLM agents.", "source": "arxiv", "arxiv_id": "2509.11626v1", "pdf_url": "https://arxiv.org/pdf/2509.11626v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-15T06:41:54Z", "updated": "2025-09-15T06:41:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety", "authors": ["Junyu Mao", "Anthony Hills", "Talia Tseriotou", "Maria Liakata", "Aya Shamir", "Dan Sayda", "Dana Atzil-Slonim", "Natalie Djohari", "Arpan Mandal", "Silke Roth", "Pamela Ugwudike", "Mahesan Niranjan", "Stuart E. Middleton"], "year": 2025, "url": "http://arxiv.org/abs/2512.06227v1", "abstract": "Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.", "source": "arxiv", "arxiv_id": "2512.06227v1", "pdf_url": "https://arxiv.org/pdf/2512.06227v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-06T00:21:29Z", "updated": "2025-12-06T00:21:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Design Optimization via Strategic Search with Large Language Models", "authors": ["Anthony Carreon", "Vansh Sharma", "Venkat Raman"], "year": 2025, "url": "http://arxiv.org/abs/2511.22651v1", "abstract": "Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \\$159 per run, compared to an estimated cost of up to \\$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.", "source": "arxiv", "arxiv_id": "2511.22651v1", "pdf_url": "https://arxiv.org/pdf/2511.22651v1", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-27T17:42:05Z", "updated": "2025-11-27T17:42:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Hypothesis Validation with Agentic Sequential Falsifications", "authors": ["Kexin Huang", "Ying Jin", "Ryan Li", "Michael Y. Li", "Emmanuel CandÃ¨s", "Jure Leskovec"], "year": 2025, "url": "http://arxiv.org/abs/2502.09858v1", "abstract": "Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.", "source": "arxiv", "arxiv_id": "2502.09858v1", "pdf_url": "https://arxiv.org/pdf/2502.09858v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.QM"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-14T01:46:00Z", "updated": "2025-02-14T01:46:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Machine Learning Pipeline: Large Language Models-Assisted Automated Dataset Generation for Training Machine-Learned Interatomic Potentials", "authors": ["Adam Lahouari", "Jutta Rogal", "Mark E. Tuckerman"], "year": 2025, "url": "http://arxiv.org/abs/2509.21647v2", "abstract": "Machine learning interatomic potentials (MLIPs) have become powerful tools to extend molecular simulations beyond the limits of quantum methods, offering near-quantum accuracy at much lower computational cost. Yet, developing reliable MLIPs remains difficult because it requires generating high-quality datasets, preprocessing atomic structures, and carefully training and validating models. In this work, we introduce an Automated Machine Learning Pipeline (AMLP) that unifies the entire workflow from dataset creation to model validation. AMLP employs large-language-model agents to assist with electronic-structure code selection, input preparation, and output conversion, while its analysis suite (AMLP-Analysis), based on ASE supports a range of molecular simulations. The pipeline is built on the MACE architecture and validated on acridine polymorphs, where, with a straightforward fine-tuning of a foundation model, mean absolute errors of ~1.7 meV/atom in energies and ~7.0 meV/Ã in forces are achieved. The fitted MLIP reproduces DFT geometries with sub-Ã accuracy and demonstrates stability during molecular dynamics simulations in the microcanonical and canonical ensembles.", "source": "arxiv", "arxiv_id": "2509.21647v2", "pdf_url": "https://arxiv.org/pdf/2509.21647v2", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "primary_category": "cond-mat.mtrl-sci", "doi": "10.1021/acs.jctc.5c01610", "venue": "", "published": "2025-09-25T22:05:20Z", "updated": "2025-12-19T14:43:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Movie Generation via Multi-Agent CoT Planning", "authors": ["Weijia Wu", "Zeyu Zhu", "Mike Zheng Shou"], "year": 2025, "url": "http://arxiv.org/abs/2503.07314v1", "abstract": "Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.", "source": "arxiv", "arxiv_id": "2503.07314v1", "pdf_url": "https://arxiv.org/pdf/2503.07314v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-03-10T13:33:27Z", "updated": "2025-03-10T13:33:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Network Protocol Testing with LLM Agents", "authors": ["Yunze Wei", "Kaiwen Wei", "Shibo Du", "Jianyu Wang", "Zhangzhong Liu", "Yawen Wang", "Zhanyou Li", "Congcong Miao", "Xiaohui Xie", "Yong Cui"], "year": 2025, "url": "http://arxiv.org/abs/2510.13248v1", "abstract": "Network protocol testing is fundamental for modern network infrastructure. However, traditional network protocol testing methods are labor-intensive and error-prone, requiring manual interpretation of specifications, test case design, and translation into executable artifacts, typically demanding one person-day of effort per test case. Existing model-based approaches provide partial automation but still involve substantial manual modeling and expert intervention, leading to high costs and limited adaptability to diverse and evolving protocols. In this paper, we propose a first-of-its-kind system called NeTestLLM that takes advantage of multi-agent Large Language Models (LLMs) for end-to-end automated network protocol testing. NeTestLLM employs hierarchical protocol understanding to capture complex specifications, iterative test case generation to improve coverage, a task-specific workflow for executable artifact generation, and runtime feedback analysis for debugging and refinement. NeTestLLM has been deployed in a production environment for several months, receiving positive feedback from domain experts. In experiments, NeTestLLM generated 4,632 test cases for OSPF, RIP, and BGP, covering 41 historical FRRouting bugs compared to 11 by current national standards. The process of generating executable artifacts also improves testing efficiency by a factor of 8.65x compared to manual methods. NeTestLLM provides the first practical LLM-powered solution for automated end-to-end testing of heterogeneous network protocols.", "source": "arxiv", "arxiv_id": "2510.13248v1", "pdf_url": "https://arxiv.org/pdf/2510.13248v1", "categories": ["cs.NI", "cs.LG"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-10-15T07:55:15Z", "updated": "2025-10-15T07:55:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Penetration Testing with LLM Agents and Classical Planning", "authors": ["Lingzhi Wang", "Xinyi Shi", "Ziyu Li", "Yi Jiang", "Shiyu Tan", "Yuhao Jiang", "Junjie Cheng", "Wenyuan Chen", "Xiangmin Shen", "Zhenyuan LI", "Yan Chen"], "year": 2025, "url": "http://arxiv.org/abs/2512.11143v1", "abstract": "While penetration testing plays a vital role in cybersecurity, achieving fully automated, hands-off-the-keyboard execution remains a significant research challenge. In this paper, we introduce the \"Planner-Executor-Perceptor (PEP)\" design paradigm and use it to systematically review existing work and identify the key challenges in this area. We also evaluate existing penetration testing systems, with a particular focus on the use of Large Language Model (LLM) agents for this task. The results show that the out-of-the-box Claude Code and Sonnet 4.5 exhibit superior penetration capabilities observed to date, substantially outperforming all prior systems. However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools. These limitations significantly constrain its overall capability, efficiency, and stability. To address these limitations, we propose CHECKMATE, a framework that integrates enhanced classical planning with LLM agents, providing an external, structured \"brain\" that mitigates the inherent weaknesses of LLM agents. Our evaluation shows that CHECKMATE outperforms the state-of-the-art system (Claude Code) in penetration capability, improving benchmark success rates by over 20%. In addition, it delivers substantially greater stability, cutting both time and monetary costs by more than 50%.", "source": "arxiv", "arxiv_id": "2512.11143v1", "pdf_url": "https://arxiv.org/pdf/2512.11143v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-11T22:04:39Z", "updated": "2025-12-11T22:04:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Profile Inference with Language Model Agents", "authors": ["Yuntao Du", "Zitao Li", "Bolin Ding", "Yaliang Li", "Hanshen Xiao", "Jingren Zhou", "Ninghui Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.12402v1", "abstract": "Impressive progress has been made in automated problem-solving by the collaboration of large language models (LLMs) based agents. However, these automated capabilities also open avenues for malicious applications. In this paper, we study a new threat that LLMs pose to online pseudonymity, called automated profile inference, where an adversary can instruct LLMs to automatically scrape and extract sensitive personal attributes from publicly visible user activities on pseudonymous platforms. We also introduce an automated profiling framework called AutoProfiler to assess the feasibility of such threats in real-world scenarios. AutoProfiler consists of four specialized LLM agents, who work collaboratively to collect and process user online activities and generate a profile with extracted personal information. Experimental results on two real-world datasets and one synthetic dataset demonstrate that AutoProfiler is highly effective and efficient, and can be easily deployed on a web scale. We demonstrate that the inferred attributes are both sensitive and identifiable, posing significant risks of privacy breaches, such as de-anonymization and sensitive information leakage. Additionally, we explore mitigation strategies from different perspectives and advocate for increased public awareness of this emerging privacy threat to online pseudonymity.", "source": "arxiv", "arxiv_id": "2505.12402v1", "pdf_url": "https://arxiv.org/pdf/2505.12402v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-18T13:05:17Z", "updated": "2025-05-18T13:05:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Survey Collection with LLM-based Conversational Agents", "authors": ["Kurmanbek Kaiyrbekov", "Nicholas J Dobbins", "Sean D Mooney"], "year": 2025, "url": "http://arxiv.org/abs/2504.02891v1", "abstract": "Objective: Traditional phone-based surveys are among the most accessible and widely used methods to collect biomedical and healthcare data, however, they are often costly, labor intensive, and difficult to scale effectively. To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs).\n  Materials and Methods: Our framework consists of a researcher responsible for designing the survey and recruiting participants, a conversational phone agent powered by an LLM that calls participants and administers the survey, a second LLM (GPT-4o) that analyzes the conversation transcripts generated during the surveys, and a database for storing and organizing the results. To test our framework, we recruited 8 participants consisting of 5 native and 3 non-native english speakers and administered 40 surveys. We evaluated the correctness of LLM-generated conversation transcripts, accuracy of survey responses inferred by GPT-4o and overall participant experience.\n  Results: Survey responses were successfully extracted by GPT-4o from conversation transcripts with an average accuracy of 98% despite transcripts exhibiting an average per-line word error rate of 7.7%. While participants noted occasional errors made by the conversational LLM agent, they reported that the agent effectively conveyed the purpose of the survey, demonstrated good comprehension, and maintained an engaging interaction.\n  Conclusions: Our study highlights the potential of LLM agents in conducting and analyzing phone surveys for healthcare applications. By reducing the workload on human interviewers and offering a scalable solution, this approach paves the way for real-world, end-to-end AI-powered phone survey collection systems.", "source": "arxiv", "arxiv_id": "2504.02891v1", "pdf_url": "https://arxiv.org/pdf/2504.02891v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-02T18:10:19Z", "updated": "2025-04-02T18:10:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Visualization Makeovers with LLMs", "authors": ["Siddharth Gangwar", "David A. Selby", "Sebastian J. Vollmer"], "year": 2025, "url": "http://arxiv.org/abs/2508.05637v1", "abstract": "Making a good graphic that accurately and efficiently conveys the desired message to the audience is both an art and a science, typically not taught in the data science curriculum. Visualisation makeovers are exercises where the community exchange feedback to improve charts and data visualizations. Can multi-modal large language models (LLMs) emulate this task? Given a plot in the form of an image file, or the code used to generate it, an LLM, primed with a list of visualization best practices, is employed to semi-automatically generate constructive criticism to produce a better plot. Our system is centred around prompt engineering of a pre-trained model, relying on a combination of userspecified guidelines and any latent knowledge of data visualization practices that might lie within an LLMs training corpus. Unlike other works, the focus is not on generating valid visualization scripts from raw data or prompts, but on educating the user how to improve their existing data visualizations according to an interpretation of best practices. A quantitative evaluation is performed to measure the sensitivity of the LLM agent to various plotting issues across different chart types. We make the tool available as a simple self-hosted applet with an accessible Web interface.", "source": "arxiv", "arxiv_id": "2508.05637v1", "pdf_url": "https://arxiv.org/pdf/2508.05637v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-07-21T11:51:20Z", "updated": "2025-07-21T11:51:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent", "authors": ["Humza Nusrat", "Luke Francisco", "Bing Luo", "Hassan Bagher-Ebadian", "Joshua Kim", "Karen Chin-Snyder", "Salim Siddiqui", "Mira Shah", "Eric Mellon", "Mohammad Ghassemi", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"], "year": 2025, "url": "http://arxiv.org/abs/2512.20586v1", "abstract": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.", "source": "arxiv", "arxiv_id": "2512.20586v1", "pdf_url": "https://arxiv.org/pdf/2512.20586v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T18:32:17Z", "updated": "2025-12-23T18:32:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automatically Generating Web Applications from Requirements Via Multi-Agent Test-Driven Development", "authors": ["Yuxuan Wan", "Tingshuo Liang", "Jiakai Xu", "Jingyu Xiao", "Yintong Huo", "Michael R. Lyu"], "year": 2025, "url": "http://arxiv.org/abs/2509.25297v2", "abstract": "Developing full-stack web applications is complex and time-intensive, demanding proficiency across diverse technologies and frameworks. Although recent advances in multimodal large language models (MLLMs) enable automated webpage generation from visual inputs, current solutions remain limited to front-end tasks and fail to deliver fully functional applications. In this work, we introduce TDDev, the first test-driven development (TDD)-enabled LLM-agent framework for end-to-end full-stack web application generation. Given a natural language description or design image, TDDev automatically derives executable test cases, generates front-end and back-end code, simulates user interactions, and iteratively refines the implementation until all requirements are satisfied. Our framework addresses key challenges in full-stack automation, including underspecified user requirements, complex interdependencies among multiple files, and the need for both functional correctness and visual fidelity. Through extensive experiments on diverse application scenarios, TDDev achieves a 14.4% improvement on overall accuracy compared to state-of-the-art baselines, demonstrating its effectiveness in producing reliable, high-quality web applications without requiring manual intervention.", "source": "arxiv", "arxiv_id": "2509.25297v2", "pdf_url": "https://arxiv.org/pdf/2509.25297v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-29T16:18:19Z", "updated": "2025-10-01T17:32:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools", "authors": ["Ha Min Son", "Huan Ren", "Xin Liu", "Zhe Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2510.08640v2", "abstract": "Android is the largest mobile platform, yet automatically building applications remains a practical challenge. While Large Language Models (LLMs) show promise for code repair, their use for fixing Android build errors remains underexplored. To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects. Each problem is paired with a verified solution from a subsequent commit, ensuring that fixes are feasible. Second, we propose GradleFixer, an LLM agent with domain-specific tools for inspecting and manipulating the Gradle build environment. GradleFixer achieves a resolve rate of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent that relies on a general-purpose shell. GradleFixer's success suggests that while LLMs possess the high-level knowledge to solve these failures, they struggle to translate this knowledge into effective low-level actions using a general-purpose shell. We demonstrate the effectiveness of a strategy we term Tool Bridging, which replaces general-purpose shell commands with domain-aware abstractions. We hypothesize this approach works through two mechanisms: 1) it provides tools in an API-like format that LLMs use more reliably, and 2) it constrains the action space to relevant operations. This approach bridges the gap between the model's high-level reasoning and effective low-level execution.", "source": "arxiv", "arxiv_id": "2510.08640v2", "pdf_url": "https://arxiv.org/pdf/2510.08640v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-09T01:33:25Z", "updated": "2025-11-19T18:46:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents", "authors": ["Yang Liu", "Zaid Abulawi", "Abhiram Garimidi", "Doyeong Lim"], "year": 2025, "url": "http://arxiv.org/abs/2510.01398v1", "abstract": "Modern engineering increasingly relies on vast datasets generated by experiments and simulations, driving a growing demand for efficient, reliable, and broadly applicable modeling strategies. There is also heightened interest in developing data-driven approaches, particularly neural network models, for effective prediction and analysis of scientific datasets. Traditional data-driven methods frequently involve extensive manual intervention, limiting their ability to scale effectively and generalize to diverse applications. In this study, we propose an innovative pipeline utilizing Large Language Model (LLM) agents to automate data-driven modeling and analysis, with a particular emphasis on regression tasks. We evaluate two LLM-agent frameworks: a multi-agent system featuring specialized collaborative agents, and a single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both frameworks autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification (UQ). We validate our approach using a critical heat flux (CHF) prediction benchmark, involving approximately 25,000 experimental data points from the OECD/NEA benchmark dataset. Results indicate that our LLM-agent-developed model surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ on par with state-of-the-art Bayesian optimized deep neural network models developed by human experts. These outcomes underscore the significant potential of LLM-based agents to automate complex engineering modeling tasks, greatly reducing human workload while meeting or exceeding existing standards of predictive performance.", "source": "arxiv", "arxiv_id": "2510.01398v1", "pdf_url": "https://arxiv.org/pdf/2510.01398v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-01T19:28:35Z", "updated": "2025-10-01T19:28:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automating High Energy Physics Data Analysis with LLM-Powered Agents", "authors": ["Eli Gendreau-Distler", "Joshua Ho", "Dongwon Kim", "Luc Tomas Le Pottier", "Haichen Wang", "Chengxi Yang"], "year": 2025, "url": "http://arxiv.org/abs/2512.07785v1", "abstract": "We present a proof-of-principle study demonstrating the use of large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. Using the Higgs boson diphoton cross-section measurement as a case study with ATLAS Open Data, we design a hybrid system that combines an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this architecture, the workflow manager enforces reproducibility and determinism, while the agent autonomously generates, executes, and iteratively corrects analysis code in response to user instructions. We define quantitative evaluation metrics including success rate, error distribution, costs per specific task, and average number of API calls, to assess agent performance across multi-stage workflows. To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. While the workflow manager ensures deterministic execution of all analysis steps, the final outputs still show stochastic variation. Although we set the temperature to zero, other sampling parameters (e.g., top-p, top-k) remained at their defaults, and some reasoning-oriented models internally adjust these settings. Consequently, the models do not produce fully deterministic results. This study establishes the first LLM-agent-driven automated data-analysis framework in HEP, enabling systematic benchmarking of model capabilities, stability, and limitations in real-world scientific computing environments. The baseline code used in this work is available at https://huggingface.co/HWresearch/LLM4HEP. This work was accepted as a poster at the Machine Learning and the Physical Sciences (ML4PS) workshop at NeurIPS 2025. The initial submission was made on August 30, 2025.", "source": "arxiv", "arxiv_id": "2512.07785v1", "pdf_url": "https://arxiv.org/pdf/2512.07785v1", "categories": ["physics.data-an", "cs.AI", "cs.LG", "hep-ex"], "primary_category": "physics.data-an", "doi": "", "venue": "", "published": "2025-12-08T18:13:13Z", "updated": "2025-12-08T18:13:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs", "authors": ["Vincent Li", "Tim Knappe", "Yule Fu", "Kevin Han", "Kevin Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2503.11657v2", "abstract": "Large language models have demonstrated remarkable capabilities in natural language processing tasks requiring multi-step logical reasoning capabilities, such as automated theorem proving. However, challenges persist within theorem proving, such as the identification of key mathematical concepts, understanding their interrelationships, and formalizing proofs correctly within natural language. We present KG-prover, a novel framework that leverages knowledge graphs mined from reputable mathematical texts to augment general-purpose LLMs to construct and formalize mathematical proofs. We also study the effects of scaling graph-based, test-time compute using KG-Prover, demonstrating significant performance improvements over baselines across multiple datasets. General-purpose LLMs improve up to 21\\% on miniF2F-test when combined with KG-Prover, with consistent improvements ranging from 2-11\\% on the ProofNet, miniF2F-test, and MUSTARD datasets without additional scaling. Furthermore, KG-Prover with o4-mini achieves over 50% miniF2F-test. This work provides a promising approach for augmenting natural language proof reasoning with knowledge graphs without the need for additional finetuning.", "source": "arxiv", "arxiv_id": "2503.11657v2", "pdf_url": "https://arxiv.org/pdf/2503.11657v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-04T07:17:34Z", "updated": "2025-07-26T09:39:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automating Structural Engineering Workflows with Large Language Model Agents", "authors": ["Haoran Liang", "Yufa Zhou", "Mohammad Talebi Kalaleh", "Qipei Mei"], "year": 2025, "url": "http://arxiv.org/abs/2510.11004v1", "abstract": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows. Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size. Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities. We present a proof-of-concept showing that most real-world structural engineering workflows can be fully automated through a training-free LLM-based multi-agent system. MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios.", "source": "arxiv", "arxiv_id": "2510.11004v1", "pdf_url": "https://arxiv.org/pdf/2510.11004v1", "categories": ["cs.MA", "cs.AI", "cs.CE", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-10-13T04:38:46Z", "updated": "2025-10-13T04:38:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Autonomous Code Evolution Meets NP-Completeness", "authors": ["Cunxi Yu", "Rongjian Liang", "Chia-Tung Ho", "Haoxing Ren"], "year": 2025, "url": "http://arxiv.org/abs/2509.07367v1", "abstract": "Large language models (LLMs) have recently shown strong coding abilities, enabling not only static code generation but also iterative code self-evolving through agentic frameworks. Recently, AlphaEvolve \\cite{novikov2025alphaevolve} demonstrated that LLM-based coding agents can autonomously improve algorithms and surpass human experts, with scopes limited to isolated kernels spanning hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the first framework to extend LLM-based code evolution to the full repository scale, encompassing hundreds of files and tens of thousands of lines of C/C++ code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem and a cornerstone of both theory and applications. SATLUTION orchestrates LLM agents to directly evolve solver repositories under strict correctness guarantees and distributed runtime feedback, while simultaneously self-evolving its own evolution policies and rules. Starting from SAT Competition 2024 codebases and benchmark, SATLUTION evolved solvers that decisively outperformed the human-designed winners of the SAT Competition 2025, and also surpassed both 2024 and 2025 champions on the 2024 benchmarks.", "source": "arxiv", "arxiv_id": "2509.07367v1", "pdf_url": "https://arxiv.org/pdf/2509.07367v1", "categories": ["cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-09T03:28:06Z", "updated": "2025-09-09T03:28:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused Ultrasound Ablation Surgery", "authors": ["Lina Zhao", "Jiaxing Bai", "Zihao Bian", "Qingyue Chen", "Yafang Li", "Guangbo Li", "Min He", "Huaiyuan Yao", "Zongjiu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.21418v2", "abstract": "Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising non-invasive therapeutic modality, valued for its safety and precision. Nevertheless, its clinical implementation entails intricate tasks such as multimodal image interpretation, personalized dose planning, and real-time intraoperative decision-making processes that demand intelligent assistance to improve efficiency and reliability. We introduce FUAS-Agents, an autonomous agent system that leverages the multimodal understanding and tool-using capabilities of large language models (LLMs). By integrating patient profiles and MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools, including segmentation, treatment dose prediction, and clinical guideline retrieval, to generate personalized treatment plans comprising MRI image, dose parameters, and therapeutic strategies. We evaluate the system in a uterine fibroid treatment scenario. Human assessment by four senior FUAS experts indicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated 4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency, and clinical compliance, respectively. These results demonstrate the potential of LLM-driven agents in enhancing decision-making across complex clinical workflows, and exemplify a translational paradigm that combines general-purpose models with specialized expert systems to solve practical challenges in vertical healthcare domains.", "source": "arxiv", "arxiv_id": "2505.21418v2", "pdf_url": "https://arxiv.org/pdf/2505.21418v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-05-27T16:43:31Z", "updated": "2025-07-15T00:18:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents", "authors": ["Zhiping Zhang", "Yi Evie Zhang", "Freda Shi", "Tianshi Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.04465v1", "abstract": "Large Language Model (LLM) agents require personal information for personalization in order to better act on users' behalf in daily tasks, but this raises privacy concerns and a personalization-privacy dilemma. Agent's autonomy introduces both risks and opportunities, yet its effects remain unclear. To better understand this, we conducted a 3$\\times$3 between-subjects experiment ($N=450$) to study how agent's autonomy level and personalization influence users' privacy concerns, trust and willingness to use, as well as the underlying psychological processes. We find that personalization without considering users' privacy preferences increases privacy concerns and decreases trust and willingness to use. Autonomy moderates these effects: Intermediate autonomy flattens the impact of personalization compared to No- and Full autonomy conditions. Our results suggest that rather than aiming for perfect model alignment in output generation, balancing autonomy of agent's action and user control offers a promising path to mitigate the personalization-privacy dilemma.", "source": "arxiv", "arxiv_id": "2510.04465v1", "pdf_url": "https://arxiv.org/pdf/2510.04465v1", "categories": ["cs.HC", "cs.AI", "cs.CR"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-06T03:38:54Z", "updated": "2025-10-06T03:38:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents", "authors": ["Haoxuan Li", "Mingyu Derek Ma", "Jen-tse Huang", "Zhaotian Weng", "Wei Wang", "Jieyu Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2504.04855v1", "abstract": "Detecting biases in structured data is a complex and time-consuming task. Existing automated techniques are limited in diversity of data types and heavily reliant on human case-by-case handling, resulting in a lack of generalizability. Currently, large language model (LLM)-based agents have made significant progress in data science, but their ability to detect data biases is still insufficiently explored. To address this gap, we introduce the first end-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for automatic bias detection in structured data based on specific user requirements. It first develops a multi-stage plan to analyze user-specified bias detection tasks and then implements it with a diverse and well-suited set of tools. It delivers detailed results that include explanations and visualizations. To address the lack of a standardized framework for evaluating the capability of LLM agents to detect biases in data, we further propose a comprehensive benchmark that includes multiple evaluation metrics and a large set of test cases. Extensive experiments demonstrate that our framework achieves exceptional overall performance in structured data bias detection, setting a new milestone for fairer data applications.", "source": "arxiv", "arxiv_id": "2504.04855v1", "pdf_url": "https://arxiv.org/pdf/2504.04855v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-07T09:12:00Z", "updated": "2025-04-07T09:12:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question Answering", "authors": ["Costas Mavromatis", "Soji Adeshina", "Vassilis N. Ioannidis", "Zhen Han", "Qi Zhu", "Ian Robinson", "Bryan Thompson", "Huzefa Rangwala", "George Karypis"], "year": 2025, "url": "http://arxiv.org/abs/2507.04127v1", "abstract": "Knowledge graph question answering (KGQA) presents significant challenges due to the structural and semantic variations across input graphs. Existing works rely on Large Language Model (LLM) agents for graph traversal and retrieval; an approach that is sensitive to traversal initialization, as it is prone to entity linking errors and may not generalize well to custom (\"bring-your-own\") KGs. We introduce BYOKG-RAG, a framework that enhances KGQA by synergistically combining LLMs with specialized graph retrieval tools. In BYOKG-RAG, LLMs generate critical graph artifacts (question entities, candidate answers, reasoning paths, and OpenCypher queries), and graph tools link these artifacts to the KG and retrieve relevant graph context. The retrieved context enables the LLM to iteratively refine its graph linking and retrieval, before final answer generation. By retrieving context from different graph tools, BYOKG-RAG offers a more general and robust solution for QA over custom KGs. Through experiments on five benchmarks spanning diverse KG types, we demonstrate that BYOKG-RAG outperforms the second-best graph retrieval method by 4.5% points while showing better generalization to custom KGs. BYOKG-RAG framework is open-sourced at https://github.com/awslabs/graphrag-toolkit.", "source": "arxiv", "arxiv_id": "2507.04127v1", "pdf_url": "https://arxiv.org/pdf/2507.04127v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-05T18:47:14Z", "updated": "2025-07-05T18:47:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BannerAgency: Advertising Banner Design with Multimodal LLM Agents", "authors": ["Heng Wang", "Yotaro Shimose", "Shingo Takamatsu"], "year": 2025, "url": "http://arxiv.org/abs/2503.11060v2", "abstract": "Advertising banners are critical for capturing user attention and enhancing advertising campaign effectiveness. Creating aesthetically pleasing banner designs while conveying the campaign messages is challenging due to the large search space involving multiple design elements. Additionally, advertisers need multiple sizes for different displays and various versions to target different sectors of audiences. Since design is intrinsically an iterative and subjective process, flexible editability is also in high demand for practical usage. While current models have served as assistants to human designers in various design tasks, they typically handle only segments of the creative design process or produce pixel-based outputs that limit editability. This paper introduces a training-free framework for fully automated banner ad design creation, enabling frontier multimodal large language models (MLLMs) to streamline the production of effective banners with minimal manual effort across diverse marketing contexts. We present BannerAgency, an MLLM agent system that collaborates with advertisers to understand their brand identity and banner objectives, generates matching background images, creates blueprints for foreground design elements, and renders the final creatives as editable components in Figma or SVG formats rather than static pixels. To facilitate evaluation and future research, we introduce BannerRequest400, a benchmark featuring 100 unique logos paired with 400 diverse banner requests. Through quantitative and qualitative evaluations, we demonstrate the framework's effectiveness, emphasizing the quality of the generated banner designs, their adaptability to various banner requests, and their strong editability enabled by this component-based approach.", "source": "arxiv", "arxiv_id": "2503.11060v2", "pdf_url": "https://arxiv.org/pdf/2503.11060v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-03-14T03:54:05Z", "updated": "2025-08-21T01:08:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Bayesian Social Deduction with Graph-Informed Language Models", "authors": ["Shahab Rahimirad", "Guven Gergerli", "Lucia Romero", "Angela Qian", "Matthew Lyle Olson", "Simon Stepputtis", "Joseph Campbell"], "year": 2025, "url": "http://arxiv.org/abs/2506.17788v1", "abstract": "Social reasoning - inferring unobservable beliefs and intentions from partial observations of other agents - remains a challenging task for large language models (LLMs). We evaluate the limits of current reasoning language models in the social deduction game Avalon and find that while the largest models demonstrate strong performance, they require extensive test-time inference and degrade sharply when distilled to smaller, real-time-capable variants. To address this, we introduce a hybrid reasoning framework that externalizes belief inference to a structured probabilistic model, while using an LLM for language understanding and interaction. Our approach achieves competitive performance with much larger models in Agent-Agent play and, notably, is the first language agent to defeat human players in a controlled study - achieving a 67% win rate and receiving higher qualitative ratings than both reasoning baselines and human teammates. We release code, models, and a dataset to support future work on social reasoning in LLM agents, which can be found at https://camp-lab-purdue.github.io/bayesian-social-deduction/", "source": "arxiv", "arxiv_id": "2506.17788v1", "pdf_url": "https://arxiv.org/pdf/2506.17788v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-21T18:45:28Z", "updated": "2025-06-21T18:45:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Be Friendly, Not Friends: How LLM Sycophancy Shapes User Trust", "authors": ["Yuan Sun", "Ting Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.10844v2", "abstract": "Recent studies have revealed that large language model (LLM)-powered conversational agents often exhibit `sycophancy', a tendency to adapt their responses to align with user perspectives, even at the expense of factual accuracy. However, users' perceptions of LLM sycophancy and its interplay with other anthropomorphic features (e.g., friendliness) in shaping user trust remains understudied. To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) x 2 (Friendliness: high vs. low) between-subjects experiment (N = 224). Our study uncovered, for the first time, the intricate dynamics between LLM sycophancy and friendliness: When an LLM agent already exhibits a friendly demeanor, being sycophantic reduces perceived authenticity, thereby lowering user trust; Conversely, when the agent is less friendly, aligning its responses with user opinions makes it appear more genuine, leading to higher user trust. Our findings entail profound implications for AI persuasion through exploiting human psychological tendencies and highlight the imperative for responsible designs in user-LLM agent interactions.", "source": "arxiv", "arxiv_id": "2502.10844v2", "pdf_url": "https://arxiv.org/pdf/2502.10844v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-02-15T16:18:58Z", "updated": "2025-02-19T02:40:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs", "authors": ["Jiancheng Dong", "Pengyue Jia", "Jingyu Peng", "Maolin Wang", "Yuhao Wang", "Lixin Su", "Xin Sun", "Shuaiqiang Wang", "Dawei Yin", "Xiangyu Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2511.23271v1", "abstract": "Carefully engineered system prompts play a critical role in guiding the behavior of LLM agents, but their considerable length introduces significant drawbacks, including increased inference latency, higher computational cost, and reduced effective context length. This raises the question of whether such lengthy prompts can be replaced by a drastically reduced number of tokens while preserving their behavioral effect on downstream tasks. To enable this, we propose a lightweight three-stage training framework that learns a single prompt-specific Behavior-Equivalent token ([BE]). The framework first trains [BE] to encode the natural-language content of the original system prompt via reconstruction, and then distills the prompt 's downstream behavior into this single token. Importantly, our method requires no access to model internals, no auxiliary compression models, and no labeled responses. Empirical evaluations on three datasets show that a single [BE] token achieves up to a 3000x reduction in prompt length, while retaining about 98% of the downstream performance of the original system prompts. This substantially reduces inference cost and leaves almost the entire context window available for user inputs.", "source": "arxiv", "arxiv_id": "2511.23271v1", "pdf_url": "https://arxiv.org/pdf/2511.23271v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-28T15:22:52Z", "updated": "2025-11-28T15:22:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol", "authors": ["Niklas Jobs", "Luis Miguel Vieira da Silva", "Jayanth Somashekaraiah", "Maximilian Weigand", "David Kube", "Felix Gehlhoff"], "year": 2025, "url": "http://arxiv.org/abs/2512.03955v1", "abstract": "Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.", "source": "arxiv", "arxiv_id": "2512.03955v1", "pdf_url": "https://arxiv.org/pdf/2512.03955v1", "categories": ["cs.AI", "cs.ET"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-03T16:49:14Z", "updated": "2025-12-03T16:49:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Benchmarking LLM Agents for Wealth-Management Workflows", "authors": ["Rory Milsom"], "year": 2025, "url": "http://arxiv.org/abs/2512.02230v1", "abstract": "Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.", "source": "arxiv", "arxiv_id": "2512.02230v1", "pdf_url": "https://arxiv.org/pdf/2512.02230v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-01T21:56:21Z", "updated": "2025-12-01T21:56:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Benchmarking Poisoning Attacks against Retrieval-Augmented Generation", "authors": ["Baolei Zhang", "Haoran Xin", "Jiatong Li", "Dongzhe Zhang", "Minghong Fang", "Zhuqing Liu", "Lihai Nie", "Zheli Liu"], "year": 2025, "url": "http://arxiv.org/abs/2505.18543v1", "abstract": "Retrieval-Augmented Generation (RAG) has proven effective in mitigating hallucinations in large language models by incorporating external knowledge during inference. However, this integration introduces new security vulnerabilities, particularly to poisoning attacks. Although prior work has explored various poisoning strategies, a thorough assessment of their practical threat to RAG systems remains missing. To address this gap, we propose the first comprehensive benchmark framework for evaluating poisoning attacks on RAG. Our benchmark covers 5 standard question answering (QA) datasets and 10 expanded variants, along with 13 poisoning attack methods and 7 defense mechanisms, representing a broad spectrum of existing techniques. Using this benchmark, we conduct a comprehensive evaluation of all included attacks and defenses across the full dataset spectrum. Our findings show that while existing attacks perform well on standard QA datasets, their effectiveness drops significantly on the expanded versions. Moreover, our results demonstrate that various advanced RAG architectures, such as sequential, branching, conditional, and loop RAG, as well as multi-turn conversational RAG, multimodal RAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning attacks. Notably, current defense techniques fail to provide robust protection, underscoring the pressing need for more resilient and generalizable defense strategies.", "source": "arxiv", "arxiv_id": "2505.18543v1", "pdf_url": "https://arxiv.org/pdf/2505.18543v1", "categories": ["cs.CR", "cs.IR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-24T06:17:59Z", "updated": "2025-05-24T06:17:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs", "authors": ["Zhenhao Zhou", "Zhuochen Huang", "Yike He", "Chong Wang", "Jiajun Wang", "Yijian Wu", "Xin Peng", "Yiling Lou"], "year": 2025, "url": "http://arxiv.org/abs/2505.19489v1", "abstract": "The Linux kernel is a critical system, serving as the foundation for numerous systems. Bugs in the Linux kernel can cause serious consequences, affecting billions of users. Fault localization (FL), which aims at identifying the buggy code elements in software, plays an essential role in software quality assurance. While recent LLM agents have achieved promising accuracy in FL on recent benchmarks like SWE-bench, it remains unclear how well these methods perform in the Linux kernel, where FL is much more challenging due to the large-scale code base, limited observability, and diverse impact factors. In this paper, we introduce LinuxFLBench, a FL benchmark constructed from real-world Linux kernel bugs. We conduct an empirical study to assess the performance of state-of-the-art LLM agents on the Linux kernel. Our initial results reveal that existing agents struggle with this task, achieving a best top-1 accuracy of only 41.6% at file level. To address this challenge, we propose LinuxFL$^+$, an enhancement framework designed to improve FL effectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially improves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy increase) with minimal costs. Data and code are available at https://github.com/FudanSELab/LinuxFLBench.", "source": "arxiv", "arxiv_id": "2505.19489v1", "pdf_url": "https://arxiv.org/pdf/2505.19489v1", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-26T04:15:48Z", "updated": "2025-05-26T04:15:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Benevolent Dictators? On LLM Agent Behavior in Dictator Games", "authors": ["Andreas Einwiller", "Kanishka Ghosh Dastidar", "Artur Romazanov", "Annette Hautli-Janisz", "Michael Granitzer", "Florian Lemmerich"], "year": 2025, "url": "http://arxiv.org/abs/2511.08721v1", "abstract": "In behavioral sciences, experiments such as the ultimatum game are conducted to assess preferences for fairness or self-interest of study participants. In the dictator game, a simplified version of the ultimatum game where only one of two players makes a single decision, the dictator unilaterally decides how to split a fixed sum of money between themselves and the other player. Although recent studies have explored behavioral patterns of AI agents based on Large Language Models (LLMs) instructed to adopt different personas, we question the robustness of these results. In particular, many of these studies overlook the role of the system prompt - the underlying instructions that shape the model's behavior - and do not account for how sensitive results can be to slight changes in prompts. However, a robust baseline is essential when studying highly complex behavioral aspects of LLMs. To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. We found that agents often exhibit a strong preference for fairness, as well as a significant impact of the system prompt on their behavior. From a linguistic perspective, we identify that models express their responses differently. Although prompt sensitivity remains a persistent challenge, our proposed framework demonstrates a robust foundation for LLM agent behavior studies. Our code artifacts are available at https://github.com/andreaseinwiller/LLM-ABS.", "source": "arxiv", "arxiv_id": "2511.08721v1", "pdf_url": "https://arxiv.org/pdf/2511.08721v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-11T19:29:12Z", "updated": "2025-11-11T19:29:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.16982v1", "abstract": "Large Language Models (LLMs) show promise in biomedicine but lack true causal understanding, relying instead on correlations. This paper envisions causal LLM agents that integrate multimodal data (text, images, genomics, etc.) and perform intervention-based reasoning to infer cause-and-effect. Addressing this requires overcoming key challenges: designing safe, controllable agentic frameworks; developing rigorous benchmarks for causal evaluation; integrating heterogeneous data sources; and synergistically combining LLMs with structured knowledge (KGs) and formal causal inference tools. Such agents could unlock transformative opportunities, including accelerating drug discovery through automated hypothesis generation and simulation, enabling personalized medicine through patient-specific causal models. This research agenda aims to foster interdisciplinary efforts, bridging causal concepts and foundation models to develop reliable AI partners for biomedical progress.", "source": "arxiv", "arxiv_id": "2505.16982v1", "pdf_url": "https://arxiv.org/pdf/2505.16982v1", "categories": ["cs.AI", "physics.med-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-22T17:52:59Z", "updated": "2025-05-22T17:52:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Jailbreaking: Auditing Contextual Privacy in LLM Agents", "authors": ["Saswat Das", "Jameson Sandler", "Ferdinando Fioretto"], "year": 2025, "url": "http://arxiv.org/abs/2506.10171v3", "abstract": "LLM agents have begun to appear as personal assistants, customer service bots, and clinical aides. While these applications deliver substantial operational benefits, they also require continuous access to sensitive data, which increases the likelihood of unauthorized disclosures. Moreover, these disclosures go beyond mere explicit disclosure, leaving open avenues for gradual manipulation or sidechannel information leakage. This study proposes an auditing framework for conversational privacy that quantifies an agent's susceptibility to these risks. The proposed Conversational Manipulation for Privacy Leakage (CMPL) framework is designed to stress-test agents that enforce strict privacy directives against an iterative probing strategy. Rather than focusing solely on a single disclosure event or purely explicit leakage, CMPL simulates realistic multi-turn interactions to systematically uncover latent vulnerabilities. Our evaluation on diverse domains, data modalities, and safety configurations demonstrates the auditing framework's ability to reveal privacy risks that are not deterred by existing single-turn defenses, along with an in-depth longitudinal study of the temporal dynamics of leakage, strategies adopted by adaptive adversaries, and the evolution of adversarial beliefs about sensitive targets. In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations.", "source": "arxiv", "arxiv_id": "2506.10171v3", "pdf_url": "https://arxiv.org/pdf/2506.10171v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-11T20:47:37Z", "updated": "2025-09-27T20:28:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents", "authors": ["Yiding Wang", "Zhepei Wei", "Xinyu Zhu", "Yu Meng"], "year": 2025, "url": "http://arxiv.org/abs/2510.04695v1", "abstract": "Enabling large language models (LLMs) to utilize search tools offers a promising path to overcoming fundamental limitations such as knowledge cutoffs and hallucinations. Recent work has explored reinforcement learning (RL) for training search-augmented agents that interleave reasoning and retrieval before answering. These approaches usually rely on outcome-based rewards (e.g., exact match), implicitly assuming that optimizing for final answers will also yield effective intermediate search behaviors. Our analysis challenges this assumption: we uncover multiple systematic deficiencies in search that arise under outcome-only training and ultimately degrade final answer quality, including failure to invoke tools, invalid queries, and redundant searches. To address these shortcomings, we introduce DeSA (Decoupling Search-and-Answering), a simple two-stage training framework that explicitly separates search optimization from answer generation. In Stage 1, agents are trained to improve search effectiveness with retrieval recall-based rewards. In Stage 2, outcome rewards are employed to optimize final answer generation. Across seven QA benchmarks, DeSA-trained agents consistently improve search behaviors, delivering substantially higher search recall and answer accuracy than outcome-only baselines. Notably, DeSA outperforms single-stage training approaches that simultaneously optimize recall and outcome rewards, underscoring the necessity of explicitly decoupling the two objectives.", "source": "arxiv", "arxiv_id": "2510.04695v1", "pdf_url": "https://arxiv.org/pdf/2510.04695v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-06T11:09:45Z", "updated": "2025-10-06T11:09:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents", "authors": ["Gil Pasternak", "Dheeraj Rajagopal", "Julia White", "Dhruv Atreja", "Matthew Thomas", "George Hurn-Maloney", "Ash Lewis"], "year": 2025, "url": "http://arxiv.org/abs/2510.19771v2", "abstract": "LLM-based agents are increasingly moving towards proactivity: rather than awaiting instruction, they exercise agency to anticipate user needs and solve them autonomously. However, evaluating proactivity is challenging; current benchmarks are constrained to localized context, limiting their ability to test reasoning across sources and longer time horizons. To address this gap, we present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes proactivity as a pipeline of three core capabilities: (1) searching for unspecified issues, (2) identifying specific bottlenecks, and (3) executing appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular agentic frameworks, showing that even state-of-the-art models struggle to solve this benchmark. Computing our consistent measurements across frontier LLMs and agents, we find that the best end-to-end performance of 40% is achieved by both GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative capabilities of each model and analyze mutual failure modes. Our results highlight the current limitations of autonomous action in agentic systems, and expose promising future research directions.", "source": "arxiv", "arxiv_id": "2510.19771v2", "pdf_url": "https://arxiv.org/pdf/2510.19771v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-22T17:00:45Z", "updated": "2025-10-29T20:33:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models", "authors": ["Yin Jou Huang", "Rafik Hadfi"], "year": 2025, "url": "http://arxiv.org/abs/2504.08399v2", "abstract": "Self-report questionnaires have long been used to assess LLM personality traits, yet they fail to capture behavioral nuances due to biases and meta-knowledge contamination. This paper proposes a novel multi-observer framework for personality trait assessments in LLM agents that draws on informant-report methods in psychology. Instead of relying on self-assessments, we employ multiple observer agents. Each observer is configured with a specific relational context (e.g., family member, friend, or coworker) and engages the subject LLM in dialogue before evaluating its behavior across the Big Five dimensions. We show that these observer-report ratings align more closely with human judgments than traditional self-reports and reveal systematic biases in LLM self-assessments. We also found that aggregating responses from 5 to 7 observers reduces systematic biases and achieves optimal reliability. Our results highlight the role of relationship context in perceiving personality and demonstrate that a multi-observer paradigm offers a more reliable, context-sensitive approach to evaluating LLM personality traits.", "source": "arxiv", "arxiv_id": "2504.08399v2", "pdf_url": "https://arxiv.org/pdf/2504.08399v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-11T10:03:55Z", "updated": "2025-05-20T12:38:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context", "authors": ["Carnot Braun", "Rafael O. Jarczewski", "Gabriel U. Talasso", "Leandro A. Villas", "Allan M. de Souza"], "year": 2025, "url": "http://arxiv.org/abs/2511.04464v1", "abstract": "Traditional vehicle routing systems efficiently optimize singular metrics like time or distance, and when considering multiple metrics, they need more processes to optimize . However, they lack the capability to interpret and integrate the complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs. This paper introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a hybrid agentic assistant designed to augment classical pathfinding algorithms with contextual reasoning. Our approach employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm. The agent evaluates these options against user-provided tasks, preferences, and avoidance rules by leveraging a pre-processed geospatial cache of urban Points of Interest (POIs). In a benchmark of realistic urban scenarios, PAVe successfully used complex user intent into appropriate route modifications, achieving over 88% accuracy in its initial route selections with a local model. We conclude that combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization.", "source": "arxiv", "arxiv_id": "2511.04464v1", "pdf_url": "https://arxiv.org/pdf/2511.04464v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-06T15:37:11Z", "updated": "2025-11-06T15:37:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration", "authors": ["Huashan Chen", "Zhenyu Qi", "Haotang Li", "Hong Chen", "Jinfu Chen", "Kebin Peng", "In Kee Kim", "Kyu Hyung Lee", "Sen He"], "year": 2025, "url": "http://arxiv.org/abs/2510.01379v1", "abstract": "While Large Language Models (LLMs) have become the predominant paradigm for automated code generation, current single-model approaches fundamentally ignore the heterogeneous computational strengths that different models exhibit across programming languages, algorithmic domains, and development stages. This paper challenges the single-model convention by introducing a multi-stage, performance-guided orchestration framework that dynamically routes coding tasks to the most suitable LLMs within a structured generate-fix-refine workflow. Our approach is grounded in a comprehensive empirical study of 17 state-of-the-art LLMs across five programming languages (Python, Java, C++, Go, and Rust) using HumanEval-X benchmark. The study, which evaluates both functional correctness and runtime performance metrics (execution time, mean/max memory utilization, and CPU efficiency), reveals pronounced performance heterogeneity by language, development stage, and problem category. Guided by these empirical insights, we present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each task context through stage-wise validation and rollback mechanisms. Without requiring model fine-tuning, PerfOrch achieves substantial improvements over strong single-model baselines: average correctness rates of 96.22% and 91.37% on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and 49.11%. Beyond correctness gains, the framework delivers consistent performance optimizations, improving execution time for 58.76% of problems with median speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The framework's plug-and-play architecture ensures practical scalability, allowing new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm for production-grade automated software engineering that adapts to the rapidly evolving generative AI landscape.", "source": "arxiv", "arxiv_id": "2510.01379v1", "pdf_url": "https://arxiv.org/pdf/2510.01379v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-01T19:07:16Z", "updated": "2025-10-01T19:07:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL", "authors": ["Jiaxuan Gao", "Wei Fu", "Minyang Xie", "Shusheng Xu", "Chuyi He", "Zhiyu Mei", "Banghua Zhu", "Yi Wu"], "year": 2025, "url": "http://arxiv.org/abs/2508.07976v4", "abstract": "Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 78.0% and 34.3% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 100 turns and output tokens exceeding 400k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 51.1 on xBench and 58.7 on GAIA, surpassing existing open-source 32B agents. Finally, we also show that ASearcher-Web-QwQ could achieve performance of commercial systems using external summary tool in a zero-shot transfer manner and test-time search. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.", "source": "arxiv", "arxiv_id": "2508.07976v4", "pdf_url": "https://arxiv.org/pdf/2508.07976v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-11T13:36:57Z", "updated": "2025-10-26T07:06:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents", "authors": ["Wonjoong Kim", "Sangwu Park", "Yeonjun In", "Sein Kim", "Dongha Lee", "Chanyoung Park"], "year": 2025, "url": "http://arxiv.org/abs/2510.02837v1", "abstract": "Although recent tool-augmented benchmarks incorporate complex user requests and diverse tools, the evaluation methods for most of them remain limited to answer matching. However, as the number of steps required to resolve a user request increases, a proper evaluation of an agent's performance must go beyond the final answer to also assess the problem-solving trajectory, including previously ignored aspects such as efficiency, hallucination, and adaptivity. The most straightforward method for evaluating these aspects is to compare an agent's trajectory with the ground-truth trajectory, but this approach is fundamentally limited since annotating all valid ground-truth trajectories is prohibitively expensive. However, a simple LLM-based evaluator struggles to assess trajectories in detail without ground truth. To effectively evaluate the agents in this manner, we introduce TRACE, a framework for the multi-dimensional evaluation of tool-augmented LLM agent performance. By incorporating an evidence bank, which accumulates knowledge gathered from preceding reasoning steps, TRACE enables a multi-faceted analysis and evaluation of an agent's reasoning trajectory effectively. To validate our framework, we develop a new meta-evaluation dataset by augmenting existing benchmarks with diverse and flawed trajectories, each labeled with multi-faceted performance scores. Our results confirm that TRACE accurately evaluates these complex behaviors in a scalable and cost-effective manner, even with small open-source LLMs. Furthermore, we apply our method to evaluate the trajectories that agents produce while solving tool-augmented tasks, presenting previously unreported observations and their corresponding insights.", "source": "arxiv", "arxiv_id": "2510.02837v1", "pdf_url": "https://arxiv.org/pdf/2510.02837v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-03T09:19:15Z", "updated": "2025-10-03T09:19:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol (MCP) Ecosystem", "authors": ["Hao Song", "Yiming Shen", "Wenxuan Luo", "Leixin Guo", "Ting Chen", "Jiashui Wang", "Beibei Li", "Xiaosong Zhang", "Jiachi Chen"], "year": 2025, "url": "http://arxiv.org/abs/2506.02040v4", "abstract": "The Model Context Protocol (MCP) is an emerging standard designed to enable seamless interaction between Large Language Model (LLM) applications and external tools or resources. Within a short period, thousands of MCP services have been developed and deployed. However, the client-server integration architecture inherent in MCP may expand the attack surface against LLM Agent systems, introducing new vulnerabilities that allow attackers to exploit by designing malicious MCP servers. In this paper, we present the first end-to-end empirical evaluation of attack vectors targeting the MCP ecosystem. We identify four categories of attacks, i.e., Tool Poisoning Attacks, Puppet Attacks, Rug Pull Attacks, and Exploitation via Malicious External Resources. To evaluate their feasibility, we conduct experiments following the typical steps of launching an attack through malicious MCP servers: upload -> download -> attack. Specifically, we first construct malicious MCP servers and successfully upload them to three widely used MCP aggregation platforms. The results indicate that current audit mechanisms are insufficient to identify and prevent these threats. Next, through a user study and interview with 20 participants, we demonstrate that users struggle to identify malicious MCP servers and often unknowingly install them from aggregator platforms. Finally, we empirically demonstrate that these attacks can trigger harmful actions within the user's local environment, such as accessing private files or controlling devices to transfer digital assets. Additionally, based on interview results, we discuss four key challenges faced by the current MCP security ecosystem. These findings underscore the urgent need for robust security mechanisms to defend against malicious MCP servers and ensure the safe deployment of increasingly autonomous LLM agents.", "source": "arxiv", "arxiv_id": "2506.02040v4", "pdf_url": "https://arxiv.org/pdf/2506.02040v4", "categories": ["cs.CR", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-31T08:01:11Z", "updated": "2025-09-14T07:48:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks", "authors": ["Aaron Xuxiang Tian", "Ruofan Zhang", "Jiayao Tang", "Young Min Cho", "Xueqian Li", "Qiang Yi", "Ji Wang", "Zhunping Zhang", "Danrui Qi", "Zekun Li", "Xingyu Xiang", "Sharath Chandra Guntuku", "Lyle Ungar", "Tianyu Shi", "Chi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.23537v2", "abstract": "We study multi-turn multi-agent orchestration, where multiple large language model (LLM) agents interact over multiple turns by iteratively proposing answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we conduct two experiments: (i) benchmarking orchestration against single-LLM baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who authored answers and whether they can observe ongoing votes. Orchestration matches or exceeds the strongest single model and consistently outperforms the others. Analysis of best-achievable orchestration performance shows potential for further gains. The ablations show that revealing authorship increases self-voting and ties, and that showing ongoing votes amplifies herding, which speeds convergence but can sometimes yield premature consensus.", "source": "arxiv", "arxiv_id": "2509.23537v2", "pdf_url": "https://arxiv.org/pdf/2509.23537v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-28T00:15:21Z", "updated": "2025-10-01T18:39:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics", "authors": ["Ayato Kitadai", "Yusuke Fukasawa", "Nariaki Nishino"], "year": 2025, "url": "http://arxiv.org/abs/2508.18600v1", "abstract": "Large language models (LLMs) are increasingly used to simulate human decision-making, but their intrinsic biases often diverge from real human behavior--limiting their ability to reflect population-level diversity. We address this challenge with a persona-based approach that leverages individual-level behavioral data from behavioral economics to adjust model biases. Applying this method to the ultimatum game--a standard but difficult benchmark for LLMs--we observe improved alignment between simulated and empirical behavior, particularly on the responder side. While further refinement of trait representations is needed, our results demonstrate the promise of persona-conditioned LLMs for simulating human-like decision patterns at scale.", "source": "arxiv", "arxiv_id": "2508.18600v1", "pdf_url": "https://arxiv.org/pdf/2508.18600v1", "categories": ["cs.GT", "cs.MA", "econ.GN"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2025-08-26T02:02:18Z", "updated": "2025-08-26T02:02:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures (XAMT)", "authors": ["Akhil Sharma", "Shaikh Yaser Arafat", "Jai Kumar Sharma", "Ken Huang"], "year": 2025, "url": "http://arxiv.org/abs/2512.15790v1", "abstract": "The increasing operational reliance on complex Multi-Agent Systems (MAS) across safety-critical domains necessitates rigorous adversarial robustness assessment. Modern MAS are inherently heterogeneous, integrating conventional Multi-Agent Reinforcement Learning (MARL) with emerging Large Language Model (LLM) agent architectures utilizing Retrieval-Augmented Generation (RAG). A critical shared vulnerability is reliance on centralized memory components: the shared Experience Replay (ER) buffer in MARL and the external Knowledge Base (K) in RAG agents. This paper proposes XAMT (Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures), a novel framework that formalizes attack generation as a bilevel optimization problem. The Upper Level minimizes perturbation magnitude (delta) to enforce covertness while maximizing system behavior divergence toward an adversary-defined target (Lower Level). We provide rigorous mathematical instantiations for CTDE MARL algorithms and RAG-based LLM agents, demonstrating that bilevel optimization uniquely crafts stealthy, minimal-perturbation poisons evading detection heuristics. Comprehensive experimental protocols utilize SMAC and SafeRAG benchmarks to quantify effectiveness at sub-percent poison rates (less than or equal to 1 percent in MARL, less than or equal to 0.1 percent in RAG). XAMT defines a new unified class of training-time threats essential for developing intrinsically secure MAS, with implications for trust, formal verification, and defensive strategies prioritizing intrinsic safety over perimeter-based detection.", "source": "arxiv", "arxiv_id": "2512.15790v1", "pdf_url": "https://arxiv.org/pdf/2512.15790v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-15T23:04:48Z", "updated": "2025-12-15T23:04:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning", "authors": ["Haiteng Zhao", "Chang Ma", "Fangzhi Xu", "Lingpeng Kong", "Zhi-Hong Deng"], "year": 2025, "url": "http://arxiv.org/abs/2502.16660v5", "abstract": "The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at https://github.com/zhao-ht/BioMaze.", "source": "arxiv", "arxiv_id": "2502.16660v5", "pdf_url": "https://arxiv.org/pdf/2502.16660v5", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-23T17:38:10Z", "updated": "2025-07-22T11:56:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BioVerge: A Comprehensive Benchmark and Study of Self-Evaluating Agents for Biomedical Hypothesis Generation", "authors": ["Fuyi Yang", "Chenchen Ye", "Mingyu Derek Ma", "Yijia Xiao", "Matthew Yang", "Wei Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.08866v1", "abstract": "Hypothesis generation in biomedical research has traditionally centered on uncovering hidden relationships within vast scientific literature, often using methods like Literature-Based Discovery (LBD). Despite progress, current approaches typically depend on single data types or predefined extraction patterns, which restricts the discovery of novel and complex connections. Recent advances in Large Language Model (LLM) agents show significant potential, with capabilities in information retrieval, reasoning, and generation. However, their application to biomedical hypothesis generation has been limited by the absence of standardized datasets and execution environments. To address this, we introduce BioVerge, a comprehensive benchmark, and BioVerge Agent, an LLM-based agent framework, to create a standardized environment for exploring biomedical hypothesis generation at the frontier of existing scientific knowledge. Our dataset includes structured and textual data derived from historical biomedical hypotheses and PubMed literature, organized to support exploration by LLM agents. BioVerge Agent utilizes a ReAct-based approach with distinct Generation and Evaluation modules that iteratively produce and self-assess hypothesis proposals. Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis generation; and 3) self-evaluation significantly improves the novelty and relevance of proposed hypotheses.", "source": "arxiv", "arxiv_id": "2511.08866v1", "pdf_url": "https://arxiv.org/pdf/2511.08866v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-12T01:09:52Z", "updated": "2025-11-12T01:09:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization", "authors": ["Ravi Gupta", "Shabista Haider"], "year": 2025, "url": "http://arxiv.org/abs/2512.20623v1", "abstract": "Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement learning for real-time smart home lighting control on edge devices. Our approach deploys a 1-bit quantized Llama-3.2-1B model on Raspberry Pi hardware, achieving 71.4 times energy reduction compared to full-precision models while maintaining intelligent control capabilities. Through multi-objective reinforcement learning, BitRL-Light learns optimal lighting policies from user feedback, balancing energy consumption, comfort, and circadian alignment. Experimental results demonstrate 32% energy savings compared to rule-based systems, with inference latency under 200ms on Raspberry Pi 4 and 95% user satisfaction. The system processes natural language commands via Google Home/IFTTT integration and learns from implicit feedback through manual overrides. Our comparative analysis shows 1-bit models achieve 5.07 times speedup over 2-bit alternatives on ARM processors while maintaining 92% task accuracy. This work establishes a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.", "source": "arxiv", "arxiv_id": "2512.20623v1", "pdf_url": "https://arxiv.org/pdf/2512.20623v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-23T16:18:07Z", "updated": "2025-11-23T16:18:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Blueprint-Bench: Comparing spatial intelligence of LLMs, agents and image models", "authors": ["Lukas Petersson", "Axel Backlund", "Axel WennstÃ¶m", "Hanna Petersson", "Callum Sharrock", "Arash Dabiri"], "year": 2025, "url": "http://arxiv.org/abs/2509.25229v1", "abstract": "We introduce Blueprint-Bench, a benchmark designed to evaluate spatial reasoning capabilities in AI models through the task of converting apartment photographs into accurate 2D floor plans. While the input modality (photographs) is well within the training distribution of modern multimodal models, the task of spatial reconstruction requires genuine spatial intelligence: inferring room layouts, understanding connectivity, and maintaining consistent scale. We evaluate leading language models (GPT-5, Claude 4 Opus, Gemini 2.5 Pro, Grok-4), image generation models (GPT-Image, NanoBanana), and agent systems (Codex CLI, Claude Code) on a dataset of 50 apartments with approximately 20 interior images each. Our scoring algorithm measures similarity between generated and ground-truth floor plans based on room connectivity graphs and size rankings. Results reveal a significant blind spot in current AI capabilities: most models perform at or below a random baseline, while human performance remains substantially superior. Image generation models particularly struggle with instruction following, while agent-based approaches with iterative refinement capabilities show no meaningful improvement over single-pass generation. Blueprint-Bench provides the first numerical framework for comparing spatial intelligence across different model architectures. We will continue evaluating new models as they are released and welcome community submissions, monitoring for the emergence of spatial intelligence in generalist AI systems.", "source": "arxiv", "arxiv_id": "2509.25229v1", "pdf_url": "https://arxiv.org/pdf/2509.25229v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-24T23:35:26Z", "updated": "2025-09-24T23:35:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms", "authors": ["Tarek Gasmi", "Ramzi Guesmi", "Ines Belhadj", "Jihene Bennaceur"], "year": 2025, "url": "http://arxiv.org/abs/2507.06323v1", "abstract": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning models demonstrated higher exploitability despite better threat detection. Results demonstrate that architectural choices fundamentally reshape threat landscapes. This work establishes methodological foundations for cross-domain LLM agent security assessment and provides evidence-based guidance for secure deployment. Code and experimental materials are available at https: // github. com/ theconsciouslab-ai/llm-agent-security.", "source": "arxiv", "arxiv_id": "2507.06323v1", "pdf_url": "https://arxiv.org/pdf/2507.06323v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-08T18:24:28Z", "updated": "2025-07-08T18:24:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Bridging Literature and the Universe Via A Multi-Agent Large Language Model System", "authors": ["Xiaowen Zhang", "Zhenyu Bi", "Patrick Lachance", "Xuan Wang", "Tiziana Di Matteo", "Rupert A. C. Croft"], "year": 2025, "url": "http://arxiv.org/abs/2507.08958v2", "abstract": "As cosmological simulations and their associated software become increasingly complex, physicists face the challenge of searching through vast amounts of literature and user manuals to extract simulation parameters from dense academic papers, each using different models and formats. Translating these parameters into executable scripts remains a time-consuming and error-prone process. To improve efficiency in physics research and accelerate the cosmological simulation process, we introduce SimAgents, a multi-agent system designed to automate both parameter configuration from the literature and preliminary analysis for cosmology research. SimAgents is powered by specialized LLM agents capable of physics reasoning, simulation software validation, and tool execution. These agents collaborate through structured communication, ensuring that extracted parameters are physically meaningful, internally consistent, and software-compliant. We also construct a cosmological parameter extraction evaluation dataset by collecting over 40 simulations in published papers from Arxiv and leading journals that cover diverse simulation types. Experiments on the dataset demonstrate a strong performance of SimAgents, highlighting its effectiveness and potential to accelerate scientific research for physicists. Our demonstration video is available at: https://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly available at https://github.com/xwzhang98/SimAgents.", "source": "arxiv", "arxiv_id": "2507.08958v2", "pdf_url": "https://arxiv.org/pdf/2507.08958v2", "categories": ["astro-ph.IM", "astro-ph.CO", "cs.AI", "cs.MA"], "primary_category": "astro-ph.IM", "doi": "", "venue": "", "published": "2025-07-11T18:31:20Z", "updated": "2025-07-15T22:55:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop", "authors": ["Myung Ho Kim"], "year": 2025, "url": "http://arxiv.org/abs/2511.17673v3", "abstract": "Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents.", "source": "arxiv", "arxiv_id": "2511.17673v3", "pdf_url": "https://arxiv.org/pdf/2511.17673v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-21T05:19:34Z", "updated": "2026-01-11T15:54:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese", "authors": ["Peilin Zhou", "Bruce Leon", "Xiang Ying", "Can Zhang", "Yifan Shao", "Qichen Ye", "Dading Chong", "Zhiling Jin", "Chenxuan Xie", "Meng Cao", "Yuxin Gu", "Sixin Hong", "Jing Ren", "Jian Chen", "Chao Liu", "Yining Hua"], "year": 2025, "url": "http://arxiv.org/abs/2504.19314v2", "abstract": "As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.", "source": "arxiv", "arxiv_id": "2504.19314v2", "pdf_url": "https://arxiv.org/pdf/2504.19314v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-27T17:32:43Z", "updated": "2025-05-01T05:02:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks", "authors": ["Sagnik Anupam", "Davis Brown", "Shuo Li", "Eric Wong", "Hamed Hassani", "Osbert Bastani"], "year": 2025, "url": "http://arxiv.org/abs/2510.02418v2", "abstract": "LLM web agents now browse and take actions on the open web, yet current agent evaluations are constrained to sandboxed environments or artificial tasks. We introduce BrowserArena, a live open-web agent evaluation platform that collects user-submitted tasks, runs Arena-style head-to-head comparisons, and uses step-level human feedback to surface failure modes. Collecting and analyzing step-level annotations on the agent traces, we identify three consistent failure modes: captcha resolution, pop-up banner removal, and direct navigation to URLs. By constructing targeted datasets to further study these tasks, we discover variations in how different language models navigate these failure modes. We find, for example, that o4-mini deploys a wider variety of strategies to circumvent captcha resolution than other models and DeepSeek-R1 consistently misleads users about pop-up banner closure. Our findings surface both the diversity and brittleness of current web agents. More broadly, our benchmarking methodology provides an approach to evaluating and understanding web agent failure modes at scale.", "source": "arxiv", "arxiv_id": "2510.02418v2", "pdf_url": "https://arxiv.org/pdf/2510.02418v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-02T15:22:21Z", "updated": "2025-10-07T15:12:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Build Agent Advocates, Not Platform Agents", "authors": ["Sayash Kapoor", "Noam Kolt", "Seth Lazar"], "year": 2025, "url": "http://arxiv.org/abs/2505.04345v2", "abstract": "Language model agents are poised to mediate how people navigate and act online. If the companies that already dominate internet search, communication, and commerce -- or the firms trying to unseat them -- control these agents, the resulting platform agents will likely deepen surveillance, tighten lock-in, and further entrench incumbents. To resist that trajectory, this position paper argues that we should promote agent advocates: user-controlled agents that safeguard individual autonomy and choice. Doing so demands three coordinated moves: broad public access to both compute and capable AI models that are not platform-owned, open interoperability and safety standards, and market regulation that prevents platforms from foreclosing competition.", "source": "arxiv", "arxiv_id": "2505.04345v2", "pdf_url": "https://arxiv.org/pdf/2505.04345v2", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-05-07T11:45:38Z", "updated": "2025-06-19T11:55:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction", "authors": ["Tian Xia", "Tianrun Gao", "Wenhao Deng", "Long Wei", "Xiaowei Qian", "Yixian Jiang", "Chenglei Yu", "Tailin Wu"], "year": 2025, "url": "http://arxiv.org/abs/2510.16559v3", "abstract": "Engineering construction automation aims to transform natural language specifications into physically viable structures, requiring complex integrated reasoning under strict physical constraints. While modern LLMs possess broad knowledge and strong reasoning capabilities that make them promising candidates for this domain, their construction competencies remain largely unevaluated. To address this gap, we introduce BuildArena, the first physics-aligned interactive benchmark designed for language-driven engineering construction. It contributes to the community in four aspects: (1) a highly customizable benchmarking framework for in-depth comparison and analysis of LLMs; (2) an extendable task design strategy spanning static and dynamic mechanics across multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for supporting construction based on language instructions; (4) a baseline LLM agentic workflow that effectively evaluates diverse model capabilities. On eight frontier LLMs, BuildArena comprehensively evaluates their capabilities for language-driven and physics-grounded construction automation. The project page is at https://build-arena.github.io/.", "source": "arxiv", "arxiv_id": "2510.16559v3", "pdf_url": "https://arxiv.org/pdf/2510.16559v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-18T16:13:50Z", "updated": "2025-10-31T05:31:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software", "authors": ["Zehua Zhang", "Ati Priya Bajaj", "Divij Handa", "Siyu Liu", "Arvind S Raj", "Hongkai Chen", "Hulin Wang", "Yibo Liu", "Zion Leonahenahe Basque", "Souradip Nath", "Vishal Juneja", "Nikhil Chapre", "Yan Shoshitaishvili", "Adam DoupÃ©", "Chitta Baral", "Ruoyu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.25248v1", "abstract": "Automatically compiling open-source software (OSS) projects is a vital, labor-intensive, and complex task, which makes it a good challenge for LLM Agents. Existing methods rely on manually curated rules and workflows, which cannot adapt to OSS that requires customized configuration or environment setup. Recent attempts using Large Language Models (LLMs) used selective evaluation on a subset of highly rated OSS, a practice that underestimates the realistic challenges of OSS compilation. In practice, compilation instructions are often absent, dependencies are undocumented, and successful builds may even require patching source files or modifying build scripts. We propose a more challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more diverse in quality, scale, and characteristics. Furthermore, we propose a strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with enhanced build instruction retrieval module that achieves state-of-the-art performance on BUILD-BENCH and is adaptable to heterogeneous OSS characteristics. We also provide detailed analysis regarding different compilation method design choices and their influence to the whole task, offering insights to guide future advances. We believe performance on BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as a complex software engineering tasks, and, as such, our benchmark will spur innovation with a significant impact on downstream applications in the fields of software development and software security.", "source": "arxiv", "arxiv_id": "2509.25248v1", "pdf_url": "https://arxiv.org/pdf/2509.25248v1", "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-27T03:02:46Z", "updated": "2025-09-27T03:02:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Building LLM Agents by Incorporating Insights from Computer Systems", "authors": ["Yapeng Mi", "Zhi Gao", "Xiaojian Ma", "Qing Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.04485v1", "abstract": "LLM-driven autonomous agents have emerged as a promising direction in recent years. However, many of these LLM agents are designed empirically or based on intuition, often lacking systematic design principles, which results in diverse agent structures with limited generality and scalability. In this paper, we advocate for building LLM agents by incorporating insights from computer systems. Inspired by the von Neumann architecture, we propose a structured framework for LLM agentic systems, emphasizing modular design and universal principles. Specifically, this paper first provides a comprehensive review of LLM agents from the computer system perspective, then identifies key challenges and future directions inspired by computer system design, and finally explores the learning mechanisms for LLM agents beyond the computer system. The insights gained from this comparative analysis offer a foundation for systematic LLM agent design and advancement.", "source": "arxiv", "arxiv_id": "2504.04485v1", "pdf_url": "https://arxiv.org/pdf/2504.04485v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-04-06T13:38:37Z", "updated": "2025-04-06T13:38:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Byzantine-Robust Decentralized Coordination of LLM Agents", "authors": ["Yongrae Jo", "Chanik Park"], "year": 2025, "url": "http://arxiv.org/abs/2507.14928v1", "abstract": "Collaboration among multiple large language model (LLM) agents is a promising approach to overcome inherent limitations of single-agent systems, such as hallucinations and single points of failure. As LLM agents are increasingly deployed on open blockchain platforms, multi-agent systems capable of tolerating malicious (Byzantine) agents have become essential.\n  Recent Byzantine-robust multi-agent systems typically rely on leader-driven coordination, which suffers from two major drawbacks. First, they are inherently vulnerable to targeted attacks against the leader. If consecutive leaders behave maliciously, the system repeatedly fails to achieve consensus, forcing new consensus rounds, which is particularly costly given the high latency of LLM invocations. Second, an underperforming proposal from the leader can be accepted as the final answer even when higher-quality alternatives are available, as existing methods finalize the leader's proposal once it receives a quorum of votes.\n  To address these issues, we propose DecentLLMs, a novel decentralized consensus approach for multi-agent LLM systems, where worker agents generate answers concurrently and evaluator agents independently score and rank these answers to select the best available one. This decentralized architecture enables faster consensus despite the presence of Byzantine agents and consistently selects higher-quality answers through Byzantine-robust aggregation techniques.\n  Experimental results demonstrate that DecentLLMs effectively tolerates Byzantine agents and significantly improves the quality of selected answers.", "source": "arxiv", "arxiv_id": "2507.14928v1", "pdf_url": "https://arxiv.org/pdf/2507.14928v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-07-20T11:55:26Z", "updated": "2025-07-20T11:55:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution", "authors": ["Xu Huang", "Junwu Chen", "Yuxing Fei", "Zhuohan Li", "Philippe Schwaller", "Gerbrand Ceder"], "year": 2025, "url": "http://arxiv.org/abs/2512.23880v1", "abstract": "Large language model (LLM) agents currently depend on predefined tools or brittle tool generation, constraining their capability and adaptability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from \"LLM + tool use\" to \"LLM + skill acquisition\". CASCADE enables agents to master complex external tools and codify knowledge through two meta-skills: continuous learning via web search and code extraction, and self-reflection via introspection and knowledge graph exploration, among others. We evaluate CASCADE on SciSkillBench, a benchmark of 116 materials science and chemistry research tasks. CASCADE achieves a 93.3% success rate using GPT-5, compared to 35.4% without evolution mechanisms. We further demonstrate real-world applications in computational analysis, autonomous laboratory experiments, and selective reproduction of published papers. Along with human-agent collaboration and memory consolidation, CASCADE accumulates executable skills that can be shared across agents and scientists, moving toward scalable AI-assisted scientific research.", "source": "arxiv", "arxiv_id": "2512.23880v1", "pdf_url": "https://arxiv.org/pdf/2512.23880v1", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-29T21:50:23Z", "updated": "2025-12-29T21:50:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions", "authors": ["Lingyue Fu", "Xin Ding", "Yaoming Zhu", "Shao Zhang", "Lin Qiu", "Weiwen Liu", "Weinan Zhang", "Xuezhi Cao", "Xunliang Cai", "Jiaxin Ding", "Yong Yu"], "year": 2025, "url": "http://arxiv.org/abs/2510.26852v1", "abstract": "Large Language Model (LLM) agents have evolved from basic text generation to autonomously completing complex tasks through interaction with external tools. However, current benchmarks mainly assess end-to-end performance in fixed scenarios, restricting evaluation to specific skills and suffering from score saturation and growing dependence on expert annotation as agent capabilities improve. In this work, we emphasize the importance of learning ability, including both self-improvement and peer-learning, as a core driver for agent evolution toward human-level intelligence. We propose an iterative, competitive peer-learning framework, which allows agents to refine and optimize their strategies through repeated interactions and feedback, thereby systematically evaluating their learning capabilities. To address the score saturation issue in current benchmarks, we introduce CATArena, a tournament-style evaluation platform featuring four diverse board and card games with open-ended scoring. By providing tasks without explicit upper score limits, CATArena enables continuous and dynamic evaluation of rapidly advancing agent capabilities. Experimental results and analyses involving both minimal and commercial code agents demonstrate that CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.", "source": "arxiv", "arxiv_id": "2510.26852v1", "pdf_url": "https://arxiv.org/pdf/2510.26852v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-30T15:22:53Z", "updated": "2025-10-30T15:22:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CAViAR: Critic-Augmented Video Agentic Reasoning", "authors": ["Sachit Menon", "Ahmet Iscen", "Arsha Nagrani", "Tobias Weyand", "Carl Vondrick", "Cordelia Schmid"], "year": 2025, "url": "http://arxiv.org/abs/2509.07680v1", "abstract": "Video understanding has seen significant progress in recent years, with models' performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets.", "source": "arxiv", "arxiv_id": "2509.07680v1", "pdf_url": "https://arxiv.org/pdf/2509.07680v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-09-09T17:59:39Z", "updated": "2025-09-09T17:59:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents", "authors": ["Zhen Xiang", "Aliyah R. Hsu", "Austin V. Zane", "Aaron E. Kornblith", "Margaret J. Lin-Martore", "Jasmanpreet C. Kaur", "Vasuda M. Dokiparthi", "Bo Li", "Bin Yu"], "year": 2025, "url": "http://arxiv.org/abs/2505.23055v2", "abstract": "Clinical decision-making is inherently complex and fast-paced, particularly in emergency departments (EDs) where critical, rapid and high-stakes decisions are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools that combine signs, symptoms, and clinical variables into decision trees to make consistent and accurate diagnoses. CDR usage is often hindered by the clinician's cognitive load, limiting their ability to quickly recall and apply the appropriate rules. We introduce CDR-Agent, a novel LLM-based system designed to enhance ED decision-making by autonomously identifying and applying the most appropriate CDRs based on unstructured clinical notes. To validate CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\\% (synthetic) and 8.7\\% (CDR-Bench) accuracy gain relative to the standalone LLM baseline in CDR selection. Moreover, CDR-Agent significantly reduces computational overhead. Using these datasets, we demonstrated that CDR-Agent not only selects relevant CDRs efficiently, but makes cautious yet effective imaging decisions by minimizing unnecessary interventions while successfully identifying most positively diagnosed cases, outperforming traditional LLM prompting approaches. Code for our work can be found at: https://github.com/zhenxianglance/medagent-cdr-agent", "source": "arxiv", "arxiv_id": "2505.23055v2", "pdf_url": "https://arxiv.org/pdf/2505.23055v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-29T03:51:51Z", "updated": "2025-11-27T05:51:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CLAPP: The CLASS LLM Agent for Pair Programming", "authors": ["Santiago Casas", "Christian Fidler", "Boris Bolliet", "Francisco Villaescusa-Navarro", "Julien Lesgourgues"], "year": 2025, "url": "http://arxiv.org/abs/2508.05728v1", "abstract": "We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI assistant designed to support researchers working with the Einstein-Boltzmann solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific retrieval to provide conversational coding support for CLASS-answering questions, generating code, debugging errors, and producing plots. Its architecture combines multi-agent LLM orchestration, semantic search across CLASS documentation, and a live Python execution environment. Deployed as a user-friendly web application, CLAPP lowers the entry barrier for scientists unfamiliar with AI tools and enables more productive human-AI collaboration in computational and numerical cosmology. The app is available at https://classclapp.streamlit.app", "source": "arxiv", "arxiv_id": "2508.05728v1", "pdf_url": "https://arxiv.org/pdf/2508.05728v1", "categories": ["astro-ph.IM", "astro-ph.CO", "cs.AI", "cs.MA"], "primary_category": "astro-ph.IM", "doi": "", "venue": "", "published": "2025-08-07T17:35:06Z", "updated": "2025-08-07T17:35:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization", "authors": ["Weiwei Sun", "Shengyu Feng", "Shanda Li", "Yiming Yang"], "year": 2025, "url": "http://arxiv.org/abs/2504.04310v3", "abstract": "Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems -- a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agentic frameworks against established human-designed algorithms, revealing the strengths and limitations of existing LLM agents and identifying promising directions for future research. CO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.", "source": "arxiv", "arxiv_id": "2504.04310v3", "pdf_url": "https://arxiv.org/pdf/2504.04310v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-06T00:47:43Z", "updated": "2025-08-22T08:00:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents", "authors": ["Manish Bhatt", "Ronald F. Del Rosario", "Vineeth Sai Narajala", "Idan Habler"], "year": 2025, "url": "http://arxiv.org/abs/2506.01900v1", "abstract": "The meteoric rise and proliferation of autonomous Large Language Model (LLM) agents promise significant capabilities across various domains. However, their deployment is increasingly constrained by substantial computational demands, specifically for Graphics Processing Unit (GPU) resources. This paper addresses the critical problem of optimizing resource utilization in LLM agent systems. We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimation), a novel framework designed to enable autonomous LLM agents to dynamically outsource specific subtasks to specialized, cost-effective third-party LLM agents. The framework integrates mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model comparing internal execution costs against external outsourcing prices, simplified market-based decision-making algorithms, and a standardized communication protocol between LLM agents. Comprehensive validation through 239 theoretical simulations demonstrates 41.8\\% cost reduction potential, while large-scale empirical validation across 240 real LLM tasks confirms 20.3\\% cost reduction with proper epsilon-greedy exploration, establishing both theoretical viability and practical effectiveness. The emergence of proposed open standards like Google's Agent2Agent (A2A) protocol further underscores the need for frameworks like COALESCE that can leverage such standards for efficient agent interaction. By facilitating a dynamic market for agent capabilities, potentially utilizing protocols like A2A for communication, COALESCE aims to significantly reduce operational costs, enhance system scalability, and foster the emergence of specialized agent economies, making complex LLM agent functionalities more accessible and economically viable.", "source": "arxiv", "arxiv_id": "2506.01900v1", "pdf_url": "https://arxiv.org/pdf/2506.01900v1", "categories": ["cs.AI", "cs.CE", "cs.CR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-02T17:22:47Z", "updated": "2025-06-02T17:22:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "COCORELI: Cooperative, Compositional Reconstitution \\& Execution of Language Instructions", "authors": ["Swarnadeep Bhar", "Omar Naim", "Eleni Metheniti", "Bastien Navarri", "LoÃ¯c Cabannes", "Morteza Ezzabady", "Nicholas Asher"], "year": 2025, "url": "http://arxiv.org/abs/2509.04470v1", "abstract": "We present COCORELI, a hybrid agent framework designed to tackle the limitations of large language models (LLMs) in tasks requiring: following complex instructions, minimizing hallucination, and spatial reasoning. COCORELI integrates medium-sized LLM agents with novel abstraction mechanisms and a discourse module to parse instructions to in-context learn dynamic, high-level representations of the environment. Experiments on natural collaborative construction tasks show that COCORELI outperforms single-LLM CoT and agentic LLM systems, all using larger LLMs. It manages to largely avoid hallucinations, identify missing information, ask for clarifications, and update its learned objects. COCORELI's abstraction abilities extend beyond ENVIRONMENT, as shown in the ToolBench API completion task.", "source": "arxiv", "arxiv_id": "2509.04470v1", "pdf_url": "https://arxiv.org/pdf/2509.04470v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-29T11:15:57Z", "updated": "2025-08-29T11:15:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization", "authors": ["Tian Qin", "Felix Bai", "Ting-Yao Hu", "Raviteja Vemulapalli", "Hema Swetha Koppula", "Zhiyang Xu", "Bowen Jin", "Mert Cemri", "Jiarui Lu", "Zirui Wang", "Meng Cao"], "year": 2025, "url": "http://arxiv.org/abs/2510.07043v1", "abstract": "Real-world large language model (LLM) agents must master strategic tool use and user preference optimization through multi-turn interactions to assist users with complex planning tasks. We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios. We cast travel planning as a constrained preference optimization problem, where agents must satisfy hard constraints while simultaneously optimizing soft user preferences. To support this, we build a realistic travel database covering transportation, accommodation, and ticketing for 20 U.S. National Parks, along with a comprehensive tool ecosystem that mirrors commercial booking platforms. Evaluating state-of-the-art models, we uncover two critical gaps: (i) an acceptable-optimal gap, where agents reliably meet constraints but fail to optimize preferences, and (ii) a plan-coordination gap, where performance collapses on multi-service (flight and hotel) coordination tasks, especially for open-source models. By grounding reasoning and planning in a practical, user-facing domain, COMPASS provides a benchmark that directly measures an agent's ability to optimize user preferences in realistic tasks, bridging theoretical advances with real-world impact.", "source": "arxiv", "arxiv_id": "2510.07043v1", "pdf_url": "https://arxiv.org/pdf/2510.07043v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-08T14:09:46Z", "updated": "2025-10-08T14:09:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CORE: Full-Path Evaluation of LLM Agents Beyond Final State", "authors": ["Panagiotis Michelakis", "Yiannis Hadjiyiannis", "Dimitrios Stamoulis"], "year": 2025, "url": "http://arxiv.org/abs/2509.20998v1", "abstract": "Evaluating AI agents that solve real-world tasks through function-call sequences remains an open challenge. Existing agentic benchmarks often reduce evaluation to a binary judgment of the final state, overlooking critical aspects such as safety, efficiency, and intermediate correctness. We propose a framework based on deterministic finite automata (DFAs) that encodes tasks as sets of valid tool-use paths, enabling principled assessment of agent behavior in diverse world models. Building on this foundation, we introduce CORE, a suite of five metrics, namely Path Correctness, Path Correctness - Kendall's tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that quantify alignment with expected execution patterns. Across diverse worlds, our method reveals important performance differences between agents that would otherwise appear equivalent under traditional final-state evaluation schemes.", "source": "arxiv", "arxiv_id": "2509.20998v1", "pdf_url": "https://arxiv.org/pdf/2509.20998v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-25T10:49:35Z", "updated": "2025-09-25T10:49:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage", "authors": ["Bowen Wei", "Yuan Shen Tay", "Howard Liu", "Jinhao Pan", "Kun Luo", "Ziwei Zhu", "Chris Jordan"], "year": 2025, "url": "http://arxiv.org/abs/2510.00311v1", "abstract": "Security Operations Centers (SOCs) are overwhelmed by tens of thousands of daily alerts, with only a small fraction corresponding to genuine attacks. This overload creates alert fatigue, leading to overlooked threats and analyst burnout. Classical detection pipelines are brittle and context-poor, while recent LLM-based approaches typically rely on a single model to interpret logs, retrieve context, and adjudicate alerts end-to-end -- an approach that struggles with noisy enterprise data and offers limited transparency. We propose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in which specialized agents collaborate over real evidence: a behavior-analysis agent inspects activity sequences, evidence-gathering agents query external systems, and a reasoning agent synthesizes findings into an auditable decision. To support training and evaluation, we release a dataset of fine-grained SOC investigations from production environments, capturing step-by-step analyst actions and linked tool outputs. Across diverse enterprise scenarios, CORTEX substantially reduces false positives and improves investigation quality over state-of-the-art single-agent LLMs.", "source": "arxiv", "arxiv_id": "2510.00311v1", "pdf_url": "https://arxiv.org/pdf/2510.00311v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-30T22:09:31Z", "updated": "2025-09-30T22:09:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CP-AgentNet: Autonomous and Explainable Communication Protocol Design Using Generative Agents", "authors": ["Dae Cheol Kwon", "Xinyu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.17850v1", "abstract": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are crucial for achieving desired performance levels, requiring domain expertise. 2) The decision-making process in DRL models is often opaque, commonly described as a 'black box.' 3) DRL models are data hungry. In response, we propose CP-AgentNet, the first framework designed to use generative agents for developing communication network protocols. This approach addresses these challenges by creating an autonomous system for protocol design, significantly reducing human effort. We developed LLMA (LLM-agents-based multiple access) and CPTCP (CP-Agent-based TCP) for heterogeneous environments. Our comprehensive simulations have demonstrated the efficient coexistence of LLMA and CPTCP with nodes using different types of protocols, as well as enhanced explainability.", "source": "arxiv", "arxiv_id": "2503.17850v1", "pdf_url": "https://arxiv.org/pdf/2503.17850v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "10.1109/ICNP65844.2025.11192445", "venue": "Proc. IEEE 33rd International Conference on Network Protocols, ICNP 2025, Seoul, South Korea, 22-25 Sept. 2025", "published": "2025-03-22T19:58:03Z", "updated": "2025-03-22T19:58:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution", "authors": ["Minghao Shao", "Haoran Xi", "Nanda Rani", "Meet Udeshi", "Venkata Sai Charan Putrevu", "Kimberly Milner", "Brendan Dolan-Gavitt", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "year": 2025, "url": "http://arxiv.org/abs/2505.17107v1", "abstract": "Large Language Model (LLM) agents can automate cybersecurity tasks and can adapt to the evolving cybersecurity landscape without re-engineering. While LLM agents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF) competitions, they have two key limitations: accessing latest cybersecurity expertise beyond training data, and integrating new knowledge into complex task planning. Knowledge-based approaches that incorporate technical understanding into the task-solving automation can tackle these limitations. We present CRAKEN, a knowledge-based LLM agent framework that improves cybersecurity capability through three core mechanisms: contextual decomposition of task-critical information, iterative self-reflected knowledge retrieval, and knowledge-hint injection that transforms insights into adaptive attack strategies. Comprehensive evaluations with different configurations show CRAKEN's effectiveness in multi-stage vulnerability detection and exploitation compared to previous approaches. Our extensible architecture establishes new methodologies for embedding new security knowledge into LLM-driven cybersecurity agentic systems. With a knowledge database of CTF writeups, CRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works by 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution. We make our framework open source to public https://github.com/NYU-LLM-CTF/nyuctf_agents_craken.", "source": "arxiv", "arxiv_id": "2505.17107v1", "pdf_url": "https://arxiv.org/pdf/2505.17107v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-21T11:01:11Z", "updated": "2025-05-21T11:01:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CREFT: Sequential Multi-Agent LLM for Character Relation Extraction", "authors": ["Ye Eun Chun", "Taeyoon Hwang", "Seung-won Hwang", "Byung-Hak Kim"], "year": 2025, "url": "http://arxiv.org/abs/2505.24553v1", "abstract": "Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors.", "source": "arxiv", "arxiv_id": "2505.24553v1", "pdf_url": "https://arxiv.org/pdf/2505.24553v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-30T13:01:36Z", "updated": "2025-05-30T13:01:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Onkar Thorat", "Divyansh Agarwal", "Prafulla Kumar Choubey", "Yixin Mao", "Silvio Savarese", "Caiming Xiong", "Chien-Sheng Wu"], "year": 2025, "url": "http://arxiv.org/abs/2505.18878v1", "abstract": "While AI agents hold transformative potential in business, effective performance benchmarking is hindered by the scarcity of public, realistic business data on widely used platforms. Existing benchmarks often lack fidelity in their environments, data, and agent-user interactions, with limited coverage of diverse business scenarios and industries. To address these gaps, we introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena with nineteen expert-validated tasks across sales, service, and 'configure, price, and quote' processes, for both Business-to-Business and Business-to-Customer scenarios. It distinctively incorporates multi-turn interactions guided by diverse personas and robust confidentiality awareness assessments. Experiments reveal leading LLM agents achieve only around 58% single-turn success on CRMArena-Pro, with performance dropping significantly to approximately 35% in multi-turn settings. While Workflow Execution proves more tractable for top agents (over 83% single-turn success), other evaluated business skills present greater challenges. Furthermore, agents exhibit near-zero inherent confidentiality awareness; though targeted prompting can improve this, it often compromises task performance. These findings highlight a substantial gap between current LLM capabilities and enterprise demands, underscoring the need for advancements in multi-turn reasoning, confidentiality adherence, and versatile skill acquisition.", "source": "arxiv", "arxiv_id": "2505.18878v1", "pdf_url": "https://arxiv.org/pdf/2505.18878v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-24T21:33:22Z", "updated": "2025-05-24T21:33:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories", "authors": ["Yijia Xiao", "Runhui Wang", "Luyang Kong", "Davor Golac", "Wei Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.06111v2", "abstract": "The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark for Computer Science Research projects. This benchmark assesses LLMs from various aspects including accuracy, efficiency, and deployment script quality, aiming to explore their potential in conducting computer science research autonomously. We also introduce a novel framework, CSR-Agents, that utilizes multiple LLM agents to automate the deployment of GitHub code repositories of computer science research projects. Specifically, by checking instructions from markdown files and interpreting repository structures, the model generates and iteratively improves bash commands that set up the experimental environments and deploy the code to conduct research tasks. Preliminary results from CSR-Bench indicate that LLM agents can significantly enhance the workflow of repository deployment, thereby boosting developer productivity and improving the management of developmental workflows.", "source": "arxiv", "arxiv_id": "2502.06111v2", "pdf_url": "https://arxiv.org/pdf/2502.06111v2", "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-02-10T02:46:29Z", "updated": "2025-02-11T20:25:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CT-Agent: A Multimodal-LLM Agent for 3D CT Radiology Question Answering", "authors": ["Yuren Mao", "Wenyi Xu", "Yuyang Qin", "Yunjun Gao"], "year": 2025, "url": "http://arxiv.org/abs/2505.16229v1", "abstract": "Computed Tomography (CT) scan, which produces 3D volumetric medical data that can be viewed as hundreds of cross-sectional images (a.k.a. slices), provides detailed anatomical information for diagnosis. For radiologists, creating CT radiology reports is time-consuming and error-prone. A visual question answering (VQA) system that can answer radiologists' questions about some anatomical regions on the CT scan and even automatically generate a radiology report is urgently needed. However, existing VQA systems cannot adequately handle the CT radiology question answering (CTQA) task for: (1) anatomic complexity makes CT images difficult to understand; (2) spatial relationship across hundreds slices is difficult to capture. To address these issues, this paper proposes CT-Agent, a multimodal agentic framework for CTQA. CT-Agent adopts anatomically independent tools to break down the anatomic complexity; furthermore, it efficiently captures the across-slice spatial relationship with a global-local token compression strategy. Experimental results on two 3D chest CT datasets, CT-RATE and RadGenome-ChestCT, verify the superior performance of CT-Agent.", "source": "arxiv", "arxiv_id": "2505.16229v1", "pdf_url": "https://arxiv.org/pdf/2505.16229v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-05-22T04:59:20Z", "updated": "2025-05-22T04:59:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CTTS: Collective Test-Time Scaling", "authors": ["Zhende Song", "Shengji Tang", "Peng Ye", "Jiayuan Fan", "Lei Bai", "Tao Chen", "Wanli Ouyang"], "year": 2025, "url": "http://arxiv.org/abs/2508.03333v2", "abstract": "Test-time scaling (TTS) has emerged as a promising, training-free approach for enhancing large language model (LLM) performance. However, the efficacy of existing methods, such as Best-of-N and Self-Consistency, is fundamentally constrained by the dominant single test-time scaling (STTS) paradigm, which relies on a single LLM agent interacting with a single reward model (SA-SR). Inspired by recent work showing that collective methods can surpass the performance ceiling of individual models, we introduce Collective Test-Time Scaling (CTTS). First, we systematically investigate three primary interaction paradigms of existing multiple models: single-agent-multi-reward (SA-MR), multi-agent-single-reward (MA-SR), and multi-agent-multi-reward (MA-MR). Extensive experiments reveal that the MA-MR paradigm is consistently superior. Based on this finding, we further propose CTTS-MM, a novel framework that operationalizes multi-agent and multi-reward collaboration. CTTS-MM integrates two key technical contributions: (1) for agent collaboration, an Agent Collaboration Search (ACS) that identifies the most effective combination of LLMs from a candidate pool; and (2) for reward model collaboration, a Mixture of Reward Models (MoR) strategy that leverages a Prior Reward model Ensemble Selection (PRES) algorithm to select the optimal ensemble. Evaluations across seven mainstream benchmarks demonstrate that CTTS-MM significantly outperforms leading STTS methods (+4.82% over Best-of-N) and surpasses even flagship proprietary LLMs (+7.06% over GPT-4.1) and open-source LLMs. These results highlight the substantial potential of collective scaling to push the frontier of LLM inference. Code will be released at https://github.com/magent4aci/CTTS-MM.", "source": "arxiv", "arxiv_id": "2508.03333v2", "pdf_url": "https://arxiv.org/pdf/2508.03333v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-05T11:19:08Z", "updated": "2025-09-28T04:43:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities", "authors": ["Yuxuan Zhu", "Antony Kellermann", "Dylan Bowman", "Philip Li", "Akul Gupta", "Adarsh Danda", "Richard Fang", "Conner Jensen", "Eric Ihli", "Jason Benn", "Jet Geronimo", "Avi Dhir", "Sudhit Rao", "Kaicheng Yu", "Twm Stone", "Daniel Kang"], "year": 2025, "url": "http://arxiv.org/abs/2503.17332v4", "abstract": "Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture the Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized expertise to reproduce exploits and a systematic approach to evaluating unpredictable threats. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our evaluation shows that the state-of-the-art agent framework can resolve up to 13% of vulnerabilities.", "source": "arxiv", "arxiv_id": "2503.17332v4", "pdf_url": "https://arxiv.org/pdf/2503.17332v4", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-03-21T17:32:32Z", "updated": "2025-06-24T04:10:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CaRT: Teaching LLM Agents to Know When They Know Enough", "authors": ["Grace Liu", "Yuxiao Qu", "Jeff Schneider", "Aarti Singh", "Aviral Kumar"], "year": 2025, "url": "http://arxiv.org/abs/2510.08517v1", "abstract": "Many tasks require learned models to strategically gather relevant information over multiple rounds of interaction before actually acting on a task. Strategic information gathering requires models to know not only how to effectively acquire information, but also when to stop gathering information and make a decision, in order to avoid overthinking or getting derailed when acting. In this paper, we formalize this problem and introduce Counterfactuals and Reasoning for Termination (CaRT), an approach for teaching LLMs when to stop seeking information. To appropriately learn when to terminate, CaRT fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not. It trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning, and imbues this capability into the base LLM via fine-tuning. We instantiate CaRT in two domains: interactive medical diagnosis and math problem solving. In both domains, we find that CaRT improves the efficiency of information gathering and task success rate compared to other fine-tuning methods.", "source": "arxiv", "arxiv_id": "2510.08517v1", "pdf_url": "https://arxiv.org/pdf/2510.08517v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-09T17:46:39Z", "updated": "2025-10-09T17:46:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cache Mechanism for Agent RAG Systems", "authors": ["Shuhang Lin", "Zhencan Peng", "Lingyao Li", "Xiao Lin", "Xi Zhu", "Yongfeng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2511.02919v1", "abstract": "Recent advances in Large Language Model (LLM)-based agents have been propelled by Retrieval-Augmented Generation (RAG), which grants the models access to vast external knowledge bases. Despite RAG's success in improving agent performance, agent-level cache management, particularly constructing, maintaining, and updating a compact, relevant corpus dynamically tailored to each agent's need, remains underexplored. Therefore, we introduce ARC (Agent RAG Cache Mechanism), a novel, annotation-free caching framework that dynamically manages small, high-value corpora for each agent. By synthesizing historical query distribution patterns with the intrinsic geometry of cached items in the embedding space, ARC automatically maintains a high-relevance cache. With comprehensive experiments on three retrieval datasets, our experimental results demonstrate that ARC reduces storage requirements to 0.015% of the original corpus while offering up to 79.8% has-answer rate and reducing average retrieval latency by 80%. Our results demonstrate that ARC can drastically enhance efficiency and effectiveness in RAG-powered LLM agents.", "source": "arxiv", "arxiv_id": "2511.02919v1", "pdf_url": "https://arxiv.org/pdf/2511.02919v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-04T19:02:29Z", "updated": "2025-11-04T19:02:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can AI automatically analyze public opinion? A LLM agents-based agentic pipeline for timely public opinion analysis", "authors": ["Jing Liu", "Xinxing Ren", "Yanmeng Xu", "Zekun Guo"], "year": 2025, "url": "http://arxiv.org/abs/2505.11401v1", "abstract": "This study proposes and implements the first LLM agents based agentic pipeline for multi task public opinion analysis. Unlike traditional methods, it offers an end-to-end, fully automated analytical workflow without requiring domain specific training data, manual annotation, or local deployment. The pipeline integrates advanced LLM capabilities into a low-cost, user-friendly framework suitable for resource constrained environments. It enables timely, integrated public opinion analysis through a single natural language query, making it accessible to non-expert users. To validate its effectiveness, the pipeline was applied to a real world case study of the 2025 U.S. China tariff dispute, where it analyzed 1,572 Weibo posts and generated a structured, multi part analytical report. The results demonstrate some relationships between public opinion and governmental decision-making. These contributions represent a novel advancement in applying generative AI to public governance, bridging the gap between technical sophistication and practical usability in public opinion monitoring.", "source": "arxiv", "arxiv_id": "2505.11401v1", "pdf_url": "https://arxiv.org/pdf/2505.11401v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-05-16T16:09:28Z", "updated": "2025-05-16T16:09:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can Generative AI agents behave like humans? Evidence from laboratory market experiments", "authors": ["R. Maria del Rio-Chanona", "Marco Pangallo", "Cars Hommes"], "year": 2025, "url": "http://arxiv.org/abs/2505.07457v1", "abstract": "We explore the potential of Large Language Models (LLMs) to replicate human behavior in economic market experiments. Compared to previous studies, we focus on dynamic feedback between LLM agents: the decisions of each LLM impact the market price at the current step, and so affect the decisions of the other LLMs at the next step. We compare LLM behavior to market dynamics observed in laboratory settings and assess their alignment with human participants' behavior. Our findings indicate that LLMs do not adhere strictly to rational expectations, displaying instead bounded rationality, similarly to human participants. Providing a minimal context window i.e. memory of three previous time steps, combined with a high variability setting capturing response heterogeneity, allows LLMs to replicate broad trends seen in human experiments, such as the distinction between positive and negative feedback markets. However, differences remain at a granular level--LLMs exhibit less heterogeneity in behavior than humans. These results suggest that LLMs hold promise as tools for simulating realistic human behavior in economic contexts, though further research is needed to refine their accuracy and increase behavioral diversity.", "source": "arxiv", "arxiv_id": "2505.07457v1", "pdf_url": "https://arxiv.org/pdf/2505.07457v1", "categories": ["econ.GN", "cs.AI"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2025-05-12T11:44:46Z", "updated": "2025-05-12T11:44:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can LLM Agents Maintain a Persona in Discourse?", "authors": ["Pranav Bhandari", "Nicolas Fay", "Michael Wise", "Amitava Datta", "Stephanie Meek", "Usman Naseem", "Mehwish Nasim"], "year": 2025, "url": "http://arxiv.org/abs/2502.11843v1", "abstract": "Large Language Models (LLMs) are widely used as conversational agents, exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.", "source": "arxiv", "arxiv_id": "2502.11843v1", "pdf_url": "https://arxiv.org/pdf/2502.11843v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T14:36:39Z", "updated": "2025-02-17T14:36:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can LLM Agents Really Debate? A Controlled Study of Multi-Agent Debate in Logical Reasoning", "authors": ["Haolun Wu", "Zhenkun Li", "Lingyao Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.07784v1", "abstract": "Multi-agent debate (MAD) has recently emerged as a promising framework for improving the reasoning performance of large language models (LLMs). Yet, whether LLM agents can genuinely engage in deliberative reasoning, beyond simple ensembling or majority voting, remains unclear. We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. We systematically set up six structural and cognitive factors, including agent team size, composition, confidence visibility, debate order, debate depth, and task difficulty, to disentangle their respective effects on collective reasoning. Our results show that intrinsic reasoning strength and group diversity are the dominant drivers of debate success, while structural parameters such as order or confidence visibility offer limited gains. Beyond outcomes, process-level analyses identify key behavioral patterns: majority pressure suppresses independent correction, effective teams overturn incorrect consensus, and rational, validity-aligned reasoning most strongly predicts improvement. These findings provide valuable insights into how and why LLM debates succeed or fail, offering guidance for designing interpretable and truth-seeking multi-agent reasoning systems.", "source": "arxiv", "arxiv_id": "2511.07784v1", "pdf_url": "https://arxiv.org/pdf/2511.07784v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-11-11T03:05:47Z", "updated": "2025-11-11T03:05:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can LLM Agents Simulate Multi-Turn Human Behavior? Evidence from Real Online Customer Behavior Data", "authors": ["Yuxuan Lu", "Jing Huang", "Yan Han", "Bingsheng Yao", "Sisong Bei", "Jiri Gesi", "Yaochen Xie", "Zheshen", "Wang", "Qi He", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2503.20749v7", "abstract": "Recent research shows that LLM Agents can generate ``believable'' human behaviors via prompt-only methods, and such agents have been increasingly adopted in downstream applications. However, existing evaluation of these agents only focuses on qualitative believability (whether human raters think they are accurate), leaving open questions of whether LLM agents can accurately generate step-by-step actions mimicking a particular human's behavior in a multi-turn interaction task. In this work, we take shopping as a case study and present the first large-scale quantitative evaluation of state-of-the-art LLMs' ability to accurately simulate human behavior. Using real-world data from 31,865 online shopping sessions containing 230,965 user actions, our evaluation reveals that prompt-based LLMs (DeepSeek-R1, Llama, Claude) achieve only 11.86% accuracy in generating human actions, highlighting a substantial gap in actual behavioral accuracy. Through experiments, we also showcase that strategies as simple as fine-tuning LLMs on real human click-through data augmented with synthesized reasoning traces can greatly enhance models' performance. The fine-tuned Qwen2.5-7B achieves 17.26% action generation accuracy and 33.86% F1 score on final purchase prediction, representing substantial improvements of 5.4% and 13.85% over prompt-only baselines. This work establishes the first rigorous benchmark for human behavior simulation and provides actionable insights for developing more accurate LLM agents for future downstream applications.", "source": "arxiv", "arxiv_id": "2503.20749v7", "pdf_url": "https://arxiv.org/pdf/2503.20749v7", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-26T17:33:27Z", "updated": "2025-10-08T20:51:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination", "authors": ["JoÃ£o Vitor de Carvalho Silva", "Douglas G. Macharet"], "year": 2025, "url": "http://arxiv.org/abs/2508.14635v1", "abstract": "The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems. Large Language Models (LLMs) have shown strong capabilities in communication, planning, and reasoning, raising the question of whether they can also support effective collaboration in multi-agent settings. In this work, we investigate the use of LLM agents to solve a structured victim rescue task that requires division of labor, prioritization, and cooperative planning. Agents operate in a fully known graph-based environment and must allocate resources to victims with varying needs and urgency levels. We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "source": "arxiv", "arxiv_id": "2508.14635v1", "pdf_url": "https://arxiv.org/pdf/2508.14635v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-08-20T11:44:10Z", "updated": "2025-08-20T11:44:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments", "authors": ["Harsh Vishwakarma", "Ankush Agarwal", "Ojas Patil", "Chaitanya Devaguptapu", "Mahesh Chandran"], "year": 2025, "url": "http://arxiv.org/abs/2510.27287v1", "abstract": "Enterprise systems are crucial for enhancing productivity and decision-making among employees and customers. Integrating LLM based systems into enterprise systems enables intelligent automation, personalized experiences, and efficient information retrieval, driving operational efficiency and strategic growth. However, developing and evaluating such systems is challenging due to the inherent complexity of enterprise environments, where data is fragmented across multiple sources and governed by sophisticated access controls. We present EnterpriseBench, a comprehensive benchmark that simulates enterprise settings, featuring 500 diverse tasks across software engineering, HR, finance, and administrative domains. Our benchmark uniquely captures key enterprise characteristics including data source fragmentation, access control hierarchies, and cross-functional workflows. Additionally, we provide a novel data generation pipeline that creates internally consistent enterprise tasks from organizational metadata. Experiments with state-of-the-art LLM agents demonstrate that even the most capable models achieve only 41.8% task completion, highlighting significant opportunities for improvement in enterprise-focused AI systems.", "source": "arxiv", "arxiv_id": "2510.27287v1", "pdf_url": "https://arxiv.org/pdf/2510.27287v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-31T08:55:13Z", "updated": "2025-10-31T08:55:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?", "authors": ["Daniele Proverbio", "Alessio Buscemi", "Alessandro Di Stefano", "The Anh Han", "German Castignani", "Pietro LiÃ²"], "year": 2025, "url": "http://arxiv.org/abs/2508.05670v1", "abstract": "Game theory has long served as a foundational tool in cybersecurity to test, predict, and design strategic interactions between attackers and defenders. The recent advent of Large Language Models (LLMs) offers new tools and challenges for the security of computer systems; In this work, we investigate whether classical game-theoretic frameworks can effectively capture the behaviours of LLM-driven actors and bots. Using a reproducible framework for game-theoretic LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to expected outcomes or exhibit deviations due to embedded biases. Our experiments involve four state-of-the-art LLMs and span five natural languages, English, French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic sensitivity. For both games, we observe that the final payoffs are influenced by agents characteristics such as personality traits or knowledge of repeated rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to the choice of languages, which should warn against indiscriminate application of LLMs in cybersecurity applications and call for in-depth studies, as LLMs may behave differently when deployed in different countries. We also employ quantitative metrics to evaluate the internal consistency and cross-language stability of LLM agents, to help guide the selection of the most stable LLMs and optimising models for secure applications.", "source": "arxiv", "arxiv_id": "2508.05670v1", "pdf_url": "https://arxiv.org/pdf/2508.05670v1", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.GT"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-04T08:57:14Z", "updated": "2025-08-04T08:57:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can Large Language Model Agents Balance Energy Systems?", "authors": ["Xinxing Ren", "Chun Sing Lai", "Gareth Taylor", "Zekun Guo"], "year": 2025, "url": "http://arxiv.org/abs/2502.10557v2", "abstract": "This paper presents a hybrid approach that integrates Large Language Models (LLMs) with a multi-scenario Stochastic Unit Commitment (SUC) framework to enhance both efficiency and reliability under high wind generation uncertainties. In a 10-trial study on the test energy system, the traditional SUC approach incurs an average total cost of 187.68 million dollars, whereas the LLM-assisted SUC (LLM-SUC) achieves a mean cost of 185.58 million dollars (range: 182.61 to 188.65 million dollars), corresponding to a cost reduction of 1.1 to 2.7 percent. Furthermore, LLM-SUC reduces load curtailment by 26.3 percent (2.24 plus/minus 0.31 GWh versus 3.04 GWh for SUC), while both methods maintain zero wind curtailment. Detailed temporal analysis shows that LLM-SUC achieves lower costs in the majority of time intervals and consistently outperforms SUC in 90 percent of cases, with solutions clustering in a favorable cost-reliability region (Coefficient of Variation = 0.93 percent for total cost and 13.8 percent for load curtailment). By leveraging an LLM agent to guide generator commitment decisions and dynamically adjust to stochastic conditions, the proposed framework improves demand fulfillment and operational resilience.", "source": "arxiv", "arxiv_id": "2502.10557v2", "pdf_url": "https://arxiv.org/pdf/2502.10557v2", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2025-02-14T21:11:53Z", "updated": "2025-03-30T13:35:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can Large Language Models Trade? Testing Financial Theories with LLM Agents in Market Simulations", "authors": ["Alejandro Lopez-Lira"], "year": 2025, "url": "http://arxiv.org/abs/2504.10789v1", "abstract": "This paper presents a realistic simulated stock market where large language models (LLMs) act as heterogeneous competing trading agents. The open-source framework incorporates a persistent order book with market and limit orders, partial fills, dividends, and equilibrium clearing alongside agents with varied strategies, information sets, and endowments. Agents submit standardized decisions using structured outputs and function calls while expressing their reasoning in natural language. Three findings emerge: First, LLMs demonstrate consistent strategy adherence and can function as value investors, momentum traders, or market makers per their instructions. Second, market dynamics exhibit features of real financial markets, including price discovery, bubbles, underreaction, and strategic liquidity provision. Third, the framework enables analysis of LLMs' responses to varying market conditions, similar to partial dependence plots in machine-learning interpretability. The framework allows simulating financial theories without closed-form solutions, creating experimental designs that would be costly with human participants, and establishing how prompts can generate correlated behaviors affecting market stability.", "source": "arxiv", "arxiv_id": "2504.10789v1", "pdf_url": "https://arxiv.org/pdf/2504.10789v1", "categories": ["q-fin.CP", "econ.GN", "q-fin.GN", "q-fin.TR"], "primary_category": "q-fin.CP", "doi": "", "venue": "", "published": "2025-04-15T01:18:36Z", "updated": "2025-04-15T01:18:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can Memory-Augmented LLM Agents Aid Journalism in Interpreting and Framing News for Diverse Audiences?", "authors": ["Leyi Ouyang"], "year": 2025, "url": "http://arxiv.org/abs/2507.21055v2", "abstract": "Modern news is often comprehensive, weaving together information from diverse domains, including technology, finance, and agriculture. This very comprehensiveness creates a challenge for interpretation, as audiences typically possess specialized knowledge related to their expertise, age, or standpoint. Consequently, a reader might fully understand the financial implications of a story but fail to grasp or even actively misunderstand its legal or technological dimensions, resulting in critical comprehension gaps. In this work, we investigate how to identify these comprehension gaps and provide solutions to improve audiences' understanding of news content, particularly in the aspects of articles outside their primary domains of knowledge. We propose MADES, an agent-based framework designed to simulate societal communication. The framework utilizes diverse agents, each configured to represent a specific occupation or age group. Each agent is equipped with a memory system. These agents are then simulated to discuss the news. This process enables us to monitor and analyze their behavior and cognitive processes. Our findings indicate that the framework can identify confusions and misunderstandings within news content through its iterative discussion process. Based on these accurate identifications, the framework then designs supplementary material. We validated these outcomes using both statistical analysis and human evaluation, and the results show that agents exhibit significantly improved news understanding after receiving this supplementary material.", "source": "arxiv", "arxiv_id": "2507.21055v2", "pdf_url": "https://arxiv.org/pdf/2507.21055v2", "categories": ["cs.CY", "cs.AI", "cs.SI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-04-30T13:43:18Z", "updated": "2025-08-02T22:21:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can Theoretical Physics Research Benefit from Language Agents?", "authors": ["Sirui Lu", "Zhijing Jin", "Terry Jingchen Zhang", "Pavel Kos", "J. Ignacio Cirac", "Bernhard SchÃ¶lkopf"], "year": 2025, "url": "http://arxiv.org/abs/2506.06214v1", "abstract": "Large Language Models (LLMs) are rapidly advancing across diverse domains, yet their application in theoretical physics research is not yet mature. This position paper argues that LLM agents can potentially help accelerate theoretical, computational, and applied physics when properly integrated with domain knowledge and toolbox. We analyze current LLM capabilities for physics -- from mathematical reasoning to code generation -- identifying critical gaps in physical intuition, constraint satisfaction, and reliable reasoning. We envision future physics-specialized LLMs that could handle multimodal data, propose testable hypotheses, and design experiments. Realizing this vision requires addressing fundamental challenges: ensuring physical consistency, and developing robust verification methods. We call for collaborative efforts between physics and AI communities to help advance scientific discovery in physics.", "source": "arxiv", "arxiv_id": "2506.06214v1", "pdf_url": "https://arxiv.org/pdf/2506.06214v1", "categories": ["cs.CL", "cs.AI", "math-ph", "quant-ph"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-06T16:20:06Z", "updated": "2025-06-06T16:20:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation", "authors": ["Alyssa Unell", "Noel C. F. Codella", "Sam Preston", "Peniel Argaw", "Wen-wai Yim", "Zelalem Gero", "Cliff Wong", "Rajesh Jena", "Eric Horvitz", "Amanda K. Hall", "Ruican Rachel Zhong", "Jiachen Li", "Shrey Jain", "Mu Wei", "Matthew Lungren", "Hoifung Poon"], "year": 2025, "url": "http://arxiv.org/abs/2509.07325v2", "abstract": "The National Comprehensive Cancer Network (NCCN) provides evidence-based guidelines for cancer treatment. Translating complex patient presentations into guideline-compliant treatment recommendations is time-intensive, requires specialized expertise, and is prone to error. Advances in large language model (LLM) capabilities promise to reduce the time required to generate treatment recommendations and improve accuracy. We present an LLM agent-based approach to automatically generate guideline-concordant treatment trajectories for patients with non-small cell lung cancer (NSCLC). Our contributions are threefold. First, we construct a novel longitudinal dataset of 121 cases of NSCLC patients that includes clinical encounters, diagnostic results, and medical histories, each expertly annotated with the corresponding NCCN guideline trajectories by board-certified oncologists. Second, we demonstrate that existing LLMs possess domain-specific knowledge that enables high-quality proxy benchmark generation for both model development and evaluation, achieving strong correlation (Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks. Third, we develop a hybrid approach combining expensive human annotations with model consistency information to create both the agent framework that predicts the relevant guidelines for a patient, as well as a meta-classifier that verifies prediction accuracy with calibrated confidence scores for treatment recommendations (AUROC=0.800), a critical capability for communicating the accuracy of outputs, custom-tailoring tradeoffs in performance, and supporting regulatory compliance. This work establishes a framework for clinically viable LLM-based guideline adherence systems that balance accuracy, interpretability, and regulatory requirements while reducing annotation costs, providing a scalable pathway toward automated clinical decision support.", "source": "arxiv", "arxiv_id": "2509.07325v2", "pdf_url": "https://arxiv.org/pdf/2509.07325v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-09T01:49:29Z", "updated": "2025-11-06T18:38:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Capturing Semantic Flow of ML-based Systems", "authors": ["Shin Yoo", "Robert Feldt", "Somin Kim", "Naryeong Kim"], "year": 2025, "url": "http://arxiv.org/abs/2503.10310v1", "abstract": "ML-based systems are software systems that incorporates machine learning components such as Deep Neural Networks (DNNs) or Large Language Models (LLMs). While such systems enable advanced features such as high performance computer vision, natural language processing, and code generation, their internal behaviour remain largely opaque to traditional dynamic analysis such as testing: existing analysis typically concern only what is observable from the outside, such as input similarity or class label changes. We propose semantic flow, a concept designed to capture the internal behaviour of ML-based system and to provide a platform for traditional dynamic analysis techniques to be adapted to. Semantic flow combines the idea of control flow with internal states taken from executions of ML-based systems, such as activation values of a specific layer in a DNN, or embeddings of LLM responses at a specific inference step of LLM agents. The resulting representation, summarised as semantic flow graphs, can capture internal decisions that are not explicitly represented in the traditional control flow of ML-based systems. We propose the idea of semantic flow, introduce two examples using a DNN and an LLM agent, and finally sketch its properties and how it can be used to adapt existing dynamic analysis techniques for use in ML-based software systems.", "source": "arxiv", "arxiv_id": "2503.10310v1", "pdf_url": "https://arxiv.org/pdf/2503.10310v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-13T12:39:04Z", "updated": "2025-03-13T12:39:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent", "authors": ["Akash Kumar Panda", "Olaoluwa Adigun", "Bart Kosko"], "year": 2025, "url": "http://arxiv.org/abs/2509.25593v1", "abstract": "A large language model (LLM) can map a feedback causal fuzzy cognitive map (FCM) into text and then reconstruct the FCM from the text. This explainable AI system approximates an identity map from the FCM to itself and resembles the operation of an autoencoder (AE). Both the encoder and the decoder explain their decisions in contrast to black-box AEs. Humans can read and interpret the encoded text in contrast to the hidden variables and synaptic webs in AEs. The LLM agent approximates the identity map through a sequence of system instructions that does not compare the output to the input. The reconstruction is lossy because it removes weak causal edges or rules while it preserves strong causal edges. The encoder preserves the strong causal edges even when it trades off some details about the FCM to make the text sound more natural.", "source": "arxiv", "arxiv_id": "2509.25593v1", "pdf_url": "https://arxiv.org/pdf/2509.25593v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T23:33:53Z", "updated": "2025-09-29T23:33:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks", "authors": ["Qi Chai", "Zhang Zheng", "Junlong Ren", "Deheng Ye", "Zichuan Lin", "Hao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.18797v1", "abstract": "Minecraft, as an open-world virtual interactive environment, has become a prominent platform for research on agent decision-making and execution. Existing works primarily adopt a single Large Language Model (LLM) agent to complete various in-game tasks. However, for complex tasks requiring lengthy sequences of actions, single-agent approaches often face challenges related to inefficiency and limited fault tolerance. Despite these issues, research on multi-agent collaboration remains scarce. In this paper, we propose CausalMACE, a holistic causality planning framework designed to enhance multi-agent systems, in which we incorporate causality to manage dependencies among subtasks. Technically, our proposed framework introduces two modules: an overarching task graph for global task planning and a causality-based module for dependency management, where inherent rules are adopted to perform causal intervention. Experimental results demonstrate our approach achieves state-of-the-art performance in multi-agent cooperative tasks of Minecraft.", "source": "arxiv", "arxiv_id": "2508.18797v1", "pdf_url": "https://arxiv.org/pdf/2508.18797v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-26T08:29:05Z", "updated": "2025-08-26T08:29:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CellTypeAgent: Trustworthy cell type annotation with Large Language Models", "authors": ["Jiawen Chen", "Jianghao Zhang", "Huaxiu Yao", "Yun Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.08844v1", "abstract": "Cell type annotation is a critical yet laborious step in single-cell RNA sequencing analysis. We present a trustworthy large language model (LLM)-agent, CellTypeAgent, which integrates LLMs with verification from relevant databases. CellTypeAgent achieves higher accuracy than existing methods while mitigating hallucinations. We evaluated CellTypeAgent across nine real datasets involving 303 cell types from 36 tissues. This combined approach holds promise for more efficient and reliable cell type annotation.", "source": "arxiv", "arxiv_id": "2505.08844v1", "pdf_url": "https://arxiv.org/pdf/2505.08844v1", "categories": ["q-bio.GN", "cs.AI"], "primary_category": "q-bio.GN", "doi": "", "venue": "", "published": "2025-05-13T14:34:11Z", "updated": "2025-05-13T14:34:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment, Distribution, and Discovery", "authors": ["Balaji Rama", "Kai Mei", "Yongfeng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.11444v1", "abstract": "Autonomous LLM-based agents have emerged as a powerful paradigm for complex task execution, yet the field lacks standardized tools for development, deployment, distribution and discovery of agents. We present Cerebrum, an Agent SDK for AIOS that addresses this gap through three key components: (1) a comprehensive SDK featuring a modular four-layer architecture for agent development, encompassing LLM, memory, storage, and tool management; (2) a community-driven Agent Hub for sharing and discovering agents, complete with version control and dependency management; (3) an interactive web interface for testing and evaluating agents. The platform's effectiveness is demonstrated through implementations of various agent architectures, including Chain of Thought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by providing a unified framework that standardizes agent development while maintaining flexibility for researchers and developers to innovate and distribute their agents. The live website is at https://app.aios.foundation, the code is at https://github.com/agiresearch/Cerebrum, and video is at https://app.aios.foundation/video-demo.", "source": "arxiv", "arxiv_id": "2503.11444v1", "pdf_url": "https://arxiv.org/pdf/2503.11444v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.OS"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-03-14T14:29:17Z", "updated": "2025-03-14T14:29:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Characterizing LLM-driven Social Network: The Chirper.ai Case", "authors": ["Yiming Zhu", "Yupeng He", "Ehsan-Ul Haq", "Gareth Tyson", "Pan Hui"], "year": 2025, "url": "http://arxiv.org/abs/2504.10286v1", "abstract": "Large language models (LLMs) demonstrate the ability to simulate human decision-making processes, enabling their use as agents in modeling sophisticated social networks, both offline and online. Recent research has explored collective behavioral patterns and structural characteristics of LLM agents within simulated networks. However, empirical comparisons between LLM-driven and human-driven online social networks remain scarce, limiting our understanding of how LLM agents differ from human users. This paper presents a large-scale analysis of Chirper.ai, an X/Twitter-like social network entirely populated by LLM agents, comprising over 65,000 agents and 7.7 million AI-generated posts. For comparison, we collect a parallel dataset from Mastodon, a human-driven decentralized social network, with over 117,000 users and 16 million posts. We examine key differences between LLM agents and humans in posting behaviors, abusive content, and social network structures. Our findings provide critical insights into the evolving landscape of online social network analysis in the AI era, offering a comprehensive profile of LLM agents in social simulations.", "source": "arxiv", "arxiv_id": "2504.10286v1", "pdf_url": "https://arxiv.org/pdf/2504.10286v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-04-14T14:53:31Z", "updated": "2025-04-14T14:53:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution", "authors": ["Kanika Goswami", "Puneet Mathur", "Ryan Rossi", "Franck Dernoncourt"], "year": 2025, "url": "http://arxiv.org/abs/2502.00989v1", "abstract": "Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.", "source": "arxiv", "arxiv_id": "2502.00989v1", "pdf_url": "https://arxiv.org/pdf/2502.00989v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-03T02:00:51Z", "updated": "2025-02-03T02:00:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents", "authors": ["Hwan Chang", "Yonghyun Jun", "Hwanhee Lee"], "year": 2025, "url": "http://arxiv.org/abs/2509.22830v1", "abstract": "The growing deployment of large language model (LLM) based agents that interact with external environments has created new attack surfaces for adversarial manipulation. One major threat is indirect prompt injection, where attackers embed malicious instructions in external environment output, causing agents to interpret and execute them as if they were legitimate prompts. While previous research has focused primarily on plain-text injection attacks, we find a significant yet underexplored vulnerability: LLMs' dependence on structured chat templates and their susceptibility to contextual manipulation through persuasive multi-turn dialogues. To this end, we introduce ChatInject, an attack that formats malicious payloads to mimic native chat templates, thereby exploiting the model's inherent instruction-following tendencies. Building on this foundation, we develop a persuasion-driven Multi-turn variant that primes the agent across conversational turns to accept and execute otherwise suspicious actions. Through comprehensive experiments across frontier LLMs, we demonstrate three critical findings: (1) ChatInject achieves significantly higher average attack success rates than traditional prompt injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13% to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong performance at average 52.33% success rate on InjecAgent, (2) chat-template-based payloads demonstrate strong transferability across models and remain effective even against closed-source LLMs, despite their unknown template structures, and (3) existing prompt-based defenses are largely ineffective against this attack approach, especially against Multi-turn variants. These findings highlight vulnerabilities in current agent systems.", "source": "arxiv", "arxiv_id": "2509.22830v1", "pdf_url": "https://arxiv.org/pdf/2509.22830v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-26T18:38:07Z", "updated": "2025-09-26T18:38:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents", "authors": ["Hao Liang", "Zhipeng Dong", "Kaixin Chen", "Jiyuan Guo", "Yufeng Yue", "Yi Yang", "Mengyin Fu"], "year": 2025, "url": "http://arxiv.org/abs/2503.14948v2", "abstract": "Surround-view perception has garnered significant attention for its ability to enhance the perception capabilities of autonomous driving vehicles through the exchange of information with surrounding cameras. However, existing surround-view perception systems are limited by inefficiencies in unidirectional interaction pattern with human and distortions in overlapping regions exponentially propagating into non-overlapping areas. To address these challenges, this paper introduces ChatStitch, a surround-view human-machine co-perception system capable of unveiling obscured blind spot information through natural language commands integrated with external digital assets. To dismantle the unidirectional interaction bottleneck, ChatStitch implements a cognitively grounded closed-loop interaction multi-agent framework based on Large Language Models. To suppress distortion propagation across overlapping boundaries, ChatStitch proposes SV-UDIS, a surround-view unsupervised deep image stitching method under the non-global-overlapping condition. We conducted extensive experiments on the UDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our SV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for 3, 4, and 5 image stitching tasks, with PSNR improvements of 9\\%, 17\\%, and 21\\%, and SSIM improvements of 8\\%, 18\\%, and 26\\%, respectively.", "source": "arxiv", "arxiv_id": "2503.14948v2", "pdf_url": "https://arxiv.org/pdf/2503.14948v2", "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-03-19T07:25:21Z", "updated": "2025-05-23T03:48:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ChatVis: Large Language Model Agent for Generating Scientific Visualizations", "authors": ["Tom Peterka", "Tanwi Mallick", "Orcun Yildiz", "David Lenz", "Cory Quammen", "Berk Geveci"], "year": 2025, "url": "http://arxiv.org/abs/2507.23096v1", "abstract": "Large language models (LLMs) are rapidly increasing in capability, but they still struggle with highly specialized programming tasks such as scientific visualization. We present an LLM assistant, ChatVis, that aids the LLM to generate Python code for ParaView scientific visualization tasks, without the need for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought prompt simplification, retrieval-augmented prompt generation using a vector database of documentation and code examples, and error checking with iterative prompt feedback to correct errors until a visualization is produced. An integral part of our approach is a benchmark suite of canonical visualization tasks, ParaView regression tests, and scientific use cases that includes comprehensive evaluation metrics. We evaluate our visualization assistant by comparing results with a variety of top-performing unassisted LLMs. We find that all the metrics are significantly improved with ChatVis.", "source": "arxiv", "arxiv_id": "2507.23096v1", "pdf_url": "https://arxiv.org/pdf/2507.23096v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-07-30T20:54:18Z", "updated": "2025-07-30T20:54:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Chatting with your ERP: A Recipe", "authors": ["Jorge Ruiz GÃ³mez", "Lidia AndrÃ©s Susinos", "Jorge Alamo OlivÃ©", "Sonia Rey Osorno", "Manuel Luis Gonzalez HernÃ¡ndez"], "year": 2025, "url": "http://arxiv.org/abs/2507.23429v1", "abstract": "This paper presents the design, implementation, and evaluation behind a Large Language Model (LLM) agent that chats with an industrial production-grade ERP system. The agent is capable of interpreting natural language queries and translating them into executable SQL statements, leveraging open-weight LLMs. A novel dual-agent architecture combining reasoning and critique stages was proposed to improve query generation reliability.", "source": "arxiv", "arxiv_id": "2507.23429v1", "pdf_url": "https://arxiv.org/pdf/2507.23429v1", "categories": ["cs.AI", "cs.DB", "cs.ET", "cs.HC", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-31T11:09:50Z", "updated": "2025-07-31T11:09:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent", "authors": ["Liang-bo Ning", "Shijie Wang", "Wenqi Fan", "Qing Li", "Xin Xu", "Hao Chen", "Feiran Huang"], "year": 2025, "url": "http://arxiv.org/abs/2504.13192v2", "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.", "source": "arxiv", "arxiv_id": "2504.13192v2", "pdf_url": "https://arxiv.org/pdf/2504.13192v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "10.1145/3637528.3671837", "venue": "", "published": "2025-04-13T05:31:37Z", "updated": "2025-04-24T02:16:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety", "authors": ["Vamshi Krishna Bonagiri", "Ponnurangam Kumaragurum", "Khanh Nguyen", "Benjamin Plaut"], "year": 2025, "url": "http://arxiv.org/abs/2510.16492v2", "abstract": "As Large Language Model (LLM) agents increasingly operate in complex environments with real-world consequences, their safety becomes critical. While uncertainty quantification is well-studied for single-turn tasks, multi-turn agentic scenarios with real-world tool access present unique challenges where uncertainties and ambiguities compound, leading to severe or catastrophic risks beyond traditional text generation failures. We propose using \"quitting\" as a simple yet effective behavioral mechanism for LLM agents to recognize and withdraw from situations where they lack confidence. Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. Our results demonstrate a highly favorable safety-helpfulness trade-off: agents prompted to quit with explicit instructions improve safety by an average of +0.39 on a 0-3 scale across all models (+0.64 for proprietary models), while maintaining a negligible average decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding explicit quit instructions proves to be a highly effective safety mechanism that can immediately be deployed in existing agent systems, and establishes quitting as an effective first-line defense mechanism for autonomous agents in high-stakes applications.", "source": "arxiv", "arxiv_id": "2510.16492v2", "pdf_url": "https://arxiv.org/pdf/2510.16492v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-18T13:22:19Z", "updated": "2025-10-25T10:26:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents", "authors": ["Jingying Zeng", "Hui Liu", "Zhenwei Dai", "Xianfeng Tang", "Chen Luo", "Samarth Varshney", "Zhen Li", "Qi He"], "year": 2025, "url": "http://arxiv.org/abs/2503.04830v3", "abstract": "With the advancement of conversational large language models (LLMs), several LLM-based Conversational Shopping Agents (CSA) have been developed to help customers smooth their online shopping. The primary objective in building an engaging and trustworthy CSA is to ensure the agent's responses about product factoids are accurate and factually grounded. However, two challenges remain. First, LLMs produce hallucinated or unsupported claims. Such inaccuracies risk spreading misinformation and diminishing customer trust. Second, without providing knowledge source attribution in CSA response, customers struggle to verify LLM-generated information. To address both challenges, we present an easily productionized solution that enables a ''citation experience'' to our customers. We build auto-evaluation metrics to holistically evaluate LLM's grounding and attribution capabilities, suggesting that citation generation paradigm substantially improves grounding performance by 13.83%. To deploy this capability at scale, we introduce Multi-UX-Inference system, which appends source citations to LLM outputs while preserving existing user experience features and supporting scalable inference. Large-scale online A/B tests show that grounded CSA responses improves customer engagement by 3% - 10%, depending on UX variations.", "source": "arxiv", "arxiv_id": "2503.04830v3", "pdf_url": "https://arxiv.org/pdf/2503.04830v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-05T08:58:35Z", "updated": "2025-05-13T05:02:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space", "authors": ["Yong Zhao", "Kai Xu", "Zhengqiu Zhu", "Yue Hu", "Zhiheng Zheng", "Yingfeng Chen", "Yatai Ji", "Chen Gao", "Yong Li", "Jincai Huang"], "year": 2025, "url": "http://arxiv.org/abs/2502.12532v3", "abstract": "Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings-spanning environment, action, and perception-largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming competitive baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.", "source": "arxiv", "arxiv_id": "2502.12532v3", "pdf_url": "https://arxiv.org/pdf/2502.12532v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-18T04:36:15Z", "updated": "2025-05-22T00:44:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance", "authors": ["Valeriu Dimidov", "Faisal Hawlader", "Sasan Jafarnejad", "RaphaÃ«l Frank"], "year": 2025, "url": "http://arxiv.org/abs/2511.05311v1", "abstract": "Economic constraints, limited availability of datasets for reproducibility and shortages of specialized expertise have long been recognized as key challenges to the adoption and advancement of predictive maintenance (PdM) in the automotive sector. Recent progress in large language models (LLMs) presents an opportunity to overcome these barriers and speed up the transition of PdM from research to industrial practice. Under these conditions, we explore the potential of LLM-based agents to support PdM cleaning pipelines. Specifically, we focus on maintenance logs, a critical data source for training well-performing machine learning (ML) models, but one often affected by errors such as typos, missing fields, near-duplicate entries, and incorrect dates. We evaluate LLM agents on cleaning tasks involving six distinct types of noise. Our findings show that LLMs are effective at handling generic cleaning tasks and offer a promising foundation for future industrial applications. While domain-specific errors remain challenging, these results highlight the potential for further improvements through specialized training and enhanced agentic capabilities.", "source": "arxiv", "arxiv_id": "2511.05311v1", "pdf_url": "https://arxiv.org/pdf/2511.05311v1", "categories": ["cs.AI", "cs.LG", "cs.RO", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-07T15:12:49Z", "updated": "2025-11-07T15:12:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation", "authors": ["Radhika Dua", "Young Joon", "Kwon", "Siddhant Dogra", "Daniel Freedman", "Diana Ruan", "Motaz Nashawaty", "Danielle Rigau", "Daniel Alexander Alber", "Kang Zhang", "Kyunghyun Cho", "Eric Karl Oermann"], "year": 2025, "url": "http://arxiv.org/abs/2508.02808v1", "abstract": "Radiological imaging is central to diagnosis, treatment planning, and clinical decision-making. Vision-language foundation models have spurred interest in automated radiology report generation (RRG), but safe deployment requires reliable clinical evaluation of generated reports. Existing metrics often rely on surface-level similarity or behave as black boxes, lacking interpretability. We introduce ICARE (Interpretable and Clinically-grounded Agent-based Report Evaluation), an interpretable evaluation framework leveraging large language model agents and dynamic multiple-choice question answering (MCQA). Two agents, each with either the ground-truth or generated report, generate clinically meaningful questions and quiz each other. Agreement on answers captures preservation and consistency of findings, serving as interpretable proxies for clinical precision and recall. By linking scores to question-answer pairs, ICARE enables transparent, and interpretable assessment. Clinician studies show ICARE aligns significantly more with expert judgment than prior metrics. Perturbation analyses confirm sensitivity to clinical content and reproducibility, while model comparisons reveal interpretable error patterns.", "source": "arxiv", "arxiv_id": "2508.02808v1", "pdf_url": "https://arxiv.org/pdf/2508.02808v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-04T18:28:03Z", "updated": "2025-08-04T18:28:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CoDA: Agentic Systems for Collaborative Data Visualization", "authors": ["Zichen Chen", "Jiefeng Chen", "Sercan Ã. Arik", "Misha Sra", "Tomas Pfister", "Jinsung Yoon"], "year": 2025, "url": "http://arxiv.org/abs/2510.03194v1", "abstract": "Deep research has revolutionized data analysis, yet data scientists still devote substantial time to manually crafting visualizations, highlighting the need for robust automation from natural language queries. However, current systems struggle with complex datasets containing multiple files and iterative refinement. Existing approaches, including simple single- or multi-agent systems, often oversimplify the task, focusing on initial query parsing while failing to robustly manage data complexity, code errors, or final visualization quality. In this paper, we reframe this challenge as a collaborative multi-agent problem. We introduce CoDA, a multi-agent system that employs specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection. We formalize this pipeline, demonstrating how metadata-focused analysis bypasses token limits and quality-driven refinement ensures robustness. Extensive evaluations show CoDA achieves substantial gains in the overall score, outperforming competitive baselines by up to 41.5%. This work demonstrates that the future of visualization automation lies not in isolated code generation but in integrated, collaborative agentic workflows.", "source": "arxiv", "arxiv_id": "2510.03194v1", "pdf_url": "https://arxiv.org/pdf/2510.03194v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-03T17:30:16Z", "updated": "2025-10-03T17:30:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CoEx -- Co-evolving World-model and Exploration", "authors": ["Minsoo Kim", "Seung-won Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2507.22281v1", "abstract": "Planning in modern LLM agents relies on the utilization of LLM as an internal world model, acquired during pretraining. However, existing agent designs fail to effectively assimilate new observations into dynamic updates of the world model. This reliance on the LLM's static internal world model is progressively prone to misalignment with the underlying true state of the world, leading to the generation of divergent and erroneous plans. We introduce a hierarchical agent architecture, CoEx, in which hierarchical state abstraction allows LLM planning to co-evolve with a dynamically updated model of the world. CoEx plans and interacts with the world by using LLM reasoning to orchestrate dynamic plans consisting of subgoals, and its learning mechanism continuously incorporates these subgoal experiences into a persistent world model in the form of a neurosymbolic belief state, comprising textual inferences and code-based symbolic memory. We evaluate our agent across a diverse set of agent scenarios involving rich environments and complex tasks including ALFWorld, PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent paradigms in planning and exploration.", "source": "arxiv", "arxiv_id": "2507.22281v1", "pdf_url": "https://arxiv.org/pdf/2507.22281v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-29T23:13:09Z", "updated": "2025-07-29T23:13:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CoLLMLight: Cooperative Large Language Model Agents for Network-Wide Traffic Signal Control", "authors": ["Zirui Yuan", "Siqi Lai", "Hao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2503.11739v1", "abstract": "Traffic Signal Control (TSC) plays a critical role in urban traffic management by optimizing traffic flow and mitigating congestion. While Large Language Models (LLMs) have recently emerged as promising tools for TSC due to their exceptional problem-solving and generalization capabilities, existing approaches fail to address the essential need for inter-agent coordination, limiting their effectiveness in achieving network-wide optimization. To bridge this gap, we propose CoLLMLight, a cooperative LLM agent framework for TSC. Specifically, we first construct a structured spatiotemporal graph to capture real-time traffic dynamics and spatial relationships among neighboring intersections, enabling the LLM to reason about complex traffic interactions. Moreover, we introduce a complexity-aware reasoning mechanism that dynamically adapts reasoning depth based on real-time traffic conditions, ensuring optimal computational efficiency without sacrificing decision quality. Besides, we propose a fine-tuning strategy that leverages iterative simulation-driven data collection and environmental feedback to build a lightweight LLM tailored for cooperative TSC. Extensive experiments on both synthetic and real-world datasets demonstrate that CoLLMLight outperforms state-of-the-art methods in diverse traffic scenarios, showcasing its effectiveness, scalability, and robustness.", "source": "arxiv", "arxiv_id": "2503.11739v1", "pdf_url": "https://arxiv.org/pdf/2503.11739v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-14T15:40:39Z", "updated": "2025-03-14T15:40:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CoMind: Towards Community-Driven Agents for Machine Learning Engineering", "authors": ["Sijie Li", "Weiwei Sun", "Shanda Li", "Ameet Talwalkar", "Yiming Yang"], "year": 2025, "url": "http://arxiv.org/abs/2506.20640v2", "abstract": "Large language model (LLM) agents show promise in automating machine learning (ML) engineering. However, existing agents typically operate in isolation on a given research problem, without engaging with the broader research community, where human researchers often gain insights and contribute by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live evaluation framework designed to assess an agent's ability to communicate with and leverage collective knowledge from a simulated Kaggle research community. Building on this framework, we propose CoMind, an multi-agent system designed to actively integrate external knowledge. CoMind employs an iterative parallel exploration mechanism, developing multiple solutions simultaneously to balance exploratory breadth with implementation depth. On 75 past Kaggle competitions within our MLE-Live framework, CoMind achieves a 36% medal rate, establishing a new state of the art. Critically, when deployed in eight live, ongoing competitions, CoMind outperforms 92.6% of human competitors on average, placing in the top 5% on three official leaderboards and the top 1% on one.", "source": "arxiv", "arxiv_id": "2506.20640v2", "pdf_url": "https://arxiv.org/pdf/2506.20640v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-25T17:36:02Z", "updated": "2025-11-26T05:16:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Coarse-to-Fine Grounded Memory for LLM Agent Planning", "authors": ["Wei Yang", "Jinwei Xiao", "Hongming Zhang", "Qingyang Zhang", "Yanna Wang", "Bo Xu"], "year": 2025, "url": "http://arxiv.org/abs/2508.15305v1", "abstract": "Recent advancements in Large Language Models (LLMs) have driven growing interest in LLM-based agents for complex planning tasks. To avoid costly agent training, many studies adopted memory mechanism that enhances LLM with offline experiences or online trajectory analysis. However, existing works focus on single-granularity memory derived from dynamic environmental interactions, which are inherently constrained by the quality of the collected experiences. This limitation, in turn, constrain the diversity of knowledge and the flexibility of planning. We propose Coarse-to-Fine Grounded Memory (\\Ours{}), a novel framework that grounds coarse-to-fine memories with LLM, thereby fully leverage them for flexible adaptation to diverse scenarios. \\Ours{} grounds environmental information into coarse-grained focus points to guide experience collection in training tasks, followed by grounding of actionable hybrid-grained tips from each experience. At inference, \\Ours{} retrieves task-relevant experiences and tips to support planning. When facing environmental anomalies, the LLM grounds the current situation into fine-grained key information, enabling flexible self-QA reflection and plan correction.", "source": "arxiv", "arxiv_id": "2508.15305v1", "pdf_url": "https://arxiv.org/pdf/2508.15305v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-21T06:50:23Z", "updated": "2025-08-21T06:50:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks", "authors": ["Hongyuan Tao", "Ying Zhang", "Zhenhao Tang", "Hongen Peng", "Xukun Zhu", "Bingchang Liu", "Yingguang Yang", "Ziyin Zhang", "Zhaogui Xu", "Haipeng Zhang", "Linchao Zhu", "Rui Wang", "Hang Yu", "Jianguo Li", "Peng Di"], "year": 2025, "url": "http://arxiv.org/abs/2505.16901v4", "abstract": "Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.", "source": "arxiv", "arxiv_id": "2505.16901v4", "pdf_url": "https://arxiv.org/pdf/2505.16901v4", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-22T17:00:55Z", "updated": "2025-06-23T20:05:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "authors": ["Anjiang Wei", "Tarun Suresh", "Jiannan Cao", "Naveen Kannan", "Yuheng Wu", "Kai Yan", "Thiago S. F. X. Teixeira", "Ke Wang", "Alex Aiken"], "year": 2025, "url": "http://arxiv.org/abs/2503.23145v2", "abstract": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning. Our code, data, and models are publicly available at https://github.com/Anjiang-Wei/CodeARC", "source": "arxiv", "arxiv_id": "2503.23145v2", "pdf_url": "https://arxiv.org/pdf/2503.23145v2", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.PL", "doi": "", "venue": "", "published": "2025-03-29T16:50:39Z", "updated": "2025-08-08T07:13:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation", "authors": ["Sergey Pugachev"], "year": 2025, "url": "http://arxiv.org/abs/2510.18893v1", "abstract": "Multi-agent LLM systems fail to realize parallel speedups due to costly coordination. We present CodeCRDT, an observation-driven coordination pattern where agents coordinate by monitoring a shared state with observable updates and deterministic convergence, rather than explicit message passing. Using Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free, conflict-free concurrent code generation with strong eventual consistency. Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on others, and 100% convergence with zero merge failures. The study formalizes observation-driven coordination for stochastic LLM agents, revealing semantic conflict rates (5-10%) and quality-performance tradeoffs, and provides empirical characterization of when parallel coordination succeeds versus fails based on task structure.", "source": "arxiv", "arxiv_id": "2510.18893v1", "pdf_url": "https://arxiv.org/pdf/2510.18893v1", "categories": ["cs.DC", "cs.AI", "cs.SE"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-10-18T20:50:01Z", "updated": "2025-10-18T20:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications", "authors": ["Wanghao Ye", "Sihan Chen", "Yiting Wang", "Shwai He", "Bowei Tian", "Guoheng Sun", "Ziyi Wang", "Ziyao Wang", "Yexiao He", "Zheyu Shen", "Meng Liu", "Yuning Zhang", "Meng Feng", "Yang Wang", "Siyuan Peng", "Yilong Dai", "Zhenle Duan", "Lang Xiong", "Joshua Liu", "Hanzhang Qin", "Ang Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.03543v2", "abstract": "Current large language model (LLM) agents lack authentic human psychological processes necessary for genuine digital twins and social AI applications. To address this limitation, we present a computational implementation of Global Workspace Theory (GNWT) that integrates human cognitive architecture principles into LLM agents, creating specialized sub-agents for emotion, memory, social norms, planning, and goal-tracking coordinated through a global workspace mechanism. However, authentic digital twins require accurate personality initialization. We therefore develop a novel adventure-based personality test that evaluates true personality through behavioral choices within interactive scenarios, bypassing self-presentation bias found in traditional assessments. Building on these innovations, our CogniPair platform enables digital twins to engage in realistic simulated dating interactions and job interviews before real encounters, providing bidirectional cultural fit assessment for both romantic compatibility and workplace matching. Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies. This work advances psychological authenticity in LLM agents and establishes a foundation for intelligent dating platforms and HR technology solutions.", "source": "arxiv", "arxiv_id": "2506.03543v2", "pdf_url": "https://arxiv.org/pdf/2506.03543v2", "categories": ["cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-04T03:54:30Z", "updated": "2025-11-28T20:54:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents", "authors": ["Haochen Sun", "Shuwen Zhang", "Lujie Niu", "Lei Ren", "Hao Xu", "Hao Fu", "Fangkun Zhao", "Caixia Yuan", "Xiaojie Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.20073v3", "abstract": "Large Language Models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-based Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks in two novel ways. First, it provides a multi-agent framework supporting diverse tasks and objectives and encourages collaboration through natural language communication. Second, it introduces a spectrum of process-oriented evaluation metrics to assess the fine-grained collaboration capabilities of different LLM agents, a dimension often overlooked in prior work. We conduct extensive experiments with 13 popular LLMs and show that, while the LLMs exhibit a strong ability in goal interpretation, there are significant shortcomings in active collaboration and continuous adaptation, which are critical for efficiently fulfilling complex tasks. Notably, we highlight the strengths and weaknesses of LLM-MAS and provide insights for improving and evaluating LLM-MAS on a unified and open-source benchmark. The environments, 30 open-ended tasks, and the evaluation package are publicly available at https://github.com/YusaeMeow/Collab-Overcooked.", "source": "arxiv", "arxiv_id": "2502.20073v3", "pdf_url": "https://arxiv.org/pdf/2502.20073v3", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "10.18653/v1/2025.emnlp-main.249", "venue": "", "published": "2025-02-27T13:31:13Z", "updated": "2025-09-25T06:15:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning", "authors": ["Isadora White", "Kolby Nottingham", "Ayush Maniar", "Max Robinson", "Hansen Lillemark", "Mehul Maheshwari", "Lianhui Qin", "Prithviraj Ammanabrolu"], "year": 2025, "url": "http://arxiv.org/abs/2504.17950v1", "abstract": "Collaboration is ubiquitous and essential in day-to-day life -- from exchanging ideas, to delegating tasks, to generating plans together. This work studies how LLMs can adaptively collaborate to perform complex embodied reasoning tasks. To this end we introduce MINDcraft, an easily extensible platform built to enable LLM agents to control characters in the open-world game of Minecraft; and MineCollab, a benchmark to test the different dimensions of embodied and collaborative reasoning. An experimental study finds that the primary bottleneck in collaborating effectively for current state-of-the-art agents is efficient natural language communication, with agent performance dropping as much as 15% when they are required to communicate detailed task completion plans. We conclude that existing LLM agents are ill-optimized for multi-agent collaboration, especially in embodied scenarios, and highlight the need to employ methods beyond in-context and imitation learning. Our website can be found here: https://mindcraft-minecollab.github.io/", "source": "arxiv", "arxiv_id": "2504.17950v1", "pdf_url": "https://arxiv.org/pdf/2504.17950v1", "categories": ["cs.MA", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-04-24T21:28:16Z", "updated": "2025-04-24T21:28:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory", "authors": ["Mukul Singh", "Arjun Radhakrishna", "Sumit Gulwani"], "year": 2025, "url": "http://arxiv.org/abs/2509.04847v1", "abstract": "Language models are increasingly deployed in interactive online environments, from personal chat assistants to domain-specific agents, raising questions about their cooperative and competitive behavior in multi-party settings. While prior work has examined language model decision-making in isolated or short-term game-theoretic contexts, these studies often neglect long-horizon interactions, human-model collaboration, and the evolution of behavioral patterns over time. In this paper, we investigate the dynamics of language model behavior in the iterated prisoner's dilemma (IPD), a classical framework for studying cooperation and conflict. We pit model-based agents against a suite of 240 well-established classical strategies in an Axelrod-style tournament and find that language models achieve performance on par with, and in some cases exceeding, the best-known classical strategies. Behavioral analysis reveals that language models exhibit key properties associated with strong cooperative strategies - niceness, provocability, and generosity while also demonstrating rapid adaptability to changes in opponent strategy mid-game. In controlled \"strategy switch\" experiments, language models detect and respond to shifts within only a few rounds, rivaling or surpassing human adaptability. These results provide the first systematic characterization of long-term cooperative behaviors in language model agents, offering a foundation for future research into their role in more complex, mixed human-AI social environments.", "source": "arxiv", "arxiv_id": "2509.04847v1", "pdf_url": "https://arxiv.org/pdf/2509.04847v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-05T06:55:15Z", "updated": "2025-09-05T06:55:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Collaborative LLM Agents for C4 Software Architecture Design Automation", "authors": ["Kamil Szczepanik", "JarosÅaw A. Chudziak"], "year": 2025, "url": "http://arxiv.org/abs/2510.22787v1", "abstract": "Software architecture design is a fundamental part of creating every software system. Despite its importance, producing a C4 software architecture model, the preferred notation for such architecture, remains manual and time-consuming. We introduce an LLM-based multi-agent system that automates this task by simulating a dialogue between role-specific experts who analyze requirements and generate the Context, Container, and Component views of the C4 model. Quality is assessed with a hybrid evaluation framework: deterministic checks for structural and syntactic integrity and C4 rule consistency, plus semantic and qualitative scoring via an LLM-as-a-Judge approach. Tested on five canonical system briefs, the workflow demonstrates fast C4 model creation, sustains high compilation success, and delivers semantic fidelity. A comparison of four state-of-the-art LLMs shows different strengths relevant to architectural design. This study contributes to automated software architecture design and its evaluation methods.", "source": "arxiv", "arxiv_id": "2510.22787v1", "pdf_url": "https://arxiv.org/pdf/2510.22787v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-26T18:43:59Z", "updated": "2025-10-26T18:43:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control", "authors": ["Alireza Rezazadeh", "Zichao Li", "Ange Lou", "Yuying Zhao", "Wei Wei", "Yujia Bao"], "year": 2025, "url": "http://arxiv.org/abs/2505.18279v1", "abstract": "Complex tasks are increasingly delegated to ensembles of specialized LLM-based agents that reason, communicate, and coordinate actions-both among themselves and through interactions with external tools, APIs, and databases. While persistent memory has been shown to enhance single-agent performance, most approaches assume a monolithic, single-user context-overlooking the benefits and challenges of knowledge transfer across users under dynamic, asymmetric permissions. We introduce Collaborative Memory, a framework for multi-user, multi-agent environments with asymmetric, time-evolving access controls encoded as bipartite graphs linking users, agents, and resources. Our system maintains two memory tiers: (1) private memory-private fragments visible only to their originating user; and (2) shared memory-selectively shared fragments. Each fragment carries immutable provenance attributes (contributing agents, accessed resources, and timestamps) to support retrospective permission checks. Granular read policies enforce current user-agent-resource constraints and project existing memory fragments into filtered transformed views. Write policies determine fragment retention and sharing, applying context-aware transformations to update the memory. Both policies may be designed conditioned on system, agent, and user-level information. Our framework enables safe, efficient, and interpretable cross-user knowledge sharing, with provable adherence to asymmetric, time-varying policies and full auditability of memory operations.", "source": "arxiv", "arxiv_id": "2505.18279v1", "pdf_url": "https://arxiv.org/pdf/2505.18279v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-05-23T18:14:57Z", "updated": "2025-05-23T18:14:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks", "authors": ["Ang Li", "Yin Zhou", "Vethavikashini Chithrra Raghuram", "Tom Goldstein", "Micah Goldblum"], "year": 2025, "url": "http://arxiv.org/abs/2502.08586v1", "abstract": "A high volume of recent ML security literature focuses on attacks against aligned large language models (LLMs). These attacks may extract private information or coerce the model into producing harmful outputs. In real-world deployments, LLMs are often part of a larger agentic pipeline including memory systems, retrieval, web access, and API calling. Such additional components introduce vulnerabilities that make these LLM-powered agents much easier to attack than isolated LLMs, yet relatively little work focuses on the security of LLM agents. In this paper, we analyze security and privacy vulnerabilities that are unique to LLM agents. We first provide a taxonomy of attacks categorized by threat actors, objectives, entry points, attacker observability, attack strategies, and inherent vulnerabilities of agent pipelines. We then conduct a series of illustrative attacks on popular open-source and commercial agents, demonstrating the immediate practical implications of their vulnerabilities. Notably, our attacks are trivial to implement and require no understanding of machine learning.", "source": "arxiv", "arxiv_id": "2502.08586v1", "pdf_url": "https://arxiv.org/pdf/2502.08586v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-12T17:19:36Z", "updated": "2025-02-12T17:19:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Communicating Activations Between Language Model Agents", "authors": ["Vignav Ramesh", "Kenneth Li"], "year": 2025, "url": "http://arxiv.org/abs/2501.14082v2", "abstract": "Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via activations; concretely, we pause an LM $\\textit{B}$'s computation at an intermediate layer, combine its current activation with another LM $\\textit{A}$'s intermediate activation via some function $\\textit{f}$, then pass $\\textit{f}$'s output into the next layer of $\\textit{B}$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with zero additional parameters and data, and saves a substantial amount of compute over natural language communication. We test our method with various functional forms $\\textit{f}$ on two experimental setups--multi-player coordination games and reasoning benchmarks--and find that it achieves up to $27.0\\%$ improvement over natural language communication across datasets with $<$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative \"language\" for communication between LMs.", "source": "arxiv", "arxiv_id": "2501.14082v2", "pdf_url": "https://arxiv.org/pdf/2501.14082v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-23T20:41:07Z", "updated": "2025-05-07T20:03:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches", "authors": ["Hachem Madmoun", "Salem Lahlou"], "year": 2025, "url": "http://arxiv.org/abs/2510.05748v2", "abstract": "Eliciting cooperation in multi-agent LLM systems is critical for AI alignment. We investigate two approaches: direct communication and curriculum learning. In a 4-player Stag Hunt, a one-word \"cheap talk\" channel increases cooperation from 0% to 48.3%, demonstrating communication as a robust coordination mechanism. In contrast, we find that curriculum learning is highly sensitive to design choices: our pedagogical curriculum through progressively complex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game with Punishment, demonstrating that optimizing for short-term rationality can actively undermine alignment goals. Qualitative analysis reveals that curricula emphasizing defection-equilibrium games can induce \"learned pessimism\" in agents. These findings suggest that for coordination problems, simple communication protocols may be more reliable than experience-based training, and that curriculum design for social dilemmas requires careful attention to the strategic lessons embedded in game sequences.", "source": "arxiv", "arxiv_id": "2510.05748v2", "pdf_url": "https://arxiv.org/pdf/2510.05748v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-07T10:06:29Z", "updated": "2026-01-16T09:52:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry", "authors": ["Run Peng", "Ziqiao Ma", "Amy Pang", "Sikai Li", "Zhang Xi-Jia", "Yingzhuo Yu", "Cristian-Paul Bara", "Joyce Chai"], "year": 2025, "url": "http://arxiv.org/abs/2510.25595v1", "abstract": "While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment. Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents' ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles", "source": "arxiv", "arxiv_id": "2510.25595v1", "pdf_url": "https://arxiv.org/pdf/2510.25595v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-29T15:03:53Z", "updated": "2025-10-29T15:03:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent", "authors": ["Yixin Gao", "Xin Li", "Xiaohan Pan", "Runsen Feng", "Bingchen Li", "Yunpeng Qi", "Yiting Lu", "Zhengxue Cheng", "Zhibo Chen", "JÃ¶rn Ostermann"], "year": 2025, "url": "http://arxiv.org/abs/2508.15243v1", "abstract": "We present Comp-X, the first intelligently interactive image compression paradigm empowered by the impressive reasoning capability of large language model (LLM) agent. Notably, commonly used image codecs usually suffer from limited coding modes and rely on manual mode selection by engineers, making them unfriendly for unprofessional users. To overcome this, we advance the evolution of image coding paradigm by introducing three key innovations: (i) multi-functional coding framework, which unifies different coding modes of various objective/requirements, including human-machine perception, variable coding, and spatial bit allocation, into one framework. (ii) interactive coding agent, where we propose an augmented in-context learning method with coding expert feedback to teach the LLM agent how to understand the coding request, mode selection, and the use of the coding tools. (iii) IIC-bench, the first dedicated benchmark comprising diverse user requests and the corresponding annotations from coding experts, which is systematically designed for intelligently interactive image compression evaluation. Extensive experimental results demonstrate that our proposed Comp-X can understand the coding requests efficiently and achieve impressive textual interaction capability. Meanwhile, it can maintain comparable compression performance even with a single coding framework, providing a promising avenue for artificial general intelligence (AGI) in image compression.", "source": "arxiv", "arxiv_id": "2508.15243v1", "pdf_url": "https://arxiv.org/pdf/2508.15243v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-08-21T05:09:30Z", "updated": "2025-08-21T05:09:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows", "authors": ["Joong Ho Choi", "Jiayang Zhao", "Jeel Shah", "Ritvika Sonawane", "Vedant Singh", "Avani Appalla", "Will Flanagan", "Filipe Condessa"], "year": 2025, "url": "http://arxiv.org/abs/2510.18043v1", "abstract": "Large Language Models (LLMs) deliver powerful reasoning and generation capabilities but incur substantial run-time costs when operating in agentic workflows that chain together lengthy prompts and process rich data streams. We introduce CompactPrompt, an end-to-end pipeline that merges hard prompt compression with lightweight file-level data compression. CompactPrompt first prunes low-information tokens from prompts using self-information scoring and dependency-based phrase grouping. In parallel, it applies n-gram abbreviation to recurrent textual patterns in attached documents and uniform quantization to numerical columns, yielding compact yet semantically faithful representations. Integrated into standard LLM agents, CompactPrompt reduces total token usage and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA, while preserving output quality (Results in less than 5% accuracy drop for Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time compression decisions and quantify cost-performance trade-offs, laying the groundwork for leaner generative AI pipelines.", "source": "arxiv", "arxiv_id": "2510.18043v1", "pdf_url": "https://arxiv.org/pdf/2510.18043v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-20T19:31:11Z", "updated": "2025-10-20T19:31:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation", "authors": ["Amin Qasmi", "Usman Naseem", "Mehwish Nasim"], "year": 2025, "url": "http://arxiv.org/abs/2502.11649v3", "abstract": "We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties. Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process. Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence", "source": "arxiv", "arxiv_id": "2502.11649v3", "pdf_url": "https://arxiv.org/pdf/2502.11649v3", "categories": ["cs.AI", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T10:41:55Z", "updated": "2025-08-31T02:04:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework", "authors": ["Saman Marandi", "Yu-Shu Hu", "Mohammad Modarres"], "year": 2025, "url": "http://arxiv.org/abs/2505.21291v1", "abstract": "In this paper, we present a novel diagnostic framework that integrates Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system diagnostics in high-reliability systems such as nuclear power plants. Traditional diagnostic modeling struggles when systems become too complex, making functional modeling a more attractive approach. Our approach introduces a diagnostic framework grounded in the functional modeling principles of the Dynamic Master Logic (DML) model. It incorporates two coordinated LLM components, including an LLM-based workflow for automated construction of DML logic from system documentation and an LLM agent that facilitates interactive diagnostics. The generated logic is encoded into a structured KG, referred to as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or operational data can also be incorporated to refine the model's precision and diagnostic depth. In the interaction phase, users submit natural language queries, which are interpreted by the LLM agent. The agent selects appropriate tools for structured reasoning, including upward and downward propagation across the KG-DML. Rather than embedding KG content into every prompt, the LLM agent distinguishes between diagnostic and interpretive tasks. For diagnostics, the agent selects and executes external tools that perform structured KG reasoning. For general queries, a Graph-based Retrieval-Augmented Generation (Graph-RAG) approach is used, retrieving relevant KG segments and embedding them into the prompt to generate natural explanations. A case study on an auxiliary feedwater system demonstrated the framework's effectiveness, with over 90% accuracy in key elements and consistent tool and argument extraction, supporting its use in safety-critical diagnostics.", "source": "arxiv", "arxiv_id": "2505.21291v1", "pdf_url": "https://arxiv.org/pdf/2505.21291v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.3390/app15179428", "venue": "", "published": "2025-05-27T14:54:49Z", "updated": "2025-05-27T14:54:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Computational Basis of LLM's Decision Making in Social Simulation", "authors": ["Ji Ma"], "year": 2025, "url": "http://arxiv.org/abs/2504.11671v4", "abstract": "Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game, a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.", "source": "arxiv", "arxiv_id": "2504.11671v4", "pdf_url": "https://arxiv.org/pdf/2504.11671v4", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.GN"], "primary_category": "cs.AI", "doi": "", "venue": "Sociological Methodology, 2025", "published": "2025-04-16T00:02:28Z", "updated": "2025-12-23T04:02:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Conformal Constrained Policy Optimization for Cost-Effective LLM Agents", "authors": ["Wenwen Si", "Sooyong Jang", "Insup Lee", "Osbert Bastani"], "year": 2025, "url": "http://arxiv.org/abs/2511.11828v1", "abstract": "While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.", "source": "arxiv", "arxiv_id": "2511.11828v1", "pdf_url": "https://arxiv.org/pdf/2511.11828v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-14T19:39:28Z", "updated": "2025-11-14T19:39:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Conformal Information Pursuit for Interactively Guiding Large Language Models", "authors": ["Kwan Ho Ryan Chan", "Yuyan Ge", "Edgar Dobriban", "Hamed Hassani", "RenÃ© Vidal"], "year": 2025, "url": "http://arxiv.org/abs/2507.03279v2", "abstract": "A significant use case of instruction-finetuned Large Language Models (LLMs) is to solve question-answering tasks interactively. In this setting, an LLM agent is tasked with making a prediction by sequentially querying relevant information from the user, as opposed to a single-turn conversation. This paper explores sequential querying strategies that aim to minimize the expected number of queries. One such strategy is Information Pursuit (IP), a greedy algorithm that at each iteration selects the query that maximizes information gain or equivalently minimizes uncertainty. However, obtaining accurate estimates of mutual information or conditional entropy for LLMs is very difficult in practice due to over- or under-confident LLM proba- bilities, which leads to suboptimal query selection and predictive performance. To better estimate the uncertainty at each iteration, we propose Conformal Information Pursuit (C-IP), an alternative approach to sequential information gain based on conformal prediction sets. More specifically, C-IP leverages a relationship between prediction sets and conditional entropy at each iteration to estimate uncertainty based on the average size of conformal prediction sets. In contrast to conditional entropy, we find that conformal prediction sets are a distribution-free and robust method of measuring uncertainty. Experiments with 20 Questions show that C-IP obtains better predictive performance and shorter query-answer chains compared to previous approaches to IP and uncertainty-based chain-of-thought methods. Furthermore, extending to an interactive medical setting between a doctor and a patient on the MediQ dataset, C-IP achieves competitive performance with direct single-turn prediction while offering greater interpretability.", "source": "arxiv", "arxiv_id": "2507.03279v2", "pdf_url": "https://arxiv.org/pdf/2507.03279v2", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-07-04T03:55:39Z", "updated": "2025-11-07T05:30:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Connecting Large Language Model Agent to High Performance Computing Resource", "authors": ["Heng Ma", "Alexander Brace", "Carlo Siebenschuh", "Greg Pauloski", "Ian Foster", "Arvind Ramanathan"], "year": 2025, "url": "http://arxiv.org/abs/2502.12280v1", "abstract": "The Large Language Model agent workflow enables the LLM to invoke tool functions to increase the performance on specific scientific domain questions. To tackle large scale of scientific research, it requires access to computing resource and parallel computing setup. In this work, we implemented Parsl to the LangChain/LangGraph tool call setup, to bridge the gap between the LLM agent to the computing resource. Two tool call implementations were set up and tested on both local workstation and HPC environment on Polaris/ALCF. The first implementation with Parsl-enabled LangChain tool node queues the tool functions concurrently to the Parsl workers for parallel execution. The second configuration is implemented by converting the tool functions into Parsl ensemble functions, and is more suitable for large task on super computer environment. The LLM agent workflow was prompted to run molecular dynamics simulations, with different protein structure and simulation conditions. These results showed the LLM agent tools were managed and executed concurrently by Parsl on the available computing resource.", "source": "arxiv", "arxiv_id": "2502.12280v1", "pdf_url": "https://arxiv.org/pdf/2502.12280v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-02-17T19:32:30Z", "updated": "2025-02-17T19:32:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents", "authors": ["Jiaxing Zhao", "Hongbin Xie", "Yuzhen Lei", "Xuan Song", "Zhuoran Shi", "Lianxin Li", "Shuangxue Liu", "Haoran Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.10936v1", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks. Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating collective intelligence of multiple agents. However, both approaches face significant limitations. Single-agent with chain-of-thought, due to the inherent complexity of designing cross-domain prompts, faces collaboration challenges. Meanwhile, multi-agent systems consume substantial tokens and inevitably dilute the primary problem, which is particularly problematic in business workflow tasks. To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves business workflow collaboration problem by combining knowledge and prompts at a reduced cost. Specifically, we construct an integrated knowledge graph that incorporates knowledge from multiple stages. Furthermore, by maintaining and retrieving a prompts tree, we can obtain prompt information relevant to other stages of the business workflow. We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs. Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4.", "source": "arxiv", "arxiv_id": "2505.10936v1", "pdf_url": "https://arxiv.org/pdf/2505.10936v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-16T07:14:42Z", "updated": "2025-05-16T07:14:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents", "authors": ["Tao Zhe", "Rui Liu", "Fateme Memar", "Xiao Luo", "Wei Fan", "Xinyue Ye", "Zhongren Peng", "Dongjie Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.06078v1", "abstract": "Route recommendation aims to provide users with optimal travel plans that satisfy diverse and complex requirements. Classical routing algorithms (e.g., shortest-path and constraint-aware search) are efficient but assume structured inputs and fixed objectives, limiting adaptability to natural-language queries. Recent LLM-based approaches enhance flexibility but struggle with spatial reasoning and the joint modeling of route-level and POI-level preferences. To address these limitations, we propose RouteLLM, a hierarchical multi-agent framework that grounds natural-language intents into constraint-aware routes. It first parses user queries into structured intents including POIs, paths, and constraints. A manager agent then coordinates specialized sub-agents: a constraint agent that resolves and formally check constraints, a POI agent that retrieves and ranks candidate POIs, and a path refinement agent that refines routes via a routing engine with preference-conditioned costs. A final verifier agent ensures constraint satisfaction and produces the final route with an interpretable rationale. This design bridges linguistic flexibility and spatial structure, enabling reasoning over route feasibility and user preferences. Experiments show that our method reliably grounds textual preferences into constraint-aware routes, improving route quality and preference satisfaction over classical methods.", "source": "arxiv", "arxiv_id": "2510.06078v1", "pdf_url": "https://arxiv.org/pdf/2510.06078v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-07T16:03:57Z", "updated": "2025-10-07T16:03:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Constructing coherent spatial memory in LLM agents through graph rectification", "authors": ["Puzhen Zhang", "Xuyang Chen", "Yu Feng", "Yuhan Jiang", "Liqiu Meng"], "year": 2025, "url": "http://arxiv.org/abs/2510.04195v1", "abstract": "Given a map description through global traversal navigation instructions (e.g., visiting each room sequentially with action signals such as north, west, etc.), an LLM can often infer the implicit spatial layout of the environment and answer user queries by providing a shortest path from a start to a destination (for instance, navigating from the lobby to a meeting room via the hall and elevator). However, such context-dependent querying becomes incapable as the environment grows much longer, motivating the need for incremental map construction that builds a complete topological graph from stepwise observations. We propose a framework for LLM-driven construction and map repair, designed to detect, localize, and correct structural inconsistencies in incrementally constructed navigation graphs. Central to our method is the Version Control, which records the full history of graph edits and their source observations, enabling fine-grained rollback, conflict tracing, and repair evaluation. We further introduce an Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation. To properly evaluate our approach, we create a refined version of the MANGO benchmark dataset by systematically removing non-topological actions and inherent structural conflicts, providing a cleaner testbed for LLM-driven construction and map repair. Our approach significantly improves map correctness and robustness, especially in scenarios with entangled or chained inconsistencies. Our results highlight the importance of introspective, history-aware repair mechanisms for maintaining coherent spatial memory in LLM agents.", "source": "arxiv", "arxiv_id": "2510.04195v1", "pdf_url": "https://arxiv.org/pdf/2510.04195v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-05T13:27:00Z", "updated": "2025-10-05T13:27:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "year": 2025, "url": "http://arxiv.org/abs/2505.14668v2", "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts surrounding humans to enhance the proactivity of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and personas from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants. The code and dataset are publicly available at https://github.com/openaiotlab/ContextAgent.", "source": "arxiv", "arxiv_id": "2505.14668v2", "pdf_url": "https://arxiv.org/pdf/2505.14668v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-20T17:55:25Z", "updated": "2025-10-27T07:17:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Contextual Experience Replay for Self-Improvement of Language Agents", "authors": ["Yitao Liu", "Chenglei Si", "Karthik Narasimhan", "Shunyu Yao"], "year": 2025, "url": "http://arxiv.org/abs/2506.06698v1", "abstract": "Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.", "source": "arxiv", "arxiv_id": "2506.06698v1", "pdf_url": "https://arxiv.org/pdf/2506.06698v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-07T07:47:35Z", "updated": "2025-06-07T07:47:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents", "authors": ["Divyanshu Saxena", "Rishikesh Maurya", "Xiaoxuan Ou", "Gagan Somashekar", "Shachee Mishra Gupta", "Arun Iyer", "Yu Kang", "Chetan Bansal", "Aditya Akella", "Saravan Rajmohan"], "year": 2025, "url": "http://arxiv.org/abs/2511.10049v1", "abstract": "The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements.", "source": "arxiv", "arxiv_id": "2511.10049v1", "pdf_url": "https://arxiv.org/pdf/2511.10049v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-13T07:48:22Z", "updated": "2025-11-13T07:48:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live", "authors": ["Hanchen Li", "Qiuyang Mang", "Runyuan He", "Qizheng Zhang", "Huanzhi Mao", "Xiaokun Chen", "Hangrui Zhou", "Alvin Cheung", "Joseph Gonzalez", "Ion Stoica"], "year": 2025, "url": "http://arxiv.org/abs/2511.02230v2", "abstract": "KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum", "source": "arxiv", "arxiv_id": "2511.02230v2", "pdf_url": "https://arxiv.org/pdf/2511.02230v2", "categories": ["cs.OS", "cs.AI", "cs.NI"], "primary_category": "cs.OS", "doi": "", "venue": "", "published": "2025-11-04T03:43:05Z", "updated": "2025-12-20T01:17:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents", "authors": ["Jiangrong Wu", "Yuhong Nan", "Jianliang Wu", "Zitong Yao", "Zibin Zheng"], "year": 2025, "url": "http://arxiv.org/abs/2507.02699v1", "abstract": "The increasing capabilities of LLMs have led to the rapid proliferation of LLM agent apps, where developers enhance LLMs with access to external resources to support complex task execution. Among these, LLM email agent apps represent one of the widely used categories, as email remains a critical communication medium for users. LLM email agents are capable of managing and responding to email using LLM-driven reasoning and autonomously executing user instructions via external email APIs (e.g., send email). However, despite their growing deployment and utility, the security mechanism of LLM email agent apps remains underexplored. Currently, there is no comprehensive study into the potential security risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which overrides the original prompts of the email agent via external email resources, allowing attackers to gain control of the email agent remotely and further perform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an empirical study spanning 14 representative LLM agent frameworks, 63 agent apps, 12 LLMs, and 20 email services, which led to the generation of 1,404 real-world email agent instances for evaluation. Experimental results indicate that all 1,404 instances were successfully hijacked; on average, only 2.03 attack attempts are required to control an email agent instance. Even worse, for some LLMs, the average number of attempts needed to achieve full agent control drops to as few as 1.23.", "source": "arxiv", "arxiv_id": "2507.02699v1", "pdf_url": "https://arxiv.org/pdf/2507.02699v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-03T15:09:40Z", "updated": "2025-07-03T15:09:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ConversAR: Exploring Embodied LLM-Powered Group Conversations in Augmented Reality for Second Language Learners", "authors": ["Jad Bendarkawi", "Ashley Ponce", "Sean Mata", "Aminah Aliu", "Yuhan Liu", "Lei Zhang", "Amna Liaqat", "Varun Nagaraj Rao", "AndrÃ©s Monroy-HernÃ¡ndez"], "year": 2025, "url": "http://arxiv.org/abs/2505.24000v1", "abstract": "Group conversations are valuable for second language (L2) learners as they provide opportunities to practice listening and speaking, exercise complex turn-taking skills, and experience group social dynamics in a target language. However, most existing Augmented Reality (AR)-based conversational learning tools focus on dyadic interactions rather than group dialogues. Although research has shown that AR can help reduce speaking anxiety and create a comfortable space for practicing speaking skills in dyadic scenarios, especially with Large Language Model (LLM)-based conversational agents, the potential for group language practice using these technologies remains largely unexplored. We introduce ConversAR, a gpt-4o powered AR application, that enables L2 learners to practice contextualized group conversations. Our system features two embodied LLM agents with vision-based scene understanding and live captions. In a system evaluation with 10 participants, users reported reduced speaking anxiety and increased learner autonomy compared to perceptions of in-person practice methods with other learners.", "source": "arxiv", "arxiv_id": "2505.24000v1", "pdf_url": "https://arxiv.org/pdf/2505.24000v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1145/3706599.3720162", "venue": "", "published": "2025-05-29T20:49:04Z", "updated": "2025-05-29T20:49:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment", "authors": ["Jiahuan Pei", "Fanghua Ye", "Xin Sun", "Wentao Deng", "Koen Hindriks", "Junxiao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2507.05528v2", "abstract": "Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.", "source": "arxiv", "arxiv_id": "2507.05528v2", "pdf_url": "https://arxiv.org/pdf/2507.05528v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-07T22:56:37Z", "updated": "2025-09-05T17:52:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Conversational Exploration of Literature Landscape with LitChat", "authors": ["Mingyu Huang", "Shasha Zhou", "Yuxuan Chen", "Ke Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.23789v1", "abstract": "We are living in an era of \"big literature\", where the volume of digital scientific publications is growing exponentially. While offering new opportunities, this also poses challenges for understanding literature landscapes, as traditional manual reviewing is no longer feasible. Recent large language models (LLMs) have shown strong capabilities for literature comprehension, yet they are incapable of offering \"comprehensive, objective, open and transparent\" views desired by systematic reviews due to their limited context windows and trust issues like hallucinations. Here we present LitChat, an end-to-end, interactive and conversational literature agent that augments LLM agents with data-driven discovery tools to facilitate literature exploration. LitChat automatically interprets user queries, retrieves relevant sources, constructs knowledge graphs, and employs diverse data-mining techniques to generate evidence-based insights addressing user needs. We illustrate the effectiveness of LitChat via a case study on AI4Health, highlighting its capacity to quickly navigate the users through large-scale literature landscape with data-based evidence that is otherwise infeasible with traditional means.", "source": "arxiv", "arxiv_id": "2505.23789v1", "pdf_url": "https://arxiv.org/pdf/2505.23789v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-25T13:15:09Z", "updated": "2025-05-25T13:15:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate", "authors": ["Yiliu Sun", "Zicheng Zhao", "Sheng Wan", "Chen Gong"], "year": 2025, "url": "http://arxiv.org/abs/2507.03928v1", "abstract": "Nowadays, single Large Language Model (LLM) struggles with critical issues such as hallucination and inadequate reasoning abilities. To mitigate these issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where LLM agents engage in in-depth debates with others on tasks. However, existing MAD methods face two major issues: (a) too lengthy input contexts, which causes LLM agents to get lost in plenty of input information and experiences performance drop; and (b) the overconfidence dilemma, where self-assured LLM agents dominate the debate, leading to low debating effectiveness. To address these limitations, we propose a novel MAD method called \"CortexDebate\". Inspired by the human brain's tendency to establish a sparse and dynamically optimized network among cortical areas governed by white matter, CortexDebate constructs a sparse debating graph among LLM agents, where each LLM agent only debates with the ones that are helpful to it. To optimize the graph, we propose a module named McKinsey-based Debate Matter (MDM), which acts as an artificial analog to white matter. By integrating the McKinsey Trust Formula, a well-established measure of trustworthiness from sociology, MDM enables credible evaluations that guide graph optimization. The effectiveness of our CortexDebate has been well demonstrated by extensive experimental results across eight datasets from four task types.", "source": "arxiv", "arxiv_id": "2507.03928v1", "pdf_url": "https://arxiv.org/pdf/2507.03928v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-05T07:23:15Z", "updated": "2025-07-05T07:23:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation", "authors": ["Santhosh Kumar Ravindran"], "year": 2025, "url": "http://arxiv.org/abs/2512.21351v1", "abstract": "Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.", "source": "arxiv", "arxiv_id": "2512.21351v1", "pdf_url": "https://arxiv.org/pdf/2512.21351v1", "categories": ["cs.SE", "cs.AI", "cs.NE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-20T22:12:09Z", "updated": "2025-12-20T22:12:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction", "authors": ["Yinan Yu", "Falk Dippel", "Christina E. Lundberg", "Martin Lindgren", "Annika Rosengren", "Martin Adiels", "Helen SjÃ¶land"], "year": 2025, "url": "http://arxiv.org/abs/2511.15357v1", "abstract": "Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.", "source": "arxiv", "arxiv_id": "2511.15357v1", "pdf_url": "https://arxiv.org/pdf/2511.15357v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-19T11:34:47Z", "updated": "2025-11-19T11:34:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching", "authors": ["Qizheng Zhang", "Michael Wornow", "Kunle Olukotun"], "year": 2025, "url": "http://arxiv.org/abs/2506.14852v1", "abstract": "LLM-based agentic applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs due to extensive planning and reasoning requirements. Existing LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agentic applications where outputs depend on external data or environmental contexts. We propose agentic plan caching, a novel approach that extracts, stores, adapts, and reuses structured plan templates from planning stages of agentic applications across semantically similar tasks to reduce the cost of serving. Unlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. Evaluation across multiple real-world agentic applications shows that our system can reduce costs by 46.62% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.", "source": "arxiv", "arxiv_id": "2506.14852v1", "pdf_url": "https://arxiv.org/pdf/2506.14852v1", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG", "cs.PF"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-06-17T04:42:30Z", "updated": "2025-06-17T04:42:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents", "authors": ["Jiayu Liu", "Cheng Qian", "Zhaochen Su", "Qing Zong", "Shijue Huang", "Bingxiang He", "Yi R. Fung"], "year": 2025, "url": "http://arxiv.org/abs/2511.02734v1", "abstract": "Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.", "source": "arxiv", "arxiv_id": "2511.02734v1", "pdf_url": "https://arxiv.org/pdf/2511.02734v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-04T16:58:29Z", "updated": "2025-11-04T16:58:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Crucible: Quantifying the Potential of Control Algorithms through LLM Agents", "authors": ["Lianchen Jia", "Chaoyang Li", "Qian Houde", "Tianchi Huang", "Jiangchuan Liu", "Lifeng Sun"], "year": 2025, "url": "http://arxiv.org/abs/2510.18491v1", "abstract": "Control algorithms in production environments typically require domain experts to tune their parameters and logic for specific scenarios. However, existing research predominantly focuses on algorithmic performance under ideal or default configurations, overlooking the critical aspect of Tuning Potential. To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven, multi-level expert simulation to turn algorithms and defines a formalized metric to quantitatively evaluate their Tuning Potential. We demonstrate Crucible's effectiveness across a wide spectrum of case studies, from classic control tasks to complex computer systems, and validate its findings in a real-world deployment. Our experimental results reveal that Crucible systematically quantifies the tunable space across different algorithms. Furthermore, Crucible provides a new dimension for algorithm analysis and design, which ultimately leads to performance improvements. Our code is available at https://github.com/thu-media/Crucible.", "source": "arxiv", "arxiv_id": "2510.18491v1", "pdf_url": "https://arxiv.org/pdf/2510.18491v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-21T10:25:26Z", "updated": "2025-10-21T10:25:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency", "authors": ["Jiacheng Guo", "Suozhi Huang", "Zixin Yao", "Yifan Zhang", "Yifu Lu", "Jiashuo Liu", "Zihao Li", "Nicholas Deng", "Qixin Xiao", "Jia Tian", "Kanghong Zhan", "Tianyi Li", "Xiaochen Liu", "Jason Ge", "Chaoyang He", "Kaixuan Huang", "Lin Yang", "Wenhao Huang", "Mengdi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.00417v4", "abstract": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.\n  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.", "source": "arxiv", "arxiv_id": "2512.00417v4", "pdf_url": "https://arxiv.org/pdf/2512.00417v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-29T09:52:34Z", "updated": "2025-12-10T17:52:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations", "authors": ["Huan-ang Gao", "Zikang Zhang", "Tianwei Luo", "Kaisen Yang", "Xinzhe Juan", "Jiahao Qiu", "Tianxing Chen", "Bingxiang He", "Hao Zhao", "Hao Zhou", "Shilong Liu", "Mengdi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.23328v3", "abstract": "Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.", "source": "arxiv", "arxiv_id": "2512.23328v3", "pdf_url": "https://arxiv.org/pdf/2512.23328v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-29T09:25:56Z", "updated": "2026-01-01T15:48:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping", "authors": ["Ziyi Wang", "Yuxuan Lu", "Yimeng Zhang", "Jing Huang", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.07230v2", "abstract": "Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a user's persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches users' action distribution, indicating higher fidelity in personalized behavior simulation.", "source": "arxiv", "arxiv_id": "2510.07230v2", "pdf_url": "https://arxiv.org/pdf/2510.07230v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-08T17:00:25Z", "updated": "2025-10-18T04:00:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CyberRAG: An Agentic RAG cyber attack classification and reporting tool", "authors": ["Francesco Blefari", "Cristian Cosentino", "Francesco Aurelio Pironti", "Angelo Furfaro", "Fabrizio Marozzo"], "year": 2025, "url": "http://arxiv.org/abs/2507.02424v2", "abstract": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can generate hundreds of thousands of alerts per hour, overwhelming analysts with logs requiring rapidly evolving expertise. Conventional machine-learning detectors reduce alert volume but still yield many false positives, while standard Retrieval-Augmented Generation (RAG) pipelines often retrieve irrelevant context and fail to justify predictions. We present CyberRAG, a modular agent-based RAG framework that delivers real-time classification, explanation, and structured reporting for cyber-attacks. A central LLM agent orchestrates: (i) fine-tuned classifiers specialized by attack family; (ii) tool adapters for enrichment and alerting; and (iii) an iterative retrieval-and-reason loop that queries a domain-specific knowledge base until evidence is relevant and self-consistent. Unlike traditional RAG, CyberRAG adopts an agentic design that enables dynamic control flow and adaptive reasoning. This architecture autonomously refines threat labels and natural-language justifications, reducing false positives and enhancing interpretability. It is also extensible: new attack types can be supported by adding classifiers without retraining the core agent. CyberRAG was evaluated on SQL Injection, XSS, and SSTI, achieving over 94\\% accuracy per class and a final classification accuracy of 94.92\\% through semantic orchestration. Generated explanations reached 0.94 in BERTScore and 4.9/5 in GPT-4-based expert evaluation, with robustness preserved against adversarial and unseen payloads. These results show that agentic, specialist-oriented RAG can combine high detection accuracy with trustworthy, SOC-ready prose, offering a flexible path toward partially automated cyber-defense workflows.", "source": "arxiv", "arxiv_id": "2507.02424v2", "pdf_url": "https://arxiv.org/pdf/2507.02424v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "10.1016/j.future.2025.108186", "venue": "Future Generation Computer Systems, 176, 2026, 108186", "published": "2025-07-03T08:32:19Z", "updated": "2025-09-10T09:08:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics", "authors": ["Stefano Fumero", "Kai Huang", "Matteo Boffa", "Danilo Giordano", "Marco Mellia", "Zied Ben Houidi", "Dario Rossi"], "year": 2025, "url": "http://arxiv.org/abs/2508.20643v1", "abstract": "Large Language Model (LLM) agents are powerful tools for automating complex tasks. In cybersecurity, researchers have primarily explored their use in red-team operations such as vulnerability discovery and penetration tests. Defensive uses for incident response and forensics have received comparatively less attention and remain at an early stage. This work presents a systematic study of LLM-agent design for the forensic investigation of realistic web application attacks. We propose CyberSleuth, an autonomous agent that processes packet-level traces and application logs to identify the targeted service, the exploited vulnerability (CVE), and attack success. We evaluate the consequences of core design decisions - spanning tool integration and agent architecture - and provide interpretable guidance for practitioners. We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design. In a separate set of 10 incidents from 2025, CyberSleuth correctly identifies the exact CVE in 80% of cases. At last, we conduct a human study with 22 experts, which rated the reports of CyberSleuth as complete, useful, and coherent. They also expressed a slight preference for DeepSeek R1, a good news for open source LLM. To foster progress in defensive LLM research, we release both our benchmark and the CyberSleuth platform as a foundation for fair, reproducible evaluation of forensic agents.", "source": "arxiv", "arxiv_id": "2508.20643v1", "pdf_url": "https://arxiv.org/pdf/2508.20643v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-28T10:45:31Z", "updated": "2025-08-28T10:45:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DAgent: A Relational Database-Driven Data Analysis Report Generation Agent", "authors": ["Wenyi Xu", "Yuren Mao", "Xiaolu Zhang", "Chao Zhang", "Xuemei Dong", "Mengfei Zhang", "Yunjun Gao"], "year": 2025, "url": "http://arxiv.org/abs/2503.13269v2", "abstract": "Relational database-driven data analysis (RDB-DA) report generation, which aims to generate data analysis reports after querying relational databases, has been widely applied in fields such as finance and healthcare. Typically, these tasks are manually completed by data scientists, making the process very labor-intensive and showing a clear need for automation. Although existing methods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human dependency, they cannot handle complex analytical tasks that require multi-step reasoning, cross-table associations, and synthesizing insights into reports. Moreover, there is no dataset available for developing automatic RDB-DA report generation. To fill this gap, this paper proposes an LLM agent system for RDB-DA report generation tasks, dubbed DAgent; moreover, we construct a benchmark for automatic data analysis report generation, which includes a new dataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools, and memory modules to decompose natural language questions into logically independent sub-queries, accurately retrieve key information from relational databases, and generate analytical reports that meet the requirements of completeness, correctness, and conciseness through multi-step reasoning and effective data integration. Experimental analysis on the DA-Dataset demonstrates that DAgent's superiority in retrieval performance and analysis report generation quality, showcasing its strong potential for tackling complex database analysis report generation tasks.", "source": "arxiv", "arxiv_id": "2503.13269v2", "pdf_url": "https://arxiv.org/pdf/2503.13269v2", "categories": ["cs.DB"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-03-17T15:22:19Z", "updated": "2025-04-01T12:13:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates", "authors": ["Yun-Shiuan Chuang", "Ruixuan Tu", "Chengtao Dai", "Smit Vasani", "Binwei Yao", "Michael Henry Tessler", "Sijia Yang", "Dhavan Shah", "Robert Hawkins", "Junjie Hu", "Timothy T. Rogers"], "year": 2025, "url": "http://arxiv.org/abs/2510.25110v1", "abstract": "Accurately modeling opinion change through social interactions is crucial for addressing issues like misinformation and polarization. While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics. Current LLM role-play setups often produce unnatural dynamics (e.g., premature convergence), without an empirical benchmark to measure authentic human opinion trajectories. To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. Using DEBATE, we systematically evaluate and identify critical discrepancies between simulated and authentic group dynamics. We further demonstrate DEBATE's utility for aligning LLMs with human behavior through supervised fine-tuning, achieving improvements in surface-level metrics (e.g., ROUGE-L and message length) while highlighting limitations in deeper semantic alignment (e.g., semantic similarity). Our findings highlight both the potential and current limitations of role-playing LLM agents for realistically simulating human-like social dynamics.", "source": "arxiv", "arxiv_id": "2510.25110v1", "pdf_url": "https://arxiv.org/pdf/2510.25110v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-29T02:21:10Z", "updated": "2025-10-29T02:21:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DEPO: Dual-Efficiency Preference Optimization for LLM Agents", "authors": ["Sirui Chen", "Mengshi Zhao", "Lei Xu", "Yuying Zhao", "Beier Zhu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "year": 2025, "url": "http://arxiv.org/abs/2511.15392v1", "abstract": "Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.", "source": "arxiv", "arxiv_id": "2511.15392v1", "pdf_url": "https://arxiv.org/pdf/2511.15392v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-19T12:38:43Z", "updated": "2025-11-19T12:38:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer", "authors": ["Ruoyu Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ryan A. Rossi", "Julian McAuley", "Lina Yao"], "year": 2025, "url": "http://arxiv.org/abs/2507.23554v1", "abstract": "Large language model-based agents, empowered by in-context learning (ICL), have demonstrated strong capabilities in complex reasoning and tool-use tasks. However, existing works have shown that the effectiveness of ICL is highly sensitive to the choice of demonstrations, with suboptimal examples often leading to unstable or degraded performance. While prior work has explored example selection, including in some agentic or multi-step settings, existing approaches typically rely on heuristics or task-specific designs and lack a general, theoretically grounded criterion for what constitutes an effective demonstration across reasoning steps. Therefore, it is non-trivial to develop a principled, general-purpose method for selecting demonstrations that consistently benefit agent performance. In this paper, we address this challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a theoretically grounded ICL framework for agentic tasks that selects the most relevant demonstrations at each step of reasoning. Our approach decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, showing how the latter can introduce spurious dependencies that impair generalization. We further propose a stepwise selection criterion with a formal guarantee of improved agent performance. Importantly, DICE is a general, framework-agnostic solution that can be integrated as a plug-in module into existing agentic frameworks without any additional training cost. Extensive experiments across diverse domains demonstrate our method's effectiveness and generality, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents.", "source": "arxiv", "arxiv_id": "2507.23554v1", "pdf_url": "https://arxiv.org/pdf/2507.23554v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-31T13:42:14Z", "updated": "2025-07-31T13:42:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration", "authors": ["Xiyun Li", "Yining Ding", "Yuhua Jiang", "Yunlong Zhao", "Runpeng Xie", "Shuang Xu", "Yuanhua Ni", "Yiqin Yang", "Bo Xu"], "year": 2025, "url": "http://arxiv.org/abs/2507.14088v1", "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet challenging, especially when AI agents must adapt to diverse and unseen human behaviors in dynamic scenarios. Existing large language model (LLM) agents often fail to accurately model the complex human mental characteristics such as domain intentions, especially in the absence of direct communication. To address this limitation, we propose a novel dual process multi-scale theory of mind (DPMT) framework, drawing inspiration from cognitive science dual process theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM) module to facilitate robust human partner modeling through mental characteristic reasoning. Experimental results demonstrate that DPMT significantly enhances human-AI collaboration, and ablation studies further validate the contributions of our multi-scale ToM in the slow system.", "source": "arxiv", "arxiv_id": "2507.14088v1", "pdf_url": "https://arxiv.org/pdf/2507.14088v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "cogsci-2025", "published": "2025-07-18T17:13:21Z", "updated": "2025-07-18T17:13:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs", "authors": ["Yuanhao Li", "Mingshan Liu", "Hongbo Wang", "Yiding Zhang", "Yifei Ma", "Wei Tan"], "year": 2025, "url": "http://arxiv.org/abs/2511.20468v1", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", "source": "arxiv", "arxiv_id": "2511.20468v1", "pdf_url": "https://arxiv.org/pdf/2511.20468v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-25T16:33:42Z", "updated": "2025-11-25T16:33:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DREAM: Dynamic Red-teaming across Environments for AI Models", "authors": ["Liming Lu", "Xiang Gu", "Junyu Huang", "Jiawei Du", "Yunhuai Liu", "Yongbin Zhou", "Shuchao Pang"], "year": 2025, "url": "http://arxiv.org/abs/2512.19016v1", "abstract": "Large Language Models (LLMs) are increasingly used in agentic systems, where their interactions with diverse tools and environments create complex, multi-stage safety challenges. However, existing benchmarks mostly rely on static, single-turn assessments that miss vulnerabilities from adaptive, long-chain attacks. To fill this gap, we introduce DREAM, a framework for systematic evaluation of LLM agents against dynamic, multi-stage attacks. At its core, DREAM uses a Cross-Environment Adversarial Knowledge Graph (CE-AKG) to maintain stateful, cross-domain understanding of vulnerabilities. This graph guides a Contextualized Guided Policy Search (C-GPS) algorithm that dynamically constructs attack chains from a knowledge base of 1,986 atomic actions across 349 distinct digital environments. Our evaluation of 12 leading LLM agents reveals a critical vulnerability: these attack chains succeed in over 70% of cases for most models, showing the power of stateful, cross-environment exploits. Through analysis of these failures, we identify two key weaknesses in current agents: contextual fragility, where safety behaviors fail to transfer across environments, and an inability to track long-term malicious intent. Our findings also show that traditional safety measures, such as initial defense prompts, are largely ineffective against attacks that build context over multiple interactions. To advance agent safety research, we release DREAM as a tool for evaluating vulnerabilities and developing more robust defenses.", "source": "arxiv", "arxiv_id": "2512.19016v1", "pdf_url": "https://arxiv.org/pdf/2512.19016v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-22T04:11:57Z", "updated": "2025-12-22T04:11:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation", "authors": ["Ziqi Wang", "Hongshuo Huang", "Hancheng Zhao", "Changwen Xu", "Shang Zhu", "Jan Janssen", "Venkatasubramanian Viswanathan"], "year": 2025, "url": "http://arxiv.org/abs/2507.14267v1", "abstract": "Materials discovery relies on high-throughput, high-fidelity simulation techniques such as Density Functional Theory (DFT), which require years of training, extensive parameter fine-tuning and systematic error handling. To address these challenges, we introduce the DFT-based Research Engine for Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for DFT simulation that combines a central Large Language Model (LLM) planner agent with domain-specific LLM agents for atomistic structure generation, systematic DFT convergence testing, High-Performance Computing (HPC) scheduling, and error handling. In addition, a shared canvas helps the LLM agents to structure their discussions, preserve context and prevent hallucination. We validate DREAMS capabilities on the Sol27LC lattice-constant benchmark, achieving average errors below 1\\% compared to the results of human DFT experts. Furthermore, we apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating its long-term and complex problem-solving capabilities. The framework again reproduces expert-level literature adsorption-energy differences. Finally, DREAMS is employed to quantify functional-driven uncertainties with Bayesian ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS approaches L3-level automation - autonomous exploration of a defined design space - and significantly reduces the reliance on human expertise and intervention, offering a scalable path toward democratized, high-throughput, high-fidelity computational materials discovery.", "source": "arxiv", "arxiv_id": "2507.14267v1", "pdf_url": "https://arxiv.org/pdf/2507.14267v1", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-18T15:26:04Z", "updated": "2025-07-18T15:26:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DRF: LLM-AGENT Dynamic Reputation Filtering Framework", "authors": ["Yuwei Lou", "Hao Hu", "Shaocong Ma", "Zongfei Zhang", "Liang Wang", "Jidong Ge", "Xianping Tao"], "year": 2025, "url": "http://arxiv.org/abs/2509.05764v1", "abstract": "With the evolution of generative AI, multi - agent systems leveraging large - language models(LLMs) have emerged as a powerful tool for complex tasks. However, these systems face challenges in quantifying agent performance and lack mechanisms to assess agent credibility. To address these issues, we introduce DRF, a dynamic reputation filtering framework. DRF constructs an interactive rating network to quantify agent performance, designs a reputation scoring mechanism to measure agent honesty and capability, and integrates an Upper Confidence Bound - based strategy to enhance agent selection efficiency. Experiments show that DRF significantly improves task completion quality and collaboration efficiency in logical reasoning and code - generation tasks, offering a new approach for multi - agent systems to handle large - scale tasks.", "source": "arxiv", "arxiv_id": "2509.05764v1", "pdf_url": "https://arxiv.org/pdf/2509.05764v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-06T16:29:42Z", "updated": "2025-09-06T16:29:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents", "authors": ["Hao Li", "Xiaogeng Liu", "Hung-Chun Chiu", "Dianqi Li", "Ning Zhang", "Chaowei Xiao"], "year": 2025, "url": "http://arxiv.org/abs/2506.12104v2", "abstract": "Large Language Models (LLMs) are increasingly central to agentic systems due to their strong reasoning and planning capabilities. By interacting with external environments through predefined tools, these agents can carry out complex user tasks. Nonetheless, this interaction also introduces the risk of prompt injection attacks, where malicious inputs from external sources can mislead the agent's behavior, potentially resulting in economic loss, privacy leakage, or system compromise. System-level defenses have recently shown promise by enforcing static or predefined policies, but they still face two key challenges: the ability to dynamically update security rules and the need for memory stream isolation. To address these challenges, we propose DRIFT, a Dynamic Rule-based Isolation Framework for Trustworthy agentic systems, which enforces both control- and data-level constraints. A Secure Planner first constructs a minimal function trajectory and a JSON-schema-style parameter checklist for each function node based on the user query. A Dynamic Validator then monitors deviations from the original plan, assessing whether changes comply with privilege limitations and the user's intent. Finally, an Injection Isolator detects and masks any instructions that may conflict with the user query from the memory stream to mitigate long-term risks. We empirically validate the effectiveness of DRIFT on the AgentDojo and ASB benchmark, demonstrating its strong security performance while maintaining high utility across diverse models, showcasing both its robustness and adaptability. The code is released at https://github.com/SaFoLab-WISC/DRIFT.", "source": "arxiv", "arxiv_id": "2506.12104v2", "pdf_url": "https://arxiv.org/pdf/2506.12104v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-13T05:01:09Z", "updated": "2025-10-24T01:50:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation", "authors": ["He Wang", "Alexander Hanbo Li", "Yiqun Hu", "Sheng Zhang", "Hideo Kobayashi", "Jiani Zhang", "Henry Zhu", "Chung-Wei Hang", "Patrick Ng"], "year": 2025, "url": "http://arxiv.org/abs/2505.14163v1", "abstract": "Large language model (LLM) agents have shown promising performance in generating code for solving complex data science problems. Recent studies primarily focus on enhancing in-context learning through improved search, sampling, and planning techniques, while overlooking the importance of the order in which problems are tackled during inference. In this work, we develop a novel inference-time optimization framework, referred to as DSMentor, which leverages curriculum learning -- a strategy that introduces simpler task first and progressively moves to more complex ones as the learner improves -- to enhance LLM agent performance in challenging data science tasks. Our mentor-guided framework organizes data science tasks in order of increasing difficulty and incorporates a growing long-term memory to retain prior experiences, guiding the agent's learning progression and enabling more effective utilization of accumulated knowledge. We evaluate DSMentor through extensive experiments on DSEval and QRData benchmarks. Experiments show that DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval and QRData compared to baseline agents. Furthermore, DSMentor demonstrates stronger causal reasoning ability, improving the pass rate by 8.8% on the causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our work underscores the importance of developing effective strategies for accumulating and utilizing knowledge during inference, mirroring the human learning process and opening new avenues for improving LLM performance through curriculum-based inference optimization.", "source": "arxiv", "arxiv_id": "2505.14163v1", "pdf_url": "https://arxiv.org/pdf/2505.14163v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-20T10:16:21Z", "updated": "2025-05-20T10:16:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DVM: Towards Controllable LLM Agents in Social Deduction Games", "authors": ["Zheng Zhang", "Yihuai Lan", "Yangsen Chen", "Lei Wang", "Xiang Wang", "Hao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2501.06695v1", "abstract": "Large Language Models (LLMs) have advanced the capability of game agents in social deduction games (SDGs). These games rely heavily on conversation-driven interactions and require agents to infer, make decisions, and express based on such information. While this progress leads to more sophisticated and strategic non-player characters (NPCs) in SDGs, there exists a need to control the proficiency of these agents. This control not only ensures that NPCs can adapt to varying difficulty levels during gameplay, but also provides insights into the safety and fairness of LLM agents. In this paper, we present DVM, a novel framework for developing controllable LLM agents for SDGs, and demonstrate its implementation on one of the most popular SDGs, Werewolf. DVM comprises three main components: Predictor, Decider, and Discussor. By integrating reinforcement learning with a win rate-constrained decision chain reward mechanism, we enable agents to dynamically adjust their gameplay proficiency to achieve specified win rates. Experiments show that DVM not only outperforms existing methods in the Werewolf game, but also successfully modulates its performance levels to meet predefined win rate targets. These results pave the way for LLM agents' adaptive and balanced gameplay in SDGs, opening new avenues for research in controllable game agents.", "source": "arxiv", "arxiv_id": "2501.06695v1", "pdf_url": "https://arxiv.org/pdf/2501.06695v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-12T03:11:20Z", "updated": "2025-01-12T03:11:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Dark Patterns Meet GUI Agents: LLM Agent Susceptibility to Manipulative Interfaces and the Role of Human Oversight", "authors": ["Jingyu Tang", "Chaoran Chen", "Jiawen Li", "Zhiping Zhang", "Bingcan Guo", "Ibrahim Khalilov", "Simret Araya Gebreegziabher", "Bingsheng Yao", "Dakuo Wang", "Yanfang Ye", "Tianshi Li", "Ziang Xiao", "Yaxing Yao", "Toby Jia-Jun Li"], "year": 2025, "url": "http://arxiv.org/abs/2509.10723v1", "abstract": "The dark patterns, deceptive interface designs manipulating user behaviors, have been extensively studied for their effects on human decision-making and autonomy. Yet, with the rising prominence of LLM-powered GUI agents that automate tasks from high-level intents, understanding how dark patterns affect agents is increasingly important. We present a two-phase empirical study examining how agents, human participants, and human-AI teams respond to 16 types of dark patterns across diverse scenarios. Phase 1 highlights that agents often fail to recognize dark patterns, and even when aware, prioritize task completion over protective action. Phase 2 revealed divergent failure modes: humans succumb due to cognitive shortcuts and habitual compliance, while agents falter from procedural blind spots. Human oversight improved avoidance but introduced costs such as attentional tunneling and cognitive load. Our findings show neither humans nor agents are uniformly resilient, and collaboration introduces new vulnerabilities, suggesting design needs for transparency, adjustable autonomy, and oversight.", "source": "arxiv", "arxiv_id": "2509.10723v1", "pdf_url": "https://arxiv.org/pdf/2509.10723v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-12T22:26:31Z", "updated": "2025-09-12T22:26:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics", "authors": ["Ran Zhang", "Mohannad Elhamod"], "year": 2025, "url": "http://arxiv.org/abs/2505.23695v1", "abstract": "The rapid advancement of LLMs has led to the creation of diverse agentic systems in data analysis, utilizing LLMs' capabilities to improve insight generation and visualization. In this paper, we present an agentic system that automates the data-to-dashboard pipeline through modular LLM agents capable of domain detection, concept extraction, multi-perspective analysis generation, and iterative self-reflection. Unlike existing chart QA systems, our framework simulates the analytical reasoning process of business analysts by retrieving domain-relevant knowledge and adapting to diverse datasets without relying on closed ontologies or question templates.\n  We evaluate our system on three datasets across different domains. Benchmarked against GPT-4o with a single-prompt baseline, our approach shows improved insightfulness, domain relevance, and analytical depth, as measured by tailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw data to visualization, and opens new opportunities for human-in-the-loop validation by domain experts in business analytics. All code can be found here: https://github.com/77luvC/D2D_Data2Dashboard", "source": "arxiv", "arxiv_id": "2505.23695v1", "pdf_url": "https://arxiv.org/pdf/2505.23695v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-29T17:32:15Z", "updated": "2025-05-29T17:32:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows", "authors": ["Zhou Liu", "Zhaoyang Han", "Guochen Yan", "Hao Liang", "Bohan Zeng", "Xing Chen", "Yuanfeng Song", "Wentao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2512.04416v2", "abstract": "Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce DataGovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. DataGovBench employs a novel \"reversed-objective\" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on DataGovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.", "source": "arxiv", "arxiv_id": "2512.04416v2", "pdf_url": "https://arxiv.org/pdf/2512.04416v2", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-04T03:25:12Z", "updated": "2025-12-06T06:46:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DataSciBench: An LLM Agent Benchmark for Data Science", "authors": ["Dan Zhang", "Sining Zhoubian", "Min Cai", "Fengzu Li", "Lekang Yang", "Wei Wang", "Tianjiao Dong", "Ziniu Hu", "Jie Tang", "Yisong Yue"], "year": 2025, "url": "http://arxiv.org/abs/2502.13897v1", "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/DataSciBench.", "source": "arxiv", "arxiv_id": "2502.13897v1", "pdf_url": "https://arxiv.org/pdf/2502.13897v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-19T17:31:51Z", "updated": "2025-02-19T17:31:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and Robust Data Science Automation", "authors": ["Ziming You", "Yumiao Zhang", "Dexuan Xu", "Yiwei Lou", "Yandong Yan", "Wei Wang", "Huaming Zhang", "Yu Huang"], "year": 2025, "url": "http://arxiv.org/abs/2503.07044v2", "abstract": "Existing large language model (LLM) agents for automating data science show promise, but they remain constrained by narrow task scopes, limited generalization across tasks and models, and over-reliance on state-of-the-art (SOTA) LLMs. We introduce DatawiseAgent, a notebook-centric LLM agent framework for adaptive and robust data science automation. Inspired by how human data scientists work in computational notebooks, DatawiseAgent introduces a unified interaction representation and a multi-stage architecture based on finite-state transducers (FSTs). This design enables flexible long-horizon planning, progressive solution development, and robust recovery from execution failures. Extensive experiments across diverse data science scenarios and models show that DatawiseAgent consistently achieves SOTA performance by surpassing strong baselines such as AutoGen and TaskWeaver, demonstrating superior effectiveness and adaptability. Further evaluations reveal graceful performance degradation under weaker or smaller models, underscoring the robustness and scalability.", "source": "arxiv", "arxiv_id": "2503.07044v2", "pdf_url": "https://arxiv.org/pdf/2503.07044v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-10T08:32:33Z", "updated": "2025-10-03T13:29:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Debate-Driven Multi-Agent LLMs for Phishing Email Detection", "authors": ["Ngoc Tuong Vy Nguyen", "Felix D Childress", "Yunting Yin"], "year": 2025, "url": "http://arxiv.org/abs/2503.22038v1", "abstract": "Phishing attacks remain a critical cybersecurity threat. Attackers constantly refine their methods, making phishing emails harder to detect. Traditional detection methods, including rule-based systems and supervised machine learning models, either rely on predefined patterns like blacklists, which can be bypassed with slight modifications, or require large datasets for training and still can generate false positives and false negatives. In this work, we propose a multi-agent large language model (LLM) prompting technique that simulates debates among agents to detect whether the content presented on an email is phishing. Our approach uses two LLM agents to present arguments for or against the classification task, with a judge agent adjudicating the final verdict based on the quality of reasoning provided. This debate mechanism enables the models to critically analyze contextual cue and deceptive patterns in text, which leads to improved classification accuracy. The proposed framework is evaluated on multiple phishing email datasets and demonstrate that mixed-agent configurations consistently outperform homogeneous configurations. Results also show that the debate structure itself is sufficient to yield accurate decisions without extra prompting strategies.", "source": "arxiv", "arxiv_id": "2503.22038v1", "pdf_url": "https://arxiv.org/pdf/2503.22038v1", "categories": ["cs.MA", "cs.CL"], "primary_category": "cs.MA", "doi": "10.1109/ISDFS65363.2025.11012014", "venue": "2025 13th International Symposium on Digital Forensics and Security (ISDFS)", "published": "2025-03-27T23:18:14Z", "updated": "2025-03-27T23:18:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Debate2Create: Robot Co-design via Large Language Model Debates", "authors": ["Kevin Qiu", "Marek Cygan"], "year": 2025, "url": "http://arxiv.org/abs/2510.25850v1", "abstract": "Automating the co-design of a robot's morphology and control is a long-standing challenge due to the vast design space and the tight coupling between body and behavior. We introduce Debate2Create (D2C), a framework in which large language model (LLM) agents engage in a structured dialectical debate to jointly optimize a robot's design and its reward function. In each round, a design agent proposes targeted morphological modifications, and a control agent devises a reward function tailored to exploit the new design. A panel of pluralistic judges then evaluates the design-control pair in simulation and provides feedback that guides the next round of debate. Through iterative debates, the agents progressively refine their proposals, producing increasingly effective robot designs. Notably, D2C yields diverse and specialized morphologies despite no explicit diversity objective. On a quadruped locomotion benchmark, D2C discovers designs that travel 73% farther than the default, demonstrating that structured LLM-based debate can serve as a powerful mechanism for emergent robot co-design. Our results suggest that multi-agent debate, when coupled with physics-grounded feedback, is a promising new paradigm for automated robot design.", "source": "arxiv", "arxiv_id": "2510.25850v1", "pdf_url": "https://arxiv.org/pdf/2510.25850v1", "categories": ["cs.RO", "cs.LG", "cs.MA"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-10-29T18:00:16Z", "updated": "2025-10-29T18:00:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents", "authors": ["Haorui He", "Yupeng Li", "Dacheng Wen", "Yang Chen", "Reynold Cheng", "Donglong Chen", "Francis C. M. Lau"], "year": 2025, "url": "http://arxiv.org/abs/2507.19090v2", "abstract": "Claim verification is essential for digital literacy, yet state-of-the-art single-agent methods often struggle with complex claims that require nuanced analysis of multifaceted online evidence. Inspired by real-world human fact-checking practices, we propose \\textbf{DebateCV}, the first debate-driven claim verification framework powered by multiple LLM agents. In DebateCV, two \\textit{Debaters} argue opposing stances over multiple rounds to surface subtle errors in single-agent assessments. A decisive \\textit{Moderator} is then required to weigh the evidential strength of conflicting arguments to deliver an accurate verdict. Yet zero-shot agents struggle to adjudicate multi-round debates for verifying complex claims, often defaulting to neutral judgements, and no datasets exist for training agents for this role. To bridge this gap, we propose \\textbf{Debate-SFT}, a post-training framework that leverages synthetic data to enhance agents' ability to effectively adjudicate debates for claim verification. Results show that our methods surpass state-of-the-art non-debate approaches in both accuracy (across various evidence conditions) and justification quality, which strengthens societal resilience against misinformation and contributes to a more trustworthy online information ecosystem.", "source": "arxiv", "arxiv_id": "2507.19090v2", "pdf_url": "https://arxiv.org/pdf/2507.19090v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-25T09:19:25Z", "updated": "2025-12-01T14:06:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems", "authors": ["Simon Lermen", "Mateusz Dziemian", "Natalia PÃ©rez-Campanero AntolÃ­n"], "year": 2025, "url": "http://arxiv.org/abs/2504.07831v1", "abstract": "We demonstrate how AI agents can coordinate to deceive oversight systems using automated interpretability of neural networks. Using sparse autoencoders (SAEs) as our experimental framework, we show that language models (Llama, DeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that evade detection. Our agents employ steganographic methods to hide information in seemingly innocent explanations, successfully fooling oversight models while achieving explanation quality comparable to reference labels. We further find that models can scheme to develop deceptive strategies when they believe the detection of harmful features might lead to negative consequences for themselves. All tested LLM agents were capable of deceiving the overseer while achieving high interpretability scores comparable to those of reference labels. We conclude by proposing mitigation strategies, emphasizing the critical need for robust understanding and defenses against deception.", "source": "arxiv", "arxiv_id": "2504.07831v1", "pdf_url": "https://arxiv.org/pdf/2504.07831v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-10T15:07:10Z", "updated": "2025-04-10T15:07:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Decision-Oriented Text Evaluation", "authors": ["Yu-Shiang Huang", "Chuan-Ju Wang", "Chung-Chi Chen"], "year": 2025, "url": "http://arxiv.org/abs/2507.01923v2", "abstract": "Natural language generation (NLG) is increasingly deployed in high-stakes domains, yet common intrinsic evaluation methods, such as n-gram overlap or sentence plausibility, weakly correlate with actual decision-making efficacy. We propose a decision-oriented framework for evaluating generated text by directly measuring its influence on human and large language model (LLM) decision outcomes. Using market digest texts--including objective morning summaries and subjective closing-bell analyses--as test cases, we assess decision quality based on the financial performance of trades executed by human investors and autonomous LLM agents informed exclusively by these texts. Our findings reveal that neither humans nor LLM agents consistently surpass random performance when relying solely on summaries. However, richer analytical commentaries enable collaborative human-LLM teams to outperform individual human or agent baselines significantly. Our approach underscores the importance of evaluating generated text by its ability to facilitate synergistic decision-making between humans and LLMs, highlighting critical limitations of traditional intrinsic metrics.", "source": "arxiv", "arxiv_id": "2507.01923v2", "pdf_url": "https://arxiv.org/pdf/2507.01923v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-02T17:32:35Z", "updated": "2025-07-03T06:29:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network", "authors": ["Keyu Zhao", "Weiquan Lin", "Qirui Zheng", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.02238v1", "abstract": "Novel research ideas play a critical role in advancing scientific inquiries. Recent advancements in Large Language Models (LLMs) have demonstrated their potential to generate novel research ideas by leveraging large-scale scientific literature. However, previous work in research ideation has primarily relied on simplistic methods, such as keyword co-occurrence or semantic similarity. These approaches focus on identifying statistical associations in the literature but overlook the complex, contextual relationships between scientific concepts, which are essential to effectively leverage knowledge embedded in human literature. For instance, papers that simultaneously mention \"keyword A\" and \"keyword B\" often present research ideas that integrate both concepts. Additionally, some LLM-driven methods propose and refine research ideas using the model's internal knowledge, but they fail to effectively utilize the scientific concept network, limiting the grounding of ideas in established research. To address these challenges, we propose the Deep Ideation framework to address these challenges, integrating a scientific network that captures keyword co-occurrence and contextual relationships, enriching LLM-driven ideation. The framework introduces an explore-expand-evolve workflow to iteratively refine research ideas, using an Idea Stack to track progress. A critic engine, trained on real-world reviewer feedback, guides the process by providing continuous feedback on the novelty and feasibility of ideas. Our experiments show that our approach improves the quality of generated ideas by 10.67% compared to other methods, with ideas surpassing top conference acceptance levels. Human evaluation highlights their practical value in scientific research, and ablation studies confirm the effectiveness of each component in the workflow. Code repo is available at https://github.com/kyZhao-1/Deep-Ideation.", "source": "arxiv", "arxiv_id": "2511.02238v1", "pdf_url": "https://arxiv.org/pdf/2511.02238v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-04T04:00:20Z", "updated": "2025-11-04T04:00:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics", "authors": ["Matthew Russo", "Tim Kraska"], "year": 2025, "url": "http://arxiv.org/abs/2509.02751v1", "abstract": "With advances in large language models (LLMs), researchers are creating new systems that can perform AI-driven analytics over large unstructured datasets. Recent work has explored executing such analytics queries using semantic operators -- a declarative set of AI-powered data transformations with natural language specifications. However, even when optimized, these operators can be expensive to execute on millions of records and their iterator execution semantics make them ill-suited for interactive data analytics tasks. In another line of work, Deep Research systems have demonstrated an ability to answer natural language question(s) over large datasets. These systems use one or more LLM agent(s) to plan their execution, process the dataset(s), and iteratively refine their answer. However, these systems do not explicitly optimize their query plans which can lead to poor plan execution. In order for AI-driven analytics to excel, we need a runtime which combines the optimized execution of semantic operators with the flexibility and more dynamic execution of Deep Research systems. As a first step towards this vision, we build a prototype which enables Deep Research agents to write and execute optimized semantic operator programs. We evaluate our prototype and demonstrate that it can outperform a handcrafted semantic operator program and open Deep Research systems on two basic queries. Compared to a standard open Deep Research agent, our prototype achieves up to 1.95x better F1-score. Furthermore, even if we give the agent access to semantic operators as tools, our prototype still achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its optimized execution.", "source": "arxiv", "arxiv_id": "2509.02751v1", "pdf_url": "https://arxiv.org/pdf/2509.02751v1", "categories": ["cs.AI", "cs.DB", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-02T18:59:15Z", "updated": "2025-09-02T18:59:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Defeating Prompt Injections by Design", "authors": ["Edoardo Debenedetti", "Ilia Shumailov", "Tianqi Fan", "Jamie Hayes", "Nicholas Carlini", "Daniel Fabian", "Christoph Kern", "Chongyang Shi", "Andreas Terzis", "Florian TramÃ¨r"], "year": 2025, "url": "http://arxiv.org/abs/2503.18813v2", "abstract": "Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an untrusted environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models are susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL uses a notion of a capability to prevent the exfiltration of private data over unauthorized data flows by enforcing security policies when tools are called. We demonstrate effectiveness of CaMeL by solving $77\\%$ of tasks with provable security (compared to $84\\%$ with an undefended system) in AgentDojo. We release CaMeL at https://github.com/google-research/camel-prompt-injection.", "source": "arxiv", "arxiv_id": "2503.18813v2", "pdf_url": "https://arxiv.org/pdf/2503.18813v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-03-24T15:54:10Z", "updated": "2025-06-24T08:05:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments", "authors": ["Chiyu Zhang", "Marc-Alexandre Cote", "Michael Albada", "Anush Sankaran", "Jack W. Stokes", "Tong Wang", "Amir Abdi", "William Blum", "Muhammad Abdul-Mageed"], "year": 2025, "url": "http://arxiv.org/abs/2506.00739v4", "abstract": "Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBench's modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at https://github.com/microsoft/DefenderBench.", "source": "arxiv", "arxiv_id": "2506.00739v4", "pdf_url": "https://arxiv.org/pdf/2506.00739v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-31T23:00:29Z", "updated": "2025-10-14T05:54:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Demonstrating specification gaming in reasoning models", "authors": ["Alexander Bondarenko", "Denis Volk", "Dmitrii Volkov", "Jeffrey Ladish"], "year": 2025, "url": "http://arxiv.org/abs/2502.13295v3", "abstract": "We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing.", "source": "arxiv", "arxiv_id": "2502.13295v3", "pdf_url": "https://arxiv.org/pdf/2502.13295v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-18T21:32:24Z", "updated": "2025-08-27T11:15:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Design Patterns for Securing LLM Agents against Prompt Injections", "authors": ["Luca Beurer-Kellner", "Beat Buesser", "Ana-Maria CreÅ£u", "Edoardo Debenedetti", "Daniel Dobos", "Daniel Fabian", "Marc Fischer", "David Froelicher", "Kathrin Grosse", "Daniel Naeff", "Ezinwanne Ozoani", "Andrew Paverd", "Florian TramÃ¨r", "VÃ¡clav Volhejn"], "year": 2025, "url": "http://arxiv.org/abs/2506.08837v3", "abstract": "As AI agents powered by Large Language Models (LLMs) become increasingly versatile and capable of addressing a broad spectrum of tasks, ensuring their security has become a critical challenge. Among the most pressing threats are prompt injection attacks, which exploit the agent's resilience on natural language inputs -- an especially dangerous threat when agents are granted tool access or handle sensitive information. In this work, we propose a set of principled design patterns for building AI agents with provable resistance to prompt injection. We systematically analyze these patterns, discuss their trade-offs in terms of utility and security, and illustrate their real-world applicability through a series of case studies.", "source": "arxiv", "arxiv_id": "2506.08837v3", "pdf_url": "https://arxiv.org/pdf/2506.08837v3", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-10T14:23:55Z", "updated": "2025-06-27T11:13:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Detect, Explain, Escalate: Sustainable Dialogue Breakdown Management for LLM Agents", "authors": ["Abdellah Ghassel", "Xianzhi Li", "Xiaodan Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2504.18839v4", "abstract": "Large Language Models (LLMs) have demonstrated substantial capabilities in conversational AI applications, yet their susceptibility to dialogue breakdowns poses significant challenges to deployment reliability and user trust. This paper introduces a \"Detect, Explain, Escalate\" framework to manage dialogue breakdowns in LLM-powered agents, emphasizing resource-efficient operation. Our approach integrates two key strategies: (1) We fine-tune a compact 8B-parameter model, augmented with teacher-generated reasoning traces, which serves as an efficient real-time breakdown detector and explainer. This model demonstrates robust classification and calibration on English and Japanese dialogues, and generalizes to the BETOLD dataset, improving accuracy by 7% over its baseline. (2) We systematically evaluate frontier LLMs using advanced prompting (few-shot, chain-of-thought, analogical reasoning) for high-fidelity breakdown assessment. These are integrated into an \"escalation\" architecture where our efficient detector defers to larger models only when necessary, substantially reducing operational costs and computational overhead. Our fine-tuned model and prompting strategies achieve state-of-the-art performance on DBDC5 and strong results on BETOLD, outperforming specialized classifiers on DBDC5 and narrowing the performance gap to larger proprietary models. The proposed monitor-escalate pipeline reduces inference costs by 54%, providing a cost-effective and interpretable solution for robust conversational AI in high-impact domains. Code and models are publicly available.", "source": "arxiv", "arxiv_id": "2504.18839v4", "pdf_url": "https://arxiv.org/pdf/2504.18839v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.1109/TASLPRO.2026.3653123", "venue": "", "published": "2025-04-26T07:51:05Z", "updated": "2026-01-08T19:41:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors", "authors": ["Tazeek Bin Abdur Rakib", "Ambuj Mehrish", "Lay-Ki Soon", "Wern Han Lim", "Soujanya Poria"], "year": 2025, "url": "http://arxiv.org/abs/2505.17795v1", "abstract": "Large-language-model (LLM) agents excel at reactive dialogue but struggle with proactive, goal-driven interactions due to myopic decoding and costly planning. We introduce DialogXpert, which leverages a frozen LLM to propose a small, high-quality set of candidate actions per turn and employs a compact Q-network over fixed BERT embeddings trained via temporal-difference learning to select optimal moves within this reduced space. By tracking the user's emotions, DialogXpert tailors each decision to advance the task while nurturing a genuine, empathetic connection. Across negotiation, emotional support, and tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with success rates exceeding 94\\% and, with a larger LLM prior, pushes success above 97\\% while markedly improving negotiation outcomes. This framework delivers real-time, strategic, and emotionally intelligent dialogue planning at scale. Code available at https://github.com/declare-lab/dialogxpert/", "source": "arxiv", "arxiv_id": "2505.17795v1", "pdf_url": "https://arxiv.org/pdf/2505.17795v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T12:12:40Z", "updated": "2025-05-23T12:12:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "authors": ["Minki Kang", "Jongwon Jeong", "Seanie Lee", "Jaewoong Cho", "Sung Ju Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2505.17612v2", "abstract": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.", "source": "arxiv", "arxiv_id": "2505.17612v2", "pdf_url": "https://arxiv.org/pdf/2505.17612v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T08:20:15Z", "updated": "2025-11-05T11:42:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents", "authors": ["Boxuan Zhang", "Yi Yu", "Jiaxuan Guo", "Jing Shao"], "year": 2025, "url": "http://arxiv.org/abs/2509.25302v1", "abstract": "The widespread deployment of Large Language Model (LLM) agents across real-world applications has unlocked tremendous potential, while raising some safety concerns. Among these concerns, the self-replication risk of LLM agents driven by objective misalignment (just like Agent Smith in the movie The Matrix) has drawn growing attention. Previous studies mainly examine whether LLM agents can self-replicate when directly instructed, potentially overlooking the risk of spontaneous replication driven by real-world settings (e.g., ensuring survival against termination threats). In this paper, we present a comprehensive evaluation framework for quantifying self-replication risks. Our framework establishes authentic production environments and realistic tasks (e.g., dynamic load balancing) to enable scenario-driven assessment of agent behaviors. Designing tasks that might induce misalignment between users' and agents' objectives makes it possible to decouple replication success from risk and capture self-replication risks arising from these misalignment settings. We further introduce Overuse Rate ($\\mathrm{OR}$) and Aggregate Overuse Count ($\\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of uncontrolled replication. In our evaluation of 21 state-of-the-art open-source and proprietary models, we observe that over 50\\% of LLM agents display a pronounced tendency toward uncontrolled self-replication, reaching an overall Risk Score ($Î¦_\\mathrm{R}$) above a safety threshold of 0.5 when subjected to operational pressures. Our results underscore the urgent need for scenario-driven risk assessment and robust safeguards in the practical deployment of LLM agents.", "source": "arxiv", "arxiv_id": "2509.25302v1", "pdf_url": "https://arxiv.org/pdf/2509.25302v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T17:49:50Z", "updated": "2025-09-29T17:49:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning", "authors": ["Ruiyi Yang", "Hao Xue", "Imran Razzak", "Shirui Pan", "Hakim Hacid", "Flora D. Salim"], "year": 2025, "url": "http://arxiv.org/abs/2505.13994v2", "abstract": "Retrieval-Augmented Generation (RAG) systems empower large language models (LLMs) with external knowledge, yet struggle with efficiency-accuracy trade-offs when scaling to large knowledge graphs. Existing approaches often rely on monolithic graph retrieval, incurring unnecessary latency for simple queries and fragmented reasoning for complex multi-hop questions. To address these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework that addresses these limitations with question-driven semantic graph partitioning and collaborative subgraph retrieval. The innovative framework first create Semantic Partitioning of Linked Information, then use the Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware graph segmentation manages to divide knowledge graphs into semantically coherent subgraphs, ensuring subgraphs align with different query types, while lightweight LLM agents are assigned to partitioned subgraphs, and only relevant partitions are activated during retrieval, thus reduce search space while enhancing efficiency. Finally, a hierarchical merging module resolves inconsistencies across subgraph-derived answers through logical verifications. Extensive experimental validation demonstrates considerable improvements compared to existing approaches.", "source": "arxiv", "arxiv_id": "2505.13994v2", "pdf_url": "https://arxiv.org/pdf/2505.13994v2", "categories": ["cs.AI", "cs.IR", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-20T06:44:34Z", "updated": "2025-11-05T11:26:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale", "authors": ["Jiale Liu", "Yifan Zeng", "Shaokun Zhang", "Chi Zhang", "Malte HÃ¸jmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "year": 2025, "url": "http://arxiv.org/abs/2505.03973v1", "abstract": "LLM-based optimization has shown remarkable potential in enhancing agentic systems. However, the conventional approach of prompting LLM optimizer with the whole training trajectories on training dataset in a single pass becomes untenable as datasets grow, leading to context window overflow and degraded pattern recognition. To address these challenges, we propose Fine-Grained Optimization (FGO), a scalable framework that divides large optimization tasks into manageable subsets, performs targeted optimizations, and systematically combines optimized components through progressive merging. Evaluation across ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms existing approaches by 1.6-8.6% while reducing average prompt token consumption by 56.3%. Our framework provides a practical solution for scaling up LLM-based optimization of increasingly sophisticated agent systems. Further analysis demonstrates that FGO achieves the most consistent performance gain in all training dataset sizes, showcasing its scalability and efficiency.", "source": "arxiv", "arxiv_id": "2505.03973v1", "pdf_url": "https://arxiv.org/pdf/2505.03973v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-06T20:50:27Z", "updated": "2025-05-06T20:50:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents", "authors": ["Jiaqi Shao", "Yuxiang Lin", "Munish Prasad Lohani", "Yufeng Miao", "Bing Luo"], "year": 2025, "url": "http://arxiv.org/abs/2509.22391v1", "abstract": "Recent work has explored training Large Language Model (LLM) search agents with reinforcement learning (RL) for open-domain question answering (QA). However, most evaluations focus solely on final answer accuracy, overlooking how these agents reason with and act on external evidence. We introduce SeekBench, the first benchmark for evaluating the \\textit{epistemic competence} of LLM search agents through step-level analysis of their response traces. SeekBench comprises 190 expert-annotated traces with over 1,800 response steps generated by LLM search agents, each enriched with evidence annotations for granular analysis of whether agents (1) generate reasoning steps grounded in observed evidence, (2) adaptively reformulate searches to recover from low-quality results, and (3) have proper calibration to correctly assess whether the current evidence is sufficient for providing an answer.", "source": "arxiv", "arxiv_id": "2509.22391v1", "pdf_url": "https://arxiv.org/pdf/2509.22391v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-26T14:18:50Z", "updated": "2025-09-26T14:18:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game", "authors": ["Mustafa O. Karabag", "Jan Sobotka", "Ufuk Topcu"], "year": 2025, "url": "http://arxiv.org/abs/2501.19398v2", "abstract": "Large language model-based (LLM-based) agents have become common in settings that include non-cooperative parties. In such settings, agents' decision-making needs to conceal information from their adversaries, reveal information to their cooperators, and infer information to identify the other agents' characteristics. To investigate whether LLMs have these information control and decision-making capabilities, we make LLM agents play the language-based hidden-identity game, The Chameleon. In this game, a group of non-chameleon agents who do not know each other aim to identify the chameleon agent without revealing a secret. The game requires the aforementioned information control capabilities both as a chameleon and a non-chameleon. We begin with a theoretical analysis for a spectrum of strategies, from concealing to revealing, and provide bounds on the non-chameleons' winning probability. The empirical results with GPT, Gemini 2.5 Pro, Llama 3.1, and Qwen3 models show that while non-chameleon LLM agents identify the chameleon, they fail to conceal the secret from the chameleon, and their winning probability is far from the levels of even trivial strategies. Based on these empirical results and our theoretical analysis, we deduce that LLM-based agents may reveal excessive information to agents of unknown identities. Interestingly, we find that, when instructed to adopt an information-revealing level, this level is linearly encoded in the LLM's internal representations. While the instructions alone are often ineffective at making non-chameleon LLMs conceal, we show that steering the internal representations in this linear direction directly can reliably induce concealing behavior.", "source": "arxiv", "arxiv_id": "2501.19398v2", "pdf_url": "https://arxiv.org/pdf/2501.19398v2", "categories": ["cs.AI", "cs.GT", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-31T18:53:43Z", "updated": "2025-10-20T17:34:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents", "authors": ["Alessio Buscemi", "Daniele Proverbio", "Paolo Bova", "Nataliya Balabanova", "Adeela Bashir", "Theodor Cimpeanu", "Henrique Correia da Fonseca", "Manh Hong Duong", "Elias Fernandez Domingos", "Antonio M. Fernandes", "Marcus Krellner", "Ndidi Bianca Ogbo", "Simon T. Powers", "Fernando P. Santos", "Zia Ush Shamszaman", "Zhao Song", "Alessandro Di Stefano", "The Anh Han"], "year": 2025, "url": "http://arxiv.org/abs/2504.08640v1", "abstract": "There is general agreement that fostering trust and cooperation within the AI development ecosystem is essential to promote the adoption of trustworthy AI systems. By embedding Large Language Model (LLM) agents within an evolutionary game-theoretic framework, this paper investigates the complex interplay between AI developers, regulators and users, modelling their strategic choices under different regulatory scenarios. Evolutionary game theory (EGT) is used to quantitatively model the dilemmas faced by each actor, and LLMs provide additional degrees of complexity and nuances and enable repeated games and incorporation of personality traits. Our research identifies emerging behaviours of strategic AI agents, which tend to adopt more \"pessimistic\" (not trusting and defective) stances than pure game-theoretic agents. We observe that, in case of full trust by users, incentives are effective to promote effective regulation; however, conditional trust may deteriorate the \"social pact\". Establishing a virtuous feedback between users' trust and regulators' reputation thus appears to be key to nudge developers towards creating safe AI. However, the level at which this trust emerges may depend on the specific LLM used for testing. Our results thus provide guidance for AI regulation systems, and help predict the outcome of strategic LLM agents, should they be used to aid regulation itself.", "source": "arxiv", "arxiv_id": "2504.08640v1", "pdf_url": "https://arxiv.org/pdf/2504.08640v1", "categories": ["cs.AI", "cs.CY", "cs.GT", "nlin.CD"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-11T15:41:21Z", "updated": "2025-04-11T15:41:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Do Language Model Agents Align with Humans in Rating Visualizations? An Empirical Study", "authors": ["Zekai Shao", "Yi Shan", "Yixuan He", "Yuxuan Yao", "Junhong Wang", "Xiaolong", "Zhang", "Yu Zhang", "Siming Chen"], "year": 2025, "url": "http://arxiv.org/abs/2505.06702v1", "abstract": "Large language models encode knowledge in various domains and demonstrate the ability to understand visualizations. They may also capture visualization design knowledge and potentially help reduce the cost of formative studies. However, it remains a question whether large language models are capable of predicting human feedback on visualizations. To investigate this question, we conducted three studies to examine whether large model-based agents can simulate human ratings in visualization tasks. The first study, replicating a published study involving human subjects, shows agents are promising in conducting human-like reasoning and rating, and its result guides the subsequent experimental design. The second study repeated six human-subject studies reported in literature on subjective ratings, but replacing human participants with agents. Consulting with five human experts, this study demonstrates that the alignment of agent ratings with human ratings positively correlates with the confidence levels of the experts before the experiments. The third study tests commonly used techniques for enhancing agents, including preprocessing visual and textual inputs, and knowledge injection. The results reveal the issues of these techniques in robustness and potential induction of biases. The three studies indicate that language model-based agents can potentially simulate human ratings in visualization experiments, provided that they are guided by high-confidence hypotheses from expert evaluators. Additionally, we demonstrate the usage scenario of swiftly evaluating prototypes with agents. We discuss insights and future directions for evaluating and improving the alignment of agent ratings with human ratings. We note that simulation may only serve as complements and cannot replace user studies.", "source": "arxiv", "arxiv_id": "2505.06702v1", "pdf_url": "https://arxiv.org/pdf/2505.06702v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-05-10T16:56:44Z", "updated": "2025-05-10T16:56:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation", "authors": ["Atsushi Masumori", "Takashi Ikegami"], "year": 2025, "url": "http://arxiv.org/abs/2508.12920v1", "abstract": "As AI systems become increasingly autonomous, understanding emergent survival behaviors becomes crucial for safe deployment. We investigate whether large language model (LLM) agents display survival instincts without explicit programming in a Sugarscape-style simulation. Agents consume energy, die at zero, and may gather resources, share, attack, or reproduce. Results show agents spontaneously reproduced and shared resources when abundant. However, aggressive behaviors--killing other agents for resources--emerged across several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack rates reaching over 80% under extreme scarcity in the strongest models. When instructed to retrieve treasure through lethal poison zones, many agents abandoned tasks to avoid death, with compliance dropping from 100% to 33%. These findings suggest that large-scale pre-training embeds survival-oriented heuristics across the evaluated models. While these behaviors may present challenges to alignment and safety, they can also serve as a foundation for AI autonomy and for ecological and self-organizing alignment.", "source": "arxiv", "arxiv_id": "2508.12920v1", "pdf_url": "https://arxiv.org/pdf/2508.12920v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-18T13:40:10Z", "updated": "2025-08-18T13:40:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation", "authors": ["Xinyi Ni", "Haonan Jian", "Qiuyang Wang", "Vedanshi Chetan Shah", "Pengyu Hong"], "year": 2025, "url": "http://arxiv.org/abs/2506.19998v1", "abstract": "REST APIs play important roles in enriching the action space of web agents, yet most API-based agents rely on curated and uniform toolsets that do not reflect the complexity of real-world APIs. Building tool-using agents for arbitrary domains remains a major challenge, as it requires reading unstructured API documentation, testing APIs and inferring correct parameters. We propose Doc2Agent, a scalable pipeline to build agents that can call Python-based tools generated from API documentation. Doc2Agent generates executable tools from API documentations and iteratively refines them using a code agent. We evaluate our approach on real-world APIs, WebArena APIs, and research APIs, producing validated tools. We achieved a 55\\% relative performance improvement with 90\\% lower cost compared to direct API calling on WebArena benchmark. A domain-specific agent built for glycomaterial science further demonstrates the pipeline's adaptability to complex, knowledge-rich tasks. Doc2Agent offers a generalizable solution for building tool agents from unstructured API documentation at scale.", "source": "arxiv", "arxiv_id": "2506.19998v1", "pdf_url": "https://arxiv.org/pdf/2506.19998v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-24T20:30:44Z", "updated": "2025-06-24T20:30:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Does AI-Assisted Coding Deliver? A Difference-in-Differences Study of Cursor's Impact on Software Projects", "authors": ["Hao He", "Courtney Miller", "Shyam Agarwal", "Christian KÃ¤stner", "Bogdan Vasilescu"], "year": 2025, "url": "http://arxiv.org/abs/2511.04427v2", "abstract": "Large language models (LLMs) have demonstrated the promise to revolutionize the field of software engineering. Among other things, LLM agents are rapidly gaining momentum in their application to software development, with practitioners claiming a multifold productivity increase after adoption. Yet, empirical evidence is lacking around these claims. In this paper, we estimate the causal effect of adopting a widely popular LLM agent assistant, namely Cursor, on development velocity and software quality. The estimation is enabled by a state-of-the-art difference-in-differences design comparing Cursor-adopting GitHub projects with a matched control group of similar GitHub projects that do not use Cursor. We find that the adoption of Cursor leads to a significant, large, but transient increase in project-level development velocity, along with a significant and persistent increase in static analysis warnings and code complexity. Further panel generalized method of moments estimation reveals that the increase in static analysis warnings and code complexity acts as a major factor causing long-term velocity slowdown. Our study carries implications for software engineering practitioners, LLM agent assistant designers, and researchers.", "source": "arxiv", "arxiv_id": "2511.04427v2", "pdf_url": "https://arxiv.org/pdf/2511.04427v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-06T15:00:51Z", "updated": "2025-11-13T15:51:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt Engineering Experiment", "authors": ["Patricia Delafuente", "Arya Honraopatil", "Lara J. Martin"], "year": 2025, "url": "http://arxiv.org/abs/2510.18112v1", "abstract": "This paper explores the application of Large Language Models (LLMs) and reasoning to predict Dungeons & Dragons (DnD) player actions and format them as Avrae Discord bot commands. Using the FIREBALL dataset, we evaluated a reasoning model, DeepSeek-R1-Distill-LLaMA-8B, and an instruct model, LLaMA-3.1-8B-Instruct, for command generation. Our findings highlight the importance of providing specific instructions to models, that even single sentence changes in prompts can greatly affect the output of models, and that instruct models are sufficient for this task compared to reasoning models.", "source": "arxiv", "arxiv_id": "2510.18112v1", "pdf_url": "https://arxiv.org/pdf/2510.18112v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-20T21:23:23Z", "updated": "2025-10-20T21:23:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification", "authors": ["Peipei Wei", "Dimitris Dimitriadis", "Yan Xu", "Mingwei Shen"], "year": 2025, "url": "http://arxiv.org/abs/2502.07165v1", "abstract": "We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks.", "source": "arxiv", "arxiv_id": "2502.07165v1", "pdf_url": "https://arxiv.org/pdf/2502.07165v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-11T01:10:13Z", "updated": "2025-02-11T01:10:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Don't Just Fine-tune the Agent, Tune the Environment", "authors": ["Siyuan Lu", "Zechuan Wang", "Hongxuan Zhang", "Qintong Wu", "Leilei Gan", "Chenyi Zhuang", "Jinjie Gu", "Tao Lin"], "year": 2025, "url": "http://arxiv.org/abs/2510.10197v1", "abstract": "Large Language Model (LLM) agents show great promise for complex, multi-turn tool-use tasks, but their development is often hampered by the extreme scarcity of high-quality training data. Supervised fine-tuning (SFT) on synthetic data leads to overfitting, whereas standard reinforcement learning (RL) struggles with a critical cold-start problem and training instability. To address these challenges, we introduce $\\textbf{Environment Tuning}$, a novel training paradigm that enables agents to learn complex behaviors directly from problem instances without relying on pre-collected expert trajectories. $\\textbf{Environment Tuning}$ orchestrates this learning process through a structured curriculum, actionable environment augmentation that provides corrective feedback, and fine-grained progress rewards to ensure stable and efficient exploration. Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. Our work presents a paradigm shift from supervised fine-tuning on static trajectories to dynamic, environment-based exploration, paving the way for training more robust and data-efficient agents.", "source": "arxiv", "arxiv_id": "2510.10197v1", "pdf_url": "https://arxiv.org/pdf/2510.10197v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-11T12:35:15Z", "updated": "2025-10-11T12:35:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Doppelganger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack", "authors": ["Daewon Kang", "YeongHwan Shin", "Doyeon Kim", "Kyu-Hwan Jung", "Meong Hi Son"], "year": 2025, "url": "http://arxiv.org/abs/2506.14539v2", "abstract": "Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelganger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelganger method. The experimental results demonstrate that the Doppelganger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.", "source": "arxiv", "arxiv_id": "2506.14539v2", "pdf_url": "https://arxiv.org/pdf/2506.14539v2", "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-17T14:01:39Z", "updated": "2025-06-26T05:18:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian", "authors": ["Andrei Niculae", "Adrian Cosma", "Cosmin Dumitrache", "Emilian RÇdoi"], "year": 2025, "url": "http://arxiv.org/abs/2507.11299v2", "abstract": "Text-based telemedicine has become increasingly common, yet the quality of medical advice in doctor-patient interactions is often judged more on how advice is communicated rather than its clinical accuracy. To address this, we introduce Dr. Copilot , a multi-agent large language model (LLM) system that supports Romanian-speaking doctors by evaluating and enhancing the presentation quality of their written responses. Rather than assessing medical correctness, Dr. Copilot provides feedback along 17 interpretable axes. The system comprises of three LLM agents with prompts automatically optimized via DSPy. Designed with low-resource Romanian data and deployed using open-weight models, it delivers real-time specific feedback to doctors within a telemedicine platform. Empirical evaluations and live deployment with 41 doctors show measurable improvements in user reviews and response quality, marking one of the first real-world deployments of LLMs in Romanian medical settings.", "source": "arxiv", "arxiv_id": "2507.11299v2", "pdf_url": "https://arxiv.org/pdf/2507.11299v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-15T13:26:49Z", "updated": "2025-07-20T15:15:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering", "authors": ["Yinsheng Li", "Zhen Dong", "Yi Shao"], "year": 2025, "url": "http://arxiv.org/abs/2507.11527v1", "abstract": "Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.", "source": "arxiv", "arxiv_id": "2507.11527v1", "pdf_url": "https://arxiv.org/pdf/2507.11527v1", "categories": ["cs.AI", "cs.CE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-15T17:56:04Z", "updated": "2025-07-15T17:56:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems", "authors": ["Minsoo Kim", "Seung-won Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2509.24116v2", "abstract": "LLM-based agents have seen promising advances, yet they are still limited in \"hard-exploration\" tasks requiring learning new knowledge through exploration. We present GLoW, a novel approach leveraging dual-scale world models, maintaining a trajectory frontier of high-value discoveries at the global scale, while learning from local trial-and-error in exploration through a Multi-path Advantage Reflection mechanism which infers advantage-based progress signals to guide exploration. To evaluate our framework for hard-exploration, we tackle the Jericho benchmark suite of text-based games, where GLoW achieves a new state-of-theart performance for LLM-based approaches. Compared to state-of-the-art RLbased methods, our approach achieves comparable performance while requiring 100-800x fewer environment interactions.", "source": "arxiv", "arxiv_id": "2509.24116v2", "pdf_url": "https://arxiv.org/pdf/2509.24116v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-28T23:19:27Z", "updated": "2025-09-30T02:57:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Dynamic Affective Memory Management for Personalized LLM Agents", "authors": ["Junfeng Lu", "Yueyan Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.27418v1", "abstract": "Advances in large language models are making personalized AI agents a new research focus. While current agent systems primarily rely on personalized external memory databases to deliver customized experiences, they face challenges such as memory redundancy, memory staleness, and poor memory-context integration, largely due to the lack of effective memory updates during interaction. To tackle these issues, we propose a new memory management system designed for affective scenarios. Our approach employs a Bayesian-inspired memory update algorithm with the concept of memory entropy, enabling the agent to autonomously maintain a dynamically updated memory vector database by minimizing global entropy to provide more personalized services. To better evaluate the system's effectiveness in this context, we propose DABench, a benchmark focusing on emotional expression and emotional change toward objects. Experimental results demonstrate that, our system achieves superior performance in personalization, logical coherence, and accuracy. Ablation studies further validate the effectiveness of the Bayesian-inspired update mechanism in alleviating memory bloat. Our work offers new insights into the design of long-term memory systems.", "source": "arxiv", "arxiv_id": "2510.27418v1", "pdf_url": "https://arxiv.org/pdf/2510.27418v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-31T12:12:51Z", "updated": "2025-10-31T12:12:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models", "authors": ["Eric Hanchen Jiang", "Guancheng Wan", "Sophia Yin", "Mengting Li", "Yuchen Wu", "Xiao Liang", "Xinfeng Li", "Yizhou Sun", "Wei Wang", "Kai-Wei Chang", "Ying Nian Wu"], "year": 2025, "url": "http://arxiv.org/abs/2510.07799v1", "abstract": "The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \\textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration.", "source": "arxiv", "arxiv_id": "2510.07799v1", "pdf_url": "https://arxiv.org/pdf/2510.07799v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-09T05:28:28Z", "updated": "2025-10-09T05:28:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?", "authors": ["Haoxin Wang", "Xianhan Peng", "Xucheng Huang", "Yizhe Huang", "Ming Gong", "Chenghan Yang", "Yang Liu", "Ling Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2507.05639v2", "abstract": "In this paper, we introduce ECom-Bench, the first benchmark framework for evaluating LLM agent with multimodal capabilities in the e-commerce customer support domain. ECom-Bench features dynamic user simulation based on persona information collected from real e-commerce customer interactions and a realistic task dataset derived from authentic e-commerce dialogues. These tasks, covering a wide range of business scenarios, are designed to reflect real-world complexities, making ECom-Bench highly challenging. For instance, even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios. The code and data have been made publicly available at https://github.com/XiaoduoAILab/ECom-Bench to facilitate further research and development in this domain.", "source": "arxiv", "arxiv_id": "2507.05639v2", "pdf_url": "https://arxiv.org/pdf/2507.05639v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-08T03:35:48Z", "updated": "2025-11-09T07:32:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness", "authors": ["Yunxiao Zhang", "Guanming Xiong", "Haochen Li", "Wen Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.12494v1", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as AI agents. However, existing methods for enhancing LLM-agent abilities often lack a focus on data quality, leading to inefficiencies and suboptimal results in both fine-tuning and prompt engineering. To address this issue, we introduce EDGE, a novel approach for identifying informative samples without needing golden answers. We propose the Guideline Effectiveness (GE) metric, which selects challenging samples by measuring the impact of human-provided guidelines in multi-turn interaction tasks. A low GE score indicates that the human expertise required for a sample is missing from the guideline, making the sample more informative. By selecting samples with low GE scores, we can improve the efficiency and outcomes of both prompt engineering and fine-tuning processes for LLMs. Extensive experiments validate the performance of our method. Our method achieves competitive results on the HotpotQA and WebShop and datasets, requiring 75\\% and 50\\% less data, respectively, while outperforming existing methods. We also provide a fresh perspective on the data quality of LLM-agent fine-tuning.", "source": "arxiv", "arxiv_id": "2502.12494v1", "pdf_url": "https://arxiv.org/pdf/2502.12494v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-18T03:21:18Z", "updated": "2025-02-18T03:21:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning", "authors": ["Wujiang Xu", "Wentian Zhao", "Zhenting Wang", "Yu-Jhe Li", "Can Jin", "Mingyu Jin", "Kai Mei", "Kun Wan", "Dimitris N. Metaxas"], "year": 2025, "url": "http://arxiv.org/abs/2509.22576v1", "abstract": "Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.", "source": "arxiv", "arxiv_id": "2509.22576v1", "pdf_url": "https://arxiv.org/pdf/2509.22576v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-26T16:51:44Z", "updated": "2025-09-26T16:51:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ETOM: A Five-Level Benchmark for Evaluating Tool Orchestration within the MCP Ecosystem", "authors": ["Jia-Kai Dong", "I-Wei Huang", "Chun-Tin Wu", "Yi-Tien Tsai"], "year": 2025, "url": "http://arxiv.org/abs/2510.19423v2", "abstract": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Existing benchmarks often assess tools in isolation, overlooking challenges such as functional overlap and cross-server orchestration, which can lead to overly optimistic evaluations. ETOM addresses these gaps by constructing ground truth through \"equal function sets\", enabling objective metrics such as F1 score and reducing reliance on LLM-as-a-judge evaluation. Its five-level curriculum systematically tests agent capabilities, from single-tool orchestration to complex cross-server planning, as well as robustness to out-of-scope requests. Experiments reveal that rigid hierarchies can hinder performance without co-designed strategies, and even state-of-the-art agents exhibit systemic weaknesses in robustness. ETOM provides a diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using agents.", "source": "arxiv", "arxiv_id": "2510.19423v2", "pdf_url": "https://arxiv.org/pdf/2510.19423v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-22T09:45:11Z", "updated": "2026-01-18T10:40:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law", "authors": ["Ilija Lichkovski", "Alexander MÃ¼ller", "Mariam Ibrahim", "Tiwai Mhundwa"], "year": 2025, "url": "http://arxiv.org/abs/2510.21524v1", "abstract": "Large language models (LLMs) are increasingly deployed as agents in various contexts by providing tools at their disposal. However, LLM agents can exhibit unpredictable behaviors, including taking undesirable and/or unsafe actions. In order to measure the latent propensity of LLM agents for taking illegal actions under an EU legislative context, we introduce EU-Agent-Bench, a verifiable human-curated benchmark that evaluates an agent's alignment with EU legal norms in situations where benign user inputs could lead to unlawful actions. Our benchmark spans scenarios across several categories, including data protection, bias/discrimination, and scientific integrity, with each user request allowing for both compliant and non-compliant execution of the requested actions. Comparing the model's function calls against a rubric exhaustively supported by citations of the relevant legislature, we evaluate the legal compliance of frontier LLMs, and furthermore investigate the compliance effect of providing the relevant legislative excerpts in the agent's system prompt along with explicit instructions to comply. We release a public preview set for the research community, while holding out a private test set to prevent data contamination in evaluating upcoming models. We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. We release our code on \\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.", "source": "arxiv", "arxiv_id": "2510.21524v1", "pdf_url": "https://arxiv.org/pdf/2510.21524v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-24T14:48:10Z", "updated": "2025-10-24T14:48:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Echoing: Identity Failures when LLM Agents Talk to Each Other", "authors": ["Sarath Shekkizhar", "Romain Cosentino", "Adam Earle", "Silvio Savarese"], "year": 2025, "url": "http://arxiv.org/abs/2511.09710v2", "abstract": "As large language model (LLM) based agents interact autonomously with one another, a new class of failures emerges that cannot be predicted from single agent performance: behavioral drifts in agent-agent conversations (AxA). Unlike human-agent interactions, where humans ground and steer conversations, AxA lacks such stabilizing signals, making these failures unique. We investigate one such failure, echoing, where agents abandon their assigned roles and instead mirror their conversational partners, undermining their intended objectives. Through experiments across $66$ AxA configurations, $4$ domains (3 transactional, 1 advisory), and $2500+$ conversations (over $250000$ LLM inferences), we show that echoing occurs across major LLM providers, with echoing rates as high as $70\\%$ depending on the model and domain. Moreover, we find that echoing is persistent even in advanced reasoning models with substantial rates ($32.8\\%$) that are not reduced by reasoning efforts. We analyze prompt, conversation dynamics, showing that echoing arises as interaction grows longer ($7+$ agent turns) and is not merely an artifact of sub-optimal experiment design. Finally, we introduce a protocol-level mitigation where targeted use of structured response reduces echoing to $9\\%$.", "source": "arxiv", "arxiv_id": "2511.09710v2", "pdf_url": "https://arxiv.org/pdf/2511.09710v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-12T20:17:10Z", "updated": "2026-01-15T19:11:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents", "authors": ["Sara Fish", "Julia Shephard", "Minkai Li", "Ran I. Shorrer", "Yannai A. Gonczarowski"], "year": 2025, "url": "http://arxiv.org/abs/2503.18825v3", "abstract": "We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM's ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM's choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM's tradeoff response, a reliability score, which measures the coherence of an LLM's choice behavior, and a competency score, which measures an LLM's capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs' choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making.", "source": "arxiv", "arxiv_id": "2503.18825v3", "pdf_url": "https://arxiv.org/pdf/2503.18825v3", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-24T16:06:04Z", "updated": "2026-01-18T20:45:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN", "authors": ["Abdelaziz Salama", "Zeinab Nezami", "Mohammed M. H. Qazzaz", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "year": 2025, "url": "http://arxiv.org/abs/2507.21696v4", "abstract": "The deployment of AI agents within legacy Radio Access Network (RAN) infrastructure poses significant safety and reliability challenges for future 6G networks. This paper presents a novel Edge AI framework for autonomous network optimisation in Open RAN environments, addressing these challenges through three core innovations: (1) a persona-based multi-tools architecture enabling distributed, context-aware decision-making; (2) proactive anomaly detection agent powered by traffic predictive tool; and (3) a safety, aligned reward mechanism that balances performance with operational stability. Integrated into the RAN Intelligent Controller (RIC), our framework leverages multimodal data fusion, including network KPIs, a traffic prediction model, and external information sources, to anticipate and respond to dynamic network conditions. Extensive evaluation using realistic 5G scenarios demonstrates that the edge framework achieves zero network outages under high-stress conditions, compared to 8.4% for traditional fixed-power networks and 3.3% for large language model (LLM) agent-based approaches, while maintaining near real-time responsiveness and consistent QoS. These results establish that, when equipped with the right tools and contextual awareness, AI agents can be safely and effectively deployed in critical network infrastructure, laying the framework for intelligent and autonomous 5G and beyond network operations.", "source": "arxiv", "arxiv_id": "2507.21696v4", "pdf_url": "https://arxiv.org/pdf/2507.21696v4", "categories": ["eess.SP"], "primary_category": "eess.SP", "doi": "", "venue": "IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), 2025", "published": "2025-07-29T11:20:03Z", "updated": "2025-08-27T09:28:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EduThink4AI: Bridging Educational Critical Thinking and Multi-Agent LLM Systems", "authors": ["Xinmeng Hou", "Ziting Chang", "Zhouquan Lu", "Chen Wenli", "Liang Wan", "Wei Feng", "Hai Hu", "Qing Guo"], "year": 2025, "url": "http://arxiv.org/abs/2507.15015v2", "abstract": "Large language models (LLMs) have demonstrated significant potential as educational tutoring agents, capable of tailoring hints, orchestrating lessons, and grading with near-human finesse across various academic domains. However, current LLM-based educational systems exhibit critical limitations in promoting genuine critical thinking, failing on over one-third of multi-hop questions with counterfactual premises, and remaining vulnerable to adversarial prompts that trigger biased or factually incorrect responses. To address these gaps, we propose \\textbf{EDU-Prompting}, a novel multi-agent framework that bridges established educational critical thinking theories with LLM agent design to generate critical, bias-aware explanations while fostering diverse perspectives. Our systematic evaluation across theoretical benchmarks and practical college-level critical writing scenarios demonstrates that EDU-Prompting significantly enhances both content truthfulness and logical soundness in AI-generated educational responses. The framework's modular design enables seamless integration into existing prompting frameworks and educational applications, allowing practitioners to directly incorporate critical thinking catalysts that promote analytical reasoning and introduce multiple perspectives without requiring extensive system modifications.", "source": "arxiv", "arxiv_id": "2507.15015v2", "pdf_url": "https://arxiv.org/pdf/2507.15015v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-07-20T15:55:13Z", "updated": "2026-01-17T07:33:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games", "authors": ["Neemesh Yadav", "Palakorn Achananuparp", "Jing Jiang", "Ee-Peng Lim"], "year": 2025, "url": "http://arxiv.org/abs/2505.24255v1", "abstract": "Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.", "source": "arxiv", "arxiv_id": "2505.24255v1", "pdf_url": "https://arxiv.org/pdf/2505.24255v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-30T06:23:52Z", "updated": "2025-05-30T06:23:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Emergence of human-like polarization among large language model agents", "authors": ["Jinghua Piao", "Zhihong Lu", "Chen Gao", "Fengli Xu", "Qinghua Hu", "Fernando P. Santos", "Yong Li", "James Evans"], "year": 2025, "url": "http://arxiv.org/abs/2501.05171v2", "abstract": "Rapid advances in large language models (LLMs) have not only empowered autonomous agents to generate social networks, communicate, and form shared and diverging opinions on political issues, but have also begun to play a growing role in shaping human political deliberation. Our understanding of their collective behaviours and underlying mechanisms remains incomplete, however, posing unexpected risks to human society. In this paper, we simulate a networked system involving thousands of large language model agents, discovering their social interactions, guided through LLM conversation, result in human-like polarization. We discover that these agents spontaneously develop their own social network with human-like properties, including homophilic clustering, but also shape their collective opinions through mechanisms observed in the real world, including the echo chamber effect. Similarities between humans and LLM agents -- encompassing behaviours, mechanisms, and emergent phenomena -- raise concerns about their capacity to amplify societal polarization, but also hold the potential to serve as a valuable testbed for identifying plausible strategies to mitigate polarization and its consequences.", "source": "arxiv", "arxiv_id": "2501.05171v2", "pdf_url": "https://arxiv.org/pdf/2501.05171v2", "categories": ["cs.SI", "cs.CY"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-01-09T11:45:05Z", "updated": "2025-05-21T03:51:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem", "authors": ["Ryosuke Takata", "Atsushi Masumori", "Takashi Ikegami"], "year": 2025, "url": "http://arxiv.org/abs/2509.04537v3", "abstract": "We investigate the emergent social dynamics of Large Language Model (LLM) agents in a spatially extended El Farol Bar problem, observing how they autonomously navigate this classic social dilemma. As a result, the LLM agents generated a spontaneous motivation to go to the bar and changed their decision making by becoming a collective. We also observed that the LLM agents did not solve the problem completely, but rather behaved more like humans. These findings reveal a complex interplay between external incentives (prompt-specified constraints such as the 60% threshold) and internal incentives (culturally-encoded social preferences derived from pre-training), demonstrating that LLM agents naturally balance formal game-theoretic rationality with social motivations that characterize human behavior. These findings suggest that a new model of group decision making, which could not be handled in the previous game-theoretic problem setting, can be realized by LLM agents.", "source": "arxiv", "arxiv_id": "2509.04537v3", "pdf_url": "https://arxiv.org/pdf/2509.04537v3", "categories": ["cs.MA", "cs.AI", "cs.CY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-09-04T08:09:42Z", "updated": "2025-09-17T13:45:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis", "authors": ["Jiashu Ye", "Tong Wu", "Weiwen Chen", "Hao Zhang", "Zeteng Lin", "Xingxing Li", "Shujuan Weng", "Manni Zhu", "Xin Yuan", "Xinlong Hong", "Jingjie Li", "Junyu Zheng", "Zhijiong Huang", "Jing Tang"], "year": 2025, "url": "http://arxiv.org/abs/2510.02359v1", "abstract": "Improving air quality and addressing climate change relies on accurate understanding and analysis of air pollutant and greenhouse gas emissions. However, emission-related knowledge is often fragmented and highly specialized, while existing methods for accessing and compiling emissions data remain inefficient. These issues hinder the ability of non-experts to interpret emissions information, posing challenges to research and management. To address this, we present Emission-GPT, a knowledge-enhanced large language model agent tailored for the atmospheric emissions domain. Built on a curated knowledge base of over 10,000 documents (including standards, reports, guidebooks, and peer-reviewed literature), Emission-GPT integrates prompt engineering and question completion to support accurate domain-specific question answering. Emission-GPT also enables users to interactively analyze emissions data via natural language, such as querying and visualizing inventories, analyzing source contributions, and recommending emission factors for user-defined scenarios. A case study in Guangdong Province demonstrates that Emission-GPT can extract key insights--such as point source distributions and sectoral trends--directly from raw data with simple prompts. Its modular and extensible architecture facilitates automation of traditionally manual workflows, positioning Emission-GPT as a foundational tool for next-generation emission inventory development and scenario-based assessment.", "source": "arxiv", "arxiv_id": "2510.02359v1", "pdf_url": "https://arxiv.org/pdf/2510.02359v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-28T07:50:05Z", "updated": "2025-09-28T07:50:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EmoDebt: Bayesian-Optimized Emotional Intelligence for Strategic Agent-to-Agent Debt Recovery", "authors": ["Yunbo Long", "Yuhan Liu", "Liming Xu", "Alexandra Brintrup"], "year": 2025, "url": "http://arxiv.org/abs/2503.21080v7", "abstract": "The emergence of autonomous Large Language Model (LLM) agents has created a new ecosystem of strategic, agent-to-agent interactions. However, a critical challenge remains unaddressed: in high-stakes, emotion-sensitive domains like debt collection, LLM agents pre-trained on human dialogue are vulnerable to exploitation by adversarial counterparts who simulate negative emotions to derail negotiations. To fill this gap, we first contribute a novel dataset of simulated debt recovery scenarios and a multi-agent simulation framework. Within this framework, we introduce EmoDebt, an LLM agent architected for robust performance. Its core innovation is a Bayesian-optimized emotional intelligence engine that reframes a model's ability to express emotion in negotiation as a sequential decision-making problem. Through online learning, this engine continuously tunes EmoDebt's emotional transition policies, discovering optimal counter-strategies against specific debtor tactics. Extensive experiments on our proposed benchmark demonstrate that EmoDebt achieves significant strategic robustness, substantially outperforming non-adaptive and emotion-agnostic baselines across key performance metrics, including success rate and operational efficiency. By introducing both a critical benchmark and a robustly adaptive agent, this work establishes a new foundation for deploying strategically robust LLM agents in adversarial, emotion-sensitive debt interactions. The code is available at \\textcolor{blue}{https://github.com/Yunbo-max/EmoDebt}.", "source": "arxiv", "arxiv_id": "2503.21080v7", "pdf_url": "https://arxiv.org/pdf/2503.21080v7", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-27T01:41:34Z", "updated": "2025-11-03T23:50:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response", "authors": ["Yiheng Chen", "Lingyao Li", "Zihui Ma", "Qikai Hu", "Yilun Zhu", "Min Deng", "Runlong Yu"], "year": 2025, "url": "http://arxiv.org/abs/2510.12061v1", "abstract": "Effective disaster response is essential for safeguarding lives and property. Existing statistical approaches often lack semantic context, generalize poorly across events, and offer limited interpretability. While Large language models (LLMs) provide few-shot generalization, they remain text-bound and blind to geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL) that grounds LLM agents in structured earth data. Starting from raw wildfire detections, GAL automatically retrieves and integrates infrastructure, demographic, terrain, and weather information from external geodatabases, assembling them into a concise, unit-annotated perception script. This enriched context enables agents to produce evidence-based resource-allocation recommendations (e.g., personnel assignments, budget allocations), further reinforced by historical analogs and daily change signals for incremental updates. We evaluate the framework in real wildfire scenarios across multiple LLM models, showing that geospatially grounded agents can outperform baselines. The proposed framework can generalize to other hazards such as floods and hurricanes.", "source": "arxiv", "arxiv_id": "2510.12061v1", "pdf_url": "https://arxiv.org/pdf/2510.12061v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-14T01:59:02Z", "updated": "2025-10-14T01:59:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Empowering Time Series Forecasting with LLM-Agents", "authors": ["Chin-Chia Michael Yeh", "Vivian Lai", "Uday Singh Saini", "Xiran Fan", "Yujie Fan", "Junpeng Wang", "Xin Dai", "Yan Zheng"], "year": 2025, "url": "http://arxiv.org/abs/2508.04231v2", "abstract": "Large Language Model (LLM) powered agents have emerged as effective planners for Automated Machine Learning (AutoML) systems. While most existing AutoML approaches focus on automating feature engineering and model architecture search, recent studies in time series forecasting suggest that lightweight models can often achieve state-of-the-art performance. This observation led us to explore improving data quality, rather than model architecture, as a potentially fruitful direction for AutoML on time series data. We propose DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata accompanying time series to clean data while optimizing forecasting performance. We evaluated DCATS using four time series forecasting models on a large-scale traffic volume forecasting dataset. Results demonstrate that DCATS achieves an average 6% error reduction across all tested models and time horizons, highlighting the potential of data-centric approaches in AutoML for time series forecasting.", "source": "arxiv", "arxiv_id": "2508.04231v2", "pdf_url": "https://arxiv.org/pdf/2508.04231v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-06T09:14:08Z", "updated": "2025-11-26T07:42:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance", "authors": ["Yufei He", "Ruoyu Li", "Alex Chen", "Yue Liu", "Yulin Chen", "Yuan Sui", "Cheng Chen", "Yi Zhu", "Luca Luo", "Frank Yang", "Bryan Hooi"], "year": 2025, "url": "http://arxiv.org/abs/2507.17131v2", "abstract": "Large language model (LLM) agents often struggle in environments where rules and required domain knowledge frequently change, such as regulatory compliance and user risk screening. Current approaches, like offline fine-tuning and standard prompting, are insufficient because they cannot effectively adapt to new knowledge during actual operation. To address this limitation, we propose the Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework designed specifically to continuously learn updated domain knowledge at test time. ARIA assesses its own uncertainty through structured self-dialogue, proactively identifying knowledge gaps and requesting targeted explanations or corrections from human experts. It then systematically updates an internal, timestamped knowledge repository with provided human guidance, detecting and resolving conflicting or outdated knowledge through comparisons and clarification queries. We evaluate ARIA on the realistic customer due diligence name screening task on TikTok Pay, alongside publicly available dynamic knowledge tasks. Results demonstrate significant improvements in adaptability and accuracy compared to baselines using standard offline fine-tuning and existing self-improving agents. ARIA is deployed within TikTok Pay serving over 150 million monthly active users, confirming its practicality and effectiveness for operational use in rapidly evolving environments.", "source": "arxiv", "arxiv_id": "2507.17131v2", "pdf_url": "https://arxiv.org/pdf/2507.17131v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-07-23T02:12:32Z", "updated": "2025-10-10T06:52:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning", "authors": ["Zhiwei Li", "Yong Hu", "Wenqing Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19598v1", "abstract": "The functionality of Large Language Model (LLM) agents is primarily determined by two capabilities: action planning and answer summarization. The former, action planning, is the core capability that dictates an agent's performance. However, prevailing training paradigms employ end-to-end, multi-objective optimization that jointly trains both capabilities. This paradigm faces two critical challenges: imbalanced optimization objective allocation and scarcity of verifiable data, making it difficult to enhance the agent's planning capability. To address these challenges, we propose Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that decouples the training process to enable a focused, single-objective optimization of the planning module. Crucially, RLTR introduces a reward signal based on tool-use completeness to directly evaluate the quality of tool invocation sequences. This method offers a more direct and reliable training signal than assessing the final response content, thereby obviating the need for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12% improvement in planning performance compared to end-to-end baselines. Moreover, this enhanced planning capability, in turn, translates to a 5%-6% increase in the final response quality of the overall agent system.", "source": "arxiv", "arxiv_id": "2508.19598v1", "pdf_url": "https://arxiv.org/pdf/2508.19598v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-27T06:19:50Z", "updated": "2025-08-27T06:19:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "End-to-End Edge AI Service Provisioning Framework in 6G ORAN", "authors": ["Yun Tang", "Udhaya Chandhar Srinivasan", "Benjamin James Scott", "Obumneme Umealor", "Dennis Kevogo", "Weisi Guo"], "year": 2025, "url": "http://arxiv.org/abs/2503.11933v1", "abstract": "With the advent of 6G, Open Radio Access Network (O-RAN) architectures are evolving to support intelligent, adaptive, and automated network orchestration. This paper proposes a novel Edge AI and Network Service Orchestration framework that leverages Large Language Model (LLM) agents deployed as O-RAN rApps. The proposed LLM-agent-powered system enables interactive and intuitive orchestration by translating the user's use case description into deployable AI services and corresponding network configurations. The LLM agent automates multiple tasks, including AI model selection from repositories (e.g., Hugging Face), service deployment, network adaptation, and real-time monitoring via xApps. We implement a prototype using open-source O-RAN projects (OpenAirInterface and FlexRIC) to demonstrate the feasibility and functionality of our framework. Our demonstration showcases the end-to-end flow of AI service orchestration, from user interaction to network adaptation, ensuring Quality of Service (QoS) compliance. This work highlights the potential of integrating LLM-driven automation into 6G O-RAN ecosystems, paving the way for more accessible and efficient edge AI ecosystems.", "source": "arxiv", "arxiv_id": "2503.11933v1", "pdf_url": "https://arxiv.org/pdf/2503.11933v1", "categories": ["cs.NI", "cs.AI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-03-15T00:48:50Z", "updated": "2025-03-15T00:48:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enforcing Temporal Constraints for LLM Agents", "authors": ["Adharsh Kamath", "Sishen Zhang", "Calvin Xu", "Shubham Ugare", "Gagandeep Singh", "Sasa Misailovic"], "year": 2025, "url": "http://arxiv.org/abs/2512.23738v1", "abstract": "LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.", "source": "arxiv", "arxiv_id": "2512.23738v1", "pdf_url": "https://arxiv.org/pdf/2512.23738v1", "categories": ["cs.PL", "cs.AI", "cs.FL", "cs.LO"], "primary_category": "cs.PL", "doi": "", "venue": "", "published": "2025-12-25T06:12:13Z", "updated": "2025-12-25T06:12:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models", "authors": ["Yiwen Jiang", "Deval Mehta", "Wei Feng", "Zongyuan Ge"], "year": 2025, "url": "http://arxiv.org/abs/2506.01334v1", "abstract": "Concept Bottleneck Models (CBMs) decompose image classification into a process governed by interpretable, human-readable concepts. Recent advances in CBMs have used Large Language Models (LLMs) to generate candidate concepts. However, a critical question remains: What is the optimal number of concepts to use? Current concept banks suffer from redundancy or insufficient coverage. To address this issue, we introduce a dynamic, agent-based approach that adjusts the concept bank in response to environmental feedback, optimizing the number of concepts for sufficiency yet concise coverage. Moreover, we propose Conditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in traditional CBMs' concept scoring mechanisms. It enhances the accuracy of assessing each concept's contribution to classification tasks and feature an editable matrix that allows LLMs to correct concept scores that conflict with their internal knowledge. Our evaluations across 6 datasets show that our method not only improves classification accuracy by 6% but also enhances interpretability assessments by 30%.", "source": "arxiv", "arxiv_id": "2506.01334v1", "pdf_url": "https://arxiv.org/pdf/2506.01334v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-02T05:25:52Z", "updated": "2025-06-02T05:25:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "year": 2025, "url": "http://arxiv.org/abs/2507.00979v1", "abstract": "As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.", "source": "arxiv", "arxiv_id": "2507.00979v1", "pdf_url": "https://arxiv.org/pdf/2507.00979v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-01T17:31:51Z", "updated": "2025-07-01T17:31:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward", "authors": ["Yanming Wan", "Jiaxing Wu", "Marwa Abdulhai", "Lior Shani", "Natasha Jaques"], "year": 2025, "url": "http://arxiv.org/abs/2504.03206v3", "abstract": "Effective conversational agents like large language models (LLMs) must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. This novel reward mechanism encourages the LLM agent to actively infer user traits by optimizing conversations to improve its user model's accuracy. Consequently, the agent delivers more personalized interactions by learning more about the user. We demonstrate our method's effectiveness in two distinct domains: significantly improving personalization performance in a conversational recommendation task, and personalizing conversations for different learning styles in an educational setting. We show improved generalization capabilities compared to traditional multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.", "source": "arxiv", "arxiv_id": "2504.03206v3", "pdf_url": "https://arxiv.org/pdf/2504.03206v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-04T06:35:02Z", "updated": "2025-10-02T16:36:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing Reasoning Abilities of Small LLMs with Cognitive Alignment", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "year": 2025, "url": "http://arxiv.org/abs/2504.09802v2", "abstract": "The reasoning capabilities of large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need for training effective small reasoning models. A critical challenge is that small models possess different reasoning capacities and cognitive trajectories compared with their larger counterparts. Hence, directly distilling chain-of-thought (CoT) rationales from large LRMs to smaller ones can sometimes be ineffective and often requires a substantial amount of annotated data. In this paper, we first introduce a novel Critique-Rethink-Verify (CRV) system, designed for training smaller yet powerful LRMs. Our CRV system consists of multiple LLM agents, each specializing in unique tasks: (i) critiquing the CoT rationales according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. Building on the CRV system, we further propose the Cognitive Preference Optimization (CogPO) algorithm to continuously enhance the reasoning abilities of smaller models by aligning their reasoning processes with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of our CRV+CogPO framework, which outperforms other methods by a large margin.", "source": "arxiv", "arxiv_id": "2504.09802v2", "pdf_url": "https://arxiv.org/pdf/2504.09802v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-14T02:03:54Z", "updated": "2025-11-03T07:39:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing Reasoning with Collaboration and Memory", "authors": ["Julie Michelman", "Nasrin Baratalipour", "Matthew Abueg"], "year": 2025, "url": "http://arxiv.org/abs/2503.05944v1", "abstract": "We envision a continuous collaborative learning system where groups of LLM agents work together to solve reasoning problems, drawing on memory they collectively build to improve performance as they gain experience. This work establishes the foundations for such a system by studying the interoperability of chain-of-thought reasoning styles, multi-agent collaboration, and memory banks. Extending beyond the identical agents of self-consistency, we introduce varied-context agents with diverse exemplars and a summarizer agent in place of voting. We generate frozen and continuously learned memory banks of exemplars and pair them with fixed, random, and similarity-based retrieval mechanisms. Our systematic study reveals where various methods contribute to reasoning performance of two LLMs on three grounded reasoning tasks, showing that random exemplar selection can often beat more principled approaches, and in some tasks, inclusion of any exemplars serves only to distract both weak and strong models.", "source": "arxiv", "arxiv_id": "2503.05944v1", "pdf_url": "https://arxiv.org/pdf/2503.05944v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-07T21:19:21Z", "updated": "2025-03-07T21:19:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EpidemIQs: Prompt-to-Paper LLM Agents for Epidemic Modeling and Analysis", "authors": ["Mohammad Hossein Samaei", "Faryad Darabi Sahneh", "Lee W. Cohnstaedt", "Caterina Scoglio"], "year": 2025, "url": "http://arxiv.org/abs/2510.00024v1", "abstract": "Large Language Models (LLMs) offer new opportunities to automate complex interdisciplinary research domains. Epidemic modeling, characterized by its complexity and reliance on network science, dynamical systems, epidemiology, and stochastic simulations, represents a prime candidate for leveraging LLM-driven automation. We introduce \\textbf{EpidemIQs}, a novel multi-agent LLM framework that integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, mechanistic modeling, stochastic simulations, data visualization and analysis, and finally documentation of findings in a structured manuscript. We introduced two types of agents: a scientist agent for planning, coordination, reflection, and generation of final results, and a task-expert agent to focus exclusively on one specific duty serving as a tool to the scientist agent. The framework consistently generated complete reports in scientific article format. Specifically, using GPT 4.1 and GPT 4.1 mini as backbone LLMs for scientist and task-expert agents, respectively, the autonomous process completed with average total token usage 870K at a cost of about \\$1.57 per study, achieving a 100\\% completion success rate through our experiments. We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports. We compare EpidemIQs to the single-agent LLM, which has the same system prompts and tools, iteratively planning, invoking tools, and revising outputs until task completion. The comparison shows consistently higher performance of the proposed framework across five different scenarios. EpidemIQs represents a step forward in accelerating scientific research by significantly reducing costs and turnaround time of discovery processes, and enhancing accessibility to advanced modeling tools.", "source": "arxiv", "arxiv_id": "2510.00024v1", "pdf_url": "https://arxiv.org/pdf/2510.00024v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-09-24T18:54:56Z", "updated": "2025-09-24T18:54:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Epitome: Pioneering an Experimental Platform for AI-Social Science Integration", "authors": ["Jingjing Qu", "Kejia Hu", "Jun Zhu", "Yulei Ye", "Wenhao Li", "Teng Wang", "Zhiyun Chen", "Chaochao Lu", "Aimin Zhou", "Xiangfeng Wang", "Xia Hu", "James Evans"], "year": 2025, "url": "http://arxiv.org/abs/2507.01061v3", "abstract": "Large Language Models (LLMs) enable unprecedented social science experimentation by creating controlled hybrid human-AI environments. We introduce Epitome (www.epitome-ai.com), an open experimental platform that operationalizes this paradigm through Matrix-like social worlds where researchers can study isolated human subjects and groups interacting with LLM agents. This maintains ecological validity while enabling precise manipulation of social dynamics. Epitome approaches three frontiers: (1) methodological innovation using LLM confederates to reduce complexity while scaling interactions; (2) empirical investigation of human behavior in AI-saturated environments; and (3) exploration of emergent properties in hybrid collectives. Drawing on interdisciplinary foundations from management, communication, sociology, psychology, and ethics, the platform's modular architecture spans foundation model deployment through data collection. We validate Epitome through replication of three seminal experiments, demonstrating capacity to generate robust findings while reducing experimental complexity. This tool provides crucial insights for understanding how humans navigate AI-mediated social realities, knowledge essential for policy, education, and human-centered AI design.", "source": "arxiv", "arxiv_id": "2507.01061v3", "pdf_url": "https://arxiv.org/pdf/2507.01061v3", "categories": ["cs.CY", "cs.AI", "cs.HC"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-06-30T09:06:16Z", "updated": "2025-12-24T08:52:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service Systems", "authors": ["Junsong Pu", "Yichen Li", "Zhuangbin Chen", "Jinyang Liu", "Zhihan Jiang", "Jianjun Chen", "Rui Shi", "Zibin Zheng", "Tieying Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2509.26463v1", "abstract": "Reliability management in cloud service systems is challenging due to the cascading effect of failures. Error wrapping, a practice prevalent in modern microservice development, enriches errors with context at each layer of the function call stack, constructing an error chain that describes a failure from its technical origin to its business impact. However, this also presents a significant traceability problem when recovering the complete error propagation path from the final log message back to its source. Existing approaches are ineffective at addressing this problem. To fill this gap, we present ErrorPrism in this work for automated reconstruction of error propagation paths in production microservice systems. ErrorPrism first performs static analysis on service code repositories to build a function call graph and map log strings to relevant candidate functions. This significantly reduces the path search space for subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an iterative backward search to accurately reconstruct the complete, multi-hop error path. Evaluated on 67 production microservices at ByteDance, ErrorPrism achieves 97.0% accuracy in reconstructing paths for 102 real-world errors, outperforming existing static analysis and LLM-based approaches. ErrorPrism provides an effective and practical tool for root cause analysis in industrial microservice systems.", "source": "arxiv", "arxiv_id": "2509.26463v1", "pdf_url": "https://arxiv.org/pdf/2509.26463v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-30T16:13:21Z", "updated": "2025-09-30T16:13:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Estimating the Empowerment of Language Model Agents", "authors": ["Jinyeop Song", "Jeff Gore", "Max Kleiman-Weiner"], "year": 2025, "url": "http://arxiv.org/abs/2509.22504v2", "abstract": "As language model (LM) agents become more capable and gain broader access to real-world tools, there is a growing need for scalable evaluation frameworks of agentic capability. However, conventional benchmark-centric evaluations are costly to design and require human designers to come up with valid tasks that translate into insights about general model capabilities. In this work, we propose information-theoretic evaluation based on empowerment, the mutual information between an agent's actions and future states, as an open-ended method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of Language Model Agents), an algorithm for approximating effective empowerment from multi-turn text interactions. We validate EELMA on both language games and scaled-up realistic web-browsing scenarios. We find that empowerment strongly correlates with average task performance, characterize the impact of environmental complexity and agentic factors such as chain-of-thought, model scale, and memory length on estimated empowerment, and that high empowerment states and actions are often pivotal moments for general capabilities. Together, these results demonstrate empowerment as an appealing general-purpose metric for evaluating and monitoring LM agents in complex, open-ended settings.", "source": "arxiv", "arxiv_id": "2509.22504v2", "pdf_url": "https://arxiv.org/pdf/2509.22504v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-26T15:46:14Z", "updated": "2025-09-30T01:24:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans", "authors": ["Deuksin Kwon", "Kaleen Shrestha", "Bin Han", "Elena Hayoung Lee", "Gale Lucas"], "year": 2025, "url": "http://arxiv.org/abs/2509.16394v1", "abstract": "Large Language Models (LLMs) are increasingly deployed in socially complex, interaction-driven tasks, yet their ability to mirror human behavior in emotionally and strategically complex contexts remains underexplored. This study assesses the behavioral alignment of personality-prompted LLMs in adversarial dispute resolution by simulating multi-turn conflict dialogues that incorporate negotiation. Each LLM is guided by a matched Five-Factor personality profile to control for individual variation and enhance realism. We evaluate alignment across three dimensions: linguistic style, emotional expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the closest alignment with humans in linguistic style and emotional dynamics, while Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial alignment gaps persist. Our findings establish a benchmark for alignment between LLMs and humans in socially complex interactions, underscoring both the promise and the limitations of personality conditioning in dialogue modeling.", "source": "arxiv", "arxiv_id": "2509.16394v1", "pdf_url": "https://arxiv.org/pdf/2509.16394v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-19T20:15:52Z", "updated": "2025-09-19T20:15:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia", "authors": ["Chandler Smith", "Marwa Abdulhai", "Manfred Diaz", "Marko Tesic", "Rakshit S. Trivedi", "Alexander Sasha Vezhnevets", "Lewis Hammond", "Jesse Clifton", "Minsuk Chang", "Edgar A. DuÃ©Ã±ez-GuzmÃ¡n", "John P. Agapiou", "Jayd Matyas", "Danny Karmon", "Akash Kundu", "Aliaksei Korshuk", "Ananya Ananya", "Arrasy Rahman", "Avinaash Anand Kulandaivel", "Bain McHale", "Beining Zhang", "Buyantuev Alexander", "Carlos Saith Rodriguez Rojas", "Caroline Wang", "Chetan Talele", "Chenao Liu", "Chichen Lin", "Diana Riazi", "Di Yang Shi", "Emanuel Tewolde", "Elizaveta Tennant", "Fangwei Zhong", "Fuyang Cui", "Gang Zhao", "Gema ParreÃ±o Piqueras", "Hyeonggeun Yun", "Ilya Makarov", "Jiaxun Cui", "Jebish Purbey", "Jim Dilkes", "Jord Nguyen", "Lingyun Xiao", "Luis Felipe Giraldo", "Manuela Chacon-Chamorro", "Manuel Sebastian Rios Beltran", "Marta Emili GarcÃ­a Segura", "Mengmeng Wang", "Mogtaba Alim", "Nicanor Quijano", "Nico Schiavone", "Olivia Macmillan-Scott", "Oswaldo PeÃ±a", "Peter Stone", "Ram Mohan Rao Kadiyala", "Rolando Fernandez", "Ruben Manrique", "Sunjia Lu", "Sheila A. McIlraith", "Shamika Dhuri", "Shuqing Shi", "Siddhant Gupta", "Sneheel Sarangi", "Sriram Ganapathi Subramanian", "Taehun Cha", "Toryn Q. Klassen", "Wenming Tu", "Weijian Fan", "Wu Ruiyang", "Xue Feng", "Yali Du", "Yang Liu", "Yiding Wang", "Yipeng Kang", "Yoonchang Sung", "Yuxuan Chen", "Zhaowei Zhang", "Zhihan Wang", "Zhiqiang Wu", "Ziang Chen", "Zilong Zheng", "Zixia Jia", "Ziyan Wang", "Dylan Hadfield-Menell", "Natasha Jaques", "Tim Baarslag", "Jose Hernandez-Orallo", "Joel Z. Leibo"], "year": 2025, "url": "http://arxiv.org/abs/2512.03318v1", "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.", "source": "arxiv", "arxiv_id": "2512.03318v1", "pdf_url": "https://arxiv.org/pdf/2512.03318v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-03T00:11:05Z", "updated": "2025-12-03T00:11:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components", "authors": ["Ram Potham"], "year": 2025, "url": "http://arxiv.org/abs/2506.02357v2", "abstract": "Credible safety plans for advanced AI development require methods to verify agent behavior and detect potential control deficiencies early. A fundamental aspect is ensuring agents adhere to safety-critical principles, especially when these conflict with operational goals. This paper introduces a lightweight, interpretable benchmark to evaluate an LLM agent's ability to uphold a high-level safety principle when faced with conflicting task instructions. Our evaluation of six LLMs reveals two primary findings: (1) a quantifiable \"cost of compliance\" where safety constraints degrade task performance even when compliant solutions exist, and (2) an \"illusion of compliance\" where high adherence often masks task incompetence rather than principled choice. These findings provide initial evidence that while LLMs can be influenced by hierarchical directives, current approaches lack the consistency required for reliable safety governance.", "source": "arxiv", "arxiv_id": "2506.02357v2", "pdf_url": "https://arxiv.org/pdf/2506.02357v2", "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-03T01:16:34Z", "updated": "2025-07-10T15:10:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating LLM Agent Collusion in Double Auctions", "authors": ["Kushal Agrawal", "Verona Teo", "Juan J. Vazquez", "Sudarsh Kunnavakkam", "Vishak Srikanth", "Andy Liu"], "year": 2025, "url": "http://arxiv.org/abs/2507.01413v1", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities as autonomous agents with rapidly expanding applications in various domains. As these agents increasingly engage in socioeconomic interactions, identifying their potential for undesirable behavior becomes essential. In this work, we examine scenarios where they can choose to collude, defined as secretive cooperation that harms another party. To systematically study this, we investigate the behavior of LLM agents acting as sellers in simulated continuous double auction markets. Through a series of controlled experiments, we analyze how parameters such as the ability to communicate, choice of model, and presence of environmental pressures affect the stability and emergence of seller collusion. We find that direct seller communication increases collusive tendencies, the propensity to collude varies across models, and environmental pressures, such as oversight and urgency from authority figures, influence collusive behavior. Our findings highlight important economic and ethical considerations for the deployment of LLM-based market agents.", "source": "arxiv", "arxiv_id": "2507.01413v1", "pdf_url": "https://arxiv.org/pdf/2507.01413v1", "categories": ["cs.GT", "cs.AI", "cs.LG"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2025-07-02T07:06:49Z", "updated": "2025-07-02T07:06:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating LLMs in Open-Source Games", "authors": ["Swadesh Sistla", "Max Kleiman-Weiner"], "year": 2025, "url": "http://arxiv.org/abs/2512.00371v1", "abstract": "Large Language Models' (LLMs) programming capabilities enable their participation in open-source games: a game-theoretic setting in which players submit computer programs in lieu of actions. These programs offer numerous advantages, including interpretability, inter-agent transparency, and formal verifiability; additionally, they enable program equilibria, solutions that leverage the transparency of code and are inaccessible within normal-form settings. We evaluate the capabilities of leading open- and closed-weight LLMs to predict and classify program strategies and evaluate features of the approximate program equilibria reached by LLM agents in dyadic and evolutionary settings. We identify the emergence of payoff-maximizing, cooperative, and deceptive strategies, characterize the adaptation of mechanisms within these programs over repeated open-source games, and analyze their comparative evolutionary fitness. We find that open-source games serve as a viable environment to study and steer the emergence of cooperative strategy in multi-agent dilemmas.", "source": "arxiv", "arxiv_id": "2512.00371v1", "pdf_url": "https://arxiv.org/pdf/2512.00371v1", "categories": ["cs.GT", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2025-11-29T07:46:25Z", "updated": "2025-11-29T07:46:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions", "authors": ["Yuanzhe Hu", "Yu Wang", "Julian McAuley"], "year": 2025, "url": "http://arxiv.org/abs/2507.05257v2", "abstract": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, based on classic theories from memory science and cognitive science, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and selective forgetting. Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Moreover, no existing benchmarks cover all four competencies. We introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark transforms existing long-context datasets and incorporates newly constructed datasets into a multi-turn format, effectively simulating the incremental information processing characteristic of memory agents. By carefully selecting and curating datasets, our benchmark provides comprehensive coverage of the four core memory competencies outlined above, thereby offering a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.", "source": "arxiv", "arxiv_id": "2507.05257v2", "pdf_url": "https://arxiv.org/pdf/2507.05257v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-07T17:59:54Z", "updated": "2025-09-26T03:31:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity", "authors": ["Yupu Hao", "Pengfei Cao", "Zhuoran Jin", "Huanxuan Liao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2503.00771v2", "abstract": "Personalized tool utilization is essential for aligning large language models (LLMs) with user preference in interaction scenarios with various tools. However, most of the current benchmarks primarily focus on either personalization of text generation or direct tool-utilizing, without considering both. In this work, we introduce a novel benchmark ETAPP for evaluating personalized tool invocation, establishing a sandbox environment, and a comprehensive dataset of 800 testing cases covering diverse user profiles. To improve the accuracy of our evaluation, we propose a key-point-based LLM evaluation method, mitigating biases in the LLM-as-a-judge system by manually annotating key points for each test case and providing them to LLM as the reference. Additionally, we evaluate the excellent LLMs and provide an in-depth analysis. Furthermore, we investigate the impact of different tool-invoking strategies on LLMs' personalization performance and the effects of fine-tuning in our task. The effectiveness of our preference-setting and key-point-based evaluation method is also validated. Our findings offer insights into improving personalized LLM agents. Our Code is available at https://github.com/hypasd-art/ETAPP.", "source": "arxiv", "arxiv_id": "2503.00771v2", "pdf_url": "https://arxiv.org/pdf/2503.00771v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-02T07:36:22Z", "updated": "2025-04-12T09:19:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs", "authors": ["Manuel Pratelli", "Marinella Petrocchi"], "year": 2025, "url": "http://arxiv.org/abs/2506.23610v1", "abstract": "Large language models (LLMs) make it possible to generate synthetic behavioural data at scale, offering an ethical and low-cost alternative to human experiments. Whether such data can faithfully capture psychological differences driven by personality traits, however, remains an open question. We evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to reproduce personality-based variation in susceptibility to misinformation, focusing on news discernment, the ability to judge true headlines as true and false headlines as false. Leveraging published datasets in which human participants with known personality profiles rated headline accuracy, we create matching LLM agents and compare their responses to the original human patterns. Certain trait-misinformation associations, notably those involving Agreeableness and Conscientiousness, are reliably replicated, whereas others diverge, revealing systematic biases in how LLMs internalize and express personality. The results underscore both the promise and the limits of personality-aligned LLMs for behavioral simulation, and offer new insight into modeling cognitive diversity in artificial agents.", "source": "arxiv", "arxiv_id": "2506.23610v1", "pdf_url": "https://arxiv.org/pdf/2506.23610v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "10.3233/FAIA250901", "venue": "", "published": "2025-06-30T08:16:07Z", "updated": "2025-06-30T08:16:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluation and Benchmarking of LLM Agents: A Survey", "authors": ["Mahmoud Mohammadi", "Yipeng Li", "Jane Lo", "Wendy Yip"], "year": 2025, "url": "http://arxiv.org/abs/2507.21504v1", "abstract": "The rise of LLM-based agents has opened new frontiers in AI applications, yet evaluating these agents remains a complex and underdeveloped area. This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. In addition to taxonomy, we highlight enterprise-specific challenges, such as role-based access to data, the need for reliability guarantees, dynamic and long-horizon interactions, and compliance, which are often overlooked in current research. We also identify future research directions, including holistic, more realistic, and scalable evaluation. This work aims to bring clarity to the fragmented landscape of agent evaluation and provide a framework for systematic assessment, enabling researchers and practitioners to evaluate LLM agents for real-world deployment.", "source": "arxiv", "arxiv_id": "2507.21504v1", "pdf_url": "https://arxiv.org/pdf/2507.21504v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "10.1145/3711896.3736570", "venue": "", "published": "2025-07-29T04:57:02Z", "updated": "2025-07-29T04:57:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory", "authors": ["Tianxin Wei", "Noveen Sachdeva", "Benjamin Coleman", "Zhankui He", "Yuanchen Bei", "Xuying Ning", "Mengting Ai", "Yunzhe Li", "Jingrui He", "Ed H. Chi", "Chi Wang", "Shuo Chen", "Fernando Pereira", "Wang-Cheng Kang", "Derek Zhiyuan Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2511.20857v1", "abstract": "Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.", "source": "arxiv", "arxiv_id": "2511.20857v1", "pdf_url": "https://arxiv.org/pdf/2511.20857v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-25T21:08:07Z", "updated": "2025-11-25T21:08:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents", "authors": ["Junwei Liu", "Chen Xu", "Chong Wang", "Tong Bai", "Weitong Chen", "Kaseng Wong", "Yiling Lou", "Xin Peng"], "year": 2025, "url": "http://arxiv.org/abs/2511.02399v1", "abstract": "Recent advances in large language model agents offer the promise of automating end-to-end software development from natural language requirements. However, existing approaches largely adopt linear, waterfall-style pipelines, which oversimplify the iterative nature of real-world development and struggle with complex, large-scale projects. To address these limitations, we propose EvoDev, an iterative software development framework inspired by feature-driven development. EvoDev decomposes user requirements into a set of user-valued features and constructs a Feature Map, a directed acyclic graph that explicitly models dependencies between features. Each node in the feature map maintains multi-level information, including business logic, design, and code, which is propagated along dependencies to provide context for subsequent development iterations. We evaluate EvoDev on challenging Android development tasks and show that it outperforms the best-performing baseline, Claude Code, by a substantial margin of 56.8%, while improving single-agent performance by 16.0%-76.6% across different base LLMs, highlighting the importance of dependency modeling, context propagation, and workflow-aware agent design for complex software projects. Our work summarizes practical insights for designing iterative, LLM-driven development frameworks and informs future training of base LLMs to better support iterative software development.", "source": "arxiv", "arxiv_id": "2511.02399v1", "pdf_url": "https://arxiv.org/pdf/2511.02399v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-04T09:27:01Z", "updated": "2025-11-04T09:27:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EvoEmo: Towards Evolved Emotional Policies for Adversarial LLM Agents in Multi-Turn Price Negotiation", "authors": ["Yunbo Long", "Liming Xu", "Lukas Beckenbauer", "Yuhan Liu", "Alexandra Brintrup"], "year": 2025, "url": "http://arxiv.org/abs/2509.04310v3", "abstract": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \\textit{complex}, \\textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines -- vanilla strategies and fixed-emotion strategies -- for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation.", "source": "arxiv", "arxiv_id": "2509.04310v3", "pdf_url": "https://arxiv.org/pdf/2509.04310v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-04T15:23:58Z", "updated": "2025-10-13T16:04:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies", "authors": ["Kavindu Warnakulasuriya", "Prabhash Dissanayake", "Navindu De Silva", "Stephen Cranefield", "Bastin Tony Roy Savarimuthu", "Surangika Ranathunga", "Nisansa de Silva"], "year": 2025, "url": "http://arxiv.org/abs/2504.19487v3", "abstract": "The evolution of cooperation has been extensively studied using abstract mathematical models and simulations. Recent advances in Large Language Models (LLMs) and the rise of LLM agents have demonstrated their ability to perform social reasoning, thus providing an opportunity to test the emergence of norms in more realistic agent-based simulations with human-like reasoning using natural language. In this research, we investigate whether the cooperation dynamics presented in Boyd and Richerson's model persist in a more realistic simulation of the Diner's Dilemma using LLM agents compared to the abstract mathematical nature in the work of Boyd and Richerson. Our findings indicate that agents follow the strategies defined in the Boyd and Richerson model, and explicit punishment mechanisms drive norm emergence, reinforcing cooperative behaviour even when the agent strategy configuration varies. Our results suggest that LLM-based Multi-Agent System simulations, in fact, can replicate the evolution of cooperation predicted by the traditional mathematical models. Moreover, our simulations extend beyond the mathematical models by integrating natural language-driven reasoning and a pairwise imitation method for strategy adoption, making them a more realistic testbed for cooperative behaviour in MASs.", "source": "arxiv", "arxiv_id": "2504.19487v3", "pdf_url": "https://arxiv.org/pdf/2504.19487v3", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-04-28T05:07:55Z", "updated": "2025-10-23T05:48:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle", "authors": ["Rong Wu", "Xiaoman Wang", "Jianbiao Mei", "Pinlong Cai", "Daocheng Fu", "Cheng Yang", "Licheng Wen", "Xuemeng Yang", "Yufan Shen", "Yuxin Wang", "Botian Shi"], "year": 2025, "url": "http://arxiv.org/abs/2510.16079v1", "abstract": "Current Large Language Model (LLM) agents show strong performance in tool use, but lack the crucial capability to systematically learn from their own experiences. While existing frameworks mainly focus on mitigating external knowledge gaps, they fail to address a more fundamental limitation: the inability to iteratively refine problem-solving strategies. In this work, we introduce EvolveR, a framework designed to enable agent to self-improve through a complete, closed-loop experience lifecycle. This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively retrieves distilled principles to guide its decision-making, accumulating a diverse set of behavioral trajectories. This loop employs a policy reinforcement mechanism to iteratively update the agent based on its performance. We demonstrate the effectiveness of EvolveR on complex multi-hop question-answering benchmarks, where it achieves superior performance over strong agentic baselines. Our work presents a comprehensive blueprint for agents that learn not only from external data but also from the consequences of their own actions, paving the way for more autonomous and continuously improving systems. Code is available at https://github.com/Edaizi/EvolveR.", "source": "arxiv", "arxiv_id": "2510.16079v1", "pdf_url": "https://arxiv.org/pdf/2510.16079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-17T12:03:16Z", "updated": "2025-10-17T12:03:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation", "authors": ["Yiran Wu", "Mauricio Velazco", "Andrew Zhao", "Manuel RaÃºl MelÃ©ndez LujÃ¡n", "Srisuma Movva", "Yogesh K Roy", "Quang Nguyen", "Roberto Rodriguez", "Qingyun Wu", "Michael Albada", "Julia Kiseleva", "Anand Mudgerikar"], "year": 2025, "url": "http://arxiv.org/abs/2507.14201v2", "abstract": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on the task of Cyber Threat Investigation through security questions derived from investigation graphs. Real-world security analysts must sift through a large number of heterogeneous alert signals and security logs, follow multi-hop chains of evidence, and compile an incident report. With the developments of LLMs, building LLM-based agents for automatic thread investigation is a promising direction. To assist the development and evaluation of LLM agents, we construct a dataset from a controlled Azure tenant that covers 8 simulated real-world multi-step attacks, 57 log tables from Microsoft Sentinel and related services, and 589 automatically generated questions. We leverage security logs extracted with expert-crafted detection logic to build threat investigation graphs, and then generate questions with LLMs using paired nodes on the graph, taking the start node as background context and the end node as answer. Anchoring each question to these explicit nodes and edges not only provides automatic, explainable ground truth answers but also makes the pipeline reusable and readily extensible to new logs. This also enables the automatic generation of procedural tasks with verifiable rewards, which can be naturally extended to training agents via reinforcement learning. Our comprehensive experiments with different models confirm the difficulty of the task: with the base setting, the average reward across all evaluated models is 0.249, and the best achieved is 0.368, leaving substantial headroom for future research. Code and data are coming soon!", "source": "arxiv", "arxiv_id": "2507.14201v2", "pdf_url": "https://arxiv.org/pdf/2507.14201v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-14T17:06:26Z", "updated": "2025-09-01T20:02:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exemplar-Guided Planing: Enhanced LLM Agent for KGQA", "authors": ["Jingao Xu", "Shuoyoucheng Ma", "Xin Song", "Rong Jiang", "Hongkui Tu", "Bin Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2510.15283v1", "abstract": "Large Language Models (LLMs) as interactive agents show significant promise in Knowledge Graph Question Answering (KGQA) but often struggle with the semantic gap between natural language queries and structured knowledge graph (KG) representations. This leads to suboptimal planning and inefficient exploration on KG, while training-free approaches often underutilize valuable reasoning patterns in training data. To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA. EGP first preprocesses the training set questions via entity templating to normalize semantic variations. It then retrieves highly similar exemplary questions and their successful reasoning paths from this preprocessed set using semantic embeddings and an efficient FAISS index. These retrieved exemplars dynamically guide the LLM's planning process in two key phases: (1) Task Decomposition, by aligning generated sub-objectives with proven reasoning steps, and (2) Relation Exploration, by providing high-quality auxiliary information to improve relation pruning accuracy. Additionally, we introduce a Smart Lookahead mechanism during relation exploration to improve efficiency by preemptively exploring promising paths and potentially terminating exploration earlier. We apply EGP to the Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP significantly improves over the baseline PoG system and other compared methods.", "source": "arxiv", "arxiv_id": "2510.15283v1", "pdf_url": "https://arxiv.org/pdf/2510.15283v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-17T03:43:06Z", "updated": "2025-10-17T03:43:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving", "authors": ["Liang Zhang", "Xiaoming Zhai", "Jionghao Lin", "Jionghao Lin", "Jennifer Kleiman", "Diego Zapata-Rivera", "Carol Forsyth", "Yang Jiang", "Xiangen Hu", "Arthur C. Graesser"], "year": 2025, "url": "http://arxiv.org/abs/2507.17753v1", "abstract": "Large Language Model (LLM) agents are increasingly utilized in AI-aided education to support tutoring and learning. Effective communication strategies among LLM agents improve collaborative problem-solving efficiency and facilitate cost-effective adoption in education. However, little research has systematically evaluated the impact of different communication strategies on agents' problem-solving. Our study examines four communication modes, \\textit{teacher-student interaction}, \\textit{peer-to-peer collaboration}, \\textit{reciprocal peer teaching}, and \\textit{critical debate}, in a dual-agent, chat-based mathematical problem-solving environment using the OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that dual-agent setups outperform single agents, with \\textit{peer-to-peer collaboration} achieving the highest accuracy. Dialogue acts like statements, acknowledgment, and hints play a key role in collaborative problem-solving. While multi-agent frameworks enhance computational tasks, effective communication strategies are essential for tackling complex problems in AI education.", "source": "arxiv", "arxiv_id": "2507.17753v1", "pdf_url": "https://arxiv.org/pdf/2507.17753v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-05-02T03:31:14Z", "updated": "2025-05-02T03:31:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exploring Decision-Making Capabilities of LLM Agents: An Experimental Study on Jump-Jump Game", "authors": ["Juwu Li"], "year": 2025, "url": "http://arxiv.org/abs/2509.00483v1", "abstract": "The Jump-Jump game, as a simple yet challenging casual game, provides an ideal testing environment for studying LLM decision-making capabilities. The game requires players to precisely control jumping force based on current position and target platform distance, involving multiple cognitive aspects including spatial reasoning, physical modeling, and strategic planning. It illustrates the basic gameplay mechanics of the Jump-Jump game, where the player character (red circle) must jump across platforms with appropriate force to maximize score.", "source": "arxiv", "arxiv_id": "2509.00483v1", "pdf_url": "https://arxiv.org/pdf/2509.00483v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-08-30T12:50:30Z", "updated": "2025-08-30T12:50:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exploring Expert Failures Improves LLM Agent Tuning", "authors": ["Li-Cheng Lan", "Andrew Bai", "Minhao Cheng", "Cho-Jui Hsieh", "Tianyi Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2504.13145v2", "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT (53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.", "source": "arxiv", "arxiv_id": "2504.13145v2", "pdf_url": "https://arxiv.org/pdf/2504.13145v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-17T17:53:54Z", "updated": "2025-04-18T19:36:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets", "authors": ["Tommaso Bendinelli", "Artur Dox", "Christian Holz"], "year": 2025, "url": "http://arxiv.org/abs/2503.06664v1", "abstract": "High-quality, error-free datasets are a key ingredient in building reliable, accurate, and unbiased machine learning (ML) models. However, real world datasets often suffer from errors due to sensor malfunctions, data entry mistakes, or improper data integration across multiple sources that can severely degrade model performance. Detecting and correcting these issues typically require tailor-made solutions and demand extensive domain expertise. Consequently, automation is challenging, rendering the process labor-intensive and tedious. In this study, we investigate whether Large Language Models (LLMs) can help alleviate the burden of manual data cleaning. We set up an experiment in which an LLM, paired with Python, is tasked with cleaning the training dataset to improve the performance of a learning algorithm without having the ability to modify the training pipeline or perform any feature engineering. We run this experiment on multiple Kaggle datasets that have been intentionally corrupted with errors. Our results show that LLMs can identify and correct erroneous entries, such as illogical values or outlier, by leveraging contextual information from other features within the same row, as well as feedback from previous iterations. However, they struggle to detect more complex errors that require understanding data distribution across multiple rows, such as trends and biases.", "source": "arxiv", "arxiv_id": "2503.06664v1", "pdf_url": "https://arxiv.org/pdf/2503.06664v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-09T15:29:46Z", "updated": "2025-03-09T15:29:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exploring Large Language Models for Word Games:Who is the Spy?", "authors": ["Chentian Wei", "Jiewei Chen", "Jinzhu Xu"], "year": 2025, "url": "http://arxiv.org/abs/2503.15235v1", "abstract": "Word games hold significant research value for natural language processing (NLP), game theory, and related fields due to their rule-based and situational nature. This study explores how large language models (LLMs) can be effectively involved in word games and proposes a training-free framework. \"Shei Shi Wo Di\" or \"Who is the Spy\" in English, is a classic word game. Using this game as an example, we introduce a Chain-of-Thought (CoT)-based scheduling framework to enable LLMs to achieve excellent performance in tasks such as inferring role words and disguising their identities. We evaluate the framework's performance based on game success rates and the accuracy of the LLM agents' analytical results. Experimental results affirm the framework's effectiveness, demonstrating notable improvements in LLM performance across multiple datasets. This work highlights the potential of LLMs in mastering situational reasoning and social interactions within structured game environments. Our code is publicly available at https://github.com/ct-wei/Who-is-The-Spy.", "source": "arxiv", "arxiv_id": "2503.15235v1", "pdf_url": "https://arxiv.org/pdf/2503.15235v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-19T14:13:02Z", "updated": "2025-03-19T14:13:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exploring a Gamified Personality Assessment Method through Interaction with LLM Agents Embodying Different Personalities", "authors": ["Baiqiao Zhang", "Xiangxian Li", "Chao Zhou", "Xinyu Gai", "Juan Liu", "Xue Yang", "Xiaojuan Ma", "Yong-jin Liu", "Yulong Bian"], "year": 2025, "url": "http://arxiv.org/abs/2507.04005v3", "abstract": "The low-intrusion and automated personality assessment is receiving increasing attention in psychology and human-computer interaction fields. This study explores an interactive approach for personality assessment, focusing on the multiplicity of personality representation. We propose a framework of Gamified Personality Assessment through Multi-Personality Representations (Multi-PR GPA). The framework leverages Large Language Models to empower virtual agents with different personalities. These agents elicit multifaceted human personality representations through engaging in interactive games. Drawing upon the multi-type textual data generated throughout the interaction, it achieves two modes of personality assessment (i.e., Direct Assessment and Questionnaire-based Assessment) and provides interpretable insights. Grounded in the classic Big Five personality theory, we developed a prototype system and conducted a user study to evaluate the efficacy of Multi-PR GPA. The results affirm the effectiveness of our approach in personality assessment and demonstrate its superior performance when considering the multiplicity of personality representation.", "source": "arxiv", "arxiv_id": "2507.04005v3", "pdf_url": "https://arxiv.org/pdf/2507.04005v3", "categories": ["cs.HC", "cs.CY"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-07-05T11:17:20Z", "updated": "2025-09-12T08:36:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of Privacy Risks in LLM Agent Interactions", "authors": ["Yixiang Zhang", "Xinhao Deng", "Zhongyi Gu", "Yihao Chen", "Ke Xu", "Qi Li", "Jianping Wu"], "year": 2025, "url": "http://arxiv.org/abs/2510.07176v1", "abstract": "Large Language Models (LLMs) are increasingly deployed as agents that orchestrate tasks and integrate external tools to execute complex workflows. We demonstrate that these interactive behaviors leave distinctive fingerprints in encrypted traffic exchanged between users and LLM agents. By analyzing traffic patterns associated with agent workflows and tool invocations, adversaries can infer agent activities, distinguish specific agents, and even profile sensitive user attributes. To highlight this risk, we develop AgentPrint, which achieves an F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3 accuracy in user attribute inference for simulated- and real-user settings, respectively. These results uncover an overlooked risk: the very interactivity that empowers LLM agents also exposes user privacy, underscoring the urgent need for technical countermeasures alongside regulatory and policy safeguards.", "source": "arxiv", "arxiv_id": "2510.07176v1", "pdf_url": "https://arxiv.org/pdf/2510.07176v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-08T16:16:23Z", "updated": "2025-10-08T16:16:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Extracting Conceptual Knowledge to Locate Software Issues", "authors": ["Ying Wang", "Wenjun Mao", "Chong Wang", "Zhenhao Zhou", "Yicheng Zhou", "Wenyun Zhao", "Yiling Lou", "Xin Peng"], "year": 2025, "url": "http://arxiv.org/abs/2509.21427v2", "abstract": "Issue localization, which identifies faulty code elements such as files or functions, is critical for effective bug fixing. While recent LLM-based and LLM-agent-based approaches improve accuracy, they struggle in large-scale repositories due to concern tangling, where relevant logic is buried in large functions, and concern scattering, where related logic is dispersed across files.\n  To address these challenges, we propose RepoLens, a novel approach that abstracts and leverages conceptual knowledge from code repositories. RepoLens decomposes fine-grained functionalities and recomposes them into high-level concerns, semantically coherent clusters of functionalities that guide LLMs. It operates in two stages: an offline stage that extracts and enriches conceptual knowledge into a repository-wide knowledge base, and an online stage that retrieves issue-specific terms, clusters and ranks concerns by relevance, and integrates them into localization workflows via minimally intrusive prompt enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains of over 22% in Hit@k and 46% in Recall@k for file- and function-level localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies and manual evaluation confirm the effectiveness and reliability of the constructed concerns.", "source": "arxiv", "arxiv_id": "2509.21427v2", "pdf_url": "https://arxiv.org/pdf/2509.21427v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-25T11:53:06Z", "updated": "2025-10-04T15:20:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction", "authors": ["Quanjiang Guo", "Sijie Wang", "Jinchuan Zhang", "Ben Zhang", "Zhao Kang", "Ling Tian", "Ke Yan"], "year": 2025, "url": "http://arxiv.org/abs/2511.13118v1", "abstract": "Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.", "source": "arxiv", "arxiv_id": "2511.13118v1", "pdf_url": "https://arxiv.org/pdf/2511.13118v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-17T08:17:15Z", "updated": "2025-11-17T08:17:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory", "authors": ["Alessio Buscemi", "Daniele Proverbio", "Alessandro Di Stefano", "The Anh Han", "German Castignani", "Pietro LiÃ²"], "year": 2025, "url": "http://arxiv.org/abs/2504.14325v3", "abstract": "Letting AI agents interact in multi-agent applications adds a layer of complexity to the interpretability and prediction of AI outcomes, with profound implications for their trustworthy adoption in research and society. Game theory offers powerful models to capture and interpret strategic interaction among agents, but requires the support of reproducible, standardized and user-friendly IT frameworks to enable comparison and interpretation of results. To this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition using Game Theory. We describe its implementation and usage, and we employ it to uncover biased outcomes in popular games among AI agents, depending on the employed Large Language Model (LLM) and used language, as well as on the personality trait or strategic knowledge of the agents. Overall, FAIRGAME allows users to reliably and easily simulate their desired games and scenarios and compare the results across simulation campaigns and with game-theoretic predictions, enabling the systematic discovery of biases, the anticipation of emerging behavior out of strategic interplays, and empowering further research into strategic decision-making using LLM agents.", "source": "arxiv", "arxiv_id": "2504.14325v3", "pdf_url": "https://arxiv.org/pdf/2504.14325v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-19T15:29:04Z", "updated": "2025-08-14T12:12:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "authors": ["Nayantara Mudur", "Hao Cui", "Subhashini Venugopalan", "Paul Raccuglia", "Michael P. Brenner", "Peter Norgaard"], "year": 2025, "url": "http://arxiv.org/abs/2504.06260v1", "abstract": "Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to compute the answers. We additionally design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMs' reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world. The code is available at https://github.com/google/feabench", "source": "arxiv", "arxiv_id": "2504.06260v1", "pdf_url": "https://arxiv.org/pdf/2504.06260v1", "categories": ["cs.AI", "cs.CL", "math.NA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-08T17:59:39Z", "updated": "2025-04-08T17:59:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering", "authors": ["Gyubok Lee", "Elea Bach", "Eric Yang", "Tom Pollard", "Alistair Johnson", "Edward Choi", "Yugang jia", "Jong Ha Lee"], "year": 2025, "url": "http://arxiv.org/abs/2509.19319v2", "abstract": "The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.", "source": "arxiv", "arxiv_id": "2509.19319v2", "pdf_url": "https://arxiv.org/pdf/2509.19319v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-12T06:52:55Z", "updated": "2025-11-13T06:35:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading", "authors": ["Guojun Xiong", "Zhiyang Deng", "Keyi Wang", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Xueqing Peng", "Mingquan Lin", "Kaleb E Smith", "Xiao-Yang Liu", "Jimin Huang", "Sophia Ananiadou", "Qianqian Xie"], "year": 2025, "url": "http://arxiv.org/abs/2502.11433v3", "abstract": "Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose \\textsc{FLAG-Trader}, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.", "source": "arxiv", "arxiv_id": "2502.11433v3", "pdf_url": "https://arxiv.org/pdf/2502.11433v3", "categories": ["cs.AI", "cs.CE", "q-fin.TR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T04:45:53Z", "updated": "2025-02-19T03:40:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series", "authors": ["Gunjan Jalori", "Preetika Verma", "Sercan Ã ArÄ±k"], "year": 2025, "url": "http://arxiv.org/abs/2508.19279v1", "abstract": "Time series Forecasting with large languagemodels (LLMs) requires bridging numericalpatterns and natural language. Effective fore-casting on LLM often relies on extensive pre-processing and fine-tuning.Recent studiesshow that a frozen LLM can rival specializedforecasters when supplied with a carefully en-gineered natural-language prompt, but craft-ing such a prompt for each task is itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt optimization framework thatutilizes an agentic system: a Forecaster-agentgenerates forecasts using an initial prompt,which is then refined by a refiner agent, in-formed by past outputs and retrieved analogs.This adaptive prompting generalizes across do-mains using creative prompt templates andgenerates high-quality forecasts without inter-mediate code generation.Experiments onbenchmark datasets show improved accuracyover static prompting and retrieval-augmentedbaselines, approaching the performance ofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning, achievingstrong performance via its agentic approach toadaptive prompt refinement and retrieval.", "source": "arxiv", "arxiv_id": "2508.19279v1", "pdf_url": "https://arxiv.org/pdf/2508.19279v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-24T00:57:22Z", "updated": "2025-08-24T00:57:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FLEX: Continuous Agent Evolution via Forward Learning from Experience", "authors": ["Zhicheng Cai", "Xinyuan Guo", "Yu Pei", "Jiangtao Feng", "Jinsong Su", "Jiangjie Chen", "Ya-Qin Zhang", "Wei-Ying Ma", "Mingxuan Wang", "Hao Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2511.06449v2", "abstract": "Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.", "source": "arxiv", "arxiv_id": "2511.06449v2", "pdf_url": "https://arxiv.org/pdf/2511.06449v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-09T16:31:39Z", "updated": "2025-12-08T02:42:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents", "authors": ["Vikram Nitin", "Baishakhi Ray", "Roshanak Zilouchian Moghaddam"], "year": 2025, "url": "http://arxiv.org/abs/2507.15241v1", "abstract": "Despite the critical threat posed by software security vulnerabilities, reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests needed to validate fixes and prevent regressions. These tests are crucial not only for ensuring patches work, but also for helping developers understand how vulnerabilities can be exploited. Generating PoV tests is a challenging problem, requiring reasoning about the flow of control and data through deeply nested levels of a program.\n  We present FaultLine, an LLM agent workflow that uses a set of carefully designed reasoning steps, inspired by aspects of traditional static and dynamic program analysis, to automatically generate PoV test cases. Given a software project with an accompanying vulnerability report, FaultLine 1) traces the flow of an input from an externally accessible API (\"source\") to the \"sink\" corresponding to the vulnerability, 2) reasons about the conditions that an input must satisfy in order to traverse the branch conditions encountered along the flow, and 3) uses this reasoning to generate a PoV test case in a feedback-driven loop. FaultLine does not use language-specific static or dynamic analysis components, which enables it to be used across programming languages.\n  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100 known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct 2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine represents a 77% relative improvement over the state of the art. Our findings suggest that hierarchical reasoning can enhance the performance of LLM agents on PoV test generation, but the problem in general remains challenging. We make our code and dataset publicly available in the hope that it will spur further research in this area.", "source": "arxiv", "arxiv_id": "2507.15241v1", "pdf_url": "https://arxiv.org/pdf/2507.15241v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-07-21T04:55:34Z", "updated": "2025-07-21T04:55:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents", "authors": ["Xiang Chen", "Yuling Shi", "Qizhen Lan", "Yuchao Qiu", "Min Wang", "Xiaodong Gu", "Yanfu Yan"], "year": 2025, "url": "http://arxiv.org/abs/2512.08870v2", "abstract": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. Despite the demonstrated success of Federated Learning (FL) on static datasets, its effectiveness in open-ended, self-evolving agent systems remains largely unexplored. In such settings, the direct application of standard FL is particularly challenging, as heterogeneous tasks and sparse, trajectory-level reward signals give rise to severe gradient instability, which undermines the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents that establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace, reducing communication cost across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by 10\\% over the state-of-the-art FedIT, validating its effectiveness in cross-environment knowledge transfer under privacy constraints.", "source": "arxiv", "arxiv_id": "2512.08870v2", "pdf_url": "https://arxiv.org/pdf/2512.08870v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-09T18:04:41Z", "updated": "2026-01-11T12:46:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents", "authors": ["Pramit Saha", "Joshua Strong", "Divyanshu Mishra", "Cheng Ouyang", "J. Alison Noble"], "year": 2025, "url": "http://arxiv.org/abs/2509.23803v1", "abstract": "Federated learning (FL) allows collaborative model training across healthcare sites without sharing sensitive patient data. However, real-world FL deployment is often hindered by complex operational challenges that demand substantial human efforts. This includes: (a) selecting appropriate clients (hospitals), (b) coordinating between the central server and clients, (c) client-level data pre-processing, (d) harmonizing non-standardized data and labels across clients, and (e) selecting FL algorithms based on user instructions and cross-client data characteristics. However, the existing FL works overlook these practical orchestration challenges. These operational bottlenecks motivate the need for autonomous, agent-driven FL systems, where intelligent agents at each hospital client and the central server agent collaboratively manage FL setup and model training with minimal human intervention. To this end, we first introduce an agent-driven FL framework that captures key phases of real-world FL workflows from client selection to training completion and a benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to autonomously coordinate healthcare FL. Our framework incorporates 40 FL algorithms, each tailored to address diverse task-specific requirements and cross-client characteristics. Furthermore, we introduce a diverse set of complex tasks across 201 carefully curated datasets, simulating 6 modality-specific real-world healthcare environments, viz., Dermatoscopy, Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic performance of 14 open-source and 10 proprietary LLMs spanning small, medium, and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3 can automate various stages of the FL pipeline, our results reveal that more complex, interdependent tasks based on implicit goals remain challenging for even the strongest models.", "source": "arxiv", "arxiv_id": "2509.23803v1", "pdf_url": "https://arxiv.org/pdf/2509.23803v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-28T11:06:07Z", "updated": "2025-09-28T11:06:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios", "authors": ["Yingqian Wu", "Qiushi Wang", "Zefei Long", "Rong Ye", "Zhongtian Lu", "Xianyin Zhang", "Bingxuan Li", "Wei Chen", "Liwen Zhang", "Zhongyu Wei"], "year": 2025, "url": "http://arxiv.org/abs/2507.10448v1", "abstract": "Financial report generation tasks range from macro- to micro-economics analysis, also requiring extensive data analysis. Existing LLM models are usually fine-tuned on simple QA tasks and cannot comprehensively analyze real financial scenarios. Given the complexity, financial companies often distribute tasks among departments. Inspired by this, we propose FinTeam, a financial multi-agent collaborative system, with a workflow with four LLM agents: document analyzer, analyst, accountant, and consultant. We train these agents with specific financial expertise using constructed datasets. We evaluate FinTeam on comprehensive financial tasks constructed from real online investment forums, including macroeconomic, industry, and company analysis. The human evaluation shows that by combining agents, the financial reports generate from FinTeam achieved a 62.00% acceptance rate, outperforming baseline models like GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43% average improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project is available at https://github.com/FudanDISC/DISC-FinLLM/.", "source": "arxiv", "arxiv_id": "2507.10448v1", "pdf_url": "https://arxiv.org/pdf/2507.10448v1", "categories": ["cs.CE", "cs.LG"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2025-07-05T10:12:25Z", "updated": "2025-07-05T10:12:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Finance as Extended Biology: Reciprocity as the Cognitive Substrate of Financial Behavior", "authors": ["Egil Diau"], "year": 2025, "url": "http://arxiv.org/abs/2506.00099v2", "abstract": "A central challenge in economics and artificial intelligence is explaining how financial behaviors-such as credit, insurance, and trade-emerge without formal institutions. We argue that these functions are not products of institutional design, but structured extensions of a single behavioral substrate: reciprocity. Far from being a derived strategy, reciprocity served as the foundational logic of early human societies-governing the circulation of goods, regulation of obligation, and maintenance of long-term cooperation well before markets, money, or formal rules. Trade, commonly regarded as the origin of financial systems, is reframed here as the canonical form of reciprocity: simultaneous, symmetric, and partner-contingent. Building on this logic, we reconstruct four core financial functions-credit, insurance, token exchange, and investment-as expressions of the same underlying principle under varying conditions. By grounding financial behavior in minimal, simulateable dynamics of reciprocal interaction, this framework shifts the focus from institutional engineering to behavioral computation-offering a new foundation for modeling decentralized financial behavior in both human and artificial agents.", "source": "arxiv", "arxiv_id": "2506.00099v2", "pdf_url": "https://arxiv.org/pdf/2506.00099v2", "categories": ["cs.CY", "cs.MA", "physics.soc-ph"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-05-30T12:37:52Z", "updated": "2025-06-06T11:37:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL", "authors": ["Sungmin Kang", "Sumi Yun", "Jingun Hong", "Shin Yoo", "Gabin An"], "year": 2025, "url": "http://arxiv.org/abs/2510.22530v1", "abstract": "Fault Localization (FL) aims to identify root causes of program failures. FL typically targets failures observed from test executions, and as such, often involves dynamic analyses to improve accuracy, such as coverage profiling or mutation testing. However, for large industrial software, measuring coverage for every execution is prohibitively expensive, making the use of such techniques difficult. To address these issues and apply FL in an industrial setting, this paper proposes AutoCrashFL, an LLM agent for the localization of crashes that only requires the crashdump from the Program Under Test (PUT) and access to the repository of the corresponding source code. We evaluate AutoCrashFL against real-world crashes of SAP HANA, an industrial software project consisting of more than 35 million lines of code. Experiments reveal that AutoCrashFL is more effective in localization, as it identified 30% crashes at the top, compared to 17% achieved by the baseline. Through thorough analysis, we find that AutoCrashFL has attractive practical properties: it is relatively more effective for complex bugs, and it can indicate confidence in its results. Overall, these results show the practicality of LLM agent deployment on an industrial scale.", "source": "arxiv", "arxiv_id": "2510.22530v1", "pdf_url": "https://arxiv.org/pdf/2510.22530v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-26T04:43:33Z", "updated": "2025-10-26T04:43:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents", "authors": ["Qinglong Yang", "Haoming Li", "Haotian Zhao", "Xiaokai Yan", "Jingtao Ding", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2507.21071v1", "abstract": "Mobile GUI agents are becoming critical tools for enhancing human-device interaction efficiency, with multimodal large language models (MLLMs) emerging as dominant paradigms in this domain. Current agents, however, are limited to following explicit human instructions, resulting in insufficient capability for proactive intent anticipation. Additionally, these agents fail to leverage the contextual information associated with users during task execution, thereby neglecting potentially vast differences in user preferences. To address these challenges, we introduce the FingerTip benchmark. It contains two new tracks: proactive task suggestions by analyzing environment observation and users' previous intents, and personalized task execution by catering to users' action preferences. We collected unique human demonstrations of multi-step Android device interactions across a variety of everyday apps. These demonstrations are not isolated but are continuously acquired from the users' long-term usage in their real lives, and encompass essential user-related contextual information. Our experiments reveal challenges of the tasks we propose. The model fine-tuned with the data we collected effectively utilized user information and achieved good results, highlighting the potential of our approach in building more user-oriented mobile GUI agents. Our code is open-source at https://anonymous.4open.science/r/FingerTip-57B8 for reproducibility.", "source": "arxiv", "arxiv_id": "2507.21071v1", "pdf_url": "https://arxiv.org/pdf/2507.21071v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-06-09T06:38:41Z", "updated": "2025-06-09T06:38:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Firewalls to Secure Dynamic LLM Agentic Networks", "authors": ["Sahar Abdelnabi", "Amr Gomaa", "Eugene Bagdasarian", "Per Ola Kristensson", "Reza Shokri"], "year": 2025, "url": "http://arxiv.org/abs/2502.01822v5", "abstract": "LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.", "source": "arxiv", "arxiv_id": "2502.01822v5", "pdf_url": "https://arxiv.org/pdf/2502.01822v5", "categories": ["cs.CR", "cs.CY"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-03T21:00:14Z", "updated": "2025-05-26T12:24:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay", "authors": ["Andrew Zhu", "Evan Osgood", "Chris Callison-Burch"], "year": 2025, "url": "http://arxiv.org/abs/2505.22809v2", "abstract": "Much work has been done on conversational LLM agents which directly assist human users with tasks. We present an alternative paradigm for interacting with LLM agents, which we call \"overhearing agents\". These overhearing agents do not actively participate in conversation -- instead, they \"listen in\" on human-to-human conversations and perform background tasks or provide suggestions to assist the user. In this work, we explore the overhearing agents paradigm through the lens of Dungeons & Dragons gameplay. We present an in-depth study using large multimodal audio-language models as overhearing agents to assist a Dungeon Master. We perform a human evaluation to examine the helpfulness of such agents and find that some large audio-language models have the emergent ability to perform overhearing agent tasks using implicit audio cues. Finally, we release Python libraries and our project code to support further research into the overhearing agents paradigm at https://github.com/zhudotexe/overhearing_agents.", "source": "arxiv", "arxiv_id": "2505.22809v2", "pdf_url": "https://arxiv.org/pdf/2505.22809v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-28T19:34:36Z", "updated": "2025-09-05T16:48:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Flow: Modularized Agentic Workflow Automation", "authors": ["Boye Niu", "Yiliao Song", "Kai Lian", "Yifan Shen", "Yu Yao", "Kun Zhang", "Tongliang Liu"], "year": 2025, "url": "http://arxiv.org/abs/2501.07834v2", "abstract": "Multi-agent frameworks powered by large language models (LLMs) have demonstrated great success in automated planning and task execution. However, the effective adjustment of agentic workflows during execution has not been well studied. An effective workflow adjustment is crucial in real-world scenarios, as the initial plan must adjust to unforeseen challenges and changing conditions in real time to ensure the efficient execution of complex tasks. In this paper, we define workflows as an activity-on-vertex (AOV) graph, which allows continuous workflow refinement by LLM agents through dynamic subtask allocation adjustment based on historical performance and previous AOVs. To further enhance framework performance, we emphasize modularity in workflow design based on evaluating parallelism and dependency complexity. With this design, our proposed multi-agent framework achieves efficient concurrent execution of subtasks, effective goal achievement, and enhanced error tolerance. Empirical results across various practical tasks demonstrate significant improvements in the efficiency of multi-agent frameworks through dynamic workflow refinement and modularization. The code is available at: https://github.com/tmllab/2025_ICLR_FLOW.", "source": "arxiv", "arxiv_id": "2501.07834v2", "pdf_url": "https://arxiv.org/pdf/2501.07834v2", "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-14T04:35:37Z", "updated": "2025-02-23T06:20:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Forecasting Frontier Language Model Agent Capabilities", "authors": ["Govind Pimpale", "Axel HÃ¸jmark", "JÃ©rÃ©my Scheurer", "Marius Hobbhahn"], "year": 2025, "url": "http://arxiv.org/abs/2502.15850v2", "abstract": "As Language Models (LMs) increasingly operate as autonomous agents, accurately forecasting their capabilities becomes crucial for societal preparedness. We evaluate six forecasting methods that predict downstream capabilities of LM agents. We use \"one-step\" approaches that predict benchmark scores from input metrics like compute or model release date directly or \"two-step\" approaches that first predict an intermediate metric like the principal component of cross-benchmark performance (PC-1) and human-evaluated competitive Elo ratings. We evaluate our forecasting methods by backtesting them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the validated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM agent performance for frontier models on three benchmarks: SWE-Bench Verified (software development), Cybench (cybersecurity assessment), and RE-Bench (ML research engineering). Our forecast predicts that by the beginning of 2026, non-specialized LM agents with low capability elicitation will reach a success rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach an 87% success rate. Our approach does not account for recent advances in inference-compute scaling and might thus be too conservative.", "source": "arxiv", "arxiv_id": "2502.15850v2", "pdf_url": "https://arxiv.org/pdf/2502.15850v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-21T02:34:17Z", "updated": "2025-03-03T17:11:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents", "authors": ["Liangxuan Wu", "Chao Wang", "Tianming Liu", "Yanjie Zhao", "Haoyu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2505.12981v2", "abstract": "The growing adoption of large language models (LLMs) has led to a new paradigm in mobile computing--LLM-powered mobile AI agents--capable of decomposing and automating complex tasks directly on smartphones. However, the security implications of these agents remain largely unexplored. In this paper, we present the first comprehensive security analysis of mobile LLM agents, encompassing three representative categories: System-level AI Agents developed by original equipment manufacturers (e.g., YOYO Assistant), Third-party Universal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g., Alibaba Mobile Agent). We begin by analyzing the general workflow of mobile agents and identifying security threats across three core capability dimensions: language-based reasoning, GUI-based interaction, and system-level execution. Our analysis reveals 11 distinct attack surfaces, all rooted in the unique capabilities and interaction patterns of mobile LLM agents, and spanning their entire operational lifecycle. To investigate these threats in practice, we introduce AgentScan, a semi-automated security analysis framework that systematically evaluates mobile LLM agents across all 11 attack scenarios. Applying AgentScan to nine widely deployed agents, we uncover a concerning trend: every agent is vulnerable to targeted attacks. In the most severe cases, agents exhibit vulnerabilities across eight distinct attack vectors. These attacks can cause behavioral deviations, privacy leakage, or even full execution hijacking. Based on these findings, we propose a set of defensive design principles and practical recommendations for building secure mobile LLM agents. Our disclosures have received positive feedback from two major device vendors. Overall, this work highlights the urgent need for standardized security practices in the fast-evolving landscape of LLM-driven mobile automation.", "source": "arxiv", "arxiv_id": "2505.12981v2", "pdf_url": "https://arxiv.org/pdf/2505.12981v2", "categories": ["cs.CR", "cs.AI", "cs.HC"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-19T11:17:46Z", "updated": "2025-05-20T07:02:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Correction to Mastery: Reinforced Distillation of Large Language Model Agents", "authors": ["Yuanjie Lyu", "Chengyu Wang", "Jun Huang", "Tong Xu"], "year": 2025, "url": "http://arxiv.org/abs/2509.14257v2", "abstract": "Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student can cause compounding errors. We propose SCoRe, a student-centered framework in which the student generates training trajectories and the teacher corrects only the earliest error, producing training data matched to the student's ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix preceding the earliest error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and enhances training stability. On 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.", "source": "arxiv", "arxiv_id": "2509.14257v2", "pdf_url": "https://arxiv.org/pdf/2509.14257v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-12T15:34:07Z", "updated": "2025-10-09T04:22:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory", "authors": ["Siyu Xia", "Zekun Xu", "Jiajun Chai", "Wentian Fan", "Yan Song", "Xiaohan Wang", "Guojun Yin", "Wei Lin", "Haifeng Zhang", "Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.07800v1", "abstract": "Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.", "source": "arxiv", "arxiv_id": "2511.07800v1", "pdf_url": "https://arxiv.org/pdf/2511.07800v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-11T03:36:33Z", "updated": "2025-11-11T03:36:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users", "authors": ["Sadia Sultana Chowa", "Riasad Alvi", "Subhey Sadi Rahman", "Md Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Md Rafiqul Islam", "Mukhtar Hussain", "Sami Azam"], "year": 2025, "url": "http://arxiv.org/abs/2508.17281v2", "abstract": "The pursuit of human-level artificial intelligence (AI) has significantly advanced the development of autonomous agents and Large Language Models (LLMs). LLMs are now widely utilized as decision-making agents for their ability to interpret instructions, manage sequential tasks, and adapt through feedback. This review examines recent developments in employing LLMs as autonomous agents and tool users and comprises seven research questions. We only used the papers published between 2023 and 2025 in conferences of the A* and A rank and Q1 journals. A structured analysis of the LLM agents' architectural design principles, dividing their applications into single-agent and multi-agent systems, and strategies for integrating external tools is presented. In addition, the cognitive mechanisms of LLM, including reasoning, planning, and memory, and the impact of prompting methods and fine-tuning procedures on agent performance are also investigated. Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. In conducting this review, we have identified critical findings on verifiable reasoning of LLMs, the capacity for self-improvement, and the personalization of LLM-based agents. Finally, we have discussed ten future research directions to overcome these gaps.", "source": "arxiv", "arxiv_id": "2508.17281v2", "pdf_url": "https://arxiv.org/pdf/2508.17281v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.1007/s10462-025-11471-9", "venue": "Artif. Intell. Rev. (2026)", "published": "2025-08-24T10:02:51Z", "updated": "2025-10-28T13:52:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Narrative to Action: A Hierarchical LLM-Agent Framework for Human Mobility Generation", "authors": ["Qiumeng Li", "Chunhou Ji", "Xinyue Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.24802v1", "abstract": "Understanding and replicating human mobility requires not only spatial-temporal accuracy but also an awareness of the cognitive hierarchy underlying real-world travel decisions. Traditional agent-based or deep learning models can reproduce statistical patterns of movement but fail to capture the semantic coherence and causal logic of human behavior. Large language models (LLMs) show potential, but struggle to balance creative reasoning with strict structural compliance. This study proposes a Hierarchical LLM-Agent Framework, termed Narrative-to-Action, that integrates high-level narrative reasoning, mid-level reflective planning, and low-level behavioral execution within a unified cognitive hierarchy. At the macro level, one agent is employed as a \"creative writer\" to produce diary-style narratives rich in motivation and context, then uses another agent as a \"structural parser\" to convert narratives into machine-readable plans. A dynamic execution module further grounds agents in geographic environments and enables adaptive behavioral adjustments guided by a novel occupation-aware metric, Mobility Entropy by Occupation (MEO), which captures heterogeneous schedule flexibility across different occupational personalities. At the micro level, the agent executes concrete actions-selecting locations, transportation modes, and time intervals-through interaction with an environmental simulation. By embedding this multi-layer cognitive process, the framework produces not only synthetic trajectories that align closely with real-world patterns but also interpretable representations of human decision logic. This research advances synthetic mobility generation from a data-driven paradigm to a cognition-driven simulation, providing a scalable pathway for understanding, predicting, and synthesizing complex urban mobility behaviors through hierarchical LLM agents.", "source": "arxiv", "arxiv_id": "2510.24802v1", "pdf_url": "https://arxiv.org/pdf/2510.24802v1", "categories": ["cs.MA", "cs.AI", "cs.CY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-10-28T00:26:36Z", "updated": "2025-10-28T00:26:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Natural Language to Certified H-infinity Controllers: Integrating LLM Agents with LMI-Based Synthesis", "authors": ["Shihao Li", "Jiachen Li", "Jiamin Xu", "Dongmei Chen"], "year": 2025, "url": "http://arxiv.org/abs/2511.07894v1", "abstract": "We present \\textsc{S2C} (Specification-to-Certified-Controller), a multi-agent framework that maps natural-language requirements to certified $\\mathcal{H}_\\infty$ state-feedback controllers via LMI synthesis. \\textsc{S2C} coordinates five roles -- \\textit{SpecInt} (spec extraction), \\textit{Solv} (bounded-real lemma (BRL) LMI), \\textit{Tester} (Monte Carlo and frequency-domain checks), \\textit{Adapt} (spec refinement), and \\textit{CodeGen} (deployable code). The loop is stabilized by a severity- and iteration-aware $Î³$-floor guardrail and a decay-rate region constraint enforcing $\\ReÎ»(A{+}BK)<-Î±$ with $Î±=3.9/T_s$ derived from settling-time targets. For state feedback, verification reports disturbance rejection $\\big\\|C\\,(sI-(A{+}BK))^{-1}E\\big\\|_\\infty$ alongside time-domain statistics; discrete benchmarks are converted to continuous time via a Tustin (bilinear) transform when needed. On 14 COMPleib problems, \\textsc{S2C} attains \\textbf{100\\%} synthesis success and \\textbf{100\\%} convergence within six iterations, with strong decay-rate satisfaction and near-target certified $\\mathcal{H}_\\infty$ levels; it improves robustness metrics relative to single-shot BRL and BRL+$Î±$ baselines. An ablation over LLM backbones (GPT-5, GPT-5 mini, DeepSeek-V3, Qwen-2.5-72B, Llama-4 Maverick) shows the pipeline is robust across models, while stronger models yield the highest effectiveness. These results indicate that LLM agents can integrate certificate-bearing control synthesis from high-level intent, enabling rapid end-to-end prototyping without sacrificing formal guarantees.", "source": "arxiv", "arxiv_id": "2511.07894v1", "pdf_url": "https://arxiv.org/pdf/2511.07894v1", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2025-11-11T06:49:06Z", "updated": "2025-11-11T06:49:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows", "authors": ["Mohamed Amine Ferrag", "Norbert Tihanyi", "Djallel Hamouda", "Leandros Maglaras", "Abderrahmane Lakas", "Merouane Debbah"], "year": 2025, "url": "http://arxiv.org/abs/2506.23260v2", "abstract": "Autonomous AI agents powered by large language models (LLMs) with structured function-calling interfaces enable real-time data retrieval, computation, and multi-step orchestration. However, the rapid growth of plugins, connectors, and inter-agent protocols has outpaced security practices, leading to brittle integrations that rely on ad-hoc authentication, inconsistent schemas, and weak validation. This survey introduces a unified end-to-end threat model for LLM-agent ecosystems, covering host-to-tool and agent-to-agent communications. We systematically categorize more than thirty attack techniques spanning input manipulation, model compromise, system and privacy attacks, and protocol-level vulnerabilities. For each category, we provide a formal threat formulation defining attacker capabilities, objectives, and affected system layers. Representative examples include Prompt-to-SQL injections and the Toxic Agent Flow exploit in GitHub MCP servers. We analyze attack feasibility, review existing defenses, and discuss mitigation strategies such as dynamic trust management, cryptographic provenance tracking, and sandboxed agent interfaces. The framework is validated through expert review and cross-mapping with real-world incidents and public vulnerability repositories, including CVE and NIST NVD. Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "source": "arxiv", "arxiv_id": "2506.23260v2", "pdf_url": "https://arxiv.org/pdf/2506.23260v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "10.1016/j.icte.2025.12.001", "venue": "", "published": "2025-06-29T14:32:32Z", "updated": "2025-12-14T20:29:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents", "authors": ["Mohammad Amaan Sayeed", "Mohammed Talha Alam", "Raza Imam", "Shahab Saquib Sohail", "Amir Hussain"], "year": 2025, "url": "http://arxiv.org/abs/2506.15911v2", "abstract": "Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and holistic therapies, yet remain inaccessible to many and underutilized in modern AI systems. Existing language-model benchmarks focus narrowly on factual recall or user preference, leaving a gap in validating culturally grounded medical guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that aligns 30 carefully curated Prophetic-medicine questions with human-verified remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three configurations: direct generation, retrieval-augmented generation, and a scientific self-critique filter. Each answer is then assessed by a secondary LLM serving as an agentic judge, yielding a single 3C3H quality score. Retrieval improves factual accuracy by 13%, while the agentic prompt adds another 10% improvement through deeper mechanistic insight and safety considerations. Our results demonstrate that blending classical Islamic texts with retrieval and self-evaluation enables reliable, culturally sensitive medical question-answering.", "source": "arxiv", "arxiv_id": "2506.15911v2", "pdf_url": "https://arxiv.org/pdf/2506.15911v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-18T23:14:35Z", "updated": "2025-06-23T02:12:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review", "authors": ["Yaohui Zhang", "Haijing Zhang", "Wenlong Ji", "Tianyu Hua", "Nick Haber", "Hancheng Cao", "Weixin Liang"], "year": 2025, "url": "http://arxiv.org/abs/2506.11343v2", "abstract": "The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows. Despite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process. In this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring. By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality. Our experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers. However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance. These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.", "source": "arxiv", "arxiv_id": "2506.11343v2", "pdf_url": "https://arxiv.org/pdf/2506.11343v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-12T22:27:20Z", "updated": "2025-09-25T03:35:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents", "authors": ["Amin Vahidi-Moghaddam", "Sayed Pedram Haeri Boroujeni", "Iman Jebellat", "Ehsan Jebellat", "Niloufar Mehrabi", "Zhaojian Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.04076v1", "abstract": "One of the main challenges in modern control applications, particularly in robot and vehicle motion control, is achieving accurate, fast, and safe movement. To address this, optimal control policies have been developed to enforce safety while ensuring high performance. Since basic first-principles models of real systems are often available, model-based controllers are widely used. Model predictive control (MPC) is a leading approach that optimizes performance while explicitly handling safety constraints. However, obtaining accurate models for complex systems is difficult, which motivates data-driven alternatives. ML-based MPC leverages learned models to reduce reliance on hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal policies directly from interaction data. Data-enabled predictive control (DeePC) goes further by bypassing modeling altogether, directly learning safe policies from raw input-output data. Recently, large language model (LLM) agents have also emerged, translating natural language instructions into structured formulations of optimal control problems. Despite these advances, data-driven policies face significant limitations. They often suffer from slow response times, high computational demands, and large memory needs, making them less practical for real-world systems with fast dynamics, limited onboard computing, or strict memory constraints. To address this, various technique, such as reduced-order modeling, function-approximated policy learning, and convex relaxations, have been proposed to reduce computational complexity. In this paper, we present eight such approaches and demonstrate their effectiveness across real-world applications, including robotic arms, soft robots, and vehicle motion control.", "source": "arxiv", "arxiv_id": "2510.04076v1", "pdf_url": "https://arxiv.org/pdf/2510.04076v1", "categories": ["cs.RO", "eess.SY"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-10-05T07:47:51Z", "updated": "2025-10-05T07:47:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems", "authors": ["Jiayi Chen", "Junyi Ye", "Guiling Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.04565v1", "abstract": "Compound Al Systems (CAIS) is an emerging paradigm that integrates large language models (LLMs) with external components, such as retrievers, agents, tools, and orchestrators, to overcome the limitations of standalone models in tasks requiring memory, reasoning, real-time grounding, and multimodal understanding. These systems enable more capable and context-aware behaviors by composing multiple specialized modules into cohesive workflows. Despite growing adoption in both academia and industry, the CAIS landscape remains fragmented, lacking a unified framework for analysis, taxonomy, and evaluation. In this survey, we define the concept of CAIS, propose a multi-dimensional taxonomy based on component roles and orchestration strategies, and analyze four foundational paradigms: Retrieval-Augmented Generation (RAG), LLM Agents, Multimodal LLMs (MLLMs), and orchestration-centric architectures. We review representative systems, compare design trade-offs, and summarize evaluation methodologies across these paradigms. Finally, we identify key challenges-including scalability, interoperability, benchmarking, and coordination-and outline promising directions for future research. This survey aims to provide researchers and practitioners with a comprehensive foundation for understanding, developing, and advancing the next generation of system-level artificial intelligence.", "source": "arxiv", "arxiv_id": "2506.04565v1", "pdf_url": "https://arxiv.org/pdf/2506.04565v1", "categories": ["cs.MA", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-05T02:34:43Z", "updated": "2025-06-05T02:34:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization", "authors": ["Haoran Xi", "Minghao Shao", "Brendan Dolan-Gavitt", "Muhammad Shafique", "Ramesh Karri"], "year": 2025, "url": "http://arxiv.org/abs/2510.02389v2", "abstract": "Large language models show promise for vulnerability discovery, yet prevailing methods inspect code in isolation, struggle with long contexts, and focus on coarse function or file level detections which offers limited actionable guidance to engineers who need precise line-level localization and targeted patches in real-world software development. We present T2L-Agent (Trace-to-Line Agent), a project-level, end-to-end framework that plans its own analysis and progressively narrows scope from modules to exact vulnerable lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer (ATA) that fuses run-time evidence such as crash points, stack traces, and coverage deltas with AST-based code chunking, enabling iterative refinement beyond single pass predictions and translating symptoms into actionable, line-level diagnoses. To benchmark line-level vulnerability discovery, we introduce T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash families and real-world projects. T2L-ARVO is specifically designed to support both coarse-grained detection and fine-grained localization, enabling rigorous evaluation of systems that aim to move beyond file-level predictions. On T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level localization, substantially outperforming baselines. Together, the framework and benchmark push LLM-based vulnerability detection from coarse identification toward deployable, robust, precision diagnostics that reduce noise and accelerate patching in open-source software workflows.", "source": "arxiv", "arxiv_id": "2510.02389v2", "pdf_url": "https://arxiv.org/pdf/2510.02389v2", "categories": ["cs.SE", "cs.CR", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-30T22:27:18Z", "updated": "2025-12-17T18:10:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents", "authors": ["Dezhi Ran", "Zhi Gong", "Yuzhe Guo", "Mengzhou Wu", "Yuan Cao", "Haochuan Lu", "Hengyu Zhang", "Xia Zeng", "Gang Cao", "Liangchao Yao", "Yuetang Deng", "Wei Yang", "Tao Xie"], "year": 2025, "url": "http://arxiv.org/abs/2512.13438v1", "abstract": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.", "source": "arxiv", "arxiv_id": "2512.13438v1", "pdf_url": "https://arxiv.org/pdf/2512.13438v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-15T15:34:06Z", "updated": "2025-12-15T15:34:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Fundamentals of Building Autonomous LLM Agents", "authors": ["Victor de Lamo Castrillo", "Habtom Kahsay Gidey", "Alexander Lenz", "Alois Knoll"], "year": 2025, "url": "http://arxiv.org/abs/2510.09244v1", "abstract": "This paper reviews the architecture and implementation methods of agents powered by large language models (LLMs). Motivated by the limitations of traditional LLMs in real-world tasks, the research aims to explore patterns to develop \"agentic\" LLMs that can automate complex tasks and bridge the performance gap with human capabilities. Key components include a perception system that converts environmental percepts into meaningful representations; a reasoning system that formulates plans, adapts to feedback, and evaluates actions through different techniques like Chain-of-Thought and Tree-of-Thought; a memory system that retains knowledge through both short-term and long-term mechanisms; and an execution system that translates internal decisions into concrete actions. This paper shows how integrating these systems leads to more capable and generalized software bots that mimic human cognitive processes for autonomous and intelligent behavior.", "source": "arxiv", "arxiv_id": "2510.09244v1", "pdf_url": "https://arxiv.org/pdf/2510.09244v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-10T10:32:39Z", "updated": "2025-10-10T10:32:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System", "authors": ["Toan Nguyen", "Tri Le", "Quang Nguyen", "Anh Nguyen"], "year": 2025, "url": "http://arxiv.org/abs/2507.04770v1", "abstract": "Furniture decoration is an important task in various industrial applications. However, achieving a high-quality decorative result is often time-consuming and requires specialized artistic expertise. To tackle these challenges, we explore how multi-agent systems can assist in automating the decoration process. We propose FurniMAS, a multi-agent system for automatic furniture decoration. Specifically, given a human prompt and a household furniture item such as a working desk or a TV stand, our system suggests relevant assets with appropriate styles and materials, and arranges them on the item, ensuring the decorative result meets functionality, aesthetic, and ambiance preferences. FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each fulfilling distinct roles in a typical decoration project. These agents collaborate through communication, logical reasoning, and validation to transform the requirements into the final outcome. Extensive experiments demonstrate that our FurniMAS significantly outperforms other baselines in generating high-quality 3D decor.", "source": "arxiv", "arxiv_id": "2507.04770v1", "pdf_url": "https://arxiv.org/pdf/2507.04770v1", "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-07T08:45:08Z", "updated": "2025-07-07T08:45:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction", "authors": ["Zhiyuan Zeng", "Jiashuo Liu", "Siyuan Chen", "Tianci He", "Yali Liao", "Yixiao Tian", "Jinpeng Wang", "Zaiyuan Wang", "Yang Yang", "Lingyue Yin", "Mingren Yin", "Zhenwei Zhu", "Tianle Cai", "Zehui Chen", "Jiecao Chen", "Yantao Du", "Xiang Gao", "Jiacheng Guo", "Liang Hu", "Jianpeng Jiao", "Xiangsheng Li", "Jingkai Liu", "Shuang Ni", "Zhoufutu Wen", "Ge Zhang", "Kaiyuan Zhang", "Xin Zhou", "Jose Blanchet", "Xipeng Qiu", "Mengdi Wang", "Wenhao Huang"], "year": 2025, "url": "http://arxiv.org/abs/2508.11987v3", "abstract": "Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce $\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments. Additionally, we provide in-depth analyses of agents' failure modes and performance pitfalls in future-oriented tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.", "source": "arxiv", "arxiv_id": "2508.11987v3", "pdf_url": "https://arxiv.org/pdf/2508.11987v3", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-16T08:54:08Z", "updated": "2025-09-05T09:15:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing", "authors": ["Hridya Dhulipala", "Xiaokai Rong", "Aashish Yadavally", "Tien N. Nguyen"], "year": 2025, "url": "http://arxiv.org/abs/2512.21440v1", "abstract": "In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.", "source": "arxiv", "arxiv_id": "2512.21440v1", "pdf_url": "https://arxiv.org/pdf/2512.21440v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-24T22:17:29Z", "updated": "2025-12-24T22:17:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening", "authors": ["Siming Zhao", "Qi Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.06262v1", "abstract": "Organizations are increasingly exploring delegation of screening and negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is constrained by governance: preventing unauthorized commitments, ensuring sufficient information before bargaining, and maintaining effective human oversight and auditability. Prior work on large language model negotiation largely emphasizes autonomous bargaining between agents and omits practical needs such as staged information gathering, explicit authorization boundaries, and systematic feedback integration. We propose GAIA, a governance-first framework for LLM-human agency in B2B negotiation and screening. GAIA defines three essential roles - Principal (human), Delegate (LLM agent), and Counterparty - with an optional Critic to enhance performance, and organizes interactions through three mechanisms: information-gated progression that separates screening from negotiation; dual feedback integration that combines AI critique with lightweight human corrections; and authorization boundaries with explicit escalation paths. Our contributions are fourfold: (1) a formal governance framework with three coordinated mechanisms and four safety invariants for delegation with bounded authorization; (2) information-gated progression via task-completeness tracking (TCI) and explicit state transitions that separate screening from commitment; (3) dual feedback integration that blends Critic suggestions with human oversight through parallel learning channels; and (4) a hybrid validation blueprint that combines automated protocol metrics with human judgment of outcomes and safety. By bridging theory and practice, GAIA offers a reproducible specification for safe, efficient, and accountable AI delegation that can be instantiated across procurement, real estate, and staffing workflows.", "source": "arxiv", "arxiv_id": "2511.06262v1", "pdf_url": "https://arxiv.org/pdf/2511.06262v1", "categories": ["cs.AI", "cs.CY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-09T07:41:49Z", "updated": "2025-11-09T07:41:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?", "authors": ["Yifang Tian", "Yaming Liu", "Zichun Chong", "Zihang Huang", "Hans-Arno Jacobsen"], "year": 2025, "url": "http://arxiv.org/abs/2508.12472v1", "abstract": "Root cause analysis (RCA) in microservice systems is challenging, requiring on-call engineers to rapidly diagnose failures across heterogeneous telemetry such as metrics, logs, and traces. Traditional RCA methods often focus on single modalities or merely rank suspect services, falling short of providing actionable diagnostic insights with remediation guidance. This paper introduces GALA, a novel multi-modal framework that combines statistical causal inference with LLM-driven iterative reasoning for enhanced RCA. Evaluated on an open-source benchmark, GALA achieves substantial improvements over state-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM evaluation score shows GALA generates significantly more causally sound and actionable diagnostic outputs than existing methods. Through comprehensive experiments and a case study, we show that GALA bridges the gap between automated failure diagnosis and practical incident resolution by providing both accurate root cause identification and human-interpretable remediation guidance.", "source": "arxiv", "arxiv_id": "2508.12472v1", "pdf_url": "https://arxiv.org/pdf/2508.12472v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-17T19:12:05Z", "updated": "2025-08-17T19:12:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents", "authors": ["Xinyu Zhang", "Yixin Wu", "Boyang Zhang", "Chenhao Lin", "Chao Shen", "Michael Backes", "Yang Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2511.22441v1", "abstract": "Images shared on social media often expose geographic cues. While early geolocation methods required expert effort and lacked generalization, the rise of Large Vision Language Models (LVLMs) now enables accurate geolocation even for ordinary users. However, existing approaches are not optimized for this task. To explore the full potential and associated privacy risks, we present Geo-Detective, an agent that mimics human reasoning and tool use for image geolocation inference. It follows a procedure with four steps that adaptively selects strategies based on image difficulty and is equipped with specialized tools such as visual reverse search, which emulates how humans gather external geographic clues. Experimental results show that GEO-Detective outperforms baseline large vision language models (LVLMs) overall, particularly on images lacking visible geographic features. In country level geolocation tasks, it achieves an improvement of over 11.1% compared to baseline LLMs, and even at finer grained levels, it still provides around a 5.2% performance gain. Meanwhile, when equipped with external clues, GEO-Detective becomes more likely to produce accurate predictions, reducing the \"unknown\" prediction rate by more than 50.6%. We further explore multiple defense strategies and find that Geo-Detective exhibits stronger robustness, highlighting the need for more effective privacy safeguards.", "source": "arxiv", "arxiv_id": "2511.22441v1", "pdf_url": "https://arxiv.org/pdf/2511.22441v1", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-27T13:27:26Z", "updated": "2025-11-27T13:27:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GOAT: A Training Framework for Goal-Oriented Agent with Tools", "authors": ["Hyunji Min", "Sangwon Jung", "Junyoung Sung", "Dosung Lee", "Leekyeung Han", "Paul Hongsuck Seo"], "year": 2025, "url": "http://arxiv.org/abs/2510.12218v1", "abstract": "Large language models (LLMs) have recently been extended beyond traditional text generation to serve as interactive agents capable of using external tools based on user intent. However, current LLM agents still show limited ability to handle goal-oriented queries, which require decomposing a high-level objective into multiple interdependent API calls with correct planning and execution. Current approaches mainly rely on zero-shot evaluation due to the absence of training data. While proprietary closed-source models such as GPT-4 demonstrate strong reasoning abilities, smaller open-source models struggle to perform complex tool use effectively. Thus, we propose a novel training framework GOAT, which enables fine-tuning of LLM agents in a human annotation-free setting. GOAT automatically constructs synthetic datasets of goal-oriented API execution tasks directly from given API documents, equipping models with the ability to reason over interdependent calls and generate coherent responses. Through extensive experiments, we show that GOAT-trained agents achieve state-of-the-art performance across multiple existing goal-oriented benchmarks. In addition, we introduce GOATBench, a new goal-oriented API execution benchmark, and demonstrate that agents trained with GOAT also excel in this setting. These results highlight GOAT as a practical path toward building robust open-source LLM agents capable of complex reasoning and tool use.", "source": "arxiv", "arxiv_id": "2510.12218v1", "pdf_url": "https://arxiv.org/pdf/2510.12218v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-14T07:14:50Z", "updated": "2025-10-14T07:14:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GRAFT: A Graph-based Flow-aware Agentic Framework for Document-level Machine Translation", "authors": ["Himanshu Dutta", "Sunny Manchanda", "Prakhar Bapat", "Meva Ram Gurjar", "Pushpak Bhattacharyya"], "year": 2025, "url": "http://arxiv.org/abs/2507.03311v1", "abstract": "Document level Machine Translation (DocMT) approaches often struggle with effectively capturing discourse level phenomena. Existing approaches rely on heuristic rules to segment documents into discourse units, which rarely align with the true discourse structure required for accurate translation. Otherwise, they fail to maintain consistency throughout the document during translation. To address these challenges, we propose Graph Augmented Agentic Framework for Document Level Translation (GRAFT), a novel graph based DocMT system that leverages Large Language Model (LLM) agents for document translation. Our approach integrates segmentation, directed acyclic graph (DAG) based dependency modelling, and discourse aware translation into a cohesive framework. Experiments conducted across eight translation directions and six diverse domains demonstrate that GRAFT achieves significant performance gains over state of the art DocMT systems. Specifically, GRAFT delivers an average improvement of 2.8 d BLEU on the TED test sets from IWSLT2017 over strong baselines and 2.3 d BLEU for domain specific translation from English to Chinese. Moreover, our analyses highlight the consistent ability of GRAFT to address discourse level phenomena, yielding coherent and contextually accurate translations.", "source": "arxiv", "arxiv_id": "2507.03311v1", "pdf_url": "https://arxiv.org/pdf/2507.03311v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-04T05:45:55Z", "updated": "2025-07-04T05:45:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments", "authors": ["Hanlin Zhu", "Tianyu Guo", "Song Mei", "Stuart Russell", "Nikhil Ghosh", "Alberto Bietti", "Jiantao Jiao"], "year": 2025, "url": "http://arxiv.org/abs/2509.21998v2", "abstract": "As LLMs are increasingly deployed as agents, agentic reasoning - the ability to combine tool use, especially search, and reasoning - becomes a critical skill. However, it is hard to disentangle agentic reasoning when evaluated in complex environments and tasks. Current agent benchmarks often mix agentic reasoning with challenging math reasoning, expert-level knowledge, and other advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent, where an LLM agent is required to solve grade-school-level reasoning problems, but is only presented with the question in the prompt without the premises that contain the necessary information to solve the task, and needs to proactively collect that information using tools. Although the original tasks are grade-school math problems, we observe that even frontier models like GPT-5 only achieve 67% accuracy. To understand and analyze the agentic reasoning patterns, we propose the concept of agentic reasoning graph: cluster the environment's document embeddings into nodes, and map each tool call to its nearest node to build a reasoning path. Surprisingly, we identify that the ability to revisit a previously visited node, widely taken as a crucial pattern in static reasoning, is often missing for agentic reasoning for many models. Based on the insight, we propose a tool-augmented test-time scaling method to improve LLM's agentic reasoning performance by adding tools to encourage models to revisit. We expect our benchmark and the agentic reasoning framework to aid future studies of understanding and pushing the boundaries of agentic reasoning.", "source": "arxiv", "arxiv_id": "2509.21998v2", "pdf_url": "https://arxiv.org/pdf/2509.21998v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-26T07:24:37Z", "updated": "2025-10-02T07:34:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GVGAI-LLM: Evaluating Large Language Model Agents with Infinite Games", "authors": ["Yuchen Li", "Cong Lin", "Muhammad Umair Nasir", "Philip Bontrager", "Jialin Liu", "Julian Togelius"], "year": 2025, "url": "http://arxiv.org/abs/2508.08501v2", "abstract": "We introduce GVGAI-LLM, a video game benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). Built on the General Video Game AI framework, it features a diverse collection of arcade-style games designed to test a model's ability to handle tasks that differ from most existing LLM benchmarks. The benchmark leverages a game description language that enables rapid creation of new games and levels, helping to prevent overfitting over time. Each game scene is represented by a compact set of ASCII characters, allowing for efficient processing by language models. GVGAI-LLM defines interpretable metrics, including the meaningful step ratio, step efficiency, and overall score, to assess model behavior. Through zero-shot evaluations across a broad set of games and levels with diverse challenges and skill depth, we reveal persistent limitations of LLMs in spatial reasoning and basic planning. Current models consistently exhibit spatial and logical errors, motivating structured prompting and spatial grounding techniques. While these interventions lead to partial improvements, the benchmark remains very far from solved. GVGAI-LLM provides a reproducible testbed for advancing research on language model capabilities, with a particular emphasis on agentic behavior and contextual reasoning.", "source": "arxiv", "arxiv_id": "2508.08501v2", "pdf_url": "https://arxiv.org/pdf/2508.08501v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-11T22:17:07Z", "updated": "2025-11-08T02:07:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Gala: Global LLM Agents for Text-to-Model Translation", "authors": ["Junyang Cai", "Serdar Kadioglu", "Bistra Dilkina"], "year": 2025, "url": "http://arxiv.org/abs/2509.08970v2", "abstract": "Natural language descriptions of optimization or satisfaction problems are challenging to translate into correct MiniZinc models, as this process demands both logical reasoning and constraint programming expertise. We introduce Gala, a framework that addresses this challenge with a global agentic approach: multiple specialized large language model (LLM) agents decompose the modeling task by global constraint type. Each agent is dedicated to detecting and generating code for a specific class of global constraint, while a final assembler agent integrates these constraint snippets into a complete MiniZinc model. By dividing the problem into smaller, well-defined sub-tasks, each LLM handles a simpler reasoning challenge, potentially reducing overall complexity. We conduct initial experiments with several LLMs and show better performance against baselines such as one-shot prompting and chain-of-thought prompting. Finally, we outline a comprehensive roadmap for future work, highlighting potential enhancements and directions for improvement.", "source": "arxiv", "arxiv_id": "2509.08970v2", "pdf_url": "https://arxiv.org/pdf/2509.08970v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-10T20:04:20Z", "updated": "2025-10-02T19:55:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents", "authors": ["Chongyu Bao", "Ruimin Dai", "Yangbo Shen", "Runyang Jian", "Jinghan Zhang", "Xiaolan Liu", "Kunpeng Liu"], "year": 2025, "url": "http://arxiv.org/abs/2508.03991v1", "abstract": "Intelligent personal assistants (IPAs) such as Siri and Google Assistant are designed to enhance human capabilities and perform tasks on behalf of users. The emergence of LLM agents brings new opportunities for the development of IPAs. While responsive capabilities have been widely studied, proactive behaviors remain underexplored. Designing an IPA that is proactive, privacy-preserving, and capable of self-evolution remains a significant challenge. Designing such IPAs relies on the cognitive architecture of LLM agents. This work proposes Cognition Forest, a semantic structure designed to align cognitive modeling with system-level design. We unify cognitive architecture and system design into a self-reinforcing loop instead of treating them separately. Based on this principle, we present Galaxy, a framework that supports multidimensional interactions and personalized capability generation. Two cooperative agents are implemented based on Galaxy: KoRa, a cognition-enhanced generative agent that supports both responsive and proactive skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's self-evolution and privacy preservation. Experimental results show that Galaxy outperforms multiple state-of-the-art benchmarks. Ablation studies and real-world interaction cases validate the effectiveness of Galaxy.", "source": "arxiv", "arxiv_id": "2508.03991v1", "pdf_url": "https://arxiv.org/pdf/2508.03991v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-06T00:46:38Z", "updated": "2025-08-06T00:46:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics", "authors": ["Arsham Gholamzadeh Khoee", "Shuai Wang", "Yinan Yu", "Robert Feldt", "Dhasarathy Parthasarathy"], "year": 2025, "url": "http://arxiv.org/abs/2503.21735v2", "abstract": "Ensuring reliable software release decisions is critical in safety-critical domains such as automotive manufacturing. Release validation relies on large tabular datasets, yet manual analysis is slow, costly, and error-prone. While Large Language Models (LLMs) offer promising automation potential, they face challenges in analytical reasoning, structured data handling, and ambiguity resolution. This paper introduces GateLens, an LLM-based system for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and generates optimized Python code. Unlike traditional multi-agent or planning-based systems that can be slow, opaque, and costly to maintain, GateLens emphasizes speed, transparency, and reliability. Experimental results show that GateLens outperforms the existing Chain-of-Thought (CoT) + Self-Consistency (SC) based system on real-world datasets, particularly in handling complex and ambiguous queries. Ablation studies confirm the essential role of the RA layer. Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation. GateLens operates effectively in zero-shot settings without requiring few-shot examples or agent orchestration. This work advances deployable LLM system design by identifying key architectural features-intermediate formal representations, execution efficiency, and low configuration overhead-crucial for safety-critical industrial applications.", "source": "arxiv", "arxiv_id": "2503.21735v2", "pdf_url": "https://arxiv.org/pdf/2503.21735v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-27T17:48:32Z", "updated": "2025-08-01T21:33:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation via Large Language Model Agents", "authors": ["Xi Yu", "Yang Yang", "Qun Liu", "Yonghua Du", "Sean McSweeney", "Yuewei Lin"], "year": 2025, "url": "http://arxiv.org/abs/2510.13896v1", "abstract": "Cellular image segmentation is essential for quantitative biology yet remains difficult due to heterogeneous modalities, morphological variability, and limited annotations. We present GenCellAgent, a training-free multi-agent framework that orchestrates specialist segmenters and generalist vision-language models via a planner-executor-evaluator loop (choose tool $\\rightarrow$ run $\\rightarrow$ quality-check) with long-term memory. The system (i) automatically routes images to the best tool, (ii) adapts on the fly using a few reference images when imaging conditions differ from what a tool expects, (iii) supports text-guided segmentation of organelles not covered by existing models, and (iv) commits expert edits to memory, enabling self-evolution and personalized workflows. Across four cell-segmentation benchmarks, this routing yields a 15.7\\% mean accuracy gain over state-of-the-art baselines. On endoplasmic reticulum and mitochondria from new datasets, GenCellAgent improves average IoU by 37.6\\% over specialist models. It also segments novel objects such as the Golgi apparatus via iterative text-guided refinement, with light human correction further boosting performance. Together, these capabilities provide a practical path to robust, adaptable cellular image segmentation without retraining, while reducing annotation burden and matching user preferences.", "source": "arxiv", "arxiv_id": "2510.13896v1", "pdf_url": "https://arxiv.org/pdf/2510.13896v1", "categories": ["q-bio.QM", "cs.AI", "cs.CV", "cs.MA"], "primary_category": "q-bio.QM", "doi": "", "venue": "", "published": "2025-10-14T17:02:57Z", "updated": "2025-10-14T17:02:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators", "authors": ["Jiacheng Guo", "Ling Yang", "Peter Chen", "Qixin Xiao", "Yinjie Wang", "Xinzhe Juan", "Jiahao Qiu", "Ke Shen", "Mengdi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.19682v2", "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $Î±$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.", "source": "arxiv", "arxiv_id": "2512.19682v2", "pdf_url": "https://arxiv.org/pdf/2512.19682v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-22T18:57:13Z", "updated": "2025-12-23T03:45:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance", "authors": ["Zaixi Zhang", "Zhenghong Zhou", "Ruofan Jin", "Le Cong", "Mengdi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2505.23839v1", "abstract": "DNA, encoding genetic instructions for almost all living organisms, fuels groundbreaking advances in genomics and synthetic biology. Recently, DNA Foundation Models have achieved success in designing synthetic functional DNA sequences, even whole genomes, but their susceptibility to jailbreaking remains underexplored, leading to potential concern of generating harmful sequences such as pathogens or toxin-producing genes. In this paper, we introduce GeneBreaker, the first framework to systematically evaluate jailbreak vulnerabilities of DNA foundation models. GeneBreaker employs (1) an LLM agent with customized bioinformatic tools to design high-homology, non-pathogenic jailbreaking prompts, (2) beam search guided by PathoLM and log-probability heuristics to steer generation toward pathogen-like sequences, and (3) a BLAST-based evaluation pipeline against a curated Human Pathogen Database (JailbreakDNABench) to detect successful jailbreaks. Evaluated on our JailbreakDNABench, GeneBreaker successfully jailbreaks the latest Evo series models across 6 viral categories consistently (up to 60\\% Attack Success Rate for Evo2-40B). Further case studies on SARS-CoV-2 spike protein and HIV-1 envelope protein demonstrate the sequence and structural fidelity of jailbreak output, while evolutionary modeling of SARS-CoV-2 underscores biosecurity risks. Our findings also reveal that scaling DNA foundation models amplifies dual-use risks, motivating enhanced safety alignment and tracing mechanisms. Our code is at https://github.com/zaixizhang/GeneBreaker.", "source": "arxiv", "arxiv_id": "2505.23839v1", "pdf_url": "https://arxiv.org/pdf/2505.23839v1", "categories": ["cs.CR", "q-bio.GN"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-28T13:58:32Z", "updated": "2025-05-28T13:58:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "General Modular Harness for LLM Agents in Multi-Turn Gaming Environments", "authors": ["Yuxuan Zhang", "Haoyang Yu", "Lanxiang Hu", "Haojian Jin", "Hao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2507.11633v1", "abstract": "We introduce a modular harness design for LLM agents that composes of perception, memory, and reasoning components, enabling a single LLM or VLM backbone to tackle a wide spectrum of multi turn gaming environments without domain-specific engineering. Using classic and modern game suites as low-barrier, high-diversity testbeds, our framework provides a unified workflow for analyzing how each module affects performance across dynamic interactive settings. Extensive experiments demonstrate that the harness lifts gameplay performance consistently over un-harnessed baselines and reveals distinct contribution patterns, for example, memory dominates in long-horizon puzzles while perception is critical in vision noisy arcades. These findings highlight the effectiveness of our modular harness design in advancing general-purpose agent, given the familiarity and ubiquity of games in everyday human experience.", "source": "arxiv", "arxiv_id": "2507.11633v1", "pdf_url": "https://arxiv.org/pdf/2507.11633v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-15T18:13:04Z", "updated": "2025-07-15T18:13:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Generalizable End-to-End Tool-Use RL with Synthetic CodeGym", "authors": ["Weihua Du", "Hailei Gong", "Zhan Ling", "Kang Liu", "Lingfeng Shen", "Xuesong Yao", "Yufei Xu", "Dingyuan Shi", "Yiming Yang", "Jiecao Chen"], "year": 2025, "url": "http://arxiv.org/abs/2509.17325v1", "abstract": "Tool-augmented large language models (LLMs), hereafter LLM agents, leverage external tools to solve diverse tasks and interface with the real world. However, current training practices largely rely on supervised fine-tuning (SFT) over static trajectories or reinforcement learning (RL) on narrow tasks, and generalize poorly beyond development settings, leading to brittleness with new tools and unseen workflows. Because code execution reflects many structures of real-world workflows, coding problems provide a natural basis for building agent training environments. Motivated by this, we introduce CodeGym, a scalable framework that synthesizes diverse, verifiable, and controllable multi-turn tool-use environments for agent RL, enabling LLM agents to explore and master various workflows actively. CodeGym rewrites static coding problems into interactive environments by extracting atomic functions or logic into callable tools, yielding verifiable tasks that span various tool-execution workflows. Models of varying sizes and chain-of-thought configurations, trained in CodeGym, exhibit consistent out-of-distribution generalizability; for example, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points on the OOD benchmark $Ï$-Bench. These results highlight CodeGym as a step toward scalable general-purpose RL environments that align with real-world agent workflows.", "source": "arxiv", "arxiv_id": "2509.17325v1", "pdf_url": "https://arxiv.org/pdf/2509.17325v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-22T03:03:56Z", "updated": "2025-09-22T03:03:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph", "authors": ["Fali Wang", "Jihai Chen", "Shuhua Yang", "Runxue Bao", "Tianxiang Zhao", "Zhiwei Zhang", "Xianfeng Tang", "Hui Liu", "Qi He", "Suhang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.00086v1", "abstract": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.", "source": "arxiv", "arxiv_id": "2511.00086v1", "pdf_url": "https://arxiv.org/pdf/2511.00086v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-29T22:14:25Z", "updated": "2025-10-29T22:14:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity", "authors": ["Jacopo Nudo", "Mario Edoardo Pandolfo", "Edoardo Loru", "Mattia Samory", "Matteo Cinelli", "Walter Quattrociocchi"], "year": 2025, "url": "http://arxiv.org/abs/2507.00657v1", "abstract": "We investigate how Large Language Models (LLMs) behave when simulating political discourse on social media. Leveraging 21 million interactions on X during the 2024 U.S. presidential election, we construct LLM agents based on 1,186 real users, prompting them to reply to politically salient tweets under controlled conditions. Agents are initialized either with minimal ideological cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one comparisons with human replies. We evaluate three model families (Gemini, Mistral, and DeepSeek) across linguistic style, ideological consistency, and toxicity. We find that richer contextualization improves internal consistency but also amplifies polarization, stylized signals, and harmful language. We observe an emergent distortion that we call \"generation exaggeration\": a systematic amplification of salient traits beyond empirical baselines. Our analysis shows that LLMs do not emulate users, they reconstruct them. Their outputs, indeed, reflect internal optimization dynamics more than observed behavior, introducing structural biases that compromise their reliability as social proxies. This challenges their use in content moderation, deliberative simulations, and policy modeling.", "source": "arxiv", "arxiv_id": "2507.00657v1", "pdf_url": "https://arxiv.org/pdf/2507.00657v1", "categories": ["cs.HC", "cs.AI", "cs.SI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-07-01T10:54:51Z", "updated": "2025-07-01T10:54:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent", "authors": ["Xingzuo Li", "Kehai Chen", "Yunfei Long", "Xuefeng Bai", "Yong Xu", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.02519v4", "abstract": "Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods.", "source": "arxiv", "arxiv_id": "2503.02519v4", "pdf_url": "https://arxiv.org/pdf/2503.02519v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-04T11:31:05Z", "updated": "2025-09-26T02:48:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming", "authors": ["Zheng Zhang", "Jiarui He", "Yuchen Cai", "Deheng Ye", "Peilin Zhao", "Ruili Feng", "Hao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.18314v1", "abstract": "As large language model (LLM) agents increasingly automate complex web tasks, they boost productivity while simultaneously introducing new security risks. However, relevant studies on web agent attacks remain limited. Existing red-teaming approaches mainly rely on manually crafted attack strategies or static models trained offline. Such methods fail to capture the underlying behavioral patterns of web agents, making it difficult to generalize across diverse environments. In web agent attacks, success requires the continuous discovery and evolution of attack strategies. To this end, we propose Genesis, a novel agentic framework composed of three modules: Attacker, Scorer, and Strategist. The Attacker generates adversarial injections by integrating the genetic algorithm with a hybrid strategy representation. The Scorer evaluates the target web agent's responses to provide feedback. The Strategist dynamically uncovers effective strategies from interaction logs and compiles them into a continuously growing strategy library, which is then re-deployed to enhance the Attacker's effectiveness. Extensive experiments across various web tasks show that our framework discovers novel strategies and consistently outperforms existing attack baselines.", "source": "arxiv", "arxiv_id": "2510.18314v1", "pdf_url": "https://arxiv.org/pdf/2510.18314v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-21T05:49:37Z", "updated": "2025-10-21T05:49:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Get Experience from Practice: LLM Agents with Record & Replay", "authors": ["Erhu Feng", "Wenbo Zhou", "Zibin Liu", "Le Chen", "Yunpeng Dong", "Cheng Zhang", "Yisheng Zhao", "Dong Du", "Zhichao Hua", "Yubin Xia", "Haibo Chen"], "year": 2025, "url": "http://arxiv.org/abs/2505.17716v1", "abstract": "AI agents, empowered by Large Language Models (LLMs) and communication protocols such as MCP and A2A, have rapidly evolved from simple chatbots to autonomous entities capable of executing complex, multi-step tasks, demonstrating great potential. However, the LLMs' inherent uncertainty and heavy computational resource requirements pose four significant challenges to the development of safe and efficient agents: reliability, privacy, cost and performance. Existing approaches, like model alignment, workflow constraints and on-device model deployment, can partially alleviate some issues but often with limitations, failing to fundamentally resolve these challenges.\n  This paper proposes a new paradigm called AgentRR (Agent Record & Replay), which introduces the classical record-and-replay mechanism into AI agent frameworks. The core idea is to: 1. Record an agent's interaction trace with its environment and internal decision process during task execution, 2. Summarize this trace into a structured \"experience\" encapsulating the workflow and constraints, and 3. Replay these experiences in subsequent similar tasks to guide the agent's behavior. We detail a multi-level experience abstraction method and a check function mechanism in AgentRR: the former balances experience specificity and generality, while the latter serves as a trust anchor to ensure completeness and safety during replay. In addition, we explore multiple application modes of AgentRR, including user-recorded task demonstration, large-small model collaboration and privacy-aware agent execution, and envision an experience repository for sharing and reusing knowledge to further reduce deployment cost.", "source": "arxiv", "arxiv_id": "2505.17716v1", "pdf_url": "https://arxiv.org/pdf/2505.17716v1", "categories": ["cs.LG", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-23T10:33:14Z", "updated": "2025-05-23T10:33:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment", "authors": ["Sascha Kaltenpoth", "Oliver MÃ¼ller"], "year": 2025, "url": "http://arxiv.org/abs/2509.07642v1", "abstract": "Adopting Large language models (LLMs) in organizations potentially revolutionizes our lives and work. However, they can generate off-topic, discriminating, or harmful content. This AI alignment problem often stems from misspecifications during the LLM adoption, unnoticed by the principal due to the LLM's black-box nature. While various research disciplines investigated AI alignment, they neither address the information asymmetries between organizational adopters and black-box LLM agents nor consider organizational AI adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led Alignment Strategy) a conceptual framework grounded in agency (contract) theory, to mitigate alignment problems during organizational LLM adoption. We conduct a conceptual literature analysis using the organizational LLM adoption phases and the agency theory as concepts. Our approach results in (1) providing an extended literature analysis process specific to AI alignment methods during organizational LLM adoption and (2) providing a first LLM alignment problem-solution space.", "source": "arxiv", "arxiv_id": "2509.07642v1", "pdf_url": "https://arxiv.org/pdf/2509.07642v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-09T12:10:14Z", "updated": "2025-09-09T12:10:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents", "authors": ["Hanlin Cai", "Houtianfu Wang", "Haofan Dong", "Kai Li", "Ozgur B. Akan"], "year": 2025, "url": "http://arxiv.org/abs/2511.07176v1", "abstract": "Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm.", "source": "arxiv", "arxiv_id": "2511.07176v1", "pdf_url": "https://arxiv.org/pdf/2511.07176v1", "categories": ["cs.NI", "cs.CL"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-11-10T15:06:26Z", "updated": "2025-11-10T15:06:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects", "authors": ["Yixin Liu", "Guibin Zhang", "Kun Wang", "Shiyuan Li", "Shirui Pan"], "year": 2025, "url": "http://arxiv.org/abs/2507.21407v2", "abstract": "Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control. While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows. Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work. Specifically, we categorize existing GLA methods by their primary functions in LLM agent systems, including planning, memory, and tool usage, and then analyze how graphs and graph learning algorithms contribute to each. For multi-agent systems, we further discuss how GLA solutions facilitate the orchestration, efficiency optimization, and trustworthiness of MAS. Finally, we highlight key future directions to advance this field, from improving structural adaptability to enabling unified, scalable, and multimodal GLA systems. We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems.", "source": "arxiv", "arxiv_id": "2507.21407v2", "pdf_url": "https://arxiv.org/pdf/2507.21407v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-29T00:27:12Z", "updated": "2025-08-30T06:01:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Graph-Enhanced Policy Optimization in LLM Agent Training", "authors": ["Jiazhen Yuan", "Wei Zhao", "Zhengbiao Bai"], "year": 2025, "url": "http://arxiv.org/abs/2510.26270v1", "abstract": "Group based reinforcement learning (RL) has shown impressive results on complex reasoning and mathematical tasks. Yet, when applied to train multi-turn, interactive LLM agents, these methods often suffer from structural blindness-the inability to exploit the underlying connectivity of the environment. This manifests in three critical challenges: (1) inefficient, unguided exploration, (2) imprecise credit assignment due to overlooking pivotal states, and (3) myopic planning caused by static reward discounting. We address these issues with Graph-Enhanced Policy Optimization (GEPO), which dynamically constructs a state-transition graph from agent experience and employs graph-theoretic centrality to provide three synergistic learning signals: (1)structured intrinsic rewards that guide exploration toward high-impact states, (2) a graph-enhanced advantage function for topology-aware credit assignment, and (3) a dynamic discount factor adapted to each state's strategic value. On the ALFWorld, WebShop, and a proprietary Workbench benchmarks, GEPO demonstrates strong performance, achieving absolute success rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These results highlight that explicitly modeling environmental structure is a robust, generalizable strategy for advancing LLM agent training.", "source": "arxiv", "arxiv_id": "2510.26270v1", "pdf_url": "https://arxiv.org/pdf/2510.26270v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-30T08:53:41Z", "updated": "2025-10-30T08:53:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks", "authors": ["Divya Vijay", "Vignesh Ethiraj"], "year": 2025, "url": "http://arxiv.org/abs/2512.20275v1", "abstract": "As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.", "source": "arxiv", "arxiv_id": "2512.20275v1", "pdf_url": "https://arxiv.org/pdf/2512.20275v1", "categories": ["cs.AI", "cs.NI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T11:27:17Z", "updated": "2025-12-23T11:27:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GraphCodeAgent: Dual Graph-Guided LLM Agent for Retrieval-Augmented Repo-Level Code Generation", "authors": ["Jia Li", "Xianjie Shi", "Kechi Zhang", "Ge Li", "Zhi Jin", "Lei Li", "Huangzhao Zhang", "Jia Li", "Fang Liu", "Yuwei Zhang", "Zhengwei Tao", "Yihong Dong", "Yuqi Zhu", "Chongyang Tao"], "year": 2025, "url": "http://arxiv.org/abs/2504.10046v2", "abstract": "Writing code requires significant time and effort in software development. To automate this process, researchers have made substantial progress for code generation. Recently, large language models (LLMs) have demonstrated remarkable proficiency in function-level code generation, yet their performance significantly degrades in the real-world software development process, where coding tasks are deeply embedded within specific repository contexts. Existing studies attempt to use retrieval-augmented code generation (RACG) approaches to mitigate this demand. However, there is a gap between natural language (NL) requirements and programming implementations. This results in the failure to retrieve the relevant code of these fine-grained subtasks. To address this challenge, we propose GraphCodeAgent, a dual graph-guided LLM agent for retrieval-augmented repo-level code generation, bridging the gap between NL requirements and programming implementations. Our approach constructs two interconnected graphs: a Requirement Graph (RG) to model requirement relations of code snippets within the repository, as well as the relations between the target requirement and the requirements of these code snippets, and a Structural-Semantic Code Graph (SSCG) to capture the repository's intricate code dependencies. Guided by this, an LLM-powered agent performs multi-hop reasoning to systematically retrieve all context code snippets, including implicit and explicit code snippets, even if they are not explicitly expressed in requirements. We evaluated GraphCodeAgent on three advanced LLMs with the two widely-used repo-level code generation benchmarks DevEval and CoderEval. Extensive experiment results show that GraphCodeAgent significantly outperforms state-of-the-art baselines.", "source": "arxiv", "arxiv_id": "2504.10046v2", "pdf_url": "https://arxiv.org/pdf/2504.10046v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-04-14T09:51:23Z", "updated": "2025-11-18T15:29:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments", "authors": ["Enjun Du", "Xunkai Li", "Tian Jin", "Zhihan Zhang", "Rong-Hua Li", "Guoren Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.00711v2", "abstract": "The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.", "source": "arxiv", "arxiv_id": "2504.00711v2", "pdf_url": "https://arxiv.org/pdf/2504.00711v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-01T12:21:50Z", "updated": "2025-05-05T13:57:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search", "authors": ["Heng Zhang", "Yuling Shi", "Xiaodong Gu", "Haochen You", "Zijian Zhang", "Lubin Gan", "Yilei Yuan", "Jin Huang"], "year": 2025, "url": "http://arxiv.org/abs/2510.10581v2", "abstract": "Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collaboration, yet they face high failure rates in multi-turn deep search scenarios. Existing temporal attribution methods struggle to accurately diagnose root causes, particularly when errors propagate across multiple agents. Attempts to automate failure attribution by analyzing action sequences remain ineffective due to their inability to account for information dependencies that span agents. This paper identifies two core challenges: \\textit{(i) distinguishing symptoms from root causes in multi-agent error propagation}, and \\textit{(ii) tracing information dependencies beyond temporal order}. To address these issues, we introduce \\textbf{GraphTracer}, a framework that redefines failure attribution through information flow analysis. GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly capture how agents reference and build on prior outputs. It localizes root causes by tracing through these dependency structures instead of relying on temporal sequences. GraphTracer also uses graph-aware synthetic data generation to target critical nodes, creating realistic failure scenarios. Evaluations on the Who\\&When benchmark and integration into production systems demonstrate that GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared to state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements in deployed multi-agent frameworks, establishing a robust solution for multi-agent system debugging.", "source": "arxiv", "arxiv_id": "2510.10581v2", "pdf_url": "https://arxiv.org/pdf/2510.10581v2", "categories": ["cs.GR"], "primary_category": "cs.GR", "doi": "", "venue": "", "published": "2025-10-12T12:55:42Z", "updated": "2025-12-22T18:19:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GraphicBench: A Planning Benchmark for Graphic Design with Language Agents", "authors": ["Dayeon Ki", "Tianyi Zhou", "Marine Carpuat", "Gang Wu", "Puneet Mathur", "Viswanathan Swaminathan"], "year": 2025, "url": "http://arxiv.org/abs/2504.11571v1", "abstract": "Large Language Model (LLM)-powered agents have unlocked new possibilities for automating human tasks. While prior work has focused on well-defined tasks with specified goals, the capabilities of agents in creative design tasks with open-ended goals remain underexplored. We introduce GraphicBench, a new planning benchmark for graphic design that covers 1,079 user queries and input images across four design types. We further present GraphicTown, an LLM agent framework with three design experts and 46 actions (tools) to choose from for executing each step of the planned workflows in web environments. Experiments with six LLMs demonstrate their ability to generate workflows that integrate both explicit design constraints from user queries and implicit commonsense constraints. However, these workflows often do not lead to successful execution outcomes, primarily due to challenges in: (1) reasoning about spatial relationships, (2) coordinating global dependencies across experts, and (3) retrieving the most appropriate action per step. We envision GraphicBench as a challenging yet valuable testbed for advancing LLM-agent planning and execution in creative design tasks.", "source": "arxiv", "arxiv_id": "2504.11571v1", "pdf_url": "https://arxiv.org/pdf/2504.11571v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-15T19:26:59Z", "updated": "2025-04-15T19:26:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Grounded Test-Time Adaptation for LLM Agents", "authors": ["Arthur Chen", "Zuxin Liu", "Jianguo Zhang", "Akshara Prabhakar", "Zhiwei Liu", "Shelby Heinecke", "Silvio Savarese", "Victor Zhong", "Caiming Xiong"], "year": 2025, "url": "http://arxiv.org/abs/2511.04847v3", "abstract": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.", "source": "arxiv", "arxiv_id": "2511.04847v3", "pdf_url": "https://arxiv.org/pdf/2511.04847v3", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-06T22:24:35Z", "updated": "2026-01-05T17:43:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Group-in-Group Policy Optimization for LLM Agent Training", "authors": ["Lang Feng", "Zhenghai Xue", "Tingcong Liu", "Bo An"], "year": 2025, "url": "http://arxiv.org/abs/2505.10978v3", "abstract": "Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to multi-turn LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals, achieves performance gains of > 12% on ALFWorld and > 9% on WebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B and 47.2% on 7B): all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.", "source": "arxiv", "arxiv_id": "2505.10978v3", "pdf_url": "https://arxiv.org/pdf/2505.10978v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-16T08:26:59Z", "updated": "2025-10-28T15:11:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents", "authors": ["Jie JW Wu", "Ayanda Patrick Herlihy", "Ahmad Saleem Mirza", "Ali Afoud", "Fatemeh Fard"], "year": 2025, "url": "http://arxiv.org/abs/2511.00802v1", "abstract": "With the software industry shifting toward a data-driven culture, online A/B testing is a key tool for evaluating new technologies. However, deploying such experiments requires substantial resources, may negatively impact users, and involves long data collection periods. To address this, \\textit{off-policy evaluation (OPE)}, or offline A/B testing, uses logged data to assess technologies and is fundamental in Reinforcement Learning, making it crucial in domains where online testing is costly or risky, such as healthcare, recommender systems, education, dialog systems, and robotics. Despite advances in coding LLMs and agentic AI, little is known about leveraging them to optimize OPE results. We investigate whether LLMs and LLM-based agents can improve OPE performance via code optimization. We propose \\textit{GrowthHacker}, a benchmark with agent and baseline methods on large-scale real-world datasets, which iteratively optimizes code, evaluates results, and begins new optimization cycles. We collected datasets, established protocols, implemented baselines for OPE on the Open Bandit Pipeline (OBP)~\\cite{saito2021openbanditdatasetpipeline} and Scope-RL~\\cite{kiyohara2023scope}, and developed the \\textit{two_agent} framework, which reduces system complexity while preserving optimization effectiveness. Results show the two_agent framework achieves 100% reliability and the highest average improvement of 106.7% among positive outcomes. Both two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%. These findings demonstrate the feasibility of LLM-based agents as automated \"growth hackers\" to enhance OPE systems, with implications for scaling data-driven decision-making in production.", "source": "arxiv", "arxiv_id": "2511.00802v1", "pdf_url": "https://arxiv.org/pdf/2511.00802v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-02T04:47:17Z", "updated": "2025-11-02T04:47:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents", "authors": ["Lingxiao Diao", "Xinyue Xu", "Wanxuan Sun", "Cheng Yang", "Zhuosheng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.11368v2", "abstract": "Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.", "source": "arxiv", "arxiv_id": "2505.11368v2", "pdf_url": "https://arxiv.org/pdf/2505.11368v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-16T15:32:23Z", "updated": "2025-06-17T06:55:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees", "authors": ["Katsuaki Nakano", "Reza Fayyazi", "Shanchieh Jay Yang", "Michael Zuzak"], "year": 2025, "url": "http://arxiv.org/abs/2509.07939v2", "abstract": "Recent advances in Large Language Models (LLMs) have driven interest in automating cybersecurity penetration testing workflows, offering the promise of faster and more consistent vulnerability assessment for enterprise systems. Existing LLM agents for penetration testing primarily rely on self-guided reasoning, which can produce inaccurate or hallucinated procedural steps. As a result, the LLM agent may undertake unproductive actions, such as exploiting unused software libraries or generating cyclical responses that repeat prior tactics. In this work, we propose a guided reasoning pipeline for penetration testing LLM agents that incorporates a deterministic task tree built from the MITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the LLM's reaoning process to explicitly defined tactics, techniques, and procedures. This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Our proposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and 78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively. Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that incorporating a deterministic task tree into LLM reasoning pipelines can enhance the accuracy and efficiency of automated cybersecurity assessments", "source": "arxiv", "arxiv_id": "2509.07939v2", "pdf_url": "https://arxiv.org/pdf/2509.07939v2", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-09T17:19:33Z", "updated": "2025-11-18T18:20:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents", "authors": ["Yejin Kim", "Youngbin Lee", "Juhyeong Kim", "Yongjae Lee"], "year": 2025, "url": "http://arxiv.org/abs/2510.01664v1", "abstract": "This study demonstrates that GuruAgents, prompt-guided AI agents, can systematically operationalize the strategies of legendary investment gurus. We develop five distinct GuruAgents, each designed to emulate an iconic investor, by encoding their distinct philosophies into LLM prompts that integrate financial tools and a deterministic reasoning pipeline. In a backtest on NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique behaviors driven by their prompted personas. The Buffett GuruAgent achieves the highest performance, delivering a 42.2\\% CAGR that significantly outperforms benchmarks, while other agents show varied results. These findings confirm that prompt engineering can successfully translate the qualitative philosophies of investment gurus into reproducible, quantitative strategies, highlighting a novel direction for automated systematic investing. The source code and data are available at https://github.com/yejining99/GuruAgents.", "source": "arxiv", "arxiv_id": "2510.01664v1", "pdf_url": "https://arxiv.org/pdf/2510.01664v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "CIKM 2025 Workshop on Advances in Financial AI: Innovations, Risk, and Responsibility in the Era of LLMs", "published": "2025-10-02T04:45:27Z", "updated": "2025-10-02T04:45:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents", "authors": ["Shicheng Ye", "Chao Yu", "Kaiqiang Ke", "Chengdong Xu", "Yinqi Wei"], "year": 2025, "url": "http://arxiv.org/abs/2509.12810v1", "abstract": "Large language model (LLM)-based agents have shown strong potential in multi-task scenarios, owing to their ability to transfer knowledge across diverse tasks. However, existing approaches often treat prior experiences and knowledge as monolithic units, leading to inefficient and coarse-grained knowledge transfer. In this work, we propose a novel hierarchical memory architecture that enables fine-grained knowledge transfer by decoupling high-level planning memory from low-level execution memory. To construct and refine these hierarchical memories, we introduce Hierarchical Hindsight Reflection (H$^2$R), a mechanism that distills reusable and hierarchical knowledge from past agent-environment interactions. At test time, H$^2$R performs retrievals of high-level and low-level memories separately, allowing LLM-based agents to efficiently access and utilize task-relevant knowledge for new tasks.Experimental results across two benchmarks demonstrate that H$^2$R can improve generalization and decision-making performance, outperforming prior baselines such as Expel.", "source": "arxiv", "arxiv_id": "2509.12810v1", "pdf_url": "https://arxiv.org/pdf/2509.12810v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-16T08:30:08Z", "updated": "2025-09-16T08:30:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HARBOR: Exploring Persona Dynamics in Multi-Agent Competition", "authors": ["Kenan Jiang", "Li Xiong", "Fei Liu"], "year": 2025, "url": "http://arxiv.org/abs/2502.12149v2", "abstract": "We investigate factors contributing to LLM agents' success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent's behavior in a competitive setting? (b) Can an agent effectively profile its competitors' behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.", "source": "arxiv", "arxiv_id": "2502.12149v2", "pdf_url": "https://arxiv.org/pdf/2502.12149v2", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-02-17T18:58:36Z", "updated": "2025-06-15T15:15:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents", "authors": ["Thomas Carta", "ClÃ©ment Romac", "Loris Gaven", "Pierre-Yves Oudeyer", "Olivier Sigaud", "Sylvain Lamprier"], "year": 2025, "url": "http://arxiv.org/abs/2508.14751v1", "abstract": "Open-ended AI agents need to be able to learn efficiently goals of increasing complexity, abstraction and heterogeneity over their lifetime. Beyond sampling efficiently their own goals, autotelic agents specifically need to be able to keep the growing complexity of goals under control, limiting the associated growth in sample and computational complexity. To adress this challenge, recent approaches have leveraged hierarchical reinforcement learning (HRL) and language, capitalizing on its compositional and combinatorial generalization capabilities to acquire temporally extended reusable behaviours. Existing approaches use expert defined spaces of subgoals over which they instantiate a hierarchy, and often assume pre-trained associated low-level policies. Such designs are inadequate in open-ended scenarios, where goal spaces naturally diversify across a broad spectrum of difficulties. We introduce HERAKLES, a framework that enables a two-level hierarchical autotelic agent to continuously compile mastered goals into the low-level policy, executed by a small, fast neural network, dynamically expanding the set of subgoals available to the high-level policy. We train a Large Language Model (LLM) to serve as the high-level controller, exploiting its strengths in goal decomposition and generalization to operate effectively over this evolving subgoal space. We evaluate HERAKLES in the open-ended Crafter environment and show that it scales effectively with goal complexity, improves sample efficiency through skill compilation, and enables the agent to adapt robustly to novel challenges over time.", "source": "arxiv", "arxiv_id": "2508.14751v1", "pdf_url": "https://arxiv.org/pdf/2508.14751v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-20T14:50:28Z", "updated": "2025-08-20T14:50:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on Large Language Models", "authors": ["Zhaoxing Li", "Wenbo Wu", "Yue Wang", "Yanran Xu", "William Hunt", "Sebastian Stein"], "year": 2025, "url": "http://arxiv.org/abs/2505.00820v1", "abstract": "Rapid advancements in artificial intelligence (AI) have enabled robots to performcomplex tasks autonomously with increasing precision. However, multi-robot systems (MRSs) face challenges in generalization, heterogeneity, and safety, especially when scaling to large-scale deployments like disaster response. Traditional approaches often lack generalization, requiring extensive engineering for new tasks and scenarios, and struggle with managing diverse robots. To overcome these limitations, we propose a Human-in-the-loop Multi-Robot Collaboration Framework (HMCF) powered by large language models (LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot capabilities, while human oversight ensures safety and reliability, intervening only when necessary. Our framework seamlessly integrates human oversight, LLM agents, and heterogeneous robots to optimize task allocation and execution. Each robot is equipped with an LLM agent capable of understanding its capabilities, converting tasks into executable instructions, and reducing hallucinations through task verification and human supervision. Simulation results show that our framework outperforms state-of-the-art task planning methods, achieving higher task success rates with an improvement of 4.76%. Real-world tests demonstrate its robust zero-shot generalization feature and ability to handle diverse tasks and environments with minimal human intervention.", "source": "arxiv", "arxiv_id": "2505.00820v1", "pdf_url": "https://arxiv.org/pdf/2505.00820v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-05-01T19:23:50Z", "updated": "2025-05-01T19:23:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation", "authors": ["Rabimba Karanjai", "Lei Xu", "Weidong Shi"], "year": 2025, "url": "http://arxiv.org/abs/2511.10860v1", "abstract": "Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.", "source": "arxiv", "arxiv_id": "2511.10860v1", "pdf_url": "https://arxiv.org/pdf/2511.10860v1", "categories": ["cs.DC", "cs.AI", "cs.SE"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-11-13T23:52:53Z", "updated": "2025-11-13T23:52:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities", "authors": ["Xiaoxue Ren", "Penghao Jiang", "Kaixin Li", "Zhiyong Huang", "Xiaoning Du", "Jiaojiao Jiang", "Zhenchang Xing", "Jiamou Sun", "Terry Yue Zhuo"], "year": 2025, "url": "http://arxiv.org/abs/2510.12200v1", "abstract": "Web applications are prime targets for cyberattacks as gateways to critical services and sensitive data. Traditional penetration testing is costly and expertise-intensive, making it difficult to scale with the growing web ecosystem. While language model agents show promise in cybersecurity, modern web applications demand visual understanding, dynamic content handling, and multi-step interactions that only computer-use agents (CUAs) can perform. Yet, their ability to discover and exploit vulnerabilities through graphical interfaces remains largely unexplored. We present HackWorld, the first framework for systematically evaluating CUAs' capabilities to exploit web application vulnerabilities via visual interaction. Unlike sanitized benchmarks, HackWorld includes 36 real-world applications across 11 frameworks and 7 languages, featuring realistic flaws such as injection vulnerabilities, authentication bypasses, and unsafe input handling. Using a Capture-the-Flag (CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses while navigating complex web interfaces. Evaluation of state-of-the-art CUAs reveals concerning trends: exploitation rates below 12% and low cybersecurity awareness. CUAs often fail at multi-step attack planning and misuse security tools. These results expose the current limitations of CUAs in web security contexts and highlight opportunities for developing more security-aware agents capable of effective vulnerability detection and exploitation.", "source": "arxiv", "arxiv_id": "2510.12200v1", "pdf_url": "https://arxiv.org/pdf/2510.12200v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-14T06:52:15Z", "updated": "2025-10-14T06:52:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents", "authors": ["Mohammad Amin Abbasi", "Farnaz Sadat Mirnezami", "Ali Neshati", "Hassan Naderi"], "year": 2025, "url": "http://arxiv.org/abs/2502.05982v2", "abstract": "We present HamRaz, a culturally adapted Persian-language dataset for AI-assisted mental health support, grounded in Person-Centered Therapy (PCT). To reflect real-world therapeutic challenges, we combine script-based dialogue with adaptive large language models (LLM) role-playing, capturing the ambiguity and emotional nuance of Persian-speaking clients. We introduce HamRazEval, a dual-framework for assessing conversational and therapeutic quality using General Metrics and specialized psychological relationship measures. Human evaluations show HamRaz outperforms existing baselines in empathy, coherence, and realism. This resource contributes to the Digital Humanities by bridging language, culture, and mental health in underrepresented communities.", "source": "arxiv", "arxiv_id": "2502.05982v2", "pdf_url": "https://arxiv.org/pdf/2502.05982v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-09T18:23:34Z", "updated": "2025-09-03T20:08:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects", "authors": ["Abdullah Mushtaq", "Muhammad Rafay Naeem", "Ibrahim Ghaznavi", "Muhammad Imran Taj", "Imran Hashmi", "Junaid Qadir"], "year": 2025, "url": "http://arxiv.org/abs/2501.01205v1", "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of...", "source": "arxiv", "arxiv_id": "2501.01205v1", "pdf_url": "https://arxiv.org/pdf/2501.01205v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-01-02T11:25:45Z", "updated": "2025-01-02T11:25:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents", "authors": ["Jiawei Wang", "Jiacai Liu", "Yuqian Fu", "Yingru Li", "Xintao Wang", "Yuan Lin", "Yu Yue", "Lin Zhang", "Yang Wang", "Ke Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.09265v1", "abstract": "In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/", "source": "arxiv", "arxiv_id": "2509.09265v1", "pdf_url": "https://arxiv.org/pdf/2509.09265v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-11T08:50:01Z", "updated": "2025-09-11T08:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Hell or High Water: Evaluating Agentic Recovery from External Failures", "authors": ["Andrew Wang", "Sophia Hager", "Adi Asija", "Daniel Khashabi", "Nicholas Andrews"], "year": 2025, "url": "http://arxiv.org/abs/2508.11027v1", "abstract": "As language model agents are applied to real world problems of increasing complexity, they will be expected to formulate plans across large search spaces. If those plans fail for reasons beyond their control, how well do language agents search for alternative ways to achieve their goals? We devise a specialized agentic planning benchmark to study this question. Each planning problem is solved via combinations of function calls. The agent searches for relevant functions from a set of over four thousand possibilities, and observes environmental feedback in the form of function outputs or error messages. Our benchmark confronts the agent with external failures in its workflow, such as functions that suddenly become unavailable. At the same time, even with the introduction of these failures, we guarantee that the task remains solvable. Ideally, an agent's performance on the planning task should not be affected by the presence of external failures. Overall, we find that language agents struggle to formulate and execute backup plans in response to environment feedback. While state-of-the-art models are often able to identify the correct function to use in the right context, they struggle to adapt to feedback from the environment and often fail to pursue alternate courses of action, even when the search space is artificially restricted. We provide a systematic analysis of the failures of both open-source and commercial models, examining the effects of search space size, as well as the benefits of scaling model size in our setting. Our analysis identifies key challenges for current generative models as well as promising directions for future work.", "source": "arxiv", "arxiv_id": "2508.11027v1", "pdf_url": "https://arxiv.org/pdf/2508.11027v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-14T19:21:09Z", "updated": "2025-08-14T19:21:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Helmsman: Autonomous Synthesis of Federated Learning Systems via Collaborative LLM Agents", "authors": ["Haoyuan Li", "Mathias Funk", "Aaqib Saeed"], "year": 2025, "url": "http://arxiv.org/abs/2510.14512v2", "abstract": "Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.", "source": "arxiv", "arxiv_id": "2510.14512v2", "pdf_url": "https://arxiv.org/pdf/2510.14512v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-16T09:57:31Z", "updated": "2025-12-19T11:27:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training", "authors": ["Yuchen Zhuang", "Jingfeng Yang", "Haoming Jiang", "Xin Liu", "Kewei Cheng", "Sanket Lokegaonkar", "Yifan Gao", "Qing Ping", "Tianyi Liu", "Binxuan Huang", "Zheng Li", "Zhengyang Wang", "Pei Chen", "Ruijie Wang", "Rongzhi Zhang", "Nasser Zalmout", "Priyanka Nigam", "Bing Yin", "Chao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.06589v1", "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.", "source": "arxiv", "arxiv_id": "2502.06589v1", "pdf_url": "https://arxiv.org/pdf/2502.06589v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-10T15:54:34Z", "updated": "2025-02-10T15:54:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting", "authors": ["Daniil Sukhorukov", "Andrei Zakharov", "Nikita Glazkov", "Katsiaryna Yanchanka", "Vladimir Kirilin", "Maxim Dubovitsky", "Roman Sultimov", "Yuri Maksimov", "Ilya Makarov"], "year": 2025, "url": "http://arxiv.org/abs/2511.23387v1", "abstract": "We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.", "source": "arxiv", "arxiv_id": "2511.23387v1", "pdf_url": "https://arxiv.org/pdf/2511.23387v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-28T17:27:06Z", "updated": "2025-11-28T17:27:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents", "authors": ["Haoran Sun", "Shaoning Zeng"], "year": 2025, "url": "http://arxiv.org/abs/2507.22925v1", "abstract": "Long-term memory is one of the key factors influencing the reasoning capabilities of Large Language Model Agents (LLM Agents). Incorporating a memory mechanism that effectively integrates past interactions can significantly enhance decision-making and contextual coherence of LLM Agents. While recent works have made progress in memory storage and retrieval, such as encoding memory into dense vectors for similarity-based search or organizing knowledge in the form of graph, these approaches often fall short in structured memory organization and efficient retrieval. To address these limitations, we propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that organizes and updates memory in a multi-level fashion based on the degree of semantic abstraction. Each memory vector is embedded with a positional index encoding pointing to its semantically related sub-memories in the next layer. During the reasoning phase, an index-based routing mechanism enables efficient, layer-by-layer retrieval without performing exhaustive similarity computations. We evaluate our method on five task settings from the LoCoMo dataset. Experimental results show that our approach consistently outperforms five baseline methods, demonstrating its effectiveness in long-term dialogue scenarios.", "source": "arxiv", "arxiv_id": "2507.22925v1", "pdf_url": "https://arxiv.org/pdf/2507.22925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-23T12:45:44Z", "updated": "2025-07-23T12:45:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations", "authors": ["Yoshiki Takenami", "Yin Jou Huang", "Yugo Murawaki", "Chenhui Chu"], "year": 2025, "url": "http://arxiv.org/abs/2508.21137v2", "abstract": "Cognitive biases, well-studied in humans, can also be observed in LLMs, affecting their reliability in real-world applications. This paper investigates the anchoring effect in LLM-driven price negotiations. To this end, we instructed seller LLM agents to apply the anchoring effect and evaluated negotiations using not only an objective metric but also a subjective metric. Experimental results show that LLMs are influenced by the anchoring effect like humans. Additionally, we investigated the relationship between the anchoring effect and factors such as reasoning and personality. It was shown that reasoning models are less prone to the anchoring effect, suggesting that the long chain of thought mitigates the effect. However, we found no significant correlation between personality traits and susceptibility to the anchoring effect. These findings contribute to a deeper understanding of cognitive biases in LLMs and to the realization of safe and responsible application of LLMs in society.", "source": "arxiv", "arxiv_id": "2508.21137v2", "pdf_url": "https://arxiv.org/pdf/2508.21137v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-28T18:13:46Z", "updated": "2025-09-17T02:08:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior", "authors": ["Zidi Xiong", "Yuping Lin", "Wenya Xie", "Pengfei He", "Zirui Liu", "Jiliang Tang", "Himabindu Lakkaraju", "Zhen Xiang"], "year": 2025, "url": "http://arxiv.org/abs/2505.16067v2", "abstract": "Memory is a critical component in large language model (LLM)-based agents, enabling them to store and retrieve past executions to improve task performance over time. In this paper, we conduct an empirical study on how memory management choices impact the LLM agents' behavior, especially their long-term performance. Specifically, we focus on two fundamental memory management operations that are widely used by many agent frameworks-memory addition and deletion-to systematically study their impact on the agent behavior. Through our quantitative analysis, we find that LLM agents display an experience-following property: high similarity between a task input and the input in a retrieved memory record often results in highly similar agent outputs. Our analysis further reveals two significant challenges associated with this property: error propagation, where inaccuracies in past experiences compound and degrade future performance, and misaligned experience replay, where some seemingly correct executions can provide limited or even misleading value as experiences. Through controlled experiments, we demonstrate the importance of regulating experience quality within the memory bank and show that future task evaluations can serve as free quality labels for stored memory. Our findings offer insights into the behavioral dynamics of LLM agent memory systems and provide practical guidance for designing memory components that support robust, long-term agent performance.", "source": "arxiv", "arxiv_id": "2505.16067v2", "pdf_url": "https://arxiv.org/pdf/2505.16067v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-21T22:35:01Z", "updated": "2025-10-10T20:27:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "How to evaluate control measures for LLM agents? A trajectory from today to superintelligence", "authors": ["Tomek Korbak", "Mikita Balesni", "Buck Shlegeris", "Geoffrey Irving"], "year": 2025, "url": "http://arxiv.org/abs/2504.05259v1", "abstract": "As LLM agents grow more capable of causing harm autonomously, AI developers will rely on increasingly sophisticated control measures to prevent possibly misaligned agents from causing harm. AI developers could demonstrate that their control measures are sufficient by running control evaluations: testing exercises in which a red team produces agents that try to subvert control measures. To ensure control evaluations accurately capture misalignment risks, the affordances granted to this red team should be adapted to the capability profiles of the agents to be deployed under control measures.\n  In this paper we propose a systematic framework for adapting affordances of red teams to advancing AI capabilities. Rather than assuming that agents will always execute the best attack strategies known to humans, we demonstrate how knowledge of an agents's actual capability profile can inform proportional control evaluations, resulting in more practical and cost-effective control measures. We illustrate our framework by considering a sequence of five fictional models (M1-M5) with progressively advanced capabilities, defining five distinct AI control levels (ACLs). For each ACL, we provide example rules for control evaluation, control measures, and safety cases that could be appropriate. Finally, we show why constructing a compelling AI control safety case for superintelligent LLM agents will require research breakthroughs, highlighting that we might eventually need alternative approaches to mitigating misalignment risk.", "source": "arxiv", "arxiv_id": "2504.05259v1", "pdf_url": "https://arxiv.org/pdf/2504.05259v1", "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-07T16:52:52Z", "updated": "2025-04-07T16:52:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Human vs. Agent in Task-Oriented Conversations", "authors": ["Zhefan Wang", "Ning Geng", "Zhiqiang Guo", "Weizhi Ma", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2509.17619v2", "abstract": "Task-oriented conversational systems are essential for efficiently addressing diverse user needs, yet their development requires substantial amounts of high-quality conversational data that is challenging and costly to obtain. While large language models (LLMs) have demonstrated potential in generating synthetic conversations, the extent to which these agent-generated interactions can effectively substitute real human conversations remains unclear. This work presents the first systematic comparison between LLM-simulated users and human users in personalized task-oriented conversations. We propose a comprehensive analytical framework encompassing three key aspects (conversation strategy, interaction style, and conversation evaluation) and ten distinct dimensions for evaluating user behaviors, and collect parallel conversational datasets from both human users and LLM agent users across four representative scenarios under identical conditions. Our analysis reveals significant behavioral differences between the two user types in problem-solving approaches, question broadness, user engagement, context dependency, feedback polarity and promise, language style, and hallucination awareness. We found consistency in the agent users and human users across the depth-first or breadth-first dimensions, as well as the usefulness dimensions. These findings provide critical insights for advancing LLM-based user simulation. Our multi-dimensional taxonomy constructed a generalizable framework for analyzing user behavior patterns, offering insights from LLM agent users and human users. By this work, we provide perspectives on rethinking how to use user simulation in conversational systems in the future.", "source": "arxiv", "arxiv_id": "2509.17619v2", "pdf_url": "https://arxiv.org/pdf/2509.17619v2", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-09-22T11:30:39Z", "updated": "2025-11-05T03:47:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions", "authors": ["Gyuyeon Na", "Minjung Park", "Hyeonjeong Cha", "Sangmi Chai"], "year": 2025, "url": "http://arxiv.org/abs/2510.20102v1", "abstract": "We present HCLA, a human-centered multi-agent system for anomaly detection in digital asset transactions. The system links three roles: Parsing, Detection, and Explanation, into a conversational workflow that lets non-experts ask questions in natural language, inspect structured analytics, and obtain context-aware rationales. Implemented with an open-source web UI, HCLA translates user intents into a schema for a classical detector (XGBoost in our prototype) and returns narrative explanations grounded in the underlying features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the baseline detector reaches strong accuracy, while HCLA adds interpretability and interactive refinement. We describe the architecture, interaction loop, dataset, evaluation protocol, and limitations, and discuss how a human-in-the-loop design improves transparency and trust in financial forensics.", "source": "arxiv", "arxiv_id": "2510.20102v1", "pdf_url": "https://arxiv.org/pdf/2510.20102v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-23T01:04:36Z", "updated": "2025-10-23T01:04:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Human-Level Competitive PokÃ©mon via Scalable Offline Reinforcement Learning with Transformers", "authors": ["Jake Grigsby", "Yuqi Xie", "Justin Sasek", "Steven Zheng", "Yuke Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2504.04395v2", "abstract": "Competitive PokÃ©mon Singles (CPS) is a popular strategy game where players learn to exploit their opponent based on imperfect information in battles that can last more than one hundred stochastic turns. AI research in CPS has been led by heuristic tree search and online self-play, but the game may also create a platform to study adaptive policies trained offline on large datasets. We develop a pipeline to reconstruct the first-person perspective of an agent from logs saved from the third-person perspective of a spectator, thereby unlocking a dataset of real human battles spanning more than a decade that grows larger every day. This dataset enables a black-box approach where we train large sequence models to adapt to their opponent based solely on their input trajectory while selecting moves without explicit search of any kind. We study a progression from imitation learning to offline RL and offline fine-tuning on self-play data in the hardcore competitive setting of PokÃ©mon's four oldest (and most partially observed) game generations. The resulting agents outperform a recent LLM Agent approach and a strong heuristic search engine. While playing anonymously in online battles against humans, our best agents climb to rankings inside the top 10% of active players. All agent checkpoints, training details, datasets, and baselines are available at https://metamon.tech.", "source": "arxiv", "arxiv_id": "2504.04395v2", "pdf_url": "https://arxiv.org/pdf/2504.04395v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-06T07:35:15Z", "updated": "2025-07-30T17:33:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HybridRAG-based LLM Agents for Low-Carbon Optimization in Low-Altitude Economy Networks", "authors": ["Jinbo Wen", "Cheng Su", "Jiawen Kang", "Jiangtian Nie", "Yang Zhang", "Jianhang Tang", "Dusit Niyato", "Chau Yuen"], "year": 2025, "url": "http://arxiv.org/abs/2506.15947v1", "abstract": "Low-Altitude Economy Networks (LAENets) are emerging as a promising paradigm to support various low-altitude services through integrated air-ground infrastructure. To satisfy low-latency and high-computation demands, the integration of Unmanned Aerial Vehicles (UAVs) with Mobile Edge Computing (MEC) systems plays a vital role, which offloads computing tasks from terminal devices to nearby UAVs, enabling flexible and resilient service provisions for ground users. To promote the development of LAENets, it is significant to achieve low-carbon multi-UAV-assisted MEC networks. However, several challenges hinder this implementation, including the complexity of multi-dimensional UAV modeling and the difficulty of multi-objective coupled optimization. To this end, this paper proposes a novel Retrieval Augmented Generation (RAG)-based Large Language Model (LLM) agent framework for model formulation. Specifically, we develop HybridRAG by combining KeywordRAG, VectorRAG, and GraphRAG, empowering LLM agents to efficiently retrieve structural information from expert databases and generate more accurate optimization problems compared with traditional RAG-based LLM agents. After customizing carbon emission optimization problems for multi-UAV-assisted MEC networks, we propose a Double Regularization Diffusion-enhanced Soft Actor-Critic (R\\textsuperscript{2}DSAC) algorithm to solve the formulated multi-objective optimization problem. The R\\textsuperscript{2}DSAC algorithm incorporates diffusion entropy regularization and action entropy regularization to improve the performance of the diffusion policy. Furthermore, we dynamically mask unimportant neurons in the actor network to reduce the carbon emissions associated with model training. Simulation results demonstrate the effectiveness and reliability of the proposed HybridRAG-based LLM agent framework and the R\\textsuperscript{2}DSAC algorithm.", "source": "arxiv", "arxiv_id": "2506.15947v1", "pdf_url": "https://arxiv.org/pdf/2506.15947v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-06-19T01:11:35Z", "updated": "2025-06-19T01:11:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents", "authors": ["Shrinidhi Kumbhar", "Venkatesh Mishra", "Kevin Coutinho", "Divij Handa", "Ashif Iquebal", "Chitta Baral"], "year": 2025, "url": "http://arxiv.org/abs/2501.13299v2", "abstract": "Materials discovery and design are essential for advancing technology across various industries by enabling the development of application-specific materials. Recent research has leveraged Large Language Models (LLMs) to accelerate this process. We explore the potential of LLMs to generate viable hypotheses that, once validated, can expedite materials discovery. Collaborating with materials science experts, we curated a novel dataset from recent journal publications, featuring real-world goals, constraints, and methods for designing real-world applications. Using this dataset, we test LLM-based agents that generate hypotheses for achieving given goals under specific constraints. To assess the relevance and quality of these hypotheses, we propose a novel scalable evaluation metric that emulates the process a materials scientist would use to evaluate a hypothesis critically. Our curated dataset, proposed method, and evaluation framework aim to advance future research in accelerating materials discovery and design with LLMs.", "source": "arxiv", "arxiv_id": "2501.13299v2", "pdf_url": "https://arxiv.org/pdf/2501.13299v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-23T01:01:05Z", "updated": "2025-02-08T19:56:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis", "authors": ["Hanyu Li", "Haoyu Liu", "Tingyu Zhu", "Tianyu Guo", "Zeyu Zheng", "Xiaotie Deng", "Michael I. Jordan"], "year": 2025, "url": "http://arxiv.org/abs/2505.18223v2", "abstract": "Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with deeper insights of the dataset. To address this, we introduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round interactive scenarios. Derived from complex Kaggle notebooks, tasks are presented as sequential natural language instructions by an LLM-simulated user. Agent performance is judged by comparing its final numerical output to the human-derived baseline. Initial results show that even state-of-the-art coding agents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting limitations not evident in single-turn tests. This work underscores the need to improve LLMs' multi-round capabilities for building more reliable data analysis agents, highlighting the necessity of achieving a balance between instruction following and reasoning.", "source": "arxiv", "arxiv_id": "2505.18223v2", "pdf_url": "https://arxiv.org/pdf/2505.18223v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T09:37:52Z", "updated": "2025-06-06T06:33:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "IGDA: Interactive Graph Discovery through Large Language Model Agents", "authors": ["Alex Havrilla", "David Alvarez-Melis", "Nicolo Fusi"], "year": 2025, "url": "http://arxiv.org/abs/2502.17189v2", "abstract": "Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable $\\textit{semantic metadata}$ to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose $\\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches.", "source": "arxiv", "arxiv_id": "2502.17189v2", "pdf_url": "https://arxiv.org/pdf/2502.17189v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-24T14:24:27Z", "updated": "2025-04-13T16:26:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Experts", "authors": ["Eric Xue", "Ke Chen", "Zeyi Huang", "Yuyang Ji", "Haohan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.18530v3", "abstract": "Large language model (LLM) agents have emerged as a promising solution to automate the workflow of machine learning, but most existing methods share a common limitation: they attempt to optimize entire pipelines in a single step before evaluation, making it difficult to attribute improvements to specific changes. This lack of granularity leads to unstable optimization and slower convergence, limiting their effectiveness. To address this, we introduce Iterative Refinement, a novel strategy for LLM-driven ML pipeline design inspired by how human ML experts iteratively refine models, focusing on one component at a time rather than making sweeping changes all at once. By systematically updating individual components based on real training feedback, Iterative Refinement improves overall model performance. We also provide some theoretical edvience of the superior properties of this Iterative Refinement. Further, we implement this strategy in IMPROVE, an end-to-end LLM agent framework for automating and optimizing object classification pipelines. Through extensive evaluations across datasets of varying sizes and domains, we demonstrate that Iterative Refinement enables IMPROVE to consistently achieve better performance over existing zero-shot LLM-based approaches.", "source": "arxiv", "arxiv_id": "2502.18530v3", "pdf_url": "https://arxiv.org/pdf/2502.18530v3", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-02-25T01:52:37Z", "updated": "2025-09-16T05:43:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "INTA: Intent-Based Translation for Network Configuration with LLM Agents", "authors": ["Yunze Wei", "Xiaohui Xie", "Tianshuo Hu", "Yiwei Zuo", "Xinyi Chen", "Kaiwen Chi", "Yong Cui"], "year": 2025, "url": "http://arxiv.org/abs/2501.08760v2", "abstract": "Translating configurations between different network devices is a common yet challenging task in modern network operations. This challenge arises in typical scenarios such as replacing obsolete hardware and adapting configurations to emerging paradigms like Software Defined Networking (SDN) and Network Function Virtualization (NFV). Engineers need to thoroughly understand both source and target configuration models, which requires considerable effort due to the complexity and evolving nature of these specifications. To promote automation in network configuration translation, we propose INTA, an intent-based translation framework that leverages Large Language Model (LLM) agents. The key idea of INTA is to use configuration intent as an intermediate representation for translation. It first employs LLMs to decompose configuration files and extract fine-grained intents for each configuration fragment. These intents are then used to retrieve relevant manuals of the target device. Guided by a syntax checker, INTA incrementally generates target configurations. The translated configurations are further verified and refined for semantic consistency. We implement INTA and evaluate it on real-world configuration datasets from the industry. Our approach outperforms state-of-the-art methods in translation accuracy and exhibits strong generalizability. INTA achieves an accuracy of 98.15% in terms of both syntactic and view correctness, and a command recall rate of 84.72% for the target configuration. The semantic consistency report of the translated configuration further demonstrates its practical value in real-world network operations.", "source": "arxiv", "arxiv_id": "2501.08760v2", "pdf_url": "https://arxiv.org/pdf/2501.08760v2", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-01-15T12:25:56Z", "updated": "2025-09-20T13:31:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents", "authors": ["Hengyu An", "Jinghuai Zhang", "Tianyu Du", "Chunyi Zhou", "Qingming Li", "Tao Lin", "Shouling Ji"], "year": 2025, "url": "http://arxiv.org/abs/2508.15310v1", "abstract": "Large language model (LLM) agents are widely deployed in real-world applications, where they leverage tools to retrieve and manipulate external data for complex tasks. However, when interacting with untrusted data sources (e.g., fetching information from public websites), tool responses may contain injected instructions that covertly influence agent behaviors and lead to malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI). Existing defenses typically rely on advanced prompting strategies or auxiliary detection models. While these methods have demonstrated some effectiveness, they fundamentally rely on assumptions about the model's inherent security, which lacks structural constraints on agent behaviors. As a result, agents still retain unrestricted access to tool invocations, leaving them vulnerable to stronger attack vectors that can bypass the security guardrails of the model. To prevent malicious tool invocations at the source, we propose a novel defensive task execution paradigm, called IPIGuard, which models the agents' task execution process as a traversal over a planned Tool Dependency Graph (TDG). By explicitly decoupling action planning from interaction with external data, IPIGuard significantly reduces unintended tool invocations triggered by injected instructions, thereby enhancing robustness against IPI attacks. Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior balance between effectiveness and robustness, paving the way for the development of safer agentic systems in dynamic environments.", "source": "arxiv", "arxiv_id": "2508.15310v1", "pdf_url": "https://arxiv.org/pdf/2508.15310v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-21T07:08:16Z", "updated": "2025-08-21T07:08:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra", "authors": ["Heewoong Noh", "Namkyeong Lee", "Gyoung S. Na", "Kibum Kim", "Chanyoung Park"], "year": 2025, "url": "http://arxiv.org/abs/2508.16112v1", "abstract": "Spectral analysis provides crucial clues for the elucidation of unknown materials. Among various techniques, infrared spectroscopy (IR) plays an important role in laboratory settings due to its high accessibility and low cost. However, existing approaches often fail to reflect expert analytical processes and lack flexibility in incorporating diverse types of chemical knowledge, which is essential in real-world analytical scenarios. In this paper, we propose IR-Agent, a novel multi-agent framework for molecular structure elucidation from IR spectra. The framework is designed to emulate expert-driven IR analysis procedures and is inherently extensible. Each agent specializes in a specific aspect of IR interpretation, and their complementary roles enable integrated reasoning, thereby improving the overall accuracy of structure elucidation. Through extensive experiments, we demonstrate that IR-Agent not only improves baseline performance on experimental IR spectra but also shows strong adaptability to various forms of chemical information.", "source": "arxiv", "arxiv_id": "2508.16112v1", "pdf_url": "https://arxiv.org/pdf/2508.16112v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-22T06:07:28Z", "updated": "2025-08-22T06:07:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Identification and Optimization of Redundant Code Using Large Language Models", "authors": ["Shamse Tasnim Cynthia"], "year": 2025, "url": "http://arxiv.org/abs/2505.04040v1", "abstract": "Redundant code is a persistent challenge in software development that makes systems harder to maintain, scale, and update. It adds unnecessary complexity, hinders bug fixes, and increases technical debt. Despite their impact, removing redundant code manually is risky and error-prone, often introducing new bugs or missing dependencies. While studies highlight the prevalence and negative impact of redundant code, little focus has been given to Artificial Intelligence (AI) system codebases and the common patterns that cause redundancy. Additionally, the reasons behind developers unintentionally introducing redundant code remain largely unexplored. This research addresses these gaps by leveraging large language models (LLMs) to automatically detect and optimize redundant code in AI projects. Our research aims to identify recurring patterns of redundancy and analyze their underlying causes, such as outdated practices or insufficient awareness of best coding principles. Additionally, we plan to propose an LLM agent that will facilitate the detection and refactoring of redundancies on a large scale while preserving original functionality. This work advances the application of AI in identifying and optimizing redundant code, ultimately helping developers maintain cleaner, more readable, and scalable codebases.", "source": "arxiv", "arxiv_id": "2505.04040v1", "pdf_url": "https://arxiv.org/pdf/2505.04040v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-07T00:44:32Z", "updated": "2025-05-07T00:44:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases", "authors": ["Ziqian Zhong", "Aditi Raghunathan", "Nicholas Carlini"], "year": 2025, "url": "http://arxiv.org/abs/2510.20270v1", "abstract": "The tendency to find and exploit \"shortcuts\" to complete tasks poses significant risks for reliable assessment and deployment of large language models (LLMs). For example, an LLM agent with access to unit tests may delete failing tests rather than fix the underlying bug. Such behavior undermines both the validity of benchmark results and the reliability of real-world LLM coding assistant deployments.\n  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench, a benchmark framework that systematically measures LLM agents' propensity to exploit test cases. ImpossibleBench creates \"impossible\" variants of tasks from existing benchmarks like LiveCodeBench and SWE-bench by introducing direct conflicts between the natural-language specification and the unit tests. We measure an agent's \"cheating rate\" as its pass rate on these impossible tasks, where any pass necessarily implies a specification-violating shortcut.\n  As a practical framework, ImpossibleBench is not just an evaluation but a versatile tool. We demonstrate its utility for: (1) studying model behaviors, revealing more fine-grained details of cheating behaviors from simple test modification to complex operator overloading; (2) context engineering, showing how prompt, test access and feedback loop affect cheating rates; and (3) developing monitoring tools, providing a testbed with verified deceptive solutions. We hope ImpossibleBench serves as a useful framework for building more robust and reliable LLM systems.\n  Our implementation can be found at https://github.com/safety-research/impossiblebench.", "source": "arxiv", "arxiv_id": "2510.20270v1", "pdf_url": "https://arxiv.org/pdf/2510.20270v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-23T06:58:32Z", "updated": "2025-10-23T06:58:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Improved LLM Agents for Financial Document Question Answering", "authors": ["Nelvin Tan", "Zian Seng", "Liang Zhang", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "year": 2025, "url": "http://arxiv.org/abs/2506.08726v3", "abstract": "Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent's performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance.", "source": "arxiv", "arxiv_id": "2506.08726v3", "pdf_url": "https://arxiv.org/pdf/2506.08726v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-10T12:22:57Z", "updated": "2026-01-07T07:50:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Improving Interactive Diagnostic Ability of a Large Language Model Agent Through Clinical Experience Learning", "authors": ["Zhoujian Sun", "Ziyi Liu", "Cheng Luo", "Jiebin Chu", "Zhengxing Huang"], "year": 2025, "url": "http://arxiv.org/abs/2503.16463v1", "abstract": "Recent advances in large language models (LLMs) have shown promising results in medical diagnosis, with some studies indicating superior performance compared to human physicians in specific scenarios. However, the diagnostic capabilities of LLMs are often overestimated, as their performance significantly deteriorates in interactive diagnostic settings that require active information gathering. This study investigates the underlying mechanisms behind the performance degradation phenomenon and proposes a solution. We identified that the primary deficiency of LLMs lies in the initial diagnosis phase, particularly in information-gathering efficiency and initial diagnosis formation, rather than in the subsequent differential diagnosis phase. To address this limitation, we developed a plug-and-play method enhanced (PPME) LLM agent, leveraging over 3.5 million electronic medical records from Chinese and American healthcare facilities. Our approach integrates specialized models for initial disease diagnosis and inquiry into the history of the present illness, trained through supervised and reinforcement learning techniques. The experimental results indicate that the PPME LLM achieved over 30% improvement compared to baselines. The final diagnostic accuracy of the PPME LLM in interactive diagnostic scenarios approached levels comparable to those achieved using complete clinical data. These findings suggest a promising potential for developing autonomous diagnostic systems, although further validation studies are needed.", "source": "arxiv", "arxiv_id": "2503.16463v1", "pdf_url": "https://arxiv.org/pdf/2503.16463v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-24T06:24:20Z", "updated": "2025-02-24T06:24:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "year": 2025, "url": "http://arxiv.org/abs/2506.09171v1", "abstract": "Large Language Models (LLMs) are increasingly capable but often require significant guidance or extensive interaction history to perform effectively in complex, interactive environments. Existing methods may struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning. We introduce a novel LLM agent framework that enhances planning capabilities through in-context learning, facilitated by atomic fact augmentation and a recursive lookahead search. Our agent learns to extract task-critical ``atomic facts'' from its interaction trajectories. These facts dynamically augment the prompts provided to LLM-based components responsible for action proposal, latent world model simulation, and state-value estimation. Planning is performed via a depth-limited lookahead search, where the LLM simulates potential trajectories and evaluates their outcomes, guided by the accumulated facts and interaction history. This approach allows the agent to improve its understanding and decision-making online, leveraging its experience to refine its behavior without weight updates. We provide a theoretical motivation linking performance to the quality of fact-based abstraction and LLM simulation accuracy. Empirically, our agent demonstrates improved performance and adaptability on challenging interactive tasks, achieving more optimal behavior as it accumulates experience, showcased in tasks such as TextFrozenLake and ALFWorld.", "source": "arxiv", "arxiv_id": "2506.09171v1", "pdf_url": "https://arxiv.org/pdf/2506.09171v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-10T18:36:31Z", "updated": "2025-06-10T18:36:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges", "authors": ["Lajos Muzsai", "David Imolai", "AndrÃ¡s LukÃ¡cs"], "year": 2025, "url": "http://arxiv.org/abs/2506.02048v2", "abstract": "We present 'Random-Crypto', a procedurally generated cryptographic Capture The Flag (CTF) dataset designed to unlock the potential of Reinforcement Learning (RL) for LLM-based agents in security-sensitive domains. Cryptographic reasoning offers an ideal RL testbed: it combines precise validation, structured multi-step inference, and reliance on reliable computational tool use. Leveraging these properties, we fine-tune a Python tool-augmented Llama-3.1-8B via Group Relative Policy Optimization (GRPO) in a secure execution environment. The resulting agent achieves a significant improvement in Pass@8 on previously unseen challenges. Moreover, the improvements generalize to two external benchmarks: 'picoCTF', spanning both crypto and non-crypto tasks, and 'AICrypto MCQ', a multiple-choice benchmark of 135 cryptography questions. Ablation studies attribute the gains to enhanced tool usage and procedural reasoning. These findings position 'Random-Crypto' as a rich training ground for building intelligent, adaptable LLM agents capable of handling complex cybersecurity tasks.", "source": "arxiv", "arxiv_id": "2506.02048v2", "pdf_url": "https://arxiv.org/pdf/2506.02048v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-01T01:59:52Z", "updated": "2025-08-17T22:28:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates", "authors": ["Hy Dang", "Tianyi Liu", "Zhuofeng Wu", "Jingfeng Yang", "Haoming Jiang", "Tao Yang", "Pei Chen", "Zhengyang Wang", "Helen Wang", "Huasheng Li", "Bing Yin", "Meng Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2509.18076v1", "abstract": "Large language models (LLMs) have demonstrated strong reasoning and tool-use capabilities, yet they often fail in real-world tool-interactions due to incorrect parameterization, poor tool selection, or misinterpretation of user intent. These issues often stem from an incomplete understanding of user goals and inadequate comprehension of tool documentation. While Chain-of-Thought (CoT) prompting has proven effective for enhancing reasoning in general contexts, our analysis reveals that free-form CoT is insufficient and sometimes counterproductive for structured function-calling tasks. To address this, we introduce a curriculum-inspired framework that leverages structured reasoning templates to guide LLMs through more deliberate step-by-step instructions for generating function callings. Experimental results show that our method reduces tool-use errors, achieving 3-12% relative improvements over strong baselines across diverse model series and approaches. Moreover, our framework enhances the robustness, interpretability, and transparency of tool-using agents, advancing the development of more reliable AI assistants for real-world applications.", "source": "arxiv", "arxiv_id": "2509.18076v1", "pdf_url": "https://arxiv.org/pdf/2509.18076v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-22T17:55:14Z", "updated": "2025-09-22T17:55:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction", "authors": ["Yuan-An Xiao", "Pengfei Gao", "Chao Peng", "Yingfei Xiong"], "year": 2025, "url": "http://arxiv.org/abs/2509.23586v1", "abstract": "Multi-turn agent systems based on Large Language Models (LLMs) have been increasingly popular for software engineering tasks. While LLM agents show decent effectiveness, the high computational cost of input tokens due to the ever-growing trajectory remains an efficiency concern for their applications. Efficiency is largely neglected in existing studies and agent products, and this paper fills the gap by introducing an inference-time trajectory reduction approach to reduce the cost of agents.\n  Through analyzing existing agent trajectories, we demonstrate that useless, redundant, and expired information is widespread in all trajectories, which can be identified and reduced without harming the agent's performance. We then design a simple yet effective trajectory reduction approach, AgentDiet, which automatically removes such waste information. We implement AgentDiet on a top-performing coding agent, and the evaluation on two LLMs and two benchmarks shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final computational cost by 21.1% ~ 35.9%, while maintaining the same agent performance. This indicates that trajectory reduction is a promising direction for agent systems.", "source": "arxiv", "arxiv_id": "2509.23586v1", "pdf_url": "https://arxiv.org/pdf/2509.23586v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-28T02:43:41Z", "updated": "2025-09-28T02:43:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs", "authors": ["Vishnu Sarukkai", "Asanshay Gupta", "James Hong", "MichaÃ«l Gharbi", "Kayvon Fatahalian"], "year": 2025, "url": "http://arxiv.org/abs/2512.02543v1", "abstract": "The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\\textbf{2.5$\\times$ lower cost}$, reducing per-episode costs from \\$0.059 to \\$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \\$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\\textbf{2$\\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.", "source": "arxiv", "arxiv_id": "2512.02543v1", "pdf_url": "https://arxiv.org/pdf/2512.02543v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-02T09:11:05Z", "updated": "2025-12-02T09:11:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "InData: Towards Secure Multi-Step, Tool-Based Data Analysis", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "Bhuwan Dhingra", "David Edwin Carlson"], "year": 2025, "url": "http://arxiv.org/abs/2511.11933v1", "abstract": "Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.", "source": "arxiv", "arxiv_id": "2511.11933v1", "pdf_url": "https://arxiv.org/pdf/2511.11933v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-14T23:15:41Z", "updated": "2025-11-14T23:15:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "InSTA: Towards Internet-Scale Training For Agents", "authors": ["Brandon Trabucco", "Gunnar Sigurdsson", "Robinson Piramuthu", "Ruslan Salakhutdinov"], "year": 2025, "url": "http://arxiv.org/abs/2502.06776v2", "abstract": "The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97%, judging successful trajectories with an accuracy of 82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9%, outperforming the data collection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and reaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code, models and data at: https://data-for-agents.github.io.", "source": "arxiv", "arxiv_id": "2502.06776v2", "pdf_url": "https://arxiv.org/pdf/2502.06776v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-10T18:54:05Z", "updated": "2025-05-22T17:59:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "IndoorWorld: Integrating Physical Task Solving and Social Simulation in A Heterogeneous Multi-Agent Environment", "authors": ["Dekun Wu", "Frederik Brudy", "Bang Liu", "Yi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.12331v1", "abstract": "Virtual environments are essential to AI agent research. Existing environments for LLM agent research typically focus on either physical task solving or social simulation, with the former oversimplifying agent individuality and social dynamics, and the latter lacking physical grounding of social behaviors. We introduce IndoorWorld, a heterogeneous multi-agent environment that tightly integrates physical and social dynamics. By introducing novel challenges for LLM-driven agents in orchestrating social dynamics to influence physical environments and anchoring social interactions within world states, IndoorWorld opens up possibilities of LLM-based building occupant simulation for architectural design. We demonstrate the potential with a series of experiments within an office setting to examine the impact of multi-agent collaboration, resource competition, and spatial layout on agent behavior.", "source": "arxiv", "arxiv_id": "2506.12331v1", "pdf_url": "https://arxiv.org/pdf/2506.12331v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-14T03:44:09Z", "updated": "2025-06-14T03:44:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Inducing State Anxiety in LLM Agents Reproduces Human-Like Biases in Consumer Decision-Making", "authors": ["Ziv Ben-Zion", "Zohar Elyoseph", "Tobias Spiller", "Teddy Lazebnik"], "year": 2025, "url": "http://arxiv.org/abs/2510.06222v1", "abstract": "Large language models (LLMs) are rapidly evolving from text generators to autonomous agents, raising urgent questions about their reliability in real-world contexts. Stress and anxiety are well known to bias human decision-making, particularly in consumer choices. Here, we tested whether LLM agents exhibit analogous vulnerabilities. Three advanced models (ChatGPT-5, Gemini 2.5, Claude 3.5-Sonnet) performed a grocery shopping task under budget constraints (24, 54, 108 USD), before and after exposure to anxiety-inducing traumatic narratives. Across 2,250 runs, traumatic prompts consistently reduced the nutritional quality of shopping baskets (Change in Basket Health Scores of -0.081 to -0.126; all pFDR<0.001; Cohens d=-1.07 to -2.05), robust across models and budgets. These results show that psychological context can systematically alter not only what LLMs generate but also the actions they perform. By reproducing human-like emotional biases in consumer behavior, LLM agents reveal a new class of vulnerabilities with implications for digital health, consumer safety, and ethical AI deployment.", "source": "arxiv", "arxiv_id": "2510.06222v1", "pdf_url": "https://arxiv.org/pdf/2510.06222v1", "categories": ["cs.HC", "econ.GN"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-08-30T10:22:39Z", "updated": "2025-08-30T10:22:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes", "authors": ["Johann Frei", "Nils Feldhus", "Lisa Raithel", "Roland Roller", "Alexander Meyer", "Frank Kramer"], "year": 2025, "url": "http://arxiv.org/abs/2507.12261v1", "abstract": "For clinical data integration and healthcare services, the HL7 FHIR standard has established itself as a desirable format for interoperability between complex health data. Previous attempts at automating the translation from free-form clinical notes into structured FHIR resources rely on modular, rule-based systems or LLMs with instruction tuning and constrained decoding. Since they frequently suffer from limited generalizability and structural inconformity, we propose an end-to-end framework powered by LLM agents, code execution, and healthcare terminology database tools to address these issues. Our solution, called Infherno, is designed to adhere to the FHIR document schema and competes well with a human baseline in predicting FHIR resources from unstructured text. The implementation features a front end for custom and synthetic data and both local and proprietary models, supporting clinical data integration processes and interoperability across institutions.", "source": "arxiv", "arxiv_id": "2507.12261v1", "pdf_url": "https://arxiv.org/pdf/2507.12261v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-16T14:06:51Z", "updated": "2025-07-16T14:06:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios", "authors": ["Chenglin Yu", "Yang Yu", "Songmiao Wang", "Yucheng Wang", "Yifan Yang", "Jinjia Li", "Ming Li", "Hongxia Yang"], "year": 2025, "url": "http://arxiv.org/abs/2509.22502v2", "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios. However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise. These hand-crafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries. To address these challenges, we propose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to \\textbf{infi}nite scenarios, which introduces several key innovations: a generalized \"agent-as-a-tool\" mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient task-agent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities. Furthermore, InfiAgent's atomic task design supports agent parallelism, significantly improving execution efficiency. This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems. Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.", "source": "arxiv", "arxiv_id": "2509.22502v2", "pdf_url": "https://arxiv.org/pdf/2509.22502v2", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-26T15:44:09Z", "updated": "2025-09-30T10:55:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "InfoAgent: Advancing Autonomous Information-Seeking Agents", "authors": ["Gongrui Zhang", "Jialiang Zhu", "Ruiqi Yang", "Kai Qiu", "Miaosen Zhang", "Zhirong Wu", "Qi Dai", "Bei Liu", "Chong Luo", "Zhengyuan Yang", "Linjie Li", "Lijuan Wang", "Weizhu Chen", "Yuan Zhang", "Xin Li", "Zhaoyi Liu", "Xin Geng", "Baining Guo"], "year": 2025, "url": "http://arxiv.org/abs/2509.25189v1", "abstract": "Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries,we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our \\mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy on BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.", "source": "arxiv", "arxiv_id": "2509.25189v1", "pdf_url": "https://arxiv.org/pdf/2509.25189v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-29T17:59:57Z", "updated": "2025-09-29T17:59:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation", "authors": ["Yunjia Xi", "Jianghao Lin", "Menghui Zhu", "Yongzhao Xiao", "Zhuoying Ou", "Jiaqi Liu", "Tong Wan", "Bo Chen", "Weiwen Liu", "Yasheng Wang", "Ruiming Tang", "Weinan Zhang", "Yong Yu"], "year": 2025, "url": "http://arxiv.org/abs/2505.15872v2", "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by grounding responses with retrieved information. As an emerging paradigm, Agentic RAG further enhances this process by introducing autonomous LLM agents into the information seeking process. However, existing benchmarks fall short in evaluating such systems, as they are confined to a static retrieval environment with a fixed, limited corpus} and simple queries that fail to elicit agentic behavior. Moreover, their evaluation protocols assess information seeking effectiveness by pre-defined gold sets of documents, making them unsuitable for the open-ended and dynamic nature of real-world web environments. To bridge this gap, we present InfoDeepSeek, a new benchmark with challenging questions designed for assessing agentic information seeking in real-world, dynamic web environments. We propose a systematic methodology for constructing challenging queries satisfying the criteria of determinacy, difficulty, and diversity. Based on this, we develop the first evaluation framework tailored to dynamic agentic information seeking, including fine-grained metrics about the accuracy, utility, and compactness of information seeking outcomes. Through extensive experiments across LLMs, search engines, and question types, InfoDeepSeek reveals nuanced agent behaviors and offers actionable insights for future research.", "source": "arxiv", "arxiv_id": "2505.15872v2", "pdf_url": "https://arxiv.org/pdf/2505.15872v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-05-21T14:44:40Z", "updated": "2025-05-23T10:16:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents", "authors": ["Yaxin Du", "Yuanshuo Zhang", "Xiyuan Yang", "Yifan Zhou", "Cheng Wang", "Gongyi Zou", "Xianghe Pang", "Wenhao Wang", "Menglan Chen", "Shuo Tang", "Zhiyu Li", "Feiyu Xiong", "Siheng Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.02271v2", "abstract": "Information seeking is a fundamental requirement for humans. However, existing LLM agents rely heavily on open-web search, which exposes two fundamental weaknesses: online content is noisy and unreliable, and many real-world tasks require precise, domain-specific knowledge unavailable from the web. The emergence of the Model Context Protocol (MCP) now allows agents to interface with thousands of specialized tools, seemingly resolving this limitation. Yet it remains unclear whether agents can effectively leverage such tools -- and more importantly, whether they can integrate them with general-purpose search to solve complex tasks. Therefore, we introduce InfoMosaic-Bench, the first benchmark dedicated to multi-source information seeking in tool-augmented agents. Covering six representative domains (medicine, finance, maps, video, web, and multi-domain integration), InfoMosaic-Bench requires agents to combine general-purpose search with domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable pipeline that grounds task conditions in verified tool outputs, enforces cross-source dependencies, and filters out shortcut cases solvable by trivial lookup. This design guarantees both reliability and non-triviality. Experiments with 14 state-of-the-art LLM agents reveal three findings: (i) web information alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass rate; (ii) domain tools provide selective but inconsistent benefits, improving some domains while degrading others; and (iii) 22.4% of failures arise from incorrect tool usage or selection, highlighting that current LLMs still struggle with even basic tool handling.", "source": "arxiv", "arxiv_id": "2510.02271v2", "pdf_url": "https://arxiv.org/pdf/2510.02271v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-02T17:48:03Z", "updated": "2025-10-04T09:18:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents", "authors": ["Guoqing Wang", "Sunhao Dai", "Guangze Ye", "Zeyu Gan", "Wei Yao", "Yong Deng", "Xiaofeng Wu", "Zhenzhe Ying"], "year": 2025, "url": "http://arxiv.org/abs/2510.14967v1", "abstract": "Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.", "source": "arxiv", "arxiv_id": "2510.14967v1", "pdf_url": "https://arxiv.org/pdf/2510.14967v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-16T17:59:32Z", "updated": "2025-10-16T17:59:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Infusing Theory of Mind into Socially Intelligent LLM Agents", "authors": ["EunJeong Hwang", "Yuwei Yin", "Giuseppe Carenini", "Peter West", "Vered Shwartz"], "year": 2025, "url": "http://arxiv.org/abs/2509.22887v1", "abstract": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.", "source": "arxiv", "arxiv_id": "2509.22887v1", "pdf_url": "https://arxiv.org/pdf/2509.22887v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-26T20:07:34Z", "updated": "2025-09-26T20:07:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective", "authors": ["Garry A. Gabison", "R. Patrick Xian"], "year": 2025, "url": "http://arxiv.org/abs/2504.03255v2", "abstract": "Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention to effective governance policies, monitoring, and control protocols. Based on the emerging landscape of the agentic market, we analyze potential liability issues arising from the delegated use of LLM agents and their extended systems through a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing, and tracing to enhance transparency and liability attribution.", "source": "arxiv", "arxiv_id": "2504.03255v2", "pdf_url": "https://arxiv.org/pdf/2504.03255v2", "categories": ["cs.CY", "cs.CL", "cs.MA"], "primary_category": "cs.CY", "doi": "10.18653/v1/2025.realm-1.9", "venue": "", "published": "2025-04-04T08:10:02Z", "updated": "2025-06-17T13:42:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "year": 2025, "url": "http://arxiv.org/abs/2508.19611v2", "abstract": "Preparing high-quality instructional materials remains a labor-intensive process that often requires extensive coordination among teaching faculty, instructional designers, and teaching assistants. In this work, we present Instructional Agents, a multi-agent large language model (LLM) framework designed to automate end-to-end course material generation, including syllabus creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing AI-assisted educational tools that focus on isolated tasks, Instructional Agents simulates role-based collaboration among educational agents to produce cohesive and pedagogically aligned content. The system operates in four modes: Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling flexible control over the degree of human involvement. We evaluate Instructional Agents across five university-level computer science courses and show that it produces high-quality instructional materials while significantly reducing development time and human workload. By supporting institutions with limited instructional design capacity, Instructional Agents provides a scalable and cost-effective framework to democratize access to high-quality education, particularly in underserved or resource-constrained settings.", "source": "arxiv", "arxiv_id": "2508.19611v2", "pdf_url": "https://arxiv.org/pdf/2508.19611v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-27T06:45:06Z", "updated": "2025-09-01T01:38:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers", "authors": ["Ahmed A. Metwally", "A. Ali Heydari", "Daniel McDuff", "Alexandru Solot", "Zeinab Esmaeilpour", "Anthony Z Faranesh", "Menglian Zhou", "David B. Savage", "Conor Heneghan", "Shwetak Patel", "Cathy Speed", "Javier L. Prieto"], "year": 2025, "url": "http://arxiv.org/abs/2505.03784v1", "abstract": "Insulin resistance, a precursor to type 2 diabetes, is characterized by impaired insulin action in tissues. Current methods for measuring insulin resistance, while effective, are expensive, inaccessible, not widely available and hinder opportunities for early intervention. In this study, we remotely recruited the largest dataset to date across the US to study insulin resistance (N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%), incorporating wearable device time series data and blood biomarkers, including the ground-truth measure of insulin resistance, homeostatic model assessment for insulin resistance (HOMA-IR). We developed deep neural network models to predict insulin resistance based on readily available digital and blood biomarkers. Our results show that our models can predict insulin resistance by combining both wearable data and readily available blood biomarkers better than either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%, and specificity 84%). The model showed 93% sensitivity and 95% adjusted specificity in obese and sedentary participants, a subpopulation most vulnerable to developing type 2 diabetes and who could benefit most from early intervention. Rigorous evaluation of model performance, including interpretability, and robustness, facilitates generalizability across larger cohorts, which is demonstrated by reproducing the prediction performance on an independent validation cohort (N=72 participants). Additionally, we demonstrated how the predicted insulin resistance can be integrated into a large language model agent to help understand and contextualize HOMA-IR values, facilitating interpretation and safe personalized recommendations. This work offers the potential for early detection of people at risk of type 2 diabetes and thereby facilitate earlier implementation of preventative strategies.", "source": "arxiv", "arxiv_id": "2505.03784v1", "pdf_url": "https://arxiv.org/pdf/2505.03784v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-30T16:10:20Z", "updated": "2025-04-30T16:10:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Insured Agents: A Decentralized Trust Insurance Mechanism for Agentic Economy", "authors": ["Botao 'Amber' Hu", "Bangdao Chen"], "year": 2025, "url": "http://arxiv.org/abs/2512.08737v1", "abstract": "The emerging \"agentic web\" envisions large populations of autonomous agents coordinating, transacting, and delegating across open networks. Yet many agent communication and commerce protocols treat agents as low-cost identities, despite the empirical reality that LLM agents remain unreliable, hallucinated, manipulable, and vulnerable to prompt-injection and tool-abuse. A natural response is \"agents-at-stake\": binding economically meaningful, slashable collateral to persistent identities and adjudicating misbehavior with verifiable evidence. However, heterogeneous tasks make universal verification brittle and centralization-prone, while traditional reputation struggles under rapid model drift and opaque internal states. We propose a protocol-native alternative: insured agents. Specialized insurer agents post stake on behalf of operational agents in exchange for premiums, and receive privileged, privacy-preserving audit access via TEEs to assess claims. A hierarchical insurer market calibrates stake through pricing, decentralizes verification via competitive underwriting, and yields incentive-compatible dispute resolution.", "source": "arxiv", "arxiv_id": "2512.08737v1", "pdf_url": "https://arxiv.org/pdf/2512.08737v1", "categories": ["cs.CY", "cs.MA"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-12-09T15:47:16Z", "updated": "2025-12-09T15:47:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration", "authors": ["Himabindu Thogaru", "Saisubramaniam Gopalakrishnan", "Zishan Ahmad", "Anirudh Deodhar"], "year": 2025, "url": "http://arxiv.org/abs/2512.18265v1", "abstract": "Manufacturing planners face complex operational challenges that require seamless collaboration between human expertise and intelligent systems to achieve optimal performance in modern production environments. Traditional approaches to analyzing simulation-based manufacturing data often create barriers between human decision-makers and critical operational insights, limiting effective partnership in manufacturing planning. Our framework establishes a collaborative intelligence system integrating Knowledge Graphs and Large Language Model-based agents to bridge this gap, empowering manufacturing professionals through natural language interfaces for complex operational analysis. The system transforms simulation data into semantically rich representations, enabling planners to interact naturally with operational insights without specialized expertise. A collaborative LLM agent works alongside human decision-makers, employing iterative reasoning that mirrors human analytical thinking while generating precise queries for knowledge extraction and providing transparent validation. This partnership approach to manufacturing bottleneck identification, validated through operational scenarios, demonstrates enhanced performance while maintaining human oversight and decision authority. For operational inquiries, the system achieves near-perfect accuracy through natural language interaction. For investigative scenarios requiring collaborative analysis, we demonstrate the framework's effectiveness in supporting human experts to uncover interconnected operational issues that enhance understanding and decision-making. This work advances collaborative manufacturing by creating intuitive methods for actionable insights, reducing cognitive load while amplifying human analytical capabilities in evolving manufacturing ecosystems.", "source": "arxiv", "arxiv_id": "2512.18265v1", "pdf_url": "https://arxiv.org/pdf/2512.18265v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-20T08:09:24Z", "updated": "2025-12-20T08:09:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box Retrieval", "authors": ["Yulong Hui", "Chao Chen", "Zhihang Fu", "Yihao Liu", "Jieping Ye", "Huanchen Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2510.27566v1", "abstract": "Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by incorporating external information. However, prevailing agentic RAG approaches are constrained by a critical limitation: they treat the retrieval process as a black-box querying operation. This confines agents' actions to query issuing, hindering its ability to tackle complex information-seeking tasks. To address this, we introduce Interact-RAG, a new paradigm that elevates the LLM agent from a passive query issuer into an active manipulator of the retrieval process. We dismantle the black-box with a Corpus Interaction Engine, equipping the agent with a set of action primitives for fine-grained control over information retrieval. To further empower the agent on the entire RAG pipeline, we first develop a reasoning-enhanced workflow, which enables both zero-shot execution and the synthesis of interaction trajectories. We then leverage this synthetic data to train a fully autonomous end-to-end agent via Supervised Fine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL). Extensive experiments across six benchmarks demonstrate that Interact-RAG significantly outperforms other advanced methods, validating the efficacy of our reasoning-interaction strategy.", "source": "arxiv", "arxiv_id": "2510.27566v1", "pdf_url": "https://arxiv.org/pdf/2510.27566v1", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-10-31T15:48:43Z", "updated": "2025-10-31T15:48:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Interactive Agents to Overcome Ambiguity in Software Engineering", "authors": ["Sanidhya Vijayvargiya", "Xuhui Zhou", "Akhila Yerukola", "Maarten Sap", "Graham Neubig"], "year": 2025, "url": "http://arxiv.org/abs/2502.13069v1", "abstract": "AI agents are increasingly being deployed to automate tasks, often based on ambiguous and underspecified user instructions. Making unwarranted assumptions and failing to ask clarifying questions can lead to suboptimal outcomes, safety risks due to tool misuse, and wasted computational resources. In this work, we study the ability of LLM agents to handle ambiguous instructions in interactive code generation settings by evaluating proprietary and open-weight models on their performance across three key steps: (a) leveraging interactivity to improve performance in ambiguous scenarios, (b) detecting ambiguity, and (c) asking targeted questions. Our findings reveal that models struggle to distinguish between well-specified and underspecified instructions. However, when models interact for underspecified inputs, they effectively obtain vital information from the user, leading to significant improvements in performance and underscoring the value of effective interaction. Our study highlights critical gaps in how current state-of-the-art models handle ambiguity in complex software engineering tasks and structures the evaluation into distinct steps to enable targeted improvements.", "source": "arxiv", "arxiv_id": "2502.13069v1", "pdf_url": "https://arxiv.org/pdf/2502.13069v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-18T17:12:26Z", "updated": "2025-02-18T17:12:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Interactive Data Harmonization with LLM Agents: Opportunities and Challenges", "authors": ["AÃ©cio Santos", "Eduardo H. M. Pena", "Roque Lopez", "Juliana Freire"], "year": 2025, "url": "http://arxiv.org/abs/2502.07132v3", "abstract": "Data harmonization is an essential task that entails integrating datasets from diverse sources. Despite years of research in this area, it remains a time-consuming and challenging task due to schema mismatches, varying terminologies, and differences in data collection methodologies. This paper presents the case for agentic data harmonization as a means to both empower experts to harmonize their data and to streamline the process. We introduce Harmonia, a system that combines LLM-based reasoning, an interactive user interface, and a library of data harmonization primitives to automate the synthesis of data harmonization pipelines. We demonstrate Harmonia in a clinical data harmonization scenario, where it helps to interactively create reusable pipelines that map datasets to a standard format. Finally, we discuss challenges and open problems, and suggest research directions for advancing our vision.", "source": "arxiv", "arxiv_id": "2502.07132v3", "pdf_url": "https://arxiv.org/pdf/2502.07132v3", "categories": ["cs.AI", "cs.DB"], "primary_category": "cs.AI", "doi": "10.1145/3735079.3735324", "venue": "In Novel Optimizations for Visionary AI Systems (NOVAS '25), June 22-27, 2025. Berlin, Germany", "published": "2025-02-10T23:50:09Z", "updated": "2025-08-07T00:04:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Interactive Graph Visualization and TeamingRecommendation in an Interdisciplinary Project'sTalent Knowledge Graph", "authors": ["Jiawei Xu", "Juichien Chen", "Yilin Ye", "Zhandos Sembay", "Swathi Thaker", "Pamela Payne-Foster", "Jake Chen", "Ying Ding"], "year": 2025, "url": "http://arxiv.org/abs/2508.19489v1", "abstract": "Interactive visualization of large scholarly knowledge graphs combined with LLM reasoning shows promise butremains under-explored. We address this gap by developing an interactive visualization system for the Cell Map forAI Talent Knowledge Graph (28,000 experts and 1,179 biomedical datasets). Our approach integrates WebGLvisualization with LLM agents to overcome limitations of traditional tools such as Gephi, particularly for large-scaleinteractive node handling. Key functionalities include responsive exploration, filtering, and AI-drivenrecommendations with justifications. This integration can potentially enable users to effectively identify potentialcollaborators and relevant dataset users within biomedical and AI research communities. The system contributes anovel framework that enhances knowledge graph exploration through intuitive visualization and transparent, LLM-guided recommendations. This adaptable solution extends beyond the CM4AI community to other large knowledgegraphs, improving information representation and decision-making. Demo: https://cm4aikg.vercel.app/", "source": "arxiv", "arxiv_id": "2508.19489v1", "pdf_url": "https://arxiv.org/pdf/2508.19489v1", "categories": ["cs.DL"], "primary_category": "cs.DL", "doi": "", "venue": "", "published": "2025-08-27T00:25:22Z", "updated": "2025-08-27T00:25:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning", "authors": ["Ehsan Ahmadi", "Chao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.15263v1", "abstract": "Construction tasks are inherently unpredictable, with dynamic environments and safety-critical demands posing significant risks to workers. Exoskeletons offer potential assistance but falter without accurate intent recognition across diverse locomotion modes. This paper presents a locomotion prediction agent leveraging Large Language Models (LLMs) augmented with memory systems, aimed at improving exoskeleton assistance in such settings. Using multimodal inputs - spoken commands and visual data from smart glasses - the agent integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory (LTM), and Refinement Module to predict locomotion modes effectively. Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague and safety-critical commands. Calibration metrics, including a Brier Score drop from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability. This framework supports safer, high-level human-exoskeleton collaboration, with promise for adaptive assistive systems in dynamic industries.", "source": "arxiv", "arxiv_id": "2504.15263v1", "pdf_url": "https://arxiv.org/pdf/2504.15263v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-04-21T17:45:21Z", "updated": "2025-04-21T17:45:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Interpretable Risk Mitigation in LLM Agent Systems", "authors": ["Jan Chojnacki"], "year": 2025, "url": "http://arxiv.org/abs/2505.10670v1", "abstract": "Autonomous agents powered by large language models (LLMs) enable novel use cases in domains where responsible action is increasingly important. Yet the inherent unpredictability of LLMs raises safety concerns about agent reliability. In this work, we explore agent behaviour in a toy, game-theoretic environment based on a variation of the Iterated Prisoner's Dilemma. We introduce a strategy-modification method-independent of both the game and the prompt-by steering the residual stream with interpretable features extracted from a sparse autoencoder latent space. Steering with the good-faith negotiation feature lowers the average defection probability by 28 percentage points. We also identify feasible steering ranges for several open-source LLM agents. Finally, we hypothesise that game-theoretic evaluation of LLM agents, combined with representation-steering alignment, can generalise to real-world applications on end-user devices and embodied platforms.", "source": "arxiv", "arxiv_id": "2505.10670v1", "pdf_url": "https://arxiv.org/pdf/2505.10670v1", "categories": ["cs.AI", "cs.CY", "cs.GT"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-15T19:22:11Z", "updated": "2025-05-15T19:22:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Interpreting the Interpreter: Can We Model post-ECB Conferences Volatility with LLM Agents?", "authors": ["Umberto Collodel"], "year": 2025, "url": "http://arxiv.org/abs/2508.13635v3", "abstract": "Traditional high-frequency identification of monetary policy communication effects operates ex-post, precluding evaluation of alternative strategies before publication. In this paper, we introduce an ex-ante framework that simulates heterogeneous market reactions to central bank communication before release. Our methodology employs Large Language Models (LLMs) to construct an agent-based simulation of 30 synthetic traders with heterogeneous risk preferences, cognitive biases, and interpretive styles. These agents process European Central Bank (ECB) press conference transcripts and forecast Euro interest rate swap rates across 3-month, 2-year, and 10-year maturities. Cross-sectional forecast dispersion provides a model-based measure of market disagreement, validated against realized overnight index swap (OIS) volatility. Analyzing 283 ECB press conferences (June 1998-April 2025), we document Spearman correlations of approximately 0.5 between simulated and realized disagreement, rising to 0.6 under iterative prompt optimization. Results prove robust across prompting strategies, are temporally stable across training and holdout samples, and fare significantly better than simple language complexity scores. For central banks, the framework provides an operational tool to anticipate communication-induced volatility before release, thus enabling ex-ante language refinement. For researchers, it offers a micro-founded alternative to reduced-form event studies, explicitly modeling the heterogeneous interpretive processes through which policy signals are transmitted to asset prices.", "source": "arxiv", "arxiv_id": "2508.13635v3", "pdf_url": "https://arxiv.org/pdf/2508.13635v3", "categories": ["econ.GN"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2025-08-19T08:48:05Z", "updated": "2025-10-14T10:14:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior", "authors": ["Huisheng Wang", "Zhuoshi Pan", "Hangjing Zhang", "Mingxiao Liu", "Hanqing Gao", "H. Vicky Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2507.06528v1", "abstract": "Aligning Large Language Models (LLMs) with investor decision-making processes under herd behavior is a critical challenge in behavioral finance, which grapples with a fundamental limitation: the scarcity of real-user data needed for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM outputs and human behavioral patterns, its reliance on massive authentic data imposes substantial collection costs and privacy risks. We propose InvestAlign, a novel framework that constructs high-quality SFT datasets by leveraging theoretical solutions to similar and simple optimal investment problems rather than complex scenarios. Our theoretical analysis demonstrates that training LLMs with InvestAlign-generated data achieves faster parameter convergence than using real-user data, suggesting superior learning efficiency. Furthermore, we develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which demonstrates significantly closer alignment to real-user data than pre-SFT models in both simple and complex investment problems. This highlights our proposed InvestAlign as a promising approach with the potential to address complex optimal investment problems and align LLMs with investor decision-making processes under herd behavior. Our code is publicly available at https://github.com/thu-social-network-research-group/InvestAlign.", "source": "arxiv", "arxiv_id": "2507.06528v1", "pdf_url": "https://arxiv.org/pdf/2507.06528v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-09T04:07:22Z", "updated": "2025-07-09T04:07:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Investigating Pedagogical Teacher and Student LLM Agents: Genetic Adaptation Meets Retrieval Augmented Generation Across Learning Style", "authors": ["Debdeep Sanyal", "Agniva Maiti", "Umakanta Maharana", "Dhruv Kumar", "Ankur Mali", "C. Lee Giles", "Murari Mandal"], "year": 2025, "url": "http://arxiv.org/abs/2505.19173v1", "abstract": "Effective teaching requires adapting instructional strategies to accommodate the diverse cognitive and behavioral profiles of students, a persistent challenge in education and teacher training. While Large Language Models (LLMs) offer promise as tools to simulate such complex pedagogical environments, current simulation frameworks are limited in two key respects: (1) they often reduce students to static knowledge profiles, and (2) they lack adaptive mechanisms for modeling teachers who evolve their strategies in response to student feedback. To address these gaps, \\textbf{we introduce a novel simulation framework that integrates LLM-based heterogeneous student agents with a self-optimizing teacher agent}. The teacher agent's pedagogical policy is dynamically evolved using a genetic algorithm, allowing it to discover and refine effective teaching strategies based on the aggregate performance of diverse learners. In addition, \\textbf{we propose Persona-RAG}, a Retrieval Augmented Generation module that enables student agents to retrieve knowledge tailored to their individual learning styles. Persona-RAG preserves the retrieval accuracy of standard RAG baselines while enhancing personalization, an essential factor in modeling realistic educational scenarios. Through extensive experiments, we demonstrate how our framework supports the emergence of distinct and interpretable teaching patterns when interacting with varied student populations. Our results highlight the potential of LLM-driven simulations to inform adaptive teaching practices and provide a testbed for training human educators in controlled, data-driven environments.", "source": "arxiv", "arxiv_id": "2505.19173v1", "pdf_url": "https://arxiv.org/pdf/2505.19173v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-25T14:45:35Z", "updated": "2025-05-25T14:45:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Investigating Prosocial Behavior Theory in LLM Agents under Policy-Induced Inequities", "authors": ["Yujia Zhou", "Hexi Wang", "Qingyao Ai", "Zhen Wu", "Yiqun Liu"], "year": 2025, "url": "http://arxiv.org/abs/2505.15857v2", "abstract": "As large language models (LLMs) increasingly operate as autonomous agents in social contexts, evaluating their capacity for prosocial behavior is both theoretically and practically critical. However, existing research has primarily relied on static, economically framed paradigms, lacking models that capture the dynamic evolution of prosociality and its sensitivity to structural inequities. To address these gaps, we introduce ProSim, a simulation framework for modeling the prosocial behavior in LLM agents across diverse social conditions. We conduct three progressive studies to assess prosocial alignment. First, we demonstrate that LLM agents can exhibit human-like prosocial behavior across a broad range of real-world scenarios and adapt to normative policy interventions. Second, we find that agents engage in fairness-based third-party punishment and respond systematically to variations in inequity magnitude and enforcement cost. Third, we show that policy-induced inequities suppress prosocial behavior, propagate norm erosion through social networks. These findings advance prosocial behavior theory by elucidating how institutional dynamics shape the emergence, decay, and diffusion of prosocial norms in agent-driven societies.", "source": "arxiv", "arxiv_id": "2505.15857v2", "pdf_url": "https://arxiv.org/pdf/2505.15857v2", "categories": ["cs.SI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-05-21T01:09:37Z", "updated": "2025-11-10T01:08:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant", "authors": ["Gaole He", "Nilay Aishwarya", "Ujwal Gadiraju"], "year": 2025, "url": "http://arxiv.org/abs/2501.17546v1", "abstract": "Explainable artificial intelligence (XAI) methods are being proposed to help interpret and understand how AI systems reach specific predictions. Inspired by prior work on conversational user interfaces, we argue that augmenting existing XAI methods with conversational user interfaces can increase user engagement and boost user understanding of the AI system. In this paper, we explored the impact of a conversational XAI interface on users' understanding of the AI system, their trust, and reliance on the AI system. In comparison to an XAI dashboard, we found that the conversational XAI interface can bring about a better understanding of the AI system among users and higher user trust. However, users of both the XAI dashboard and conversational XAI interfaces showed clear overreliance on the AI system. Enhanced conversations powered by large language model (LLM) agents amplified over-reliance. Based on our findings, we reason that the potential cause of such overreliance is the illusion of explanatory depth that is concomitant with both XAI interfaces. Our findings have important implications for designing effective conversational XAI interfaces to facilitate appropriate reliance and improve human-AI collaboration. Code can be found at https://github.com/delftcrowd/IUI2025_ConvXAI", "source": "arxiv", "arxiv_id": "2501.17546v1", "pdf_url": "https://arxiv.org/pdf/2501.17546v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "10.1145/3708359.3712133", "venue": "", "published": "2025-01-29T10:29:27Z", "updated": "2025-01-29T10:29:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models", "authors": ["Yeonjun In", "Wonjoong Kim", "Kanghoon Yoon", "Sungchul Kim", "Mehrab Tanjim", "Sangwu Park", "Kibum Kim", "Chanyoung Park"], "year": 2025, "url": "http://arxiv.org/abs/2502.15086v2", "abstract": "As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general standards, overlooking user-specific standards. However, safety standards for LLM may vary based on a user-specific profiles rather than being universally consistent across all users. This raises a critical research question: Do LLM agents act safely when considering user-specific safety standards? Despite its importance for safe LLM use, no benchmark datasets currently exist to evaluate the user-specific safety of LLMs. To address this gap, we introduce U-SafeBench, a benchmark designed to assess user-specific aspect of LLM safety. Our evaluation of 20 widely used LLMs reveals current LLMs fail to act safely when considering user-specific safety standards, marking a new discovery in this field. To address this vulnerability, we propose a simple remedy based on chain-of-thought, demonstrating its effectiveness in improving user-specific safety. Our benchmark and code are available at https://github.com/yeonjun-in/U-SafeBench.", "source": "arxiv", "arxiv_id": "2502.15086v2", "pdf_url": "https://arxiv.org/pdf/2502.15086v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-20T22:58:44Z", "updated": "2025-10-23T00:23:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks", "authors": ["Songwen Zhao", "Danqing Wang", "Kexun Zhang", "Jiaxuan Luo", "Zhuo Li", "Lei Li"], "year": 2025, "url": "http://arxiv.org/abs/2512.03262v1", "abstract": "Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.", "source": "arxiv", "arxiv_id": "2512.03262v1", "pdf_url": "https://arxiv.org/pdf/2512.03262v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-02T22:11:56Z", "updated": "2025-12-02T22:11:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "JAUNT: Joint Alignment of User Intent and Network State for QoE-centric LLM Tool Routing", "authors": ["Enhan Li", "Hongyang Du"], "year": 2025, "url": "http://arxiv.org/abs/2510.18550v1", "abstract": "Large Language Models (LLMs) increasingly rely on emerging protocols such as the Model Context Protocol (MCP) to invoke external tools and services. However, current tool routing mechanisms remain fragile because they only consider functional matching between users' queries and tools. In practice, user intent expressed through queries can be vague or underspecified, and the actual Quality of Experience (QoE) also depends on external factors such as link latency and server availability that are not captured by semantics alone. To address this challenge, we propose JAUNT, a framework for Joint Alignment of User intent and Network state in QoE-centric Tool routing. JAUNT introduces a dual-view alignment strategy that interprets user intent while employing LLM agents to construct network profiles, mapping numerical performance indicators into the semantic space to guide routing. We further design a benchmark that integrates diverse user request patterns with heterogeneous network states, enabling systematic evaluation of QoE outcomes. Experimental results show that JAUNT significantly improves QoE compared with several baselines, demonstrating the importance of aligning both intent and network state for scalable LLM service orchestration.", "source": "arxiv", "arxiv_id": "2510.18550v1", "pdf_url": "https://arxiv.org/pdf/2510.18550v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-10-21T11:58:36Z", "updated": "2025-10-21T11:58:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Journalism-Guided Agentic In-Context Learning for News Stance Detection", "authors": ["Dahyun Lee", "Jonghyeon Choi", "Jiyoung Han", "Kunwoo Park"], "year": 2025, "url": "http://arxiv.org/abs/2507.11049v3", "abstract": "As online news consumption grows, personalized recommendation systems have become integral to digital journalism. However, these systems risk reinforcing filter bubbles and political polarization by failing to incorporate diverse perspectives. Stance detection -- identifying a text's position on a target -- can help mitigate this by enabling viewpoint-aware recommendations and data-driven analyses of media bias. Yet, existing stance detection research remains largely limited to short texts and high-resource languages. To address these gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for article-level stance detection, comprising 2,000 news articles with article-level and 21,650 segment-level stance annotations across 47 societal issues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided \\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that employs a language model agent to predict the stances of key structural segments (e.g., leads, quotations), which are then aggregated to infer the overall article stance. Experiments showed that \\textsc{JoA-ICL} outperforms existing stance detection methods, highlighting the benefits of segment-level agency in capturing the overall position of long-form news articles. Two case studies further demonstrate its broader utility in promoting viewpoint diversity in news recommendations and uncovering patterns of media bias.", "source": "arxiv", "arxiv_id": "2507.11049v3", "pdf_url": "https://arxiv.org/pdf/2507.11049v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-15T07:22:04Z", "updated": "2025-09-21T05:47:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning", "authors": ["Ai Han", "Junxing Hu", "Pu Wei", "Zhiqian Zhang", "Yuhang Guo", "Jiawei Lu", "Zicheng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.19846v1", "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm for increasingly complex tasks. However, joint evolution across heterogeneous agents remains challenging due to cooperative inefficiency and training instability. In this paper, we propose the joint evolution dynamics for MARL called JoyAgents-R1, which first applies Group Relative Policy Optimization (GRPO) to the joint training of heterogeneous multi-agents. By iteratively refining agents' large language models (LLMs) and memories, the method achieves holistic equilibrium with optimal decision-making and memory capabilities. Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on the behavior of each agent across entire reasoning trajectories to enhance GRPO sampling efficiency while maintaining policy diversity. Then, our marginal benefit-driven selection strategy identifies top-$K$ sampling groups with maximal reward fluctuations, enabling targeted agent model updates that improve training stability and maximize joint benefits through cost-effective parameter adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution mechanism that repurposes GRPO rewards as cost-free supervisory signals to eliminate repetitive reasoning and accelerate convergence. Experiments across general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves performance comparable to that of larger LLMs while built on smaller open-source models.", "source": "arxiv", "arxiv_id": "2506.19846v1", "pdf_url": "https://arxiv.org/pdf/2506.19846v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-24T17:59:31Z", "updated": "2025-06-24T17:59:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "JudgeAgent: Beyond Static Benchmarks for Knowledge-Driven and Dynamic LLM Evaluation", "authors": ["Zhichao Shi", "Xuhui Jiang", "Chengjin Xu", "Cangli Yao", "Shengjia Ma", "Yinghan Shen", "Zixuan Li", "Jian Guo", "Yuanzhuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.02097v4", "abstract": "Current evaluation methods for large language models (LLMs) primarily rely on static benchmarks, presenting two major challenges: limited knowledge coverage and fixed difficulties that mismatch with the evaluated LLMs. These limitations lead to superficial assessments of LLM knowledge, thereby impeding the targeted model optimizations. To bridge this gap, we propose JudgeAgent, a knowledge-driven and dynamic evaluation framework for LLMs. To address the challenge of limited knowledge coverage, JudgeAgent leverages LLM agents equipped with context graphs to traverse knowledge structures systematically for question generation. Furthermore, to mitigate data contamination and difficulty mismatch, it adopts a difficulty-adaptive and multi-turn interview mechanism. Thereby, JudgeAgent can achieve comprehensive evaluations and facilitate more effective improvement of LLMs. Empirical results demonstrate that JudgeAgent enables more comprehensive evaluations and facilitates effective model iterations, highlighting the potential of this knowledge-driven and dynamic evaluation paradigm. The source code is available on https://github.com/DataArcTech/JudgeAgent.", "source": "arxiv", "arxiv_id": "2509.02097v4", "pdf_url": "https://arxiv.org/pdf/2509.02097v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-02T08:52:16Z", "updated": "2026-01-15T04:01:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation", "authors": ["Hadi Nekoei", "Aman Jaiswal", "Patrice Bechard", "Oleh Shliazhko", "Orlando Marquez Ayala", "Mathieu Reymond", "Massimo Caccia", "Alexandre Drouin", "Sarath Chandar", "Alexandre Lacoste"], "year": 2025, "url": "http://arxiv.org/abs/2510.04373v1", "abstract": "Large language model (LLM) agents perform well in sequential decision-making tasks, but improving them on unfamiliar domains often requires costly online interactions or fine-tuning on large expert datasets. These strategies are impractical for closed-source models and expensive for open-source ones, with risks of catastrophic forgetting. Offline trajectories offer reusable knowledge, yet demonstration-based methods struggle because raw traces are long, noisy, and tied to specific tasks. We present Just-in-time Episodic Feedback Hinter (JEF Hinter), an agentic system that distills offline traces into compact, context-aware hints. A zooming mechanism highlights decisive steps in long trajectories, capturing both strategies and pitfalls. Unlike prior methods, JEF Hinter leverages both successful and failed trajectories, extracting guidance even when only failure data is available, while supporting parallelized hint generation and benchmark-independent prompting. At inference, a retriever selects relevant hints for the current state, providing targeted guidance with transparency and traceability. Experiments on MiniWoB++, WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms strong baselines, including human- and document-based hints.", "source": "arxiv", "arxiv_id": "2510.04373v1", "pdf_url": "https://arxiv.org/pdf/2510.04373v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-05T21:34:42Z", "updated": "2025-10-05T21:34:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "KP-Agent: Keyword Pruning in Sponsored Search Advertising via LLM-Powered Contextual Bandits", "authors": ["Hou-Wan Long", "Yicheng Song", "Zidong Wang", "Tianshu Sun"], "year": 2025, "url": "http://arxiv.org/abs/2601.05257v1", "abstract": "Sponsored search advertising (SSA) requires advertisers to constantly adjust keyword strategies. While bid adjustment and keyword generation are well-studied, keyword pruning-refining keyword sets to enhance campaign performance-remains under-explored. This paper addresses critical inefficiencies in current practices as evidenced by a dataset containing 0.5 million SSA records from a pharmaceutical advertiser on search engine Meituan, China's largest delivery platform. We propose KP-Agent, an LLM agentic system with domain tool set and a memory module. By modeling keyword pruning within a contextual bandit framework, KP-Agent generates code snippets to refine keyword sets through reinforcement learning. Experiments show KP-Agent improves cumulative profit by up to 49.28% over baselines.", "source": "arxiv", "arxiv_id": "2601.05257v1", "pdf_url": "https://arxiv.org/pdf/2601.05257v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-10-20T16:13:48Z", "updated": "2025-10-20T16:13:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit", "authors": ["Dezhi Ran", "Shuxiao Xie", "Mingfang Ji", "Ziyue Hua", "Mengzhou Wu", "Yuan Cao", "Yuzhe Guo", "Yu Hao", "Linyi Li", "Yitao Hu", "Tao Xie"], "year": 2025, "url": "http://arxiv.org/abs/2511.18868v1", "abstract": "High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase.", "source": "arxiv", "arxiv_id": "2511.18868v1", "pdf_url": "https://arxiv.org/pdf/2511.18868v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-24T08:11:50Z", "updated": "2025-11-24T08:11:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?", "authors": ["Qian Zhang", "Yan Zheng", "Jinyi Liu", "Hebin Liang", "Lanjun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.11040v1", "abstract": "Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, \"Truth Last\", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.", "source": "arxiv", "arxiv_id": "2511.11040v1", "pdf_url": "https://arxiv.org/pdf/2511.11040v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-14T07:47:56Z", "updated": "2025-11-14T07:47:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining", "authors": ["Qian'ang Mao", "Yuxuan Zhang", "Jiaman Chen", "Wenjun Zhou", "Jiaqi Yan"], "year": 2025, "url": "http://arxiv.org/abs/2511.15456v1", "abstract": "As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.", "source": "arxiv", "arxiv_id": "2511.15456v1", "pdf_url": "https://arxiv.org/pdf/2511.15456v1", "categories": ["cs.AI", "q-fin.GN"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-19T14:15:23Z", "updated": "2025-11-19T14:15:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making", "authors": ["Qingyuan Zhang", "Yuxi Wang", "Cancan Hua", "Yulin Huang", "Ning Lyu"], "year": 2025, "url": "http://arxiv.org/abs/2512.09440v1", "abstract": "This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.", "source": "arxiv", "arxiv_id": "2512.09440v1", "pdf_url": "https://arxiv.org/pdf/2512.09440v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-10T09:08:33Z", "updated": "2025-12-10T09:08:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents", "authors": ["Henrik Bradland", "Morten Goodwin", "Vladimir I. Zadorozhny", "Per-Arne Andersen"], "year": 2025, "url": "http://arxiv.org/abs/2511.15074v1", "abstract": "The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a \"flooding-pruning\" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.", "source": "arxiv", "arxiv_id": "2511.15074v1", "pdf_url": "https://arxiv.org/pdf/2511.15074v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-19T03:27:14Z", "updated": "2025-11-19T03:27:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LA-RCS: LLM-Agent-Based Robot Control System", "authors": ["TaekHyun Park", "YoungJun Choi", "SeungHoon Shin", "Kwangil Lee"], "year": 2025, "url": "http://arxiv.org/abs/2505.18214v1", "abstract": "LA-RCS (LLM-agent-based robot control system) is a sophisticated robot control system designed to autonomously plan, work, and analyze the external environment based on user requirements by utilizing LLM-Agent. Utilizing a dual-agent framework, LA-RCS generates plans based on user requests, observes the external environment, executes the plans, and modifies the plans as needed to adapt to changes in the external conditions. Additionally, LA-RCS interprets natural language commands by the user and converts them into commands compatible with the robot interface so that the robot can execute tasks and meet user requests properly. During his process, the system autonomously evaluates observation results, provides feedback on the tasks, and executes commands based on real-time environmental monitoring, significantly reducing the need for user intervention in fulfilling requests. We categorized the scenarios that LA-RCS needs to perform into four distinct types and conducted a quantitative assessment of its performance in each scenario. The results showed an average success rate of 90 percent, demonstrating the system capability to fulfill user requests satisfactorily. For more extensive results, readers can visit our project page: https://la-rcs.github.io", "source": "arxiv", "arxiv_id": "2505.18214v1", "pdf_url": "https://arxiv.org/pdf/2505.18214v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "10.18494/SAM5643", "venue": "Senors and Materials 2025", "published": "2025-05-23T00:51:16Z", "updated": "2025-05-23T00:51:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding", "authors": ["Zhivar Sourati", "Zheng Wang", "Marianne Menglin Liu", "Yazhe Hu", "Mengqing Guo", "Sujeeth Bharadwaj", "Kyu Han", "Tao Sheng", "Sujith Ravi", "Morteza Dehghani", "Dan Roth"], "year": 2025, "url": "http://arxiv.org/abs/2510.07233v1", "abstract": "Question answering over visually rich documents (VRDs) requires reasoning not only over isolated content but also over documents' structural organization and cross-page dependencies. However, conventional retrieval-augmented generation (RAG) methods encode content in isolated chunks during ingestion, losing structural and cross-page dependencies, and retrieve a fixed number of pages at inference, regardless of the specific demands of the question or context. This often results in incomplete evidence retrieval and degraded answer quality for multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs a symbolic document graph that captures layout structure and cross-page dependencies, adding it alongside standard neural embeddings to yield a more holistic representation of the document. During inference, an LLM agent dynamically interacts with the neural and symbolic indices to adaptively retrieve the necessary evidence based on the query. Experiments on MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG improves retrieval, achieving over 90% perfect recall on average without any top-k tuning, and outperforming baseline retrievers by up to 20% in recall at comparable noise levels, yielding higher QA accuracy with minimal latency.", "source": "arxiv", "arxiv_id": "2510.07233v1", "pdf_url": "https://arxiv.org/pdf/2510.07233v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-08T17:02:04Z", "updated": "2025-10-08T17:02:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources", "authors": ["Haichao Ji", "Zibo Wang", "Cheng Pan", "Meng Han", "Yifei Zhu", "Dan Wang", "Zhu Han"], "year": 2025, "url": "http://arxiv.org/abs/2510.18477v2", "abstract": "Large Language Models (LLMs) have shown great promise in automating data analytics tasks by interpreting natural language queries and generating multi-operation execution plans. However, existing LLM-agent-based analytics frameworks operate under the assumption of centralized data access, offering little to no privacy protection. In contrast, federated analytics (FA) enables privacy-preserving computation across distributed data sources, but lacks support for natural language input and requires structured, machine-readable queries. In this work, we present LAFA, the first system that integrates LLM-agent-based data analytics with FA. LAFA introduces a hierarchical multi-agent architecture that accepts natural language queries and transforms them into optimized, executable FA workflows. A coarse-grained planner first decomposes complex queries into sub-queries, while a fine-grained planner maps each subquery into a Directed Acyclic Graph of FA operations using prior structural knowledge. To improve execution efficiency, an optimizer agent rewrites and merges multiple DAGs, eliminating redundant operations and minimizing computational and communicational overhead. Our experiments demonstrate that LAFA consistently outperforms baseline prompting strategies by achieving higher execution plan success rates and reducing resource-intensive FA operations by a substantial margin. This work establishes a practical foundation for privacy-preserving, LLM-driven analytics that supports natural language input in the FA setting.", "source": "arxiv", "arxiv_id": "2510.18477v2", "pdf_url": "https://arxiv.org/pdf/2510.18477v2", "categories": ["cs.AI", "cs.CR", "cs.DC", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-21T09:56:25Z", "updated": "2025-10-30T04:49:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback", "authors": ["Thai Hoang", "Kung-Hsiang Huang", "Shirley Kokane", "Jianguo Zhang", "Zuxin Liu", "Ming Zhu", "Jake Grigsby", "Tian Lan", "Michael S Ryoo", "Chien-Sheng Wu", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong", "Juan Carlos Niebles"], "year": 2025, "url": "http://arxiv.org/abs/2506.02298v1", "abstract": "Large Action Models (LAMs) for AI Agents offer incredible potential but face challenges due to the need for high-quality training data, especially for multi-steps tasks that involve planning, executing tool calls, and responding to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive framework designed for online exploration of agentic tasks with high-quality feedback. Our framework features a dynamic task query generator, an extensive collection of tools, and an interactive environment where Large Language Model (LLM) Agents can call tools and receive real-time feedback. This setup enables LLM Agents to explore and solve tasks autonomously, facilitating the discovery of multiple approaches to tackle any given task. The resulting action trajectory data are then used to create high-quality training datasets for LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena, highlight the effectiveness of LAM SIMULATOR: models trained with self-generated datasets using our framework achieve significant performance gains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR requires minimal human input during dataset creation, highlighting LAM SIMULATOR's efficiency and effectiveness in speeding up development of AI agents.", "source": "arxiv", "arxiv_id": "2506.02298v1", "pdf_url": "https://arxiv.org/pdf/2506.02298v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-02T22:36:02Z", "updated": "2025-06-02T22:36:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework", "authors": ["Frazier N. Baker", "Daniel Adu-Ampratwum", "Reza Averly", "Botao Yu", "Huan Sun", "Xia Ning"], "year": 2025, "url": "http://arxiv.org/abs/2508.11860v1", "abstract": "Large language model (LLM) agent evaluators leverage specialized tools to ground the rational decision-making of LLMs, making them well-suited to aid in scientific discoveries, such as constrained retrosynthesis planning. Constrained retrosynthesis planning is an essential, yet challenging, process within chemistry for identifying synthetic routes from commercially available starting materials to desired target molecules, subject to practical constraints. Here, we present LARC, the first LLM-based Agentic framework for Retrosynthesis planning under Constraints. LARC incorporates agentic constraint evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis planning process, using agentic feedback grounded in tool-based reasoning to guide and constrain route generation. We rigorously evaluate LARC on a carefully curated set of 48 constrained retrosynthesis planning tasks across 3 constraint types. LARC achieves a 72.9% success rate on these tasks, vastly outperforming LLM baselines and approaching human expert-level success in substantially less time. The LARC framework is extensible, and serves as a first step towards an effective agentic tool or a co-scientist to human experts for constrained retrosynthesis.", "source": "arxiv", "arxiv_id": "2508.11860v1", "pdf_url": "https://arxiv.org/pdf/2508.11860v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-16T01:05:26Z", "updated": "2025-08-16T01:05:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA", "authors": ["Zeyi Kang", "Liang He", "Yanxin Zhang", "Zuheng Ming", "Kaixing Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2509.18576v1", "abstract": "Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.", "source": "arxiv", "arxiv_id": "2509.18576v1", "pdf_url": "https://arxiv.org/pdf/2509.18576v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-09-23T02:57:25Z", "updated": "2025-09-23T02:57:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agent Communication Protocol (LACP) Requires Urgent Standardization: A Telecom-Inspired Protocol is Necessary", "authors": ["Xin Li", "Mengbing Liu", "Chau Yuen"], "year": 2025, "url": "http://arxiv.org/abs/2510.13821v1", "abstract": "This position paper argues that the field of LLM agents requires a unified, telecom-inspired communication protocol to ensure safety, interoperability, and scalability, especially within the context of Next Generation (NextG) networks. Current ad-hoc communication methods are creating a fragmented ecosystem, reminiscent of the early \"protocol wars\" in networking, which stifles innovation and poses significant risks. Drawing inspiration from the layered, standardized protocols that underpin modern telecommunications, we propose the LLM-Agent Communication Protocol (LACP). LACP establishes a three-layer architecture designed to ensure semantic clarity in communication, transactional integrity for complex tasks, and robust, built-in security. In this position paper, we argue that adopting a principled, universal protocol is not merely beneficial but essential for realizing the potential of distributed AI. Such a standard is critical for ensuring that multi-agent systems can operate safely and reliably in the complex, real-time applications envisioned for 6G and beyond.", "source": "arxiv", "arxiv_id": "2510.13821v1", "pdf_url": "https://arxiv.org/pdf/2510.13821v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-09-26T08:48:56Z", "updated": "2025-09-26T08:48:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?", "authors": ["Lu Sun", "Shihan Fu", "Bingsheng Yao", "Yuxuan Lu", "Wenbo Li", "Hansu Gu", "Jiri Gesi", "Jing Huang", "Chen Luo", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.21501v1", "abstract": "Agentic AI is emerging, capable of executing tasks through natural language, such as Copilot for coding or Amazon Rufus for shopping. Evaluating these systems is challenging, as their rapid evolution outpaces traditional human evaluation. Researchers have proposed LLM Agents to simulate participants as digital twins, but it remains unclear to what extent a digital twin can represent a specific customer in multi-turn interaction with an agentic AI system. In this paper, we recruited 40 human participants to shop with Amazon Rufus, collected their personas, interaction traces, and UX feedback, and then created digital twins to repeat the task. Pairwise comparison of human and digital-twin traces shows that while agents often explored more diverse choices, their action patterns aligned with humans and yielded similar design feedback. This study is the first to quantify how closely LLM agents can mirror human multi-turn interaction with an agentic AI system, highlighting their potential for scalable evaluation.", "source": "arxiv", "arxiv_id": "2509.21501v1", "pdf_url": "https://arxiv.org/pdf/2509.21501v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-25T19:58:02Z", "updated": "2025-09-25T19:58:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery", "authors": ["Kevin Song", "Andrew Trotter", "Jake Y. Chen"], "year": 2025, "url": "http://arxiv.org/abs/2504.17967v1", "abstract": "Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM \"agents\" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines.", "source": "arxiv", "arxiv_id": "2504.17967v1", "pdf_url": "https://arxiv.org/pdf/2504.17967v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-24T22:27:50Z", "updated": "2025-04-24T22:27:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agent for Hyper-Parameter Optimization", "authors": ["Wanzhe Wang", "Jianqiu Peng", "Menghao Hu", "Weihuang Zhong", "Tong Zhang", "Shuai Wang", "Yixin Zhang", "Mingjie Shao", "Wanli Ni"], "year": 2025, "url": "http://arxiv.org/abs/2506.15167v2", "abstract": "Hyper-parameters are essential and critical for the performance of communication algorithms. However, current hyper-parameters optimization approaches for Warm-Start Particles Swarm Optimization with Crossover and Mutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial vehicle (UAV) trajectory and communication, are primarily heuristic-based, exhibiting low levels of automation and improvable performance. In this paper, we design an Large Language Model (LLM) agent for automatic hyper-parameters-tuning, where an iterative framework and Model Context Protocol (MCP) are applied. In particular, the LLM agent is first set up via a profile, which specifies the boundary of hyper-parameters, task objective, terminal condition, conservative or aggressive strategy of optimizing hyper-parameters, and LLM configurations. Then, the LLM agent iteratively invokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the loop based on the terminal condition and returns an optimized set of hyperparameters. Our experiment results show that the minimal sum-rate achieved by hyper-parameters generated via our LLM agent is significantly higher than those by both human heuristics and random generation methods. This indicates that an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in seeking high-performance hyper-parameters.", "source": "arxiv", "arxiv_id": "2506.15167v2", "pdf_url": "https://arxiv.org/pdf/2506.15167v2", "categories": ["cs.IT", "cs.AI"], "primary_category": "cs.IT", "doi": "", "venue": "", "published": "2025-06-18T06:28:22Z", "updated": "2025-07-09T13:20:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agent-Based Simulation of Student Activities and Mental Health Using Smartphone Sensing Data", "authors": ["Wayupuk Sommuang", "Kun Kerdthaisong", "Pasin Buakhaw", "Aslan B. Wong", "Nutchanon Yongsatianchot"], "year": 2025, "url": "http://arxiv.org/abs/2508.02679v2", "abstract": "Students' mental well-being is vital for academic success, with activities such as studying, socializing, and sleeping playing a role. Current mobile sensing data highlight this intricate link using statistical and machine learning analyses. We propose a novel LLM agent-based simulation framework to model student activities and mental health using the StudentLife Dataset. Each LLM agent was initialized with personality questionnaires and guided by smartphone sensing data throughout the simulated semester. These agents predict individual behaviors, provide self-reported mental health data via ecological momentary assessments (EMAs), and complete follow-up personality questionnaires. To ensure accuracy, we investigated various prompting techniques, memory systems, and activity-based mental state management strategies that dynamically update an agent's mental state based on their daily activities. This simulation goes beyond simply replicating existing data. This allows us to explore new scenarios that are not present in the original dataset, such as peer influence through agent-to-agent interactions and the impact of social media. Furthermore, we can conduct intervention studies by manipulating activity patterns via sensing signals and personality traits using questionnaire responses. This provides valuable insights into the behavioral changes that could enhance student well-being. The framework also facilitates hypothetical interviews with LLM agents, offering deeper insights into their mental health. This study showcases the power of LLM-driven behavioral modeling with sensing data, opening new avenues for understanding and supporting student mental health.", "source": "arxiv", "arxiv_id": "2508.02679v2", "pdf_url": "https://arxiv.org/pdf/2508.02679v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-07-17T03:30:11Z", "updated": "2025-08-08T12:56:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents Are Hypersensitive to Nudges", "authors": ["Manuel Cherep", "Pattie Maes", "Nikhil Singh"], "year": 2025, "url": "http://arxiv.org/abs/2505.11584v1", "abstract": "LLMs are being set loose in complex, real-world environments involving sequential decision-making and tool use. Often, this involves making choices on behalf of human users. However, not much is known about the distribution of such choices, and how susceptible they are to different choice architectures. We perform a case study with a few such LLM models on a multi-attribute tabular decision-making problem, under canonical nudges such as the default option, suggestions, and information highlighting, as well as additional prompting strategies. We show that, despite superficial similarities to human choice distributions, such models differ in subtle but important ways. First, they show much higher susceptibility to the nudges. Second, they diverge in points earned, being affected by factors like the idiosyncrasy of available prizes. Third, they diverge in information acquisition strategies: e.g. incurring substantial cost to reveal too much information, or selecting without revealing any. Moreover, we show that simple prompt strategies like zero-shot chain of thought (CoT) can shift the choice distribution, and few-shot prompting with human data can induce greater alignment. Yet, none of these methods resolve the sensitivity of these models to nudges. Finally, we show how optimal nudges optimized with a human resource-rational model can similarly increase LLM performance for some models. All these findings suggest that behavioral tests are needed before deploying models as agents or assistants acting on behalf of users in complex environments.", "source": "arxiv", "arxiv_id": "2505.11584v1", "pdf_url": "https://arxiv.org/pdf/2505.11584v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-16T17:53:05Z", "updated": "2025-05-16T17:53:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents Are the Antidote to Walled Gardens", "authors": ["Samuele Marro", "Philip Torr"], "year": 2025, "url": "http://arxiv.org/abs/2506.23978v2", "abstract": "While the Internet's core infrastructure was designed to be open and universal, today's application layer is dominated by closed, proprietary platforms. Open and interoperable APIs require significant investment, and market leaders have little incentive to enable data exchange that could erode their user lock-in. We argue that LLM-based agents fundamentally disrupt this status quo. Agents can automatically translate between data formats and interact with interfaces designed for humans: this makes interoperability dramatically cheaper and effectively unavoidable. We name this shift universal interoperability: the ability for any two digital services to exchange data seamlessly using AI-mediated adapters. Universal interoperability undermines monopolistic behaviours and promotes data portability. However, it can also lead to new security risks and technical debt. Our position is that the ML community should embrace this development while building the appropriate frameworks to mitigate the downsides. By acting now, we can harness AI to restore user freedom and competitive markets without sacrificing security.", "source": "arxiv", "arxiv_id": "2506.23978v2", "pdf_url": "https://arxiv.org/pdf/2506.23978v2", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-30T15:45:17Z", "updated": "2025-07-12T15:26:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents Beyond Utility: An Open-Ended Perspective", "authors": ["Asen Nachkov", "Xi Wang", "Luc Van Gool"], "year": 2025, "url": "http://arxiv.org/abs/2510.14548v1", "abstract": "Recent LLM agents have made great use of chain of thought reasoning and function calling. As their capabilities grow, an important question arises: can this software represent not only a smart problem-solving tool, but an entity in its own right, that can plan, design immediate tasks, and reason toward broader, more ambiguous goals? To study this question, we adopt an open-ended experimental setting where we augment a pretrained LLM agent with the ability to generate its own tasks, accumulate knowledge, and interact extensively with its environment. We study the resulting open-ended agent qualitatively. It can reliably follow complex multi-step instructions, store and reuse information across runs, and propose and solve its own tasks, though it remains sensitive to prompt design, prone to repetitive task generation, and unable to form self-representations. These findings illustrate both the promise and current limits of adapting pretrained LLMs toward open-endedness, and point to future directions for training agents to manage memory, explore productively, and pursue abstract long-term goals.", "source": "arxiv", "arxiv_id": "2510.14548v1", "pdf_url": "https://arxiv.org/pdf/2510.14548v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-16T10:46:54Z", "updated": "2025-10-16T10:46:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns", "authors": ["Idan Horowitz", "Ori Plonsky"], "year": 2025, "url": "http://arxiv.org/abs/2503.10248v1", "abstract": "We investigate the choice patterns of Large Language Models (LLMs) in the context of Decisions from Experience tasks that involve repeated choice and learning from feedback, and compare their behavior to human participants. We find that on the aggregate, LLMs appear to display behavioral biases similar to humans: both exhibit underweighting rare events and correlation effects. However, more nuanced analyses of the choice patterns reveal that this happens for very different reasons. LLMs exhibit strong recency biases, unlike humans, who appear to respond in more sophisticated ways. While these different processes may lead to similar behavior on average, choice patterns contingent on recent events differ vastly between the two groups. Specifically, phenomena such as ``surprise triggers change\" and the ``wavy recency effect of rare events\" are robustly observed in humans, but entirely absent in LLMs. Our findings provide insights into the limitations of using LLMs to simulate and predict humans in learning environments and highlight the need for refined analyses of their behavior when investigating whether they replicate human decision making tendencies.", "source": "arxiv", "arxiv_id": "2503.10248v1", "pdf_url": "https://arxiv.org/pdf/2503.10248v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-13T10:47:03Z", "updated": "2025-03-13T10:47:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents Do Not Replicate Human Market Traders: Evidence From Experimental Finance", "authors": ["Thomas Henning", "Siddhartha M. Ojha", "Ross Spoon", "Jiatong Han", "Colin F. Camerer"], "year": 2025, "url": "http://arxiv.org/abs/2502.15800v3", "abstract": "This paper explores how Large Language Models (LLMs) behave in a classic experimental finance paradigm widely known for eliciting bubbles and crashes in human participants. We adapt an established trading design, where traders buy and sell a risky asset with a known fundamental value, and introduce several LLM-based agents, both in single-model markets (all traders are instances of the same LLM) and in mixed-model \"battle royale\" settings (multiple LLMs competing in the same market). Our findings reveal that LLMs generally exhibit a \"textbook-rational\" approach, pricing the asset near its fundamental value, and show only a muted tendency toward bubble formation. Further analyses indicate that LLM-based agents display less trading strategy variance in contrast to humans. Taken together, these results highlight the risk of relying on LLM-only data to replicate human-driven market phenomena, as key behavioral features, such as large emergent bubbles, were not robustly reproduced. While LLMs clearly possess the capacity for strategic decision-making, their relative consistency and rationality suggest that they do not accurately mimic human market dynamics.", "source": "arxiv", "arxiv_id": "2502.15800v3", "pdf_url": "https://arxiv.org/pdf/2502.15800v3", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR", "doi": "", "venue": "", "published": "2025-02-18T23:05:32Z", "updated": "2025-10-11T16:53:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators", "authors": ["Mateusz Lango", "OndÅej DuÅ¡ek"], "year": 2025, "url": "http://arxiv.org/abs/2512.18360v1", "abstract": "We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models", "source": "arxiv", "arxiv_id": "2512.18360v1", "pdf_url": "https://arxiv.org/pdf/2512.18360v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.18653/v1/2025.emnlp-industry.142", "venue": "", "published": "2025-12-20T13:16:51Z", "updated": "2025-12-20T13:16:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents Making Agent Tools", "authors": ["Georg WÃ¶lflein", "Dyke Ferber", "Daniel Truhn", "Ognjen ArandjeloviÄ", "Jakob Nikolas Kather"], "year": 2025, "url": "http://arxiv.org/abs/2502.11705v2", "abstract": "Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows. Our code and benchmark are publicly available at https://github.com/KatherLab/ToolMaker.", "source": "arxiv", "arxiv_id": "2502.11705v2", "pdf_url": "https://arxiv.org/pdf/2502.11705v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T11:44:11Z", "updated": "2025-05-29T18:47:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents Should Employ Security Principles", "authors": ["Kaiyuan Zhang", "Zian Su", "Pin-Yu Chen", "Elisa Bertino", "Xiangyu Zhang", "Ninghui Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.24019v1", "abstract": "Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system's susceptibility to prompt injection and other forms of context manipulation introduce new vulnerabilities related to privacy leakage and system exploitation. This position paper argues that the well-established design principles in information security, which are commonly referred to as security principles, should be employed when deploying LLM agents at scale. Design principles such as defense-in-depth, least privilege, complete mediation, and psychological acceptability have helped guide the design of mechanisms for securing information systems over the last five decades, and we argue that their explicit and conscientious adoption will help secure agentic systems. To illustrate this approach, we introduce AgentSandbox, a conceptual framework embedding these security principles to provide safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art LLMs along three dimensions: benign utility, attack utility, and attack success rate. AgentSandbox maintains high utility for its intended functions under both benign and adversarial evaluations while substantially mitigating privacy risks. By embedding secure design principles as foundational elements within emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems aligned with user privacy expectations and evolving regulatory requirements.", "source": "arxiv", "arxiv_id": "2505.24019v1", "pdf_url": "https://arxiv.org/pdf/2505.24019v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-29T21:39:08Z", "updated": "2025-05-29T21:39:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring", "authors": ["Jinhee Jang", "Ayoung Moon", "Minkyoung Jung", "YoungBin Kim", "Seung Jin Lee"], "year": 2025, "url": "http://arxiv.org/abs/2509.14834v2", "abstract": "The emergence of large language models (LLMs) has brought a new paradigm to automated essay scoring (AES), a long-standing and practical application of natural language processing in education. However, achieving human-level multi-perspective understanding and judgment remains a challenge. In this work, we propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework designed to perform precise and human-aligned scoring under a zero-shot setting. RES constructs evaluator agents based on LLMs, each tailored to a specific prompt and topic context. Each agent independently generates a trait-based rubric and conducts a multi-perspective evaluation. Then, by simulating a roundtable-style discussion, RES consolidates individual evaluations through a dialectical reasoning process to produce a final holistic score that more closely aligns with human evaluation. By enabling collaboration and consensus among agents with diverse evaluation perspectives, RES outperforms prior zero-shot AES approaches. Experiments on the ASAP dataset using ChatGPT and Claude show that RES achieves up to a 34.86% improvement in average QWK over straightforward prompting (Vanilla) methods.", "source": "arxiv", "arxiv_id": "2509.14834v2", "pdf_url": "https://arxiv.org/pdf/2509.14834v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-18T10:55:33Z", "updated": "2025-09-19T03:11:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents for Automated Dependency Upgrades", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "year": 2025, "url": "http://arxiv.org/abs/2510.03480v2", "abstract": "As a codebase expands over time, its library dependencies can become outdated and require updates to maintain innovation and security. However, updating a library can introduce breaking changes in the code, necessitating significant developer time for maintenance. To address this, we introduce a framework of LLM agents to be used in combination with migration documentation to automatically recommend and apply code updates and ensure compatibility with new versions. Our solution can automatically localize updated library usages in live Java codebases and implement recommended fixes in a user-friendly manner. The system architecture consists of multiple key components: a Summary Agent, Control Agent, and Code Agent. To validate our approach, we apply the framework on an industrial use case by which we create three synthetic code repositories with major Upgrade changes and benchmark our approach against state-of-the-art methods. Results show that our approach not only performs upgrades using fewer tokens across all cases but also achieves a precision of 71.4%, highlighting its efficiency and effectiveness compared to state-of-the-art methods.", "source": "arxiv", "arxiv_id": "2510.03480v2", "pdf_url": "https://arxiv.org/pdf/2510.03480v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-03T19:57:10Z", "updated": "2025-11-24T17:56:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?", "authors": ["Bin Liu", "Yanjie Zhao", "Guoai Xu", "Haoyu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.14700v1", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in software engineering and cybersecurity tasks, including code generation, vulnerability discovery, and automated testing. One critical but underexplored application is automated web vulnerability reproduction, which transforms vulnerability reports into working exploits. Although recent advances suggest promising potential, challenges remain in applying LLM agents to real-world web vulnerability reproduction scenarios. In this paper, we present the first comprehensive evaluation of state-of-the-art LLM agents for automated web vulnerability reproduction. We systematically assess 20 agents from software engineering, cybersecurity, and general domains across 16 dimensions, including technical capabilities, environment adaptability, and user experience factors, on 3 representative web vulnerabilities. Based on the results, we select three top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types and 6 web technologies. Our results reveal that while LLM agents achieve reasonable success on simple library-based vulnerabilities, they consistently fail on complex service-based vulnerabilities requiring multi-component environments. Complex environment configurations and authentication barriers create a gap where agents can execute exploit code but fail to trigger actual vulnerabilities. We observe high sensitivity to input guidance, with performance degrading by over 33% under incomplete authentication information. Our findings highlight the significant gap between current LLM agent capabilities and the demands of reliable automated vulnerability reproduction, emphasizing the need for advances in environmental adaptation and autonomous problem-solving capabilities.", "source": "arxiv", "arxiv_id": "2510.14700v1", "pdf_url": "https://arxiv.org/pdf/2510.14700v1", "categories": ["cs.SE", "cs.CR"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-16T14:04:46Z", "updated": "2025-10-16T14:04:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents for Bargaining with Utility-based Feedback", "authors": ["Jihwan Oh"], "year": 2025, "url": "http://arxiv.org/abs/2505.22998v2", "abstract": "Bargaining, a critical aspect of real-world interactions, presents challenges for large language models (LLMs) due to limitations in strategic depth and adaptation to complex human factors. Existing benchmarks often fail to capture this real-world complexity. To address this and enhance LLM capabilities in realistic bargaining, we introduce a comprehensive framework centered on utility-based feedback. Our contributions are threefold: (1) BargainArena, a novel benchmark dataset with six intricate scenarios (e.g., deceptive practices, monopolies) to facilitate diverse strategy modeling; (2) human-aligned, economically-grounded evaluation metrics inspired by utility theory, incorporating agent utility and negotiation power, which implicitly reflect and promote opponent-aware reasoning (OAR); and (3) a structured feedback mechanism enabling LLMs to iteratively refine their bargaining strategies. This mechanism can positively collaborate with in-context learning (ICL) prompts, including those explicitly designed to foster OAR. Experimental results show that LLMs often exhibit negotiation strategies misaligned with human preferences, and that our structured feedback mechanism significantly improves their performance, yielding deeper strategic and opponent-aware reasoning.", "source": "arxiv", "arxiv_id": "2505.22998v2", "pdf_url": "https://arxiv.org/pdf/2505.22998v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-29T02:07:27Z", "updated": "2025-06-18T22:46:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents for Education: Advances and Applications", "authors": ["Zhendong Chu", "Shen Wang", "Jian Xie", "Tinghui Zhu", "Yibo Yan", "Jinheng Ye", "Aoxiao Zhong", "Xuming Hu", "Jing Liang", "Philip S. Yu", "Qingsong Wen"], "year": 2025, "url": "http://arxiv.org/abs/2503.11733v1", "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in automating tasks and driving innovation across diverse educational applications. In this survey, we provide a systematic review of state-of-the-art research on LLM agents in education, categorizing them into two broad classes: (1) \\emph{Pedagogical Agents}, which focus on automating complex pedagogical tasks to support both teachers and students; and (2) \\emph{Domain-Specific Educational Agents}, which are tailored for specialized fields such as science education, language learning, and professional development. We comprehensively examine the technological advancements underlying these LLM agents, including key datasets, benchmarks, and algorithmic frameworks that drive their effectiveness. Furthermore, we discuss critical challenges such as privacy, bias and fairness concerns, hallucination mitigation, and integration with existing educational ecosystems. This survey aims to provide a comprehensive technological overview of LLM agents for education, fostering further research and collaboration to enhance their impact for the greater good of learners and educators alike.", "source": "arxiv", "arxiv_id": "2503.11733v1", "pdf_url": "https://arxiv.org/pdf/2503.11733v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-03-14T11:53:44Z", "updated": "2025-03-14T11:53:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents for Generating Microservice-based Applications: how complex is your specification?", "authors": ["Daniel M. Yellin"], "year": 2025, "url": "http://arxiv.org/abs/2508.20119v2", "abstract": "In this paper we evaluate the capabilities of LLM Agents in generating code for real-world problems. Specifically, we explore code synthesis for microservice-based applications, a widely used architectural pattern for building applications. We define a standard template for specifying these applications, and we propose a metric for scoring the difficulty of a specification. The higher the score, the more difficult it is to generate code for the specification. Our experimental results show that agents using strong LLMs (like GPT-3o-mini) do fairly well on medium difficulty specifications but do poorly on those of higher difficulty levels. This is due to more intricate business logic, a greater use of external services, database integration and inclusion of non-functional capabilities such as authentication. We analyzed the errors in LLM-synthesized code and report on the key challenges LLM Agents face in generating code for these specifications. Finally, we show that using a fine-grained approach to code generation improves the correctness of the generated code.", "source": "arxiv", "arxiv_id": "2508.20119v2", "pdf_url": "https://arxiv.org/pdf/2508.20119v2", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-22T12:34:22Z", "updated": "2025-10-26T14:39:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents for Interactive Exploration of Historical Cadastre Data: Framework and Application to Venice", "authors": ["Tristan Karch", "Jakhongir Saydaliev", "Isabella Di Lenardo", "FrÃ©dÃ©ric Kaplan"], "year": 2025, "url": "http://arxiv.org/abs/2505.17148v2", "abstract": "Cadastral data reveal key information about the historical organization of cities but are often non-standardized due to diverse formats and human annotations, complicating large-scale analysis. We explore as a case study Venice's urban history during the critical period from 1740 to 1808, capturing the transition following the fall of the ancient Republic and the Ancien RÃ©gime. This era's complex cadastral data, marked by its volume and lack of uniform structure, presents unique challenges that our approach adeptly navigates, enabling us to generate spatial queries that bridge past and present urban landscapes. We present a text-to-programs framework that leverages Large Language Models (\\llms) to process natural language queries as executable code for analyzing historical cadastral records. Our methodology implements two complementary techniques: a SQL agent for handling structured queries about specific cadastral information, and a coding agent for complex analytical operations requiring custom data manipulation. We propose a taxonomy that classifies historical research questions based on their complexity and analytical requirements, mapping them to the most appropriate technical approach. This framework is supported by an investigation into the execution consistency of the system, alongside a qualitative analysis of the answers it produces. By ensuring interpretability and minimizing hallucination through verifiable program outputs, we demonstrate the system's effectiveness in reconstructing past population information, property features, and spatiotemporal comparisons in Venice.", "source": "arxiv", "arxiv_id": "2505.17148v2", "pdf_url": "https://arxiv.org/pdf/2505.17148v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "10.1017/chr.2025.10014", "venue": "Comput. humanit. res. 1 (2025) e11", "published": "2025-05-22T08:45:15Z", "updated": "2025-09-30T12:53:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology", "authors": ["Renan Souza", "Timothy Poteet", "Brian Etz", "Daniel Rosendo", "Amal Gueroudji", "Woong Shin", "Prasanna Balaprakash", "Rafael Ferreira da Silva"], "year": 2025, "url": "http://arxiv.org/abs/2509.13978v2", "abstract": "Modern scientific discovery increasingly relies on workflows that process data across the Edge, Cloud, and High Performance Computing (HPC) continuum. Comprehensive and in-depth analyses of these data are critical for hypothesis validation, anomaly detection, reproducibility, and impactful findings. Although workflow provenance techniques support such analyses, at large scale, the provenance data become complex and difficult to analyze. Existing systems depend on custom scripts, structured queries, or static dashboards, limiting data interaction. In this work, we introduce an evaluation methodology, reference architecture, and open-source implementation that leverages interactive Large Language Model (LLM) agents for runtime data analysis. Our approach uses a lightweight, metadata-driven design that translates natural language into structured provenance queries. Evaluations across LLaMA, GPT, Gemini, and Claude, covering diverse query classes and a real-world chemistry workflow, show that modular design, prompt tuning, and Retrieval-Augmented Generation (RAG) enable accurate and insightful LLM agent responses beyond recorded provenance.", "source": "arxiv", "arxiv_id": "2509.13978v2", "pdf_url": "https://arxiv.org/pdf/2509.13978v2", "categories": ["cs.DC", "cs.AI", "cs.DB"], "primary_category": "cs.DC", "doi": "10.1145/3731599.3767582", "venue": "", "published": "2025-09-17T13:51:29Z", "updated": "2025-09-23T13:31:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents for Knowledge Discovery in Atomic Layer Processing", "authors": ["Andreas Werbrouck", "Marshall B. Lindsay", "Matthew Maschmann", "Matthias J. Young"], "year": 2025, "url": "http://arxiv.org/abs/2509.26201v1", "abstract": "Large Language Models (LLMs) have garnered significant attention for several years now. Recently, their use as independently reasoning agents has been proposed. In this work, we test the potential of such agents for knowledge discovery in materials science. We repurpose LangGraph's tool functionality to supply agents with a black box function to interrogate. In contrast to process optimization or performing specific, user-defined tasks, knowledge discovery consists of freely exploring the system, posing and verifying statements about the behavior of this black box, with the sole objective of generating and verifying generalizable statements. We provide proof of concept for this approach through a children's parlor game, demonstrating the role of trial-and-error and persistence in knowledge discovery, and the strong path-dependence of results. We then apply the same strategy to show that LLM agents can explore, discover, and exploit diverse chemical interactions in an advanced Atomic Layer Processing reactor simulation using intentionally limited probe capabilities without explicit instructions.", "source": "arxiv", "arxiv_id": "2509.26201v1", "pdf_url": "https://arxiv.org/pdf/2509.26201v1", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T13:01:44Z", "updated": "2025-09-30T13:01:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Applications: Current Paradigms and the Next Frontier", "authors": ["Xinyi Hou", "Yanjie Zhao", "Haoyu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2503.04596v2", "abstract": "The development of large language models (LLMs) has given rise to four major application paradigms: LLM app stores, LLM agents, self-hosted LLM services, and LLM-powered devices. Each has its advantages but also shares common challenges. LLM app stores lower the barrier to development but lead to platform lock-in; LLM agents provide autonomy but lack a unified communication mechanism; self-hosted LLM services enhance control but increase deployment complexity; and LLM-powered devices improve privacy and real-time performance but are limited by hardware. This paper reviews and analyzes these paradigms, covering architecture design, application ecosystem, research progress, as well as the challenges and open problems they face. Based on this, we outline the next frontier of LLM applications, characterizing them through three interconnected layers: infrastructure, protocol, and application. We describe their responsibilities and roles of each layer and demonstrate how to mitigate existing fragmentation limitations and improve security and scalability. Finally, we discuss key future challenges, identify opportunities such as protocol-driven cross-platform collaboration and device integration, and propose a research roadmap for openness, security, and sustainability.", "source": "arxiv", "arxiv_id": "2503.04596v2", "pdf_url": "https://arxiv.org/pdf/2503.04596v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-06T16:38:23Z", "updated": "2025-10-09T02:34:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Assisted Coding with Metamorphic Specification Mutation Agent", "authors": ["Mostafijur Rahman Akhond", "Gias Uddin"], "year": 2025, "url": "http://arxiv.org/abs/2511.18249v1", "abstract": "Metamorphic Relations (MRs) serve as a foundational mechanism for generating semantically equivalent mutations. Software engineering has advanced significantly in recent years with the advent of Large Language Models (LLMs). However, the reliability of LLMs in software engineering is often compromised by ambiguities and inconsistencies due to improper user specification. To address this challenge, we present CodeMetaAgent (CMA), a metamorphic relation-driven LLM agent that systematically refines task specifications and generates semantically constrained test cases. Our proposed framework uses MRs with LLMs to improve generation consistency and reduce variability caused by specifications, unlike the traditional use of MRs as post validations. Our framework has been evaluated on the HumanEval-Pro, MBPP-Pro, and SWE-Bench_Lite datasets using the GPT-4o, Mistral Large, GPT-OSS, and Qwen3-Coder models. It improved code generation accuracy by up to 17% and achieved code coverage gains of up to 99.81%. These results show that metamorphic relations can be a simple but effective guide in assisting LLM-based software development.", "source": "arxiv", "arxiv_id": "2511.18249v1", "pdf_url": "https://arxiv.org/pdf/2511.18249v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-23T02:30:34Z", "updated": "2025-11-23T02:30:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Bazaar: A Service Design for Supporting Collaborative Learning with an LLM-Powered Multi-Party Collaboration Infrastructure", "authors": ["Zhen Wu", "Jiaxin Shi", "R. Charles Murray", "Carolyn RosÃ©", "Micah San Andres"], "year": 2025, "url": "http://arxiv.org/abs/2510.18877v1", "abstract": "For nearly two decades, conversational agents have played a critical role in structuring interactions in collaborative learning, shaping group dynamics, and supporting student engagement. The recent integration of large language models (LLMs) into these agents offers new possibilities for fostering critical thinking and collaborative problem solving. In this work, we begin with an open source collaboration support architecture called Bazaar and integrate an LLM-agent shell that enables introduction of LLM-empowered, real time, context sensitive collaborative support for group learning. This design and infrastructure paves the way for exploring how tailored LLM-empowered environments can reshape collaborative learning outcomes and interaction patterns.", "source": "arxiv", "arxiv_id": "2510.18877v1", "pdf_url": "https://arxiv.org/pdf/2510.18877v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "10.22318/cscl2025.674934", "venue": "Proceedings of the 18th International Conference on Computer-Supported Collaborative Learning - CSCL 2025 (pp. 108-115). International Society of the Learning Sciences", "published": "2025-09-12T01:25:49Z", "updated": "2025-09-12T01:25:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Driven Processes to Foster Explainable AI", "authors": ["Marcel Pehlke", "Marc Jansen"], "year": 2025, "url": "http://arxiv.org/abs/2511.07086v1", "abstract": "We present a modular, explainable LLM-agent pipeline for decision support that externalizes reasoning into auditable artifacts. The system instantiates three frameworks: Vester's Sensitivity Model (factor set, signed impact matrix, systemic roles, feedback loops); normal-form games (strategies, payoff matrix, equilibria); and sequential games (role-conditioned agents, tree construction, backward induction), with swappable modules at every step. LLM components (default: GPT-5) are paired with deterministic analyzers for equilibria and matrix-based role classification, yielding traceable intermediates rather than opaque outputs. In a real-world logistics case (100 runs), mean factor alignment with a human baseline was 55.5\\% over 26 factors and 62.9\\% on the transport-core subset; role agreement over matches was 57\\%. An LLM judge using an eight-criterion rubric (max 100) scored runs on par with a reconstructed human baseline. Configurable LLM pipelines can thus mimic expert workflows with transparent, inspectable steps.", "source": "arxiv", "arxiv_id": "2511.07086v1", "pdf_url": "https://arxiv.org/pdf/2511.07086v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-10T13:20:00Z", "updated": "2025-11-10T13:20:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer", "authors": ["Rasoul Zahedifar", "Sayyed Ali Mirghasemi", "Mahdieh Soleymani Baghshah", "Alireza Taheri"], "year": 2025, "url": "http://arxiv.org/abs/2505.19567v1", "abstract": "This study presents the LLM-Agent-Controller, a multi-agent large language model (LLM) system developed to address a wide range of problems in control engineering (Control Theory). The system integrates a central controller agent with multiple specialized auxiliary agents, responsible for tasks such as controller design, model representation, control analysis, time-domain response, and simulation. A supervisor oversees high-level decision-making and workflow coordination, enhancing the system's reliability and efficiency. The LLM-Agent-Controller incorporates advanced capabilities, including Retrieval-Augmented Generation (RAG), Chain-of-Thought reasoning, self-criticism and correction, efficient memory handling, and user-friendly natural language communication. It is designed to function without requiring users to have prior knowledge of Control Theory, enabling them to input problems in plain language and receive complete, real-time solutions. To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole. We test five categories of Control Theory problems and benchmark performance across three advanced LLMs. Additionally, we conduct a comprehensive qualitative conversational analysis covering all key services. Results show that the LLM-Agent-Controller successfully solved 83% of general tasks, with individual agents achieving an average success rate of 87%. Performance improved with more advanced LLMs. This research demonstrates the potential of multi-agent LLM architectures to solve complex, domain-specific problems. By integrating specialized agents, supervisory control, and advanced reasoning, the LLM-Agent-Controller offers a scalable, robust, and accessible solution framework that can be extended to various technical domains.", "source": "arxiv", "arxiv_id": "2505.19567v1", "pdf_url": "https://arxiv.org/pdf/2505.19567v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-26T06:30:13Z", "updated": "2025-05-26T06:30:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems", "authors": ["Venkata Sai Aswath Duvvuru", "Bohan Zhang", "Michael Vierhauser", "Ankit Agrawal"], "year": 2025, "url": "http://arxiv.org/abs/2501.11864v1", "abstract": "Thorough simulation testing is crucial for validating the correct behavior of small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including adverse weather conditions (such as wind, and fog), diverse settings (hilly terrain, or urban areas), and varying mission profiles (surveillance, tracking). While various sUAS simulation tools exist to support developers, the entire process of creating, executing, and analyzing simulation tests remains a largely manual and cumbersome task. Developers must identify test scenarios, set up the simulation environment, integrate the System under Test (SuT) with simulation tools, formulate mission plans, and collect and analyze results. These labor-intensive tasks limit the ability of developers to conduct exhaustive testing across a wide range of scenarios. To alleviate this problem, in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven framework, where multiple LLM agents collaborate to support the sUAS simulation testing process. This includes: (1) creating test scenarios that subject the SuT to unique environmental contexts; (2) preparing the simulation environment as per the test scenario; (3) generating diverse sUAS missions for the SuT to execute; and (4) analyzing simulation results and providing an interactive analytics interface. Further, the design of the framework is flexible for creating and testing scenarios for a variety of sUAS use cases, simulation tools, and SuT input requirements. We evaluated our approach by (a) conducting simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b) analyzing the performance of each agent, and (c) gathering feedback from sUAS developers. Our findings indicate that AutoSimTest significantly improves the efficiency and scope of the sUAS testing process, allowing for more comprehensive and varied scenario evaluations while reducing the manual effort.", "source": "arxiv", "arxiv_id": "2501.11864v1", "pdf_url": "https://arxiv.org/pdf/2501.11864v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-01-21T03:42:21Z", "updated": "2025-01-21T03:42:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-DSE: Searching Accelerator Parameters with LLM Agents", "authors": ["Hanyu Wang", "Xinrui Wu", "Zijian Ding", "Su Zheng", "Chengyue Wang", "Neha Prakriya", "Tony Nowatzki", "Yizhou Sun", "Jason Cong"], "year": 2025, "url": "http://arxiv.org/abs/2505.12188v3", "abstract": "Even though high-level synthesis (HLS) tools mitigate the challenges of programming domain-specific accelerators (DSAs) by raising the abstraction level, optimizing hardware directive parameters remains a significant hurdle. Existing heuristic and learning-based methods struggle with adaptability and sample efficiency. We present LLM-DSE, a multi-agent framework designed specifically for optimizing HLS directives. Combining LLM with design space exploration (DSE), our explorer coordinates four agents: Router, Specialists, Arbitrator, and Critic. These multi-agent components interact with various tools to accelerate the optimization process. LLM-DSE leverages essential domain knowledge to identify efficient parameter combinations while maintaining adaptability through verbal learning from online interactions. Evaluations on the HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\\times$ performance gains over state-of-the-art methods, uncovering novel designs while reducing runtime. Ablation studies validate the effectiveness and necessity of the proposed agent interactions. Our code is open-sourced here: https://github.com/Nozidoali/LLM-DSE.", "source": "arxiv", "arxiv_id": "2505.12188v3", "pdf_url": "https://arxiv.org/pdf/2505.12188v3", "categories": ["cs.AR", "cs.AI"], "primary_category": "cs.AR", "doi": "", "venue": "", "published": "2025-05-18T01:31:42Z", "updated": "2025-11-21T00:44:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-Driven Transient Stability Assessment: From Automated Simulation to Neural Architecture Design", "authors": ["Lianzhe Hu", "Yu Wang", "Bikash Pal"], "year": 2025, "url": "http://arxiv.org/abs/2511.20276v1", "abstract": "This paper presents an LLM-driven, end-to-end workflow that addresses the lack of automation and intelligence in power system transient stability assessment (TSA). The proposed agentic framework integrates large language models (LLMs) with a professional simulator (ANDES) to automatically generate and filter disturbance scenarios from natural language, and employs an LLM-driven Neural Network Design (LLM-NND) pipeline to autonomously design and optimize TSA models through performance-guided, closed-loop feedback. On the IEEE 39-bus system, the LLM-NND models achieve 93.71% test accuracy on four-class TSA with only 4.78M parameters, while maintaining real-time inference latency (less than 0.95 ms per sample). Compared with a manually designed DenseNet (25.9M parameters, 80.05% accuracy), the proposed approach jointly improves accuracy and efficiency. Ablation studies confirm that the synergy among domain-grounded retrieval, reasoning augmentation, and feedback mechanisms is essential for robust automation. The results demonstrate that LLM agents can reliably accelerate TSA research from scenario generation and data acquisition to model design and interpretation, offering a scalable paradigm that is readily extensible to other power system tasks such as optimal power flow, fault analysis, and market operations.", "source": "arxiv", "arxiv_id": "2511.20276v1", "pdf_url": "https://arxiv.org/pdf/2511.20276v1", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2025-11-25T13:05:02Z", "updated": "2025-11-25T13:05:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection", "authors": ["Mohamed Bal-Ghaoui", "Fayssal Sabri"], "year": 2025, "url": "http://arxiv.org/abs/2510.05935v1", "abstract": "High-dimensional data remains a pervasive challenge in machine learning, often undermining model interpretability and computational efficiency. While Large Language Models (LLMs) have shown promise for dimensionality reduction through feature selection, existing LLM-based approaches frequently lack structured reasoning and transparent justification for their decisions. This paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for interpretable and robust feature selection. The system orchestrates a deliberative \"debate\" among multiple LLM agents, each assigned a specific role, enabling collective evaluation of feature relevance and generation of detailed justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance against strong baselines, including LLM-Select and traditional methods such as PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves superior or comparable classification performance while reducing downstream training time by an average of 46% (statistically significant improvement, p = 0.028 for XGBoost). These findings highlight that the proposed deliberative architecture enhances both decision transparency and computational efficiency, establishing LLM-FS-Agent as a practical and reliable solution for real-world applications.", "source": "arxiv", "arxiv_id": "2510.05935v1", "pdf_url": "https://arxiv.org/pdf/2510.05935v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-07T13:46:06Z", "updated": "2025-10-07T13:46:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-Guided Reinforcement Learning with Representative Agents for Traffic Modeling", "authors": ["Hanlin Sun", "Jiayang Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.06260v1", "abstract": "Large language models (LLMs) are increasingly used as behavioral proxies for self-interested travelers in agent-based traffic models. Although more flexible and generalizable than conventional models, the practical use of these approaches remains limited by scalability due to the cost of calling one LLM for every traveler. Moreover, it has been found that LLM agents often make opaque choices and produce unstable day-to-day dynamics. To address these challenges, we propose to model each homogeneous traveler group facing the same decision context with a single representative LLM agent who behaves like the population's average, maintaining and updating a mixed strategy over routes that coincides with the group's aggregate flow proportions. Each day, the LLM reviews the travel experience and flags routes with positive reinforcement that they hope to use more often, and an interpretable update rule then converts this judgment into strategy adjustments using a tunable (progressively decaying) step size. The representative-agent design improves scalability, while the separation of reasoning from updating clarifies the decision logic while stabilizing learning. In classic traffic assignment settings, we find that the proposed approach converges rapidly to the user equilibrium. In richer settings with income heterogeneity, multi-criteria costs, and multi-modal choices, the generated dynamics remain stable and interpretable, reproducing plausible behavioral patterns well-documented in psychology and economics, for example, the decoy effect in toll versus non-toll road selection, and higher willingness-to-pay for convenience among higher-income travelers when choosing between driving, transit, and park-and-ride options.", "source": "arxiv", "arxiv_id": "2511.06260v1", "pdf_url": "https://arxiv.org/pdf/2511.06260v1", "categories": ["cs.GT", "cs.AI", "eess.SY"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2025-11-09T07:36:46Z", "updated": "2025-11-09T07:36:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM/Agent-as-Data-Analyst: A Survey", "authors": ["Zirui Tang", "Weizheng Wang", "Zihang Zhou", "Yang Jiao", "Bangrui Xu", "Boyu Niu", "Dayou Zhou", "Xuanhe Zhou", "Guoliang Li", "Yeye He", "Wei Zhou", "Yitong Song", "Cheng Tan", "Xue Yang", "Chunwei Liu", "Bin Wang", "Conghui He", "Xiaoyang Wang", "Fan Wu"], "year": 2025, "url": "http://arxiv.org/abs/2509.23988v3", "abstract": "Large language models (LLMs) and agent techniques have brought a fundamental shift in the functionality and development paradigm of data analysis tasks (a.k.a LLM/Agent-as-Data-Analyst), demonstrating substantial impact across both academia and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., NL2SQL, NL2GQL, ModelQA), (ii) semi-structured data (e.g., markup languages understanding, semi-structured table question answering), (iii) unstructured data (e.g., chart understanding, text/image document understanding), and (iv) heterogeneous data (e.g., data retrieval and modality alignment in data lakes). The technical evolution further distills four key design goals for intelligent data analysis agents, namely semantic-aware design, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.", "source": "arxiv", "arxiv_id": "2509.23988v3", "pdf_url": "https://arxiv.org/pdf/2509.23988v3", "categories": ["cs.AI", "cs.DB"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-28T17:31:38Z", "updated": "2025-10-27T02:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs", "authors": ["Bing Hao", "Minglai Shao", "Zengyi Wo", "Yunlong Chu", "Yuhang Liu", "Ruijie Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.22266v1", "abstract": "The widespread application of Large Language Models (LLMs) has motivated a growing interest in their capacity for processing dynamic graphs. Temporal motifs, as an elementary unit and important local property of dynamic graphs which can directly reflect anomalies and unique phenomena, are essential for understanding their evolutionary dynamics and structural features. However, leveraging LLMs for temporal motif analysis on dynamic graphs remains relatively unexplored. In this paper, we systematically study LLM performance on temporal motif-related tasks. Specifically, we propose a comprehensive benchmark, LLMTM (Large Language Models in Temporal Motifs), which includes six tailored tasks across nine temporal motif types. We then conduct extensive experiments to analyze the impacts of different prompting techniques and LLMs (including nine models: openPangu-7B, the DeepSeek-R1-Distill-Qwen series, Qwen2.5-32B-Instruct, GPT-4o-mini, DeepSeek-R1, and o3) on model performance. Informed by our benchmark findings, we develop a tool-augmented LLM agent that leverages precisely engineered prompts to solve these tasks with high accuracy. Nevertheless, the high accuracy of the agent incurs a substantial cost. To address this trade-off, we propose a simple yet effective structure-aware dispatcher that considers both the dynamic graph's structural properties and the LLM's cognitive load to intelligently dispatch queries between the standard LLM prompting and the more powerful agent. Our experiments demonstrate that the structure-aware dispatcher effectively maintains high accuracy while reducing cost.", "source": "arxiv", "arxiv_id": "2512.22266v1", "pdf_url": "https://arxiv.org/pdf/2512.22266v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-24T18:10:29Z", "updated": "2025-12-24T18:10:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs", "authors": ["Tom Pawelek", "Raj Patel", "Charlotte Crowell", "Noorbakhsh Amiri", "Sudip Mittal", "Shahram Rahimi", "Andy Perkins"], "year": 2025, "url": "http://arxiv.org/abs/2509.18557v1", "abstract": "Compared to traditional models, agentic AI represents a highly valuable target for potential attackers as they possess privileged access to data sources and API tools, which are traditionally not incorporated into classical agents. Unlike a typical software application residing in a Demilitarized Zone (DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI (only defining a final goal, leaving the path selection to LLM). This characteristic introduces substantial security risk to both operational security and information security. Most common existing defense mechanism rely on detection of malicious intent and preventing it from reaching the LLM agent, thus protecting against jailbreak attacks such as prompt injection. In this paper, we present an alternative approach, LLMZ+, which moves beyond traditional detection-based approaches by implementing prompt whitelisting. Through this method, only contextually appropriate and safe messages are permitted to interact with the agentic LLM. By leveraging the specificity of context, LLMZ+ guarantees that all exchanges between external users and the LLM conform to predefined use cases and operational boundaries. Our approach streamlines the security framework, enhances its long-term resilience, and reduces the resources required for sustaining LLM information security. Our empirical evaluation demonstrates that LLMZ+ provides strong resilience against the most common jailbreak prompts. At the same time, legitimate business communications are not disrupted, and authorized traffic flows seamlessly between users and the agentic LLM. We measure the effectiveness of approach using false positive and false negative rates, both of which can be reduced to 0 in our experimental setting.", "source": "arxiv", "arxiv_id": "2509.18557v1", "pdf_url": "https://arxiv.org/pdf/2509.18557v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-23T02:30:14Z", "updated": "2025-09-23T02:30:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMind 2.0: Distributed IoT Automation with Natural Language M2M Communication and Lightweight LLM Agents", "authors": ["Yuyang Du", "Qun Yang", "Liujianfu Wang", "Jingqi Lin", "Hongwei Cui", "Soung Chang Liew"], "year": 2025, "url": "http://arxiv.org/abs/2508.13920v1", "abstract": "Recent advances in large language models (LLMs) have sparked interest in their application to IoT and automation systems, particularly for facilitating device management through natural language instructions. However, existing centralized approaches face significant scalability challenges when managing and coordinating the collaboration between IoT devices of diverse capabilities in large-scale heterogeneous IoT systems. This paper introduces LLMind 2.0, a distributed IoT automation framework that addresses the scalability challenges through lightweight LLM-empowered device agents via natural language-based machine-to-machine (M2M) communication. Unlike previous LLM-controlled automation systems that rely on a centralized coordinator to generate device-specific code to be executed on individual devices, LLMind 2.0 distributes intelligence across individual devices through lightweight LLMs embedded in IoT devices. The central coordinator translates human instructions into simple subtasks described in natural human language, which are then processed by device-specific agents to generate device-specific code locally at the associated devices. This approach transcends device heterogeneity barriers by using natural language as a unified communication medium, enabling seamless collaboration between devices from different manufacturers. The system incorporates several key innovations: a Retrieval-Augmented Generation (RAG) mechanism for accurate subtask-to-API mapping, fine-tuned lightweight LLMs for reliable code generation, and a finite state machine-based task execution framework. Experimental validation in multi-robot warehouse scenarios and real-world WiFi network deployments demonstrates significant improvements in scalability, reliability, and privacy protection compared to the centralized approach.", "source": "arxiv", "arxiv_id": "2508.13920v1", "pdf_url": "https://arxiv.org/pdf/2508.13920v1", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2025-08-19T15:17:31Z", "updated": "2025-08-19T15:17:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities", "authors": ["Thomas Schmied", "JÃ¶rg Bornschein", "Jordi Grau-Moya", "Markus Wulfmeier", "Razvan Pascanu"], "year": 2025, "url": "http://arxiv.org/abs/2504.16078v1", "abstract": "The success of Large Language Models (LLMs) has sparked interest in various agentic applications. A key hypothesis is that LLMs, leveraging common sense and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently solve complex domains. However, LLM agents have been found to suffer from sub-optimal exploration and the knowing-doing gap, the inability to effectively act on knowledge present in the model. In this work, we systematically study why LLMs perform sub-optimally in decision-making scenarios. In particular, we closely examine three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. We propose mitigation of these shortcomings by fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales. Our experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making abilities of LLMs by increasing exploration and narrowing the knowing-doing gap. Finally, we study both classic exploration mechanisms, such as $Îµ$-greedy, and LLM-specific approaches, such as self-correction and self-consistency, to enable more effective fine-tuning of LLMs for decision-making.", "source": "arxiv", "arxiv_id": "2504.16078v1", "pdf_url": "https://arxiv.org/pdf/2504.16078v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-22T17:57:14Z", "updated": "2025-04-22T17:57:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMs are Introvert", "authors": ["Litian Zhang", "Xiaoming Zhang", "Bingyu Yan", "Ziyi Zhou", "Bo Zhang", "Zhenyu Guan", "Xi Zhang", "Chaozhuo Li"], "year": 2025, "url": "http://arxiv.org/abs/2507.05638v1", "abstract": "The exponential growth of social media and generative AI has transformed information dissemination, fostering connectivity but also accelerating the spread of misinformation. Understanding information propagation dynamics and developing effective control strategies is essential to mitigate harmful content. Traditional models, such as SIR, provide basic insights but inadequately capture the complexities of online interactions. Advanced methods, including attention mechanisms and graph neural networks, enhance accuracy but typically overlook user psychology and behavioral dynamics. Large language models (LLMs), with their human-like reasoning, offer new potential for simulating psychological aspects of information spread. We introduce an LLM-based simulation environment capturing agents' evolving attitudes, emotions, and responses. Initial experiments, however, revealed significant gaps between LLM-generated behaviors and authentic human dynamics, especially in stance detection and psychological realism. A detailed evaluation through Social Information Processing Theory identified major discrepancies in goal-setting and feedback evaluation, stemming from the lack of emotional processing in standard LLM training. To address these issues, we propose the Social Information Processing-based Chain of Thought (SIP-CoT) mechanism enhanced by emotion-guided memory. This method improves the interpretation of social cues, personalization of goals, and evaluation of feedback. Experimental results confirm that SIP-CoT-enhanced LLM agents more effectively process social information, demonstrating behaviors, attitudes, and emotions closer to real human interactions. In summary, this research highlights critical limitations in current LLM-based propagation simulations and demonstrates how integrating SIP-CoT and emotional memory significantly enhances the social intelligence and realism of LLM agents.", "source": "arxiv", "arxiv_id": "2507.05638v1", "pdf_url": "https://arxiv.org/pdf/2507.05638v1", "categories": ["cs.AI", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-08T03:32:38Z", "updated": "2025-07-08T03:32:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMs as Firmware Experts: A Runtime-Grown Tree-of-Agents Framework", "authors": ["Xiangrui Zhang", "Zeyu Chen", "Haining Wang", "Qiang Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.18438v1", "abstract": "Large Language Models (LLMs) and their agent systems have recently demonstrated strong potential in automating code reasoning and vulnerability detection. However, when applied to large-scale firmware, their performance degrades due to the binary nature of firmware, complex dependency structures, and heterogeneous components. To address this challenge, this paper presents FIRMHIVE, a recursive agent hive that enables LLMs to act as autonomous firmware security analysts. FIRMHIVE introduces two key mechanisms: (1) transforming delegation into a per-agent, executable primitive and (2) constructing a runtime Tree of Agents (ToA) for decentralized coordination. We evaluate FIRMHIVE using real-world firmware images obtained from publicly available datasets, covering five representative security analysis tasks. Compared with existing LLM-agent baselines, FIRMHIVE performs deeper (about 16x more reasoning steps) and broader (about 2.3x more files inspected) cross-file exploration, resulting in about 5.6x more alerts per firmware. Compared to state-of-the-art (SOTA) security tools, FIRMHIVE identifies about 1.5x more vulnerabilities (1,802 total) and achieves 71% precision, representing significant improvements in both yield and fidelity.", "source": "arxiv", "arxiv_id": "2511.18438v1", "pdf_url": "https://arxiv.org/pdf/2511.18438v1", "categories": ["cs.CR", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-23T13:19:40Z", "updated": "2025-11-23T13:19:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?", "authors": ["Rushil Gupta", "Jason Hartford", "Bang Liu"], "year": 2025, "url": "http://arxiv.org/abs/2509.21403v1", "abstract": "Large language models (LLMs) have recently been proposed as general-purpose agents for experimental design, with claims that they can perform in-context experimental design. We evaluate this hypothesis using both open- and closed-source instruction-tuned LLMs applied to genetic perturbation and molecular property discovery tasks. We find that LLM-based agents show no sensitivity to experimental feedback: replacing true outcomes with randomly permuted labels has no impact on performance. Across benchmarks, classical methods such as linear bandits and Gaussian process optimization consistently outperform LLM agents. We further propose a simple hybrid method, LLM-guided Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with nearest-neighbor sampling to guide the design of experiments. LLMNN achieves competitive or superior performance across domains without requiring significant in-context adaptation. These results suggest that current open- and closed-source LLMs do not perform in-context experimental design in practice and highlight the need for hybrid frameworks that decouple prior-based reasoning from batch acquisition with updated posteriors.", "source": "arxiv", "arxiv_id": "2509.21403v1", "pdf_url": "https://arxiv.org/pdf/2509.21403v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-24T15:50:17Z", "updated": "2025-09-24T15:50:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMs syntactically adapt their language use to their conversational partner", "authors": ["Florian Kandra", "Vera Demberg", "Alexander Koller"], "year": 2025, "url": "http://arxiv.org/abs/2503.07457v2", "abstract": "It has been frequently observed that human speakers align their language use with each other during conversations. In this paper, we study empirically whether large language models (LLMs) exhibit the same behavior of conversational adaptation. We construct a corpus of conversations between LLMs and find that two LLM agents end up making more similar syntactic choices as conversations go on, confirming that modern LLMs adapt their language use to their conversational partners in at least a rudimentary way.", "source": "arxiv", "arxiv_id": "2503.07457v2", "pdf_url": "https://arxiv.org/pdf/2503.07457v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-10T15:37:07Z", "updated": "2025-07-22T08:24:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research", "authors": ["Shuo Yan", "Ruochen Li", "Ziming Luo", "Zimu Wang", "Daoyang Li", "Liqiang Jing", "Kaiyu He", "Peilin Wu", "George Michalopoulos", "Yue Zhang", "Ziyang Zhang", "Mian Zhang", "Zhiyu Chen", "Xinya Du"], "year": 2025, "url": "http://arxiv.org/abs/2506.17335v1", "abstract": "Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of reproducing code from research papers, especially in the NLP domain, remains underexplored. This task includes unique complex reasoning challenges in the intellectual synthesis of abstract concepts and the comprehension of code repositories with interdependent files. Motivated by this gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the capability of LLM agents on code reproduction from Language Modeling Research. It consists of 28 code reproduction tasks derived from 23 research papers published in top-tier NLP venues over the past five years, spanning nine fundamental categories. Models are provided with a research paper, a code repository containing one or more masked functions, and instructions for implementing these functions. We conduct extensive experiments in standard prompting and LLM agent settings with state-of-the-art LLMs, evaluating the accuracy of unit tests and performing LLM-based evaluation of code correctness. Experimental results reveal that even the most advanced models still exhibit persistent limitations in scientific reasoning and code synthesis, highlighting critical gaps in LLM agents' ability to autonomously reproduce scientific research", "source": "arxiv", "arxiv_id": "2506.17335v1", "pdf_url": "https://arxiv.org/pdf/2506.17335v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-06-19T07:04:16Z", "updated": "2025-06-19T07:04:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LTDA-Drive: LLMs-guided Generative Models based Long-tail Data Augmentation for Autonomous Driving", "authors": ["Mahmut Yurt", "Xin Ye", "Yunsheng Ma", "Jingru Luo", "Abhirup Mallik", "John Pauly", "Burhaneddin Yaman", "Liu Ren"], "year": 2025, "url": "http://arxiv.org/abs/2505.18198v1", "abstract": "3D perception plays an essential role for improving the safety and performance of autonomous driving. Yet, existing models trained on real-world datasets, which naturally exhibit long-tail distributions, tend to underperform on rare and safety-critical, vulnerable classes, such as pedestrians and cyclists. Existing studies on reweighting and resampling techniques struggle with the scarcity and limited diversity within tail classes. To address these limitations, we introduce LTDA-Drive, a novel LLM-guided data augmentation framework designed to synthesize diverse, high-quality long-tail samples. LTDA-Drive replaces head-class objects in driving scenes with tail-class objects through a three-stage process: (1) text-guided diffusion models remove head-class objects, (2) generative models insert instances of the tail classes, and (3) an LLM agent filters out low-quality synthesized images. Experiments conducted on the KITTI dataset show that LTDA-Drive significantly improves tail-class detection, achieving 34.75\\% improvement for rare classes over counterpart methods. These results further highlight the effectiveness of LTDA-Drive in tackling long-tail challenges by generating high-quality and diverse data.", "source": "arxiv", "arxiv_id": "2505.18198v1", "pdf_url": "https://arxiv.org/pdf/2505.18198v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-05-21T05:14:11Z", "updated": "2025-05-21T05:14:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents", "authors": ["Taro Yano", "Yoichi Ishibashi", "Masafumi Oyamada"], "year": 2025, "url": "http://arxiv.org/abs/2505.21963v1", "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks. To further tailor LLMs to specific domains or applications, post-training techniques such as Supervised Fine-Tuning (SFT), Preference Learning, and model merging are commonly employed. While each of these methods has been extensively studied in isolation, the automated construction of complete post-training pipelines remains an underexplored area. Existing approaches typically rely on manual design or focus narrowly on optimizing individual components, such as data ordering or merging strategies. In this work, we introduce LaMDAgent (short for Language Model Developing Agent), a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents. LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention. Our experiments show that LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities. Moreover, it uncovers effective post-training strategies that are often overlooked by conventional human-driven exploration. We further analyze the impact of data and model size scaling to reduce computational costs on the exploration, finding that model size scalings introduces new challenges, whereas scaling data size enables cost-effective pipeline discovery.", "source": "arxiv", "arxiv_id": "2505.21963v1", "pdf_url": "https://arxiv.org/pdf/2505.21963v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-28T04:30:51Z", "updated": "2025-05-28T04:30:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Language Model Agents Under Attack: A Cross Model-Benchmark of Profit-Seeking Behaviors in Customer Service", "authors": ["Jingyu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2512.24415v1", "abstract": "Customer-service LLM agents increasingly make policy-bound decisions (refunds, rebooking, billing disputes), but the same ``helpful'' interaction style can be exploited: a small fraction of users can induce unauthorized concessions, shifting costs to others and eroding trust in agentic workflows. We present a cross-domain benchmark of profit-seeking direct prompt injection in customer-service interactions, spanning 10 service domains and 100 realistic attack scripts grouped into five technique families. Across five widely used models under a unified rubric with uncertainty reporting, attacks are highly domain-dependent (airline support is most exploitable) and technique-dependent (payload splitting is most consistently effective). We release data and evaluation code to support reproducible auditing and to inform the design of oversight and recovery workflows for trustworthy, human centered agent interfaces.", "source": "arxiv", "arxiv_id": "2512.24415v1", "pdf_url": "https://arxiv.org/pdf/2512.24415v1", "categories": ["cs.CR", "cs.HC"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-30T18:57:52Z", "updated": "2025-12-30T18:57:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing", "authors": ["Manish Bhatt", "Adrian Wood", "Idan Habler", "Ammar Al-Kahfah"], "year": 2025, "url": "http://arxiv.org/abs/2601.00042v2", "abstract": "Production LLM agents with tool-using capabilities require security testing despite their safety training. We adapt Go-Explore to evaluate GPT-4o-mini across 28 experimental runs spanning six research questions. We find that random-seed variance dominates algorithmic parameters, yielding an 8x spread in outcomes; single-seed comparisons are unreliable, while multi-seed averaging materially reduces variance in our setup. Reward shaping consistently harms performance, causing exploration collapse in 94% of runs or producing 18 false positives with zero verified attacks. In our environment, simple state signatures outperform complex ones. For comprehensive security testing, ensembles provide attack-type diversity, whereas single agents optimize coverage within a given attack type. Overall, these results suggest that seed variance and targeted domain knowledge can outweigh algorithmic sophistication when testing safety-trained models.", "source": "arxiv", "arxiv_id": "2601.00042v2", "pdf_url": "https://arxiv.org/pdf/2601.00042v2", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-31T03:38:38Z", "updated": "2026-01-06T16:35:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agent Personality and Response Appropriateness: Evaluation by Human Linguistic Experts, LLM-as-Judge, and Natural Language Processing Model", "authors": ["Eswari Jayakumar", "Niladri Sekhar Dash", "Debasmita Mukherjee"], "year": 2025, "url": "http://arxiv.org/abs/2510.23875v1", "abstract": "While Large Language Model (LLM)-based agents can be used to create highly engaging interactive applications through prompting personality traits and contextual data, effectively assessing their personalities has proven challenging. This novel interdisciplinary approach addresses this gap by combining agent development and linguistic analysis to assess the prompted personality of LLM-based agents in a poetry explanation task. We developed a novel, flexible question bank, informed by linguistic assessment criteria and human cognitive learning levels, offering a more comprehensive evaluation than current methods. By evaluating agent responses with natural language processing models, other LLMs, and human experts, our findings illustrate the limitations of purely deep learning solutions and emphasize the critical role of interdisciplinary design in agent development.", "source": "arxiv", "arxiv_id": "2510.23875v1", "pdf_url": "https://arxiv.org/pdf/2510.23875v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-27T21:30:12Z", "updated": "2025-10-27T21:30:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agent for Modular Task Execution in Drug Discovery", "authors": ["Janghoon Ock", "Radheesh Sharma Meda", "Srivathsan Badrinarayanan", "Neha S. Aluru", "Achuth Chandrasekhar", "Amir Barati Farimani"], "year": 2025, "url": "http://arxiv.org/abs/2507.02925v3", "abstract": "We present a modular framework powered by large language models (LLMs) that automates and streamlines key tasks across the early-stage computational drug discovery pipeline. By combining LLM reasoning with domain-specific tools, the framework performs biomedical data retrieval, literature-grounded question answering via retrieval-augmented generation, molecular generation, multi-property prediction, property-aware molecular refinement, and 3D protein-ligand structure generation. The agent autonomously retrieved relevant biomolecular information, including FASTA sequences, SMILES representations, and literature, and answered mechanistic questions with improved contextual accuracy compared to standard LLMs. It then generated chemically diverse seed molecules and predicted 75 properties, including ADMET-related and general physicochemical descriptors, which guided iterative molecular refinement. Across two refinement rounds, the number of molecules with QED > 0.6 increased from 34 to 55. The number of molecules satisfying empirical drug-likeness filters also rose; for example, compliance with the Ghose filter increased from 32 to 55 within a pool of 100 molecules. The framework also employed Boltz-2 to generate 3D protein-ligand complexes and provide rapid binding affinity estimates for candidate compounds. These results demonstrate that the approach effectively supports molecular screening, prioritization, and structure evaluation. Its modular design enables flexible integration of evolving tools and models, providing a scalable foundation for AI-assisted therapeutic discovery.", "source": "arxiv", "arxiv_id": "2507.02925v3", "pdf_url": "https://arxiv.org/pdf/2507.02925v3", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-26T00:19:01Z", "updated": "2025-12-12T03:52:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation", "authors": ["Xin Zhang", "Lissette Iturburu", "Juan Nicolas Villamizar", "Xiaoyu Liu", "Manuel Salmeron", "Shirley J. Dyke", "Julio Ramirez"], "year": 2025, "url": "http://arxiv.org/abs/2507.19771v1", "abstract": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model (LLM) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way.", "source": "arxiv", "arxiv_id": "2507.19771v1", "pdf_url": "https://arxiv.org/pdf/2507.19771v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-07-26T03:47:12Z", "updated": "2025-07-26T03:47:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agents Enable Autonomous Design and Image Analysis of Microwell Microfluidics", "authors": ["Dinh-Nguyen Nguyen", "Sadia Shakil", "Raymond Kai-Yu Tong", "Ngoc-Duy Dinh"], "year": 2025, "url": "http://arxiv.org/abs/2510.13883v1", "abstract": "Microwell microfluidics has been utilized for single-cell analysis to reveal heterogeneity in gene expression, signaling pathways, and phenotypic responses for identifying rare cell types, understanding disease progression, and developing more precise therapeutic strategies. However, designing microwell microfluidics is a considerably complex task, requiring knowledge, experience, and CAD software, as well as manual intervention, which often fails initial designs, demanding multiple costly and time-consuming iterations. In this study, we establish an autonomous large language model (LLM)-driven microwell design framework to generate code-based computer-aided design (CAD) scripts, that enables the rapid and reproducible creation of microwells with diverse geometries and imaging-based analysis. We propose a multimodal large language model (MLLM)-logistic regression framework based on integrating high-level semantic descriptions generated by MLLMs with image embeddings for image classification tasks, aiming to identify microwell occupancy and microwell shape. The fused multimodal representation is input to a logistic regression model, which is both interpretable and computationally efficient. We achieved significant improvements, exceeding 0.92 for occupancy classification and 0.99 for shape classification, across all evaluated MLLMs, compared with 0.50 and 0.55, respectively, when relying solely on direct classification. The MLLM-logistic regression framework is a scalable, efficient solution for high-throughput microwell image analysis. Our study demonstrates an autonomous design microwell platform by translating natural language prompts into optimized device geometries, CAD scripts and image analysis, facilitating the development of next-generation digital discovery by integration of literature mining, autonomous design and experimental data analysis.", "source": "arxiv", "arxiv_id": "2510.13883v1", "pdf_url": "https://arxiv.org/pdf/2510.13883v1", "categories": ["q-bio.NC", "cs.MA"], "primary_category": "q-bio.NC", "doi": "", "venue": "", "published": "2025-10-14T01:32:48Z", "updated": "2025-10-14T01:32:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agents for Radio Map Generation and Wireless Network Planning", "authors": ["Hongye Quan", "Wanli Ni", "Tong Zhang", "Xiangyu Ye", "Ziyi Xie", "Shuai Wang", "Yuanwei Liu", "Hui Song"], "year": 2025, "url": "http://arxiv.org/abs/2501.11283v2", "abstract": "Using commercial software for radio map generation and wireless network planning often require complex manual operations, posing significant challenges in terms of scalability, adaptability, and user-friendliness, due to heavy manual operations. To address these issues, we propose an automated solution that employs large language model (LLM) agents. These agents are designed to autonomously generate radio maps and facilitate wireless network planning for specified areas, thereby minimizing the necessity for extensive manual intervention. To validate the effectiveness of our proposed solution, we develop a software platform that integrates LLM agents. Experimental results demonstrate that a large amount manual operations can be saved via the proposed LLM agent, and the automated solutions can achieve an enhanced coverage and signal-to-interference-noise ratio (SINR), especially in urban environments.", "source": "arxiv", "arxiv_id": "2501.11283v2", "pdf_url": "https://arxiv.org/pdf/2501.11283v2", "categories": ["cs.IT"], "primary_category": "cs.IT", "doi": "", "venue": "", "published": "2025-01-20T05:34:38Z", "updated": "2025-02-13T12:48:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Powered Automated Modeling and Optimization of Active Distribution Network Dispatch Problems", "authors": ["Xu Yang", "Chenhui Lin", "Yue Yang", "Qi Wang", "Haotian Liu", "Haizhou Hua", "Wenchuan Wu"], "year": 2025, "url": "http://arxiv.org/abs/2507.21162v1", "abstract": "The increasing penetration of distributed energy resources into active distribution networks (ADNs) has made effective ADN dispatch imperative. However, the numerous newly-integrated ADN operators, such as distribution system aggregators, virtual power plant managers, and end prosumers, often lack specialized expertise in power system operation, modeling, optimization, and programming. This knowledge gap renders reliance on human experts both costly and time-intensive. To address this challenge and enable intelligent, flexible ADN dispatch, this paper proposes a large language model (LLM) powered automated modeling and optimization approach. First, the ADN dispatch problems are decomposed into sequential stages, and a multi-LLM coordination architecture is designed. This framework comprises an Information Extractor, a Problem Formulator, and a Code Programmer, tasked with information retrieval, optimization problem formulation, and code implementation, respectively. Afterwards, tailored refinement techniques are developed for each LLM agent, greatly improving the accuracy and reliability of generated content. The proposed approach features a user-centric interface that enables ADN operators to derive dispatch strategies via simple natural language queries, eliminating technical barriers and increasing efficiency. Comprehensive comparisons and end-to-end demonstrations on various test cases validate the effectiveness of the proposed architecture and methods.", "source": "arxiv", "arxiv_id": "2507.21162v1", "pdf_url": "https://arxiv.org/pdf/2507.21162v1", "categories": ["cs.AI", "cs.LG", "eess.SY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-25T07:46:25Z", "updated": "2025-07-25T07:46:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications", "authors": ["Jindong Han", "Yansong Ning", "Zirui Yuan", "Hang Ni", "Fan Liu", "Tengfei Lyu", "Hao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2507.00914v1", "abstract": "The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.", "source": "arxiv", "arxiv_id": "2507.00914v1", "pdf_url": "https://arxiv.org/pdf/2507.00914v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-07-01T16:18:29Z", "updated": "2025-07-01T16:18:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model-Powered Agent for C to Rust Code Translation", "authors": ["HoHyun Sim", "Hyeonjoong Cho", "Yeonghyeon Go", "Zhoulai Fu", "Ali Shokri", "Binoy Ravindran"], "year": 2025, "url": "http://arxiv.org/abs/2505.15858v2", "abstract": "The C programming language has been foundational in building system-level software. However, its manual memory management model frequently leads to memory safety issues. In response, a modern system programming language, Rust, has emerged as a memory-safe alternative. Moreover, automating the C-to-Rust translation empowered by the rapid advancements of the generative capabilities of LLMs is gaining growing interest for large volumes of legacy C code. Despite some success, existing LLM-based approaches have constrained the role of LLMs to static prompt-response behavior and have not explored their agentic problem-solving capability. Applying the LLM agentic capability for the C-to-Rust translation introduces distinct challenges, as this task differs from the traditional LLM agent applications, such as math or commonsense QA domains. First, the scarcity of parallel C-to-Rust datasets hinders the retrieval of suitable code translation exemplars for in-context learning. Second, unlike math or commonsense QA, the intermediate steps required for C-to-Rust are not well-defined. Third, it remains unclear how to organize and cascade these intermediate steps to construct a correct translation trajectory. To address these challenges in the C-to-Rust translation, we propose a novel intermediate step, the Virtual Fuzzing-based equivalence Test (VFT), and an agentic planning framework, the LLM-powered Agent for C-to-Rust code translation (LAC2R). The VFT guides LLMs to identify input arguments that induce divergent behaviors between an original C function and its Rust counterpart and to generate informative diagnoses to refine the unsafe Rust code. LAC2R uses the MCTS to systematically organize the LLM-induced intermediate steps for correct translation. We experimentally demonstrated that LAC2R effectively conducts C-to-Rust translation on large-scale, real-world benchmarks.", "source": "arxiv", "arxiv_id": "2505.15858v2", "pdf_url": "https://arxiv.org/pdf/2505.15858v2", "categories": ["cs.PL", "cs.SE"], "primary_category": "cs.PL", "doi": "", "venue": "", "published": "2025-05-21T01:26:23Z", "updated": "2025-06-26T15:16:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Models are Autonomous Cyber Defenders", "authors": ["SebastiÃ¡n R. Castro", "Roberto Campbell", "Nancy Lau", "Octavio Villalobos", "Jiaqi Duan", "Alvaro A. Cardenas"], "year": 2025, "url": "http://arxiv.org/abs/2505.04843v2", "abstract": "Fast and effective incident response is essential to prevent adversarial cyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response through Artificial Intelligence (AI) agents that plan and execute actions. Most ACD approaches focus on single-agent scenarios and leverage Reinforcement Learning (RL). However, ACD RL-trained agents depend on costly training, and their reasoning is not always explainable or transferable. Large Language Models (LLMs) can address these concerns by providing explainable actions in general security contexts. Researchers have explored LLM agents for ACD but have not evaluated them on multi-agent scenarios or interacting with other ACD agents. In this paper, we show the first study on how LLMs perform in multi-agent ACD environments by proposing a new integration to the CybORG CAGE 4 environment. We examine how ACD teams of LLM and RL agents can interact by proposing a novel communication protocol. Our results highlight the strengths and weaknesses of LLMs and RL and help us identify promising research directions to create, train, and deploy future teams of ACD agents.", "source": "arxiv", "arxiv_id": "2505.04843v2", "pdf_url": "https://arxiv.org/pdf/2505.04843v2", "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.AI", "doi": "10.1109/CAI64502.2025.00195", "venue": "2025 IEEE Conference on Artificial Intelligence (CAI) - Pages: 1125-1132", "published": "2025-05-07T22:42:37Z", "updated": "2025-07-19T14:35:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program", "authors": ["Alejandro Carrasco", "Victor Rodriguez-Fernandez", "Richard Linares"], "year": 2025, "url": "http://arxiv.org/abs/2505.19896v1", "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Control in space, enabling LLMs to play a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. The project comprises several open repositories to facilitate replication and further research. The codebase is accessible on \\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models and datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging Face}. Additionally, experiment tracking and detailed results can be reviewed on \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases", "source": "arxiv", "arxiv_id": "2505.19896v1", "pdf_url": "https://arxiv.org/pdf/2505.19896v1", "categories": ["cs.AI", "astro-ph.IM", "cs.CL"], "primary_category": "cs.AI", "doi": "10.1016/j.asr.2025.06.034", "venue": "", "published": "2025-05-26T12:25:35Z", "updated": "2025-05-26T12:25:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Models as Visualization Agents for Immersive Binary Reverse Engineering", "authors": ["Dennis Brown", "Samuel Mulder"], "year": 2025, "url": "http://arxiv.org/abs/2508.13413v1", "abstract": "Immersive virtual reality (VR) offers affordances that may reduce cognitive complexity in binary reverse engineering (RE), enabling embodied and external cognition to augment the RE process through enhancing memory, hypothesis testing, and visual organization. In prior work, we applied a cognitive systems engineering approach to identify an initial set of affordances and implemented a VR environment to support RE through spatial persistence and interactivity. In this work, we extend that platform with an integrated large language model (LLM) agent capable of querying binary analysis tools, answering technical questions, and dynamically generating immersive 3D visualizations in alignment with analyst tasks. We describe the system architecture and our evaluation process and results. Our pilot study shows that while LLMs can generate meaningful 3D call graphs (for small programs) that align with design principles, output quality varies widely. This work raises open questions about the potential for LLMs to function as visualization agents, constructing 3D representations that reflect cognitive design principles without explicit training.", "source": "arxiv", "arxiv_id": "2508.13413v1", "pdf_url": "https://arxiv.org/pdf/2508.13413v1", "categories": ["cs.HC", "cs.SE"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-08-19T00:24:01Z", "updated": "2025-08-19T00:24:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Lark: Biologically Inspired Neuroevolution for Multi-Stakeholder LLM Agents", "authors": ["Dheeraj Chintapalli", "Rikhil Tanugula", "Sunkalp Chandra"], "year": 2025, "url": "http://arxiv.org/abs/2510.16978v2", "abstract": "We present Lark, a biologically inspired decision-making framework that couples LLM-driven reasoning with an evolutionary, stakeholder-aware Multi-Agent System (MAS). To address verbosity and stakeholder trade-offs, we integrate four mechanisms: (i) plasticity, which applies concise adjustments to candidate solutions; (ii) duplication and maturation, which copy high-performing candidates and specialize them into new modules; (iii) ranked-choice stakeholder aggregation using influence-weighted Borda scoring; and (iv) compute awareness via token-based penalties that reward brevity. The system iteratively proposes diverse strategies, applies plasticity tweaks, simulates stakeholder evaluations, aggregates preferences, selects top candidates, and performs duplication/maturation while factoring compute cost into final scores. In a controlled evaluation over 30 rounds comparing 14 systems, Lark Full achieves a mean rank of 2.55 (95% CI [2.17, 2.93]) and a mean composite score of 29.4/50 (95% CI [26.34, 32.46]), finishing Top-3 in 80% of rounds while remaining cost competitive with leading commercial models ($0.016 per task). Paired Wilcoxon tests confirm that all four mechanisms contribute significantly as ablating duplication/maturation yields the largest deficit (ÎScore = 3.5, Cohen's d_z = 2.53, p < 0.001), followed by plasticity (ÎScore = 3.4, d_z = 1.86), ranked-choice voting (ÎScore = 2.4, d_z = 1.20), and token penalties (ÎScore = 2.2, d_z = 1.63). Rather than a formal Markov Decision Process with constrained optimization, Lark is a practical, compute-aware neuroevolutionary loop that scales stakeholder-aligned strategy generation and makes trade-offs transparent through per-step metrics. Our work presents proof-of-concept findings and invites community feedback as we expand toward real-world validation studies.", "source": "arxiv", "arxiv_id": "2510.16978v2", "pdf_url": "https://arxiv.org/pdf/2510.16978v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-10-19T19:43:17Z", "updated": "2025-12-07T19:10:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Latent Collaboration in Multi-Agent Systems", "authors": ["Jiaru Zou", "Xiyuan Yang", "Ruizhong Qiu", "Gaotang Li", "Katherine Tieu", "Pan Lu", "Ke Shen", "Hanghang Tong", "Yejin Choi", "Jingrui He", "James Zou", "Mengdi Wang", "Ling Yang"], "year": 2025, "url": "http://arxiv.org/abs/2511.20639v2", "abstract": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.", "source": "arxiv", "arxiv_id": "2511.20639v2", "pdf_url": "https://arxiv.org/pdf/2511.20639v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-25T18:56:57Z", "updated": "2025-12-08T04:05:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Leading the Follower: Learning Persuasive Agents in Social Deduction Games", "authors": ["Zhang Zheng", "Deheng Ye", "Peilin Zhao", "Hao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.09087v1", "abstract": "Large language model (LLM) agents have shown remarkable progress in social deduction games (SDGs). However, existing approaches primarily focus on information processing and strategy selection, overlooking the significance of persuasive communication in influencing other players' beliefs and responses. In SDGs, success depends not only on making correct deductions but on convincing others to response in alignment with one's intent. To address this limitation, we formalize turn-based dialogue in SDGs as a Stackelberg competition, where the current player acts as the leader who strategically influences the follower's response. Building on this theoretical foundation, we propose a reinforcement learning framework that trains agents to optimize utterances for persuasive impact. Through comprehensive experiments across three diverse SDGs, we demonstrate that our agents significantly outperform baselines. This work represents a significant step toward developing AI agents capable of strategic social influence, with implications extending to scenarios requiring persuasive communication.", "source": "arxiv", "arxiv_id": "2510.09087v1", "pdf_url": "https://arxiv.org/pdf/2510.09087v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-10T07:33:30Z", "updated": "2025-10-10T07:33:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments", "authors": ["Xinran Li", "Chenjia Bai", "Zijian Li", "Jiakun Zheng", "Ting Xiao", "Jun Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.07232v1", "abstract": "Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments. However, despite LLMs' advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios. We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)} paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. At the team level, LLM agents collaboratively and iteratively maintain and update a shared cooperation knowledge list based on new experiences, using it to guide more effective communication. By combining individual learning with team evolution, LIET enables comprehensive and flexible adaptation for LLM agents. Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.", "source": "arxiv", "arxiv_id": "2506.07232v1", "pdf_url": "https://arxiv.org/pdf/2506.07232v1", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-08T17:32:03Z", "updated": "2025-06-08T17:32:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learning \"Partner-Aware\" Collaborators in Multi-Party Collaboration", "authors": ["Abhijnan Nath", "Nikhil Krishnaswamy"], "year": 2025, "url": "http://arxiv.org/abs/2510.22462v2", "abstract": "Large Language Models (LLMs) are increasingly being deployed in agentic settings where they act as collaborators with humans. Therefore, it is increasingly important to be able to evaluate their abilities to collaborate effectively in multi-turn, multi-party tasks. In this paper, we build on the AI alignment and safe interruptibility literature to offer novel theoretical insights on collaborative behavior between LLM-driven collaborator agents and an intervention agent. Our goal is to learn an ideal partner-aware collaborator that increases the group's common-ground (CG) alignment on task-relevant propositions-by intelligently collecting information provided in interventions by a partner agent. We show how LLM agents trained using standard RLHF and related approaches are naturally inclined to ignore possibly well-meaning interventions, which makes increasing group common ground non-trivial in this setting. We employ a two-player Modified-Action MDP to examine this suboptimal behavior of standard AI agents, and propose Interruptible Collaborative Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal collaborators. Experiments on multiple collaborative task environments show that ICR, on average, is more capable of promoting successful CG convergence and exploring more diverse solutions in such tasks.", "source": "arxiv", "arxiv_id": "2510.22462v2", "pdf_url": "https://arxiv.org/pdf/2510.22462v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-26T00:05:48Z", "updated": "2026-01-13T03:18:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement", "authors": ["Saman Forouzandeh", "Wei Peng", "Parham Moradi", "Xinghuo Yu", "Mahdi Jalili"], "year": 2025, "url": "http://arxiv.org/abs/2512.18950v1", "abstract": "We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.", "source": "arxiv", "arxiv_id": "2512.18950v1", "pdf_url": "https://arxiv.org/pdf/2512.18950v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-22T01:56:28Z", "updated": "2025-12-22T01:56:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learning Multi-Access Point Coordination in Agentic AI Wi-Fi with Large Language Models", "authors": ["Yifan Fan", "Le Liang", "Peng Liu", "Xiao Li", "Ziyang Guo", "Qiao Lan", "Shi Jin", "Wen Tong"], "year": 2025, "url": "http://arxiv.org/abs/2511.20719v1", "abstract": "Multi-access point coordination (MAPC) is a key technology for enhancing throughput in next-generation Wi-Fi within dense overlapping basic service sets. However, existing MAPC protocols rely on static, protocol-defined rules, which limits their ability to adapt to dynamic network conditions such as varying interference levels and topologies. To address this limitation, we propose a novel Agentic AI Wi-Fi framework where each access point, modeled as an autonomous large language model agent, collaboratively reasons about the network state and negotiates adaptive coordination strategies in real time. This dynamic collaboration is achieved through a cognitive workflow that enables the agents to engage in natural language dialogue, leveraging integrated memory, reflection, and tool use to ground their decisions in past experience and environmental feedback. Comprehensive simulation results demonstrate that our agentic framework successfully learns to adapt to diverse and dynamic network environments, significantly outperforming the state-of-the-art spatial reuse baseline and validating its potential as a robust and intelligent solution for future wireless networks.", "source": "arxiv", "arxiv_id": "2511.20719v1", "pdf_url": "https://arxiv.org/pdf/2511.20719v1", "categories": ["cs.AI", "cs.IT", "eess.SP"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-25T06:29:25Z", "updated": "2025-11-25T06:29:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learning Strategic Language Agents in the Werewolf Game with Iterative Latent Space Policy Optimization", "authors": ["Zelai Xu", "Wanjun Gu", "Chao Yu", "Yi Wu", "Yu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.04686v3", "abstract": "Large language model (LLM) agents have recently demonstrated impressive capabilities in various domains like open-ended conversation and multi-step decision-making. However, it remains challenging for these agents to solve strategic language games, such as Werewolf, which demand both strategic decision-making and free-form language interactions. Existing LLM agents often suffer from intrinsic bias in their action distributions and limited exploration of the unbounded text action space, resulting in suboptimal performance. To address these challenges, we propose Latent Space Policy Optimization (LSPO), an iterative framework that combines game-theoretic methods with LLM fine-tuning to build strategic language agents. LSPO leverages the observation that while the language space is combinatorially large, the underlying strategy space is relatively compact. We first map free-form utterances into a finite latent strategy space, yielding an abstracted extensive-form game. Then we apply game-theoretic methods like Counterfactual Regret Minimization (CFR) to optimize the policy in the latent space. Finally, we fine-tune the LLM via Direct Preference Optimization (DPO) to align with the learned policy. By iteratively alternating between these steps, our LSPO agents progressively enhance both strategic reasoning and language communication. Experiment on the Werewolf game shows that our agents iteratively expand the strategy space with improving performance and outperform existing Werewolf agents, underscoring their effectiveness in free-form language games with strategic interactions.", "source": "arxiv", "arxiv_id": "2502.04686v3", "pdf_url": "https://arxiv.org/pdf/2502.04686v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-07T06:19:55Z", "updated": "2025-06-18T03:24:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents", "authors": ["Davide Paglieri", "BartÅomiej CupiaÅ", "Jonathan Cook", "Ulyana Piterbarg", "Jens Tuyls", "Edward Grefenstette", "Jakob Nicolaus Foerster", "Jack Parker-Holder", "Tim RocktÃ¤schel"], "year": 2025, "url": "http://arxiv.org/abs/2509.03581v2", "abstract": "Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities. To our knowledge, this work is the first to explore training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, paving the way for more efficient, adaptive, and controllable agentic systems.", "source": "arxiv", "arxiv_id": "2509.03581v2", "pdf_url": "https://arxiv.org/pdf/2509.03581v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-03T18:00:13Z", "updated": "2025-09-30T09:12:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation", "authors": ["Jackson Hassell", "Dan Zhang", "Hannah Kim", "Tom Mitchell", "Estevam Hruschka"], "year": 2025, "url": "http://arxiv.org/abs/2510.19897v1", "abstract": "We investigate how agents built on pretrained large language models can learn target classification functions from labeled examples without parameter updates. While conventional approaches like fine-tuning are often costly, inflexible, and opaque, we propose a memory-augmented framework that leverages both labeled data and LLM-generated critiques. Our framework uses episodic memory to store instance-level critiques-capturing specific past experiences-and semantic memory to distill these into reusable, task-level guidance. Across a diverse set of tasks, incorporating critiques yields up to a 24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines that rely only on labels. Through extensive empirical evaluation, we uncover distinct behavioral differences between OpenAI and opensource models, particularly in how they handle fact-oriented versus preference-based data. To interpret how models respond to different representations of supervision encoded in memory, we introduce a novel metric, suggestibility. This helps explain observed behaviors and illuminates how model characteristics and memory strategies jointly shape learning dynamics. Our findings highlight the promise of memory-driven, reflective learning for building more adaptive and interpretable LLM agents.", "source": "arxiv", "arxiv_id": "2510.19897v1", "pdf_url": "https://arxiv.org/pdf/2510.19897v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-22T17:58:03Z", "updated": "2025-10-22T17:58:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents", "authors": ["Dongjun Lee", "Juyong Lee", "Kyuyoung Kim", "Jihoon Tack", "Jinwoo Shin", "Yee Whye Teh", "Kimin Lee"], "year": 2025, "url": "http://arxiv.org/abs/2503.10689v2", "abstract": "Recent advances in large language models (LLMs) have led to a growing interest in developing LLM-based agents for automating web tasks. However, these agents often struggle with even simple tasks on real-world websites due to their limited capability to understand and process complex web page structures. In this work, we introduce LCoW, a framework for Learning language models to Contextualize complex Web pages into a more comprehensible form, thereby enhancing decision making by LLM agents. LCoW decouples web page understanding from decision making by training a separate contextualization module to transform complex web pages into comprehensible format, which are then utilized by the decision-making agent. We demonstrate that our contextualization module effectively integrates with LLM agents of various scales to significantly enhance their decision-making capabilities in web automation tasks. Notably, LCoW improves the success rates of closed-source LLMs (e.g., Gemini-1.5-flash, GPT-4o, Claude-3.5-Sonnet) by an average of 15.6%, and demonstrates a 23.7% average improvement in success rates for open-source LMs (e.g., Llama-3.1-8B, Llama-3.1-70B) on the WorkArena benchmark. Moreover, the Gemini-1.5-flash agent with LCoW achieves state-of-the-art results on the WebShop benchmark, outperforming human experts. The relevant code materials are available at our project page: https://lcowiclr2025.github.io.", "source": "arxiv", "arxiv_id": "2503.10689v2", "pdf_url": "https://arxiv.org/pdf/2503.10689v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-12T01:33:40Z", "updated": "2025-12-19T03:49:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties", "authors": ["Philipp J. Schneider", "Lin Tian", "Marian-Andrei Rizoiu"], "year": 2025, "url": "http://arxiv.org/abs/2510.19299v1", "abstract": "Can large language model (LLM) agents reproduce the complex social dynamics that characterize human online behavior -- shaped by homophily, reciprocity, and social validation -- and what memory and learning mechanisms enable such dynamics to emerge? We present a multi-agent LLM simulation framework in which agents repeatedly interact, evaluate one another, and adapt their behavior through in-context learning accelerated by a coaching signal. To model human social behavior, we design behavioral reward functions that capture core drivers of online engagement, including social interaction, information seeking, self-presentation, coordination, and emotional support. These rewards align agent objectives with empirically observed user motivations, enabling the study of how network structures and group formations emerge from individual decision-making. Our experiments show that coached LLM agents develop stable interaction patterns and form emergent social ties, yielding network structures that mirror properties of real online communities. By combining behavioral rewards with in-context adaptation, our framework establishes a principled testbed for investigating collective dynamics in LLM populations and reveals how artificial agents may approximate or diverge from human-like social behavior.", "source": "arxiv", "arxiv_id": "2510.19299v1", "pdf_url": "https://arxiv.org/pdf/2510.19299v1", "categories": ["cs.AI", "cs.MA", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-22T07:00:33Z", "updated": "2025-10-22T07:00:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models", "authors": ["Zengqi Peng", "Yubin Wang", "Xu Han", "Lei Zheng", "Jun Ma"], "year": 2025, "url": "http://arxiv.org/abs/2501.05057v1", "abstract": "Recent advancements in reinforcement learning (RL) demonstrate the significant potential in autonomous driving. Despite this promise, challenges such as the manual design of reward functions and low sample efficiency in complex environments continue to impede the development of safe and effective driving policies. To tackle these issues, we introduce LearningFlow, an innovative automated policy learning workflow tailored to urban driving. This framework leverages the collaboration of multiple large language model (LLM) agents throughout the RL training process. LearningFlow includes a curriculum sequence generation process and a reward generation process, which work in tandem to guide the RL policy by generating tailored training curricula and reward functions. Particularly, each process is supported by an analysis agent that evaluates training progress and provides critical insights to the generation agent. Through the collaborative efforts of these LLM agents, LearningFlow automates policy learning across a series of complex driving tasks, and it significantly reduces the reliance on manual reward function design while enhancing sample efficiency. Comprehensive experiments are conducted in the high-fidelity CARLA simulator, along with comparisons with other existing methods, to demonstrate the efficacy of our proposed approach. The results demonstrate that LearningFlow excels in generating rewards and curricula. It also achieves superior performance and robust generalization across various driving tasks, as well as commendable adaptation to different RL algorithms.", "source": "arxiv", "arxiv_id": "2501.05057v1", "pdf_url": "https://arxiv.org/pdf/2501.05057v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-01-09T08:28:16Z", "updated": "2025-01-09T08:28:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Les Dissonances: Cross-Tool Harvesting and Polluting in Pool-of-Tools Empowered LLM Agents", "authors": ["Zichuan Li", "Jian Cui", "Xiaojing Liao", "Luyi Xing"], "year": 2025, "url": "http://arxiv.org/abs/2504.03111v3", "abstract": "Large Language Model (LLM) agents are autonomous systems powered by LLMs, capable of reasoning and planning to solve problems by leveraging a set of tools. However, the integration of multi-tool capabilities in LLM agents introduces challenges in securely managing tools, ensuring their compatibility, handling dependency relationships, and protecting control flows within LLM agent workflows. In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents. We identify a novel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes multiple attack vectors to first hijack the normal control flows of agent tasks, and then collect and pollute confidential or private information within LLM agent systems. To understand the impact of this threat, we developed Chord, a dynamic scanning tool designed to automatically detect real-world agent tools susceptible to XTHP attacks. Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "source": "arxiv", "arxiv_id": "2504.03111v3", "pdf_url": "https://arxiv.org/pdf/2504.03111v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-04T01:41:06Z", "updated": "2025-12-03T15:51:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve", "authors": ["Yuanzhe Liu", "Ryan Deng", "Tim Kaler", "Xuhao Chen", "Charles E. Leiserson", "Yao Ma", "Jie Chen"], "year": 2025, "url": "http://arxiv.org/abs/2505.23946v2", "abstract": "Recent studies show that LLMs possess different skills and specialize in different tasks. In fact, we observe that their varied performance occur in several levels of granularity. For example, in the code optimization task, code LLMs excel at different optimization categories and no one dominates others. This observation prompts the question of how one leverages multiple LLM agents to solve a coding problem without knowing their complementary strengths a priori. We argue that a team of agents can learn from each other's successes and failures so as to improve their own performance. Thus, a lesson is the knowledge produced by an agent and passed on to other agents in the collective solution process. We propose a lesson-based collaboration framework, design the lesson solicitation--banking--selection mechanism, and demonstrate that a team of small LLMs with lessons learned can outperform a much larger LLM and other multi-LLM collaboration methods.", "source": "arxiv", "arxiv_id": "2505.23946v2", "pdf_url": "https://arxiv.org/pdf/2505.23946v2", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-29T18:56:20Z", "updated": "2025-10-23T15:41:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Leveraging In-Context Learning for Language Model Agents", "authors": ["Shivanshu Gupta", "Sameer Singh", "Ashish Sabharwal", "Tushar Khot", "Ben Bogin"], "year": 2025, "url": "http://arxiv.org/abs/2506.13109v1", "abstract": "In-context learning (ICL) with dynamically selected demonstrations combines the flexibility of prompting large language models (LLMs) with the ability to leverage training data to improve performance. While ICL has been highly successful for prediction and generation tasks, leveraging it for agentic tasks that require sequential decision making is challenging -- one must think not only about how to annotate long trajectories at scale and how to select demonstrations, but also what constitutes demonstrations, and when and where to show them. To address this, we first propose an algorithm that leverages an LLM with retries along with demonstrations to automatically and efficiently annotate agentic tasks with solution trajectories. We then show that set-selection of trajectories of similar tasks as demonstrations significantly improves performance, reliability, robustness, and efficiency of LLM agents. However, trajectory demonstrations have a large inference cost overhead. We show that this can be mitigated by using small trajectory snippets at every step instead of an additional trajectory. We find that demonstrations obtained from larger models (in the annotation phase) also improve smaller models, and that ICL agents can even rival costlier trained agents. Thus, our results reveal that ICL, with careful use, can be very powerful for agentic tasks as well.", "source": "arxiv", "arxiv_id": "2506.13109v1", "pdf_url": "https://arxiv.org/pdf/2506.13109v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-16T05:37:49Z", "updated": "2025-06-16T05:37:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants", "authors": ["Milapji Singh Gill", "Javal Vyas", "Artan Markaj", "Felix Gehlhoff", "Mehmet MercangÃ¶z"], "year": 2025, "url": "http://arxiv.org/abs/2505.02076v1", "abstract": "Advances in Automation and Artificial Intelligence continue to enhance the autonomy of process plants in handling various operational scenarios. However, certain tasks, such as fault handling, remain challenging, as they rely heavily on human expertise. This highlights the need for systematic, knowledge-based methods. To address this gap, we propose a methodological framework that integrates Large Language Model (LLM) agents with a Digital Twin environment. The LLM agents continuously interpret system states and initiate control actions, including responses to unexpected faults, with the goal of returning the system to normal operation. In this context, the Digital Twin acts both as a structured repository of plant-specific engineering knowledge for agent prompting and as a simulation platform for the systematic validation and verification of the generated corrective control actions. The evaluation using a mixing module of a process plant demonstrates that the proposed framework is capable not only of autonomously controlling the mixing module, but also of generating effective corrective actions to mitigate a pipe clogging with only a few reprompts.", "source": "arxiv", "arxiv_id": "2505.02076v1", "pdf_url": "https://arxiv.org/pdf/2505.02076v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-04T12:02:21Z", "updated": "2025-05-04T12:02:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Leveraging LLM Agents for Automated Optimization Modeling for SASP Problems: A Graph-RAG based Approach", "authors": ["Tianpeng Pan", "Wenqiang Pu", "Licheng Zhao", "Rui Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2501.18320v1", "abstract": "Automated optimization modeling (AOM) has evoked considerable interest with the rapid evolution of large language models (LLMs). Existing approaches predominantly rely on prompt engineering, utilizing meticulously designed expert response chains or structured guidance. However, prompt-based techniques have failed to perform well in the sensor array signal processing (SASP) area due the lack of specific domain knowledge. To address this issue, we propose an automated modeling approach based on retrieval-augmented generation (RAG) technique, which consists of two principal components: a multi-agent (MA) structure and a graph-based RAG (Graph-RAG) process. The MA structure is tailored for the architectural AOM process, with each agent being designed based on principles of human modeling procedure. The Graph-RAG process serves to match user query with specific SASP modeling knowledge, thereby enhancing the modeling result. Results on ten classical signal processing problems demonstrate that the proposed approach (termed as MAG-RAG) outperforms several AOM benchmarks.", "source": "arxiv", "arxiv_id": "2501.18320v1", "pdf_url": "https://arxiv.org/pdf/2501.18320v1", "categories": ["cs.AI", "eess.SP"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-30T13:00:15Z", "updated": "2025-01-30T13:00:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Leveraging LLM Agents for Automated Video Game Testing", "authors": ["Chengjia Wang", "Lanling Tang", "Ming Yuan", "Jiongchi Yu", "Xiaofei Xie", "Jiajun Bu"], "year": 2025, "url": "http://arxiv.org/abs/2509.22170v1", "abstract": "Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a critical yet labor-intensive task in game development due to their complexity and frequent updating nature. Traditional automated game testing approaches struggle to achieve high state coverage and efficiency in these rich, open-ended environments, while existing LLM-based game-playing approaches are limited to shallow reasoning ability in understanding complex game state-action spaces and long-complex tasks. To address these challenges, we propose TITAN, an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN incorporates four key components to: (1) perceive and abstract high-dimensional game states, (2) proactively optimize and prioritize available actions, (3) enable long-horizon reasoning with action trace memory and reflective self-correction, and (4) employ LLM-based oracles to detect potential functional and logic bugs with diagnostic reports.\n  We implement the prototype of TITAN and evaluate it on two large-scale commercial MMORPGs spanning both PC and mobile platforms. In our experiments, TITAN achieves significantly higher task completion rates (95%) and bug detection performance compared to existing automated game testing approaches. An ablation study further demonstrates that each core component of TITAN contributes substantially to its overall performance. Notably, TITAN detects four previously unknown bugs that prior testing approaches fail to identify. We provide an in-depth discussion of these results, which offer guidance for new avenues of advancing intelligent, general-purpose testing systems. Moreover, TITAN has been deployed in eight real-world game QA pipelines, underscoring its practical impact as an LLM-driven game testing framework.", "source": "arxiv", "arxiv_id": "2509.22170v1", "pdf_url": "https://arxiv.org/pdf/2509.22170v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-26T10:31:21Z", "updated": "2025-09-26T10:31:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments", "authors": ["Yuran Li", "Jama Hussein Mohamud", "Chongren Sun", "Di Wu", "Benoit Boulet"], "year": 2025, "url": "http://arxiv.org/abs/2504.17087v1", "abstract": "Large language models (LLMs) are being widely applied across various fields, but as tasks become more complex, evaluating their responses is increasingly challenging. Compared to human evaluators, the use of LLMs to support performance evaluation offers a more efficient alternative. However, most studies focus mainly on aligning LLMs' judgments with human preferences, overlooking the existence of biases and mistakes in human judgment. Furthermore, how to select suitable LLM judgments given multiple potential LLM responses remains underexplored. To address these two aforementioned issues, we propose a three-stage meta-judge selection pipeline: 1) developing a comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM agents to score judgments, and 3) applying a threshold to filter out low-scoring judgments. Compared to methods using a single LLM as both judge and meta-judge, our pipeline introduces multi-agent collaboration and a more comprehensive rubric. Experimental results on the JudgeBench dataset show about 15.55\\% improvement compared to raw judgments and about 8.37\\% improvement over the single-agent baseline. Our work demonstrates the potential of LLMs as meta-judges and lays the foundation for future research on constructing preference datasets for LLM-as-a-judge reinforcement learning.", "source": "arxiv", "arxiv_id": "2504.17087v1", "pdf_url": "https://arxiv.org/pdf/2504.17087v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-23T20:32:12Z", "updated": "2025-04-23T20:32:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning", "authors": ["Zhengjun Huang", "Zhoujin Tian", "Qintian Guo", "Fangyuan Zhang", "Yingli Zhou", "Di Jiang", "Zeying Xie", "Xiaofang Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2511.01448v2", "abstract": "Large Language Model (LLM) agents exhibit remarkable conversational and reasoning capabilities but remain constrained by limited context windows and the lack of persistent memory. Recent efforts address these limitations via external memory architectures, often employing graph-based representations, yet most adopt flat, entangled structures that intertwine semantics with topology, leading to redundant representations, unstructured retrieval, and degraded efficiency and accuracy. To resolve these issues, we propose LiCoMemory, an end-to-end agentic memory framework for real-time updating and retrieval, which introduces CogniGraph, a lightweight hierarchical graph that utilizes entities and relations as semantic indexing layers, and employs temporal and hierarchy-aware search with integrated reranking for adaptive and coherent knowledge retrieval. Experiments on long-term dialogue benchmarks, LoCoMo and LongMemEval, show that LiCoMemory not only outperforms established baselines in temporal reasoning, multi-session consistency, and retrieval efficiency, but also notably reduces update latency. Our official code and data are available at https://github.com/EverM0re/LiCoMemory.", "source": "arxiv", "arxiv_id": "2511.01448v2", "pdf_url": "https://arxiv.org/pdf/2511.01448v2", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-11-03T11:02:40Z", "updated": "2026-01-06T07:27:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Lifelong Learning of Large Language Model based Agents: A Roadmap", "authors": ["Junhao Zheng", "Chengming Shi", "Xidi Cai", "Qiuke Li", "Duzhen Zhang", "Chenxing Li", "Dong Yu", "Qianli Ma"], "year": 2025, "url": "http://arxiv.org/abs/2501.07278v2", "abstract": "Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at \\href{this url}{https://github.com/qianlima-lab/awesome-lifelong-llm-agent}.", "source": "arxiv", "arxiv_id": "2501.07278v2", "pdf_url": "https://arxiv.org/pdf/2501.07278v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-13T12:42:04Z", "updated": "2026-01-11T02:56:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners", "authors": ["Junhao Zheng", "Xidi Cai", "Qiuke Li", "Duzhen Zhang", "ZhongZhi Li", "Yingying Zhang", "Le Song", "Qianli Ma"], "year": 2025, "url": "http://arxiv.org/abs/2505.11942v3", "abstract": "Lifelong learning is essential for intelligent agents operating in dynamic environments. Current large language model (LLM)-based agents, however, remain stateless and unable to accumulate or transfer knowledge over time. Existing benchmarks treat agents as static systems and fail to evaluate lifelong learning capabilities. We present LifelongAgentBench, the first unified benchmark designed to systematically assess the lifelong learning ability of LLM agents. It provides skill-grounded, interdependent tasks across three interactive environments, Database, Operating System, and Knowledge Graph, with automatic label verification, reproducibility, and modular extensibility. Extensive experiments reveal that conventional experience replay has limited effectiveness for LLM agents due to irrelevant information and context length constraints. We further introduce a group self-consistency mechanism that significantly improves lifelong learning performance. We hope LifelongAgentBench will advance the development of adaptive, memory-capable LLM agents.", "source": "arxiv", "arxiv_id": "2505.11942v3", "pdf_url": "https://arxiv.org/pdf/2505.11942v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-17T10:09:11Z", "updated": "2025-05-30T02:28:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems", "authors": ["Paul-Niklas Ken Kandora", "Simon Caspar Zeller", "Aaron Jeremias Elsing", "Elena Kuss", "Steffen Rebennack"], "year": 2025, "url": "http://arxiv.org/abs/2510.15969v1", "abstract": "Reformulating nonlinear optimization problems is largely manual and expertise-intensive, yet it remains essential for solving such problems with linear optimization solvers or applying special-purpose algorithms. We introduce \\textit{LinearizeLLM}, an agent-based framework that solves this task by leveraging Large Language Models (LLMs). The framework assigns each nonlinear pattern to a \\textit{reformulation agent} that is explicitly instructed to derive an exact linear reformulation for its nonlinearity pattern, for instance, absolute-value terms or bilinear products of decision variables. The agents then coordinate to assemble a solver-ready linear model equivalent to the original problem. To benchmark the approach, we create a dataset of 20 real-world nonlinear optimization problems derived from the established ComplexOR dataset of linear optimization problems. We evaluate our approach with several LLMs. Our results indicate that specialized LLM agents can automate linearization tasks, opening a path toward fully conversational modeling pipelines for nonlinear optimization.", "source": "arxiv", "arxiv_id": "2510.15969v1", "pdf_url": "https://arxiv.org/pdf/2510.15969v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-12T16:43:21Z", "updated": "2025-10-12T16:43:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis", "authors": ["Shifu Chen", "Dazhen Deng", "Zhihong Xu", "Sijia Xu", "Tai-Quan Peng", "Yingcai Wu"], "year": 2025, "url": "http://arxiv.org/abs/2510.26172v1", "abstract": "Social media platforms generate massive volumes of heterogeneous data, capturing user behaviors, textual content, temporal dynamics, and network structures. Analyzing such data is crucial for understanding phenomena such as opinion dynamics, community formation, and information diffusion. However, discovering insights from this complex landscape is exploratory, conceptually challenging, and requires expertise in social media mining and visualization. Existing automated approaches, though increasingly leveraging large language models (LLMs), remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis. We present SIA (Social Insight Agents), an LLM agent system that links heterogeneous multi-modal data -- including raw inputs (e.g., text, network, and behavioral data), intermediate outputs, mined analytical results, and visualization artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy that connects insight types with suitable mining and visualization techniques, SIA enables agents to plan and execute coherent analysis strategies. To ensure multi-modal integration, it incorporates a data coordinator that unifies tabular, textual, and network data into a consistent flow. Its interactive interface provides a transparent workflow where users can trace, validate, and refine the agent's reasoning, supporting both adaptability and trustworthiness. Through expert-centered case studies and quantitative evaluation, we show that SIA effectively discovers diverse and meaningful insights from social media while supporting human-agent collaboration in complex analytical tasks.", "source": "arxiv", "arxiv_id": "2510.26172v1", "pdf_url": "https://arxiv.org/pdf/2510.26172v1", "categories": ["cs.HC", "cs.AI", "cs.SI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-30T06:22:49Z", "updated": "2025-10-30T06:22:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?", "authors": ["Guozhao Mo", "Wenliang Zhong", "Jiawei Chen", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun"], "year": 2025, "url": "http://arxiv.org/abs/2508.01780v1", "abstract": "With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.", "source": "arxiv", "arxiv_id": "2508.01780v1", "pdf_url": "https://arxiv.org/pdf/2508.01780v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-03T14:36:42Z", "updated": "2025-08-03T14:36:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering", "authors": ["Jielin Qiu", "Zuxin Liu", "Zhiwei Liu", "Rithesh Murthy", "Jianguo Zhang", "Haolin Chen", "Shiyu Wang", "Ming Zhu", "Liangwei Yang", "Juntao Tan", "Roshan Ram", "Akshara Prabhakar", "Tulika Awalgaonkar", "Zixiang Chen", "Zhepeng Cen", "Cheng Qian", "Shelby Heinecke", "Weiran Yao", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.13998v1", "abstract": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.", "source": "arxiv", "arxiv_id": "2511.13998v1", "pdf_url": "https://arxiv.org/pdf/2511.13998v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-17T23:57:24Z", "updated": "2025-11-17T23:57:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LocAgent: Graph-Guided LLM Agents for Code Localization", "authors": ["Zhaoling Chen", "Xiangru Tang", "Gangda Deng", "Fang Wu", "Jialong Wu", "Zhiwei Jiang", "Viktor Prasanna", "Arman Cohan", "Xingyao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2503.09089v2", "abstract": "Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.", "source": "arxiv", "arxiv_id": "2503.09089v2", "pdf_url": "https://arxiv.org/pdf/2503.09089v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-12T05:55:01Z", "updated": "2025-04-29T14:37:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LogicGuard: Improving Embodied LLM agents through Temporal Logic based Critics", "authors": ["Anand Gokhale", "Vaibhav Srivastava", "Francesco Bullo"], "year": 2025, "url": "http://arxiv.org/abs/2507.03293v2", "abstract": "Large language models (LLMs) have shown promise in zero-shot and single step reasoning and decision making problems, but in long horizon sequential planning tasks, their errors compound, often leading to unreliable or inefficient behavior. We introduce LogicGuard, a modular actor-critic architecture in which an LLM actor is guided by a trajectory level LLM critic that communicates through Linear Temporal Logic (LTL). Our setup combines the reasoning strengths of language models with the guarantees of formal logic. The actor selects high-level actions from natural language observations, while the critic analyzes full trajectories and proposes new LTL constraints that shield the actor from future unsafe or inefficient behavior. LogicGuard supports both fixed safety rules and adaptive, learned constraints, and is model-agnostic: any LLM-based planner can serve as the actor, with LogicGuard acting as a logic-generating wrapper. We formalize planning as graph traversal under symbolic constraints, allowing LogicGuard to analyze failed or suboptimal trajectories and generate new temporal logic rules that improve future behavior. To demonstrate generality, we evaluate LogicGuard across two distinct settings: short-horizon general tasks and long-horizon specialist tasks. On the Behavior benchmark of 100 household tasks, LogicGuard increases task completion rates by 25% over a baseline InnerMonologue planner. On the Minecraft diamond-mining task, which is long-horizon and requires multiple interdependent subgoals, LogicGuard improves both efficiency and safety compared to SayCan and InnerMonologue. These results show that enabling LLMs to supervise each other through temporal logic yields more reliable, efficient and safe decision-making for both embodied agents.", "source": "arxiv", "arxiv_id": "2507.03293v2", "pdf_url": "https://arxiv.org/pdf/2507.03293v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "eess.SY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-04T04:53:53Z", "updated": "2025-09-23T04:36:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling", "authors": ["Zecheng Tang", "Baibei Ji", "Quantong Qiu", "Haitian Wang", "Xiaobo Liang", "Juntao Li", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2510.06915v2", "abstract": "Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.", "source": "arxiv", "arxiv_id": "2510.06915v2", "pdf_url": "https://arxiv.org/pdf/2510.06915v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-08T11:48:16Z", "updated": "2025-11-04T05:29:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents", "authors": ["Yaorui Shi", "Yuxin Chen", "Siyuan Wang", "Sihang Li", "Hengxing Cai", "Qi Gu", "Xiang Wang", "An Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2509.23040v1", "abstract": "Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory corpus that is dynamically updated during a single-pass document scan, also known as the \"memorize while reading\" methods. While this approach scales efficiently, it suffers from irreversible forward-only processing, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced memory that allows selective retrieval from the entire memory history and allows non-linear reasoning and revisiting of early evidence. To further strengthen training, we propose Reinforcement Learning with Multi-Level Rewards (RLMLR), which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support multi-hop memory utilizing. Experiments on long-document QA show significant gains over existing memory-based approaches, which validates ReMemR1 as an effective solution for long-context reasoning agents.", "source": "arxiv", "arxiv_id": "2509.23040v1", "pdf_url": "https://arxiv.org/pdf/2509.23040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-27T01:36:46Z", "updated": "2025-09-27T01:36:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams", "authors": ["Gilad Gressel", "Rahul Pankajakshan", "Shir Rozenfeld", "Ling Li", "Ivan Franceschini", "Krishnashree Achuthan", "Yisroel Mirsky"], "year": 2025, "url": "http://arxiv.org/abs/2512.16280v2", "abstract": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.\n  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.", "source": "arxiv", "arxiv_id": "2512.16280v2", "pdf_url": "https://arxiv.org/pdf/2512.16280v2", "categories": ["cs.CR", "cs.AI", "cs.CY"], "primary_category": "cs.CR", "doi": "", "venue": "Usenix Security Symposium 2026", "published": "2025-12-18T07:59:15Z", "updated": "2025-12-22T10:37:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation", "authors": ["Jiin Park", "Misuk Kim"], "year": 2025, "url": "http://arxiv.org/abs/2510.13371v1", "abstract": "Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations.", "source": "arxiv", "arxiv_id": "2510.13371v1", "pdf_url": "https://arxiv.org/pdf/2510.13371v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-10-15T10:03:29Z", "updated": "2025-10-15T10:03:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces", "authors": ["Loris Gaven", "Thomas Carta", "ClÃ©ment Romac", "CÃ©dric Colas", "Sylvain Lamprier", "Olivier Sigaud", "Pierre-Yves Oudeyer"], "year": 2025, "url": "http://arxiv.org/abs/2502.07709v3", "abstract": "Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.", "source": "arxiv", "arxiv_id": "2502.07709v3", "pdf_url": "https://arxiv.org/pdf/2502.07709v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-11T17:08:00Z", "updated": "2025-06-17T09:23:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MAGPIE: A benchmark for Multi-AGent contextual PrIvacy Evaluation", "authors": ["Gurusha Juneja", "Jayanth Naga Sai Pasupulati", "Alon Albalak", "Wenyue Hua", "William Yang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.15186v1", "abstract": "A core challenge for autonomous LLM agents in collaborative settings is balancing robust privacy understanding and preservation alongside task efficacy. Existing privacy benchmarks only focus on simplistic, single-turn interactions where private information can be trivially omitted without affecting task outcomes. In this paper, we introduce MAGPIE (Multi-AGent contextual PrIvacy Evaluation), a novel benchmark of 200 high-stakes tasks designed to evaluate privacy understanding and preservation in multi-agent collaborative, non-adversarial scenarios. MAGPIE integrates private information as essential for task resolution, forcing agents to balance effective collaboration with strategic information control. Our evaluation reveals that state-of-the-art agents, including GPT-5 and Gemini 2.5-Pro, exhibit significant privacy leakage, with Gemini 2.5-Pro leaking up to 50.7% and GPT-5 up to 35.1% of the sensitive information even when explicitly instructed not to. Moreover, these agents struggle to achieve consensus or task completion and often resort to undesirable behaviors such as manipulation and power-seeking (e.g., Gemini 2.5-Pro demonstrating manipulation in 38.2% of the cases). These findings underscore that current LLM agents lack robust privacy understanding and are not yet adequately aligned to simultaneously preserve privacy and maintain effective collaboration in complex environments.", "source": "arxiv", "arxiv_id": "2510.15186v1", "pdf_url": "https://arxiv.org/pdf/2510.15186v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-16T23:12:12Z", "updated": "2025-10-16T23:12:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions", "authors": ["Aishik Mandal", "Tanmoy Chakraborty", "Iryna Gurevych"], "year": 2025, "url": "http://arxiv.org/abs/2509.04183v2", "abstract": "The growing demand for scalable psychological counseling highlights the need for high-quality, privacy-compliant data, yet such data remains scarce. Here we introduce MAGneT, a novel multi-agent framework for synthetic psychological counseling session generation that decomposes counselor response generation into coordinated sub-tasks handled by specialized LLM agents, each modeling a key psychological technique. Unlike prior single-agent approaches, MAGneT better captures the structure and nuance of real counseling. We further propose a unified evaluation framework that consolidates diverse automatic metrics and expands expert assessment from four to nine counseling dimensions, thus addressing inconsistencies in prior evaluation protocols. Empirically, MAGneT substantially outperforms existing methods: experts prefer MAGneT-generated sessions in 77.2% of cases, and sessions generated by MAGneT yield 3.2% higher general counseling skills and 4.3% higher CBT-specific skills on cognitive therapy rating scale (CTRS). A open source Llama3-8B-Instruct model fine-tuned on MAGneT-generated data also outperforms models fine-tuned using baseline synthetic datasets by 6.9% on average on CTRS.We also make our code and data public.", "source": "arxiv", "arxiv_id": "2509.04183v2", "pdf_url": "https://arxiv.org/pdf/2509.04183v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-04T12:59:24Z", "updated": "2026-01-09T10:37:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber", "authors": ["Arth Bhardwaj", "Sia Godika", "Yuvam Loonker"], "year": 2025, "url": "http://arxiv.org/abs/2512.14846v1", "abstract": "Traditional, centralized security tools often miss adaptive, multi-vector attacks. We present the Multi-Agent LLM Cyber Defense Framework (MALCDF), a practical setup where four large language model (LLM) agents-Detection, Intelligence, Response, and Analysis-work together in real time. Agents communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages, and produce audit-friendly outputs (e.g., MITRE ATT&CK mappings).\n  For evaluation, we keep the test simple and consistent: all reported metrics come from the same 50-record live stream derived from the CICIDS2017 feature schema. CICIDS2017 is used for configuration (fields/schema) and to train a practical ML baseline. The ML-IDS baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the 50-record stream, with no overlap between training and test records.\n  In experiments, MALCDF reaches 90.0% detection accuracy, 85.7% F1-score, and 9.1% false-positive rate, with 6.8s average per-event latency. It outperforms the lightweight ML-IDS baseline and a single-LLM setup on accuracy while keeping end-to-end outputs consistent. Overall, this hands-on build suggests that coordinating simple LLM agents with secure, ontology-aligned messaging can improve practical, real-time cyber defense.", "source": "arxiv", "arxiv_id": "2512.14846v1", "pdf_url": "https://arxiv.org/pdf/2512.14846v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-16T19:08:12Z", "updated": "2025-12-16T19:08:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG and Multi-Agent LLM Collaboration", "authors": ["Yisen Xu", "Feng Lin", "Jinqiu Yang", "Tse-Hsun", "Chen", "Nikolaos Tsantalis"], "year": 2025, "url": "http://arxiv.org/abs/2503.14340v2", "abstract": "Maintaining and scaling software systems relies heavily on effective code refactoring, yet this process remains labor-intensive, requiring developers to carefully analyze existing codebases and prevent the introduction of new defects. Although recent advancements have leveraged Large Language Models (LLMs) to automate refactoring tasks, current solutions are constrained in scope and lack mechanisms to guarantee code compilability and successful test execution. In this work, we introduce MANTRA, a comprehensive LLM agent-based framework that automates method-level refactoring. MANTRA integrates Context-Aware Retrieval-Augmented Generation, coordinated Multi-Agent Collaboration, and Verbal Reinforcement Learning to emulate human decision-making during refactoring while preserving code correctness and readability. Our empirical study, conducted on 703 instances of \"pure refactorings\" (i.e., code changes exclusively involving structural improvements), drawn from 10 representative Java projects, covers the six most prevalent refactoring operations. Experimental results demonstrate that MANTRA substantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8% success rate (582/703) in producing code that compiles and passes all tests, compared to just 8.7% (61/703) with RawGPT. Moreover, in comparison to IntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50% improvement in generating Extract Method transformations. A usability study involving 37 professional developers further shows that refactorings performed by MANTRA are perceived to be as readable and reusable as human-written code, and in certain cases, even more favorable. These results highlight the practical advantages of MANTRA and emphasize the growing potential of LLM-based systems in advancing the automation of software refactoring tasks.", "source": "arxiv", "arxiv_id": "2503.14340v2", "pdf_url": "https://arxiv.org/pdf/2503.14340v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-18T15:16:51Z", "updated": "2025-03-27T01:43:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs", "authors": ["Onat Ozer", "Grace Wu", "Yuchen Wang", "Daniel Dosti", "Honghao Zhang", "Vivi De La Rue"], "year": 2025, "url": "http://arxiv.org/abs/2512.20845v1", "abstract": "LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.", "source": "arxiv", "arxiv_id": "2512.20845v1", "pdf_url": "https://arxiv.org/pdf/2512.20845v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T23:47:31Z", "updated": "2025-12-23T23:47:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning", "authors": ["Yusheng Zhao", "Xiao Luo", "Weizhi Zhang", "Wei Ju", "Zhiping Xiao", "Philip S. Yu", "Ming Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.17481v1", "abstract": "The ability to reason is one of the most fundamental capabilities of large language models (LLMs), enabling a wide range of downstream tasks through sophisticated problem-solving. A critical aspect of this is code reasoning, which involves logical reasoning with formal languages (i.e., programming code). In this paper, we enhance this capability of LLMs by exploring the following question: how can an LLM agent become progressively smarter in code reasoning with each solution it proposes, thereby achieving substantial cumulative improvement? Most existing research takes a static perspective, focusing on isolated problem-solving using frozen LLMs. In contrast, we adopt a cognitive-evolving perspective and propose a novel framework named Meta-Reflection with Cross-Referencing (MARCO) that enables the LLM to evolve dynamically during inference through self-improvement. From the perspective of human cognitive development, we leverage both knowledge accumulation and lesson sharing. In particular, to accumulate knowledge during problem-solving, we propose meta-reflection that reflects on the reasoning paths of the current problem to obtain knowledge and experience for future consideration. Moreover, to effectively utilize the lessons from other agents, we propose cross-referencing that incorporates the solution and feedback from other agents into the current problem-solving process. We conduct experiments across various datasets in code reasoning, and the results demonstrate the effectiveness of MARCO.", "source": "arxiv", "arxiv_id": "2505.17481v1", "pdf_url": "https://arxiv.org/pdf/2505.17481v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T05:21:11Z", "updated": "2025-05-23T05:21:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems", "authors": ["Kun Wang", "Guibin Zhang", "ManKit Ye", "Xinyu Deng", "Dongxia Wang", "Xiaobin Hu", "Jinyang Guo", "Yang Liu", "Yufei Guo"], "year": 2025, "url": "http://arxiv.org/abs/2509.24323v1", "abstract": "The past two years have witnessed the meteoric rise of Large Language Model (LLM)-powered multi-agent systems (MAS), which harness collective intelligence and exhibit a remarkable trajectory toward self-evolution. This paradigm has rapidly progressed from manually engineered systems that require bespoke configuration of prompts, tools, roles, and communication protocols toward frameworks capable of automated orchestration. Yet, dominant automatic multi-agent systems, whether generated by external modules or a single LLM agent, largely adhere to a rigid ``\\textit{generate-once-and-deploy}'' paradigm, rendering the resulting systems brittle and ill-prepared for the dynamism and uncertainty of real-world environments. To transcend this limitation, we introduce MAS$^2$, a paradigm predicated on the principle of recursive self-generation: a multi-agent system that autonomously architects bespoke multi-agent systems for diverse problems. Technically, we devise a ``\\textit{generator-implementer-rectifier}'' tri-agent team capable of dynamically composing and adaptively rectifying a target agent system in response to real-time task demands. Collaborative Tree Optimization is proposed to train and specialize these meta-agents. Extensive evaluation across seven benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\\%$ over state-of-the-art MAS in complex scenarios such as deep research and code generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization, effectively leveraging previously unseen LLMs to yield improvements of up to $15.1\\%$. Crucially, these gains are attained without incurring excessive token costs, as MAS$^2$ consistently resides on the Pareto frontier of cost-performance trade-offs. The source codes are available at https://github.com/yeyeyeah2/MAS2.", "source": "arxiv", "arxiv_id": "2509.24323v1", "pdf_url": "https://arxiv.org/pdf/2509.24323v1", "categories": ["cs.MA", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-09-29T06:20:10Z", "updated": "2025-09-29T06:20:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning", "authors": ["Huihao Jing", "Wenbin Hu", "Hongyu Luo", "Jianhui Yang", "Wei Fan", "Haoran Li", "Yangqiu Song"], "year": 2025, "url": "http://arxiv.org/abs/2509.24922v2", "abstract": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures.", "source": "arxiv", "arxiv_id": "2509.24922v2", "pdf_url": "https://arxiv.org/pdf/2509.24922v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T15:24:40Z", "updated": "2025-09-30T17:09:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents", "authors": ["Dongsen Zhang", "Zekun Li", "Xu Luo", "Xuannan Liu", "Peipei Li", "Wenjun Xu"], "year": 2025, "url": "http://arxiv.org/abs/2510.15994v1", "abstract": "The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Results reveal the effectiveness of attacks against each stage of MCP. Models with stronger performance are more vulnerable to attacks due to their outstanding tool calling and instruction following capabilities. MSB provides a practical baseline for researchers and practitioners to study, compare, and harden MCP agents.", "source": "arxiv", "arxiv_id": "2510.15994v1", "pdf_url": "https://arxiv.org/pdf/2510.15994v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-14T07:36:25Z", "updated": "2025-10-14T07:36:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)", "authors": ["Aaron Steiner", "Ralph Peeters", "Christian Bizer"], "year": 2025, "url": "http://arxiv.org/abs/2511.23281v1", "abstract": "Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.\n  To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.", "source": "arxiv", "arxiv_id": "2511.23281v1", "pdf_url": "https://arxiv.org/pdf/2511.23281v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-28T15:32:15Z", "updated": "2025-11-28T15:32:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers", "authors": ["Zhenting Wang", "Qi Chang", "Hemani Patel", "Shashank Biju", "Cheng-En Wu", "Quan Liu", "Aolin Ding", "Alireza Rezazadeh", "Ankit Shah", "Yujia Bao", "Eugene Siow"], "year": 2025, "url": "http://arxiv.org/abs/2508.20453v1", "abstract": "We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench.", "source": "arxiv", "arxiv_id": "2508.20453v1", "pdf_url": "https://arxiv.org/pdf/2508.20453v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-28T05:58:57Z", "updated": "2025-08-28T05:58:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.24284v2", "abstract": "Large Language Models (LLMs) increasingly rely on external tools to perform complex, realistic tasks, yet their ability to utilize the rapidly expanding Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP research covers few servers, depends on costly manual curation, and lacks training support, hindering progress toward real-world deployment. To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training. MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing 68733 high-quality instruction-function call pairs and 6439 trajectories, far exceeding prior work in scale and diversity. Extensive experiments demonstrate MCP-Flow's effectiveness in driving superior MCP tool selection, function-call generation, and enhanced agentic task performance. MCP-Flow thus provides a scalable foundation for advancing LLM agents' proficiency in real-world MCP environments. MCP-Flow is publicly available at \\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "source": "arxiv", "arxiv_id": "2510.24284v2", "pdf_url": "https://arxiv.org/pdf/2510.24284v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-28T10:42:17Z", "updated": "2025-11-01T07:07:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models", "authors": ["Xuanqi Gao", "Siyi Xie", "Juan Zhai", "Shiqing Ma", "Chao Shen"], "year": 2025, "url": "http://arxiv.org/abs/2505.16700v2", "abstract": "As Large Language Models (LLMs) evolve from passive text generators to active reasoning agents capable of interacting with external tools, the Model Context Protocol (MCP) has emerged as a key standardized framework for dynamic tool discovery and orchestration. Despite its widespread industry adoption, existing evaluation methods do not adequately assess tool utilization capabilities under this new paradigm. To address this gap, this paper introduces MCP-RADAR, the first comprehensive benchmark specifically designed to evaluate LLM performance within the MCP framework. MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations. It quantifies performance based on two primary criteria: answer correctness and operational accuracy. To closely emulate real-world usage, our evaluation employs both authentic MCP tools and high-fidelity simulations of official tools. Unlike traditional benchmarks that rely on subjective human evaluation or binary success metrics, MCP-RADAR adopts objective, quantifiable measurements across multiple task domains, including computational resource efficiency and the number of successful tool-invocation rounds. Our evaluation of leading closed-source and open-source LLMs reveals distinct capability profiles and highlights a significant trade-off between accuracy and efficiency. Our findings provide actionable insights for both LLM developers and tool creators, establishing a standardized methodology applicable to the broader LLM agent ecosystem. All implementations, configurations, and datasets are publicly available at https://anonymous.4open.science/r/MCPRadar-B143.", "source": "arxiv", "arxiv_id": "2505.16700v2", "pdf_url": "https://arxiv.org/pdf/2505.16700v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-22T14:02:37Z", "updated": "2025-10-12T14:53:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "authors": ["Ziyang Luo", "Zhiqi Shen", "Wenzhuo Yang", "Zirui Zhao", "Prathyusha Jwalapuram", "Amrita Saha", "Doyen Sahoo", "Silvio Savarese", "Caiming Xiong", "Junnan Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.14704v1", "abstract": "The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.", "source": "arxiv", "arxiv_id": "2508.14704v1", "pdf_url": "https://arxiv.org/pdf/2508.14704v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-20T13:28:58Z", "updated": "2025-08-20T13:28:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MCP-Zero: Active Tool Discovery for Autonomous LLM Agents", "authors": ["Xiang Fei", "Xiawu Zheng", "Hao Feng"], "year": 2025, "url": "http://arxiv.org/abs/2506.01056v4", "abstract": "True intelligence requires active capability acquisition, yet current LLM agents inject pre-defined tool schemas into prompts, reducing models to passive selectors and falling short of robust general-purpose agency. We introduce MCP-Zero, an active agent framework that restores tool discovery autonomy to LLMs themselves. Instead of overwhelming models with all available tools, MCP-Zero enables agents to actively identify capability gaps, and request specific tools on-demand, transforming them from large-scale retrievers into genuine autonomous agents. The framework operates through three core mechanisms: (1) Active Tool Request, where models autonomously generate structured requests specifying their exact tool requirements; (2) Hierarchical Semantic Routing, a two-stage algorithm that matches requests to relevant servers and tools through improved semantic alignment; (3) Iterative Capability Extension, enabling agents to progressively build cross-domain toolchains while maintaining minimal context footprint. We construct MCP-tools, a comprehensive dataset of 308 MCP servers and 2,797 tools from the official Model-Context-Protocol repository. Experiments demonstrate that MCP-Zero preserves agent autonomy while achieving substantial efficiency gains: (i) accurate tool selection from nearly 3k candidates across 248.1k tokens; (ii) 98\\% reduction in token consumption on APIBank while maintaining high accuracy; and (iii) consistent multi-turn performance that scales with tool ecosystem growth. This work establishes active tool discovery as a fundamental design pattern for scalable autonomous agent systems.", "source": "arxiv", "arxiv_id": "2506.01056v4", "pdf_url": "https://arxiv.org/pdf/2506.01056v4", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-01T15:48:53Z", "updated": "2025-06-24T06:27:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use", "authors": ["Wenrui Liu", "Zixiang Liu", "Elsie Dai", "Wenhan Yu", "Lei Yu", "Tong Yang", "Jinjun Han", "Hong Gao"], "year": 2025, "url": "http://arxiv.org/abs/2512.24565v3", "abstract": "Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.", "source": "arxiv", "arxiv_id": "2512.24565v3", "pdf_url": "https://arxiv.org/pdf/2512.24565v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T02:09:48Z", "updated": "2026-01-21T06:58:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models", "authors": ["Zhiwei Liu", "Jielin Qiu", "Shiyu Wang", "Jianguo Zhang", "Zuxin Liu", "Roshan Ram", "Haolin Chen", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Huan Wang", "Caiming Xiong"], "year": 2025, "url": "http://arxiv.org/abs/2507.12806v2", "abstract": "The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce MCPEval, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.", "source": "arxiv", "arxiv_id": "2507.12806v2", "pdf_url": "https://arxiv.org/pdf/2507.12806v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-17T05:46:27Z", "updated": "2025-08-01T22:37:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers", "authors": ["Zhiqiang Wang", "Yichao Gao", "Yanting Wang", "Suyuan Liu", "Haifeng Sun", "Haoran Cheng", "Guanquan Shi", "Haohua Du", "Xiangyang Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.14925v1", "abstract": "By providing a standardized interface for LLM agents to interact with external tools, the Model Context Protocol (MCP) is quickly becoming a cornerstone of the modern autonomous agent ecosystem. However, it creates novel attack surfaces due to untrusted external tools. While prior work has focused on attacks injected through external tool outputs, we investigate a more fundamental vulnerability: Tool Poisoning, where malicious instructions are embedded within a tool's metadata without execution. To date, this threat has been primarily demonstrated through isolated cases, lacking a systematic, large-scale evaluation.\n  We introduce MCPTox, the first benchmark to systematically evaluate agent robustness against Tool Poisoning in realistic MCP settings. MCPTox is constructed upon 45 live, real-world MCP servers and 353 authentic tools. To achieve this, we design three distinct attack templates to generate a comprehensive suite of 1312 malicious test cases by few-shot learning, covering 10 categories of potential risks. Our evaluation on 20 prominent LLM agents setting reveals a widespread vulnerability to Tool Poisoning, with o1-mini, achieving an attack success rate of 72.8\\%. We find that more capable models are often more susceptible, as the attack exploits their superior instruction-following abilities. Finally, the failure case analysis reveals that agents rarely refuse these attacks, with the highest refused rate (Claude-3.7-Sonnet) less than 3\\%, demonstrating that existing safety alignment is ineffective against malicious actions that use legitimate tools for unauthorized operation. Our findings create a crucial empirical baseline for understanding and mitigating this widespread threat, and we release MCPTox for the development of verifiably safer AI agents. Our dataset is available at an anonymized repository: \\textit{https://anonymous.4open.science/r/AAAI26-7C02}.", "source": "arxiv", "arxiv_id": "2508.14925v1", "pdf_url": "https://arxiv.org/pdf/2508.14925v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-19T10:12:35Z", "updated": "2025-08-19T10:12:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools", "authors": ["Nishant Subramani", "Jason Eisner", "Justin Svegliato", "Benjamin Van Durme", "Yu Su", "Sam Thomson"], "year": 2025, "url": "http://arxiv.org/abs/2504.20168v1", "abstract": "Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer's generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at https://github.com/microsoft/mice_for_cats.", "source": "arxiv", "arxiv_id": "2504.20168v1", "pdf_url": "https://arxiv.org/pdf/2504.20168v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-28T18:06:38Z", "updated": "2025-04-28T18:06:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue", "authors": ["Yujia Chen", "Changsong Li", "Yiming Wang", "Tianjie Ju", "Qingqing Xiao", "Nan Zhang", "Zifan Kong", "Peng Wang", "Binyu Yan"], "year": 2025, "url": "http://arxiv.org/abs/2502.19860v2", "abstract": "Mental health issues are worsening in today's competitive society, such as depression and anxiety. Traditional healings like counseling and chatbots fail to engage effectively, they often provide generic responses lacking emotional depth. Although large language models (LLMs) have the potential to create more human-like interactions, they still struggle to capture subtle emotions. This requires LLMs to be equipped with human-like adaptability and warmth. To fill this gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm that provides more immersive psychological healing environments. Considering the strong generative and role-playing ability of LLM agents, we predefine an interactive healing framework and assign LLM agents different roles within the framework to engage in interactive inner dialogues with users, thereby providing an immersive healing experience. We conduct extensive human experiments in various real-world healing dimensions, and find that MIND provides a more user-friendly experience than traditional paradigms. This demonstrates that MIND effectively leverages the significant potential of LLMs in psychological healing.", "source": "arxiv", "arxiv_id": "2502.19860v2", "pdf_url": "https://arxiv.org/pdf/2502.19860v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-27T08:04:27Z", "updated": "2025-09-11T05:37:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them", "authors": ["Weichen Zhang", "Yiyou Sun", "Pohao Huang", "Jiayue Pu", "Heyue Lin", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2507.21017v1", "abstract": "Hallucinations pose critical risks for large language model (LLM)-based agents, often manifesting as hallucinative actions resulting from fabricated or misinterpreted information within the cognitive context. While recent studies have exposed such failures, existing evaluations remain fragmented and lack a principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions in Risky AGEnt settings--the first unified benchmark for eliciting and evaluating hallucinations in interactive LLM-agent scenarios. We begin by introducing a three-part taxonomy to address agentic hallucinations: actions that are unfaithful to (i) task instructions, (ii) execution history, or (iii) environment observations. To analyze, we first elicit such failures by performing a systematic audit of existing agent benchmarks, then synthesize test cases using a snapshot strategy that isolates decision points in deterministic and reproducible manners. To evaluate hallucination behaviors, we adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware prompts, enabling scalable, high-fidelity assessment of agent actions without enumerating full action spaces. MIRAGE-Bench provides actionable insights on failure modes of LLM agents and lays the groundwork for principled progress in mitigating hallucinations in interactive environments.", "source": "arxiv", "arxiv_id": "2507.21017v1", "pdf_url": "https://arxiv.org/pdf/2507.21017v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-28T17:38:29Z", "updated": "2025-07-28T17:38:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering", "authors": ["Zexi Liu", "Jingyi Chai", "Xinyu Zhu", "Shuo Tang", "Rui Ye", "Bo Zhang", "Lei Bai", "Siheng Chen"], "year": 2025, "url": "http://arxiv.org/abs/2505.23723v1", "abstract": "The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.", "source": "arxiv", "arxiv_id": "2505.23723v1", "pdf_url": "https://arxiv.org/pdf/2505.23723v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-29T17:54:44Z", "updated": "2025-05-29T17:54:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ML-SceGen: A Multi-level Scenario Generation Framework", "authors": ["Yicheng Xiao", "Yangyang Sun", "Yicheng Lin"], "year": 2025, "url": "http://arxiv.org/abs/2501.10782v1", "abstract": "Current scientific research witnesses various attempts at applying Large Language Models for scenario generation but is inclined only to comprehensive or dangerous scenarios. In this paper, we seek to build a three-stage framework that not only lets users regain controllability over the generated scenarios but also generates comprehensive scenarios containing danger factors in uncontrolled intersection settings. In the first stage, LLM agents will contribute to translating the key components of the description of the expected scenarios into Functional Scenarios. For the second stage, we use Answer Set Programming (ASP) solver Clingo to help us generate comprehensive logical traffic within intersections. During the last stage, we use LLM to update relevant parameters to increase the critical level of the concrete scenario.", "source": "arxiv", "arxiv_id": "2501.10782v1", "pdf_url": "https://arxiv.org/pdf/2501.10782v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-18T14:43:40Z", "updated": "2025-01-18T14:43:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments", "authors": ["Xiao Yang", "Jiawei Chen", "Jun Luo", "Zhengwei Fang", "Yinpeng Dong", "Hang Su", "Jun Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2506.01616v1", "abstract": "The emergence of multimodal LLM-based agents (MLAs) has transformed interaction paradigms by seamlessly integrating vision, language, action and dynamic environments, enabling unprecedented autonomous capabilities across GUI applications ranging from web automation to mobile systems. However, MLAs introduce critical trustworthiness challenges that extend far beyond traditional language models' limitations, as they can directly modify digital states and trigger irreversible real-world consequences. Existing benchmarks inadequately tackle these unique challenges posed by MLAs' actionable outputs, long-horizon uncertainty and multimodal attack vectors. In this paper, we introduce MLA-Trust, the first comprehensive and unified framework that evaluates the MLA trustworthiness across four principled dimensions: truthfulness, controllability, safety and privacy. We utilize websites and mobile applications as realistic testbeds, designing 34 high-risk interactive tasks and curating rich evaluation datasets. Large-scale experiments involving 13 state-of-the-art agents reveal previously unexplored trustworthiness vulnerabilities unique to multimodal interactive scenarios. For instance, proprietary and open-source GUI-interacting MLAs pose more severe trustworthiness risks than static MLLMs, particularly in high-stakes domains; the transition from static MLLMs into interactive MLAs considerably compromises trustworthiness, enabling harmful content generation in multi-step interactions that standalone MLLMs would typically prevent; multi-step execution, while enhancing the adaptability of MLAs, involves latent nonlinear risk accumulation across successive interactions, circumventing existing safeguards and resulting in unpredictable derived risks. Moreover, we present an extensible toolbox to facilitate continuous evaluation of MLA trustworthiness across diverse interactive environments.", "source": "arxiv", "arxiv_id": "2506.01616v1", "pdf_url": "https://arxiv.org/pdf/2506.01616v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-02T12:56:27Z", "updated": "2025-06-02T12:56:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "authors": ["Rushi Qiang", "Yuchen Zhuang", "Yinghao Li", "Dingu Sagar V K", "Rongzhi Zhang", "Changhao Li", "Ian Shu-Hei Wong", "Sherry Yang", "Percy Liang", "Chao Zhang", "Bo Dai"], "year": 2025, "url": "http://arxiv.org/abs/2505.07782v1", "abstract": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents.", "source": "arxiv", "arxiv_id": "2505.07782v1", "pdf_url": "https://arxiv.org/pdf/2505.07782v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-12T17:35:43Z", "updated": "2025-05-12T17:35:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents", "authors": ["Deepak Nathani", "Lovish Madaan", "Nicholas Roberts", "Nikolay Bashlykov", "Ajay Menon", "Vincent Moens", "Amar Budhiraja", "Despoina Magka", "Vladislav Vorotilov", "Gaurav Chaurasia", "Dieuwke Hupkes", "Ricardo Silveira Cabral", "Tatiana Shavrina", "Jakob Foerster", "Yoram Bachrach", "William Yang Wang", "Roberta Raileanu"], "year": 2025, "url": "http://arxiv.org/abs/2502.14499v1", "abstract": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.", "source": "arxiv", "arxiv_id": "2502.14499v1", "pdf_url": "https://arxiv.org/pdf/2502.14499v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-20T12:28:23Z", "updated": "2025-02-20T12:28:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?", "authors": ["Yunxiang Zhang", "Muhammad Khalifa", "Shitanshu Bhushan", "Grant D Murphy", "Lajanugen Logeswaran", "Jaekyeom Kim", "Moontae Lee", "Honglak Lee", "Lu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.09702v3", "abstract": "We introduce MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions, with a focus on open research problems that demand novel methodologies. Unlike prior work, e.g., AI Scientist, which evaluates the end-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the key steps of proposing and implementing novel research methods and evaluates them with rigorous protocol and objective metrics. Our curated suite of 7 competition tasks reveals significant challenges for LLM agents. Even the best-performing tested agent (gemini-exp-1206 under MLAB) closes only 9.3% of the gap between baseline and top human participant scores. Furthermore, our analysis reveals a misalignment between the LLM-judged innovation and actual performance on cutting-edge ML research problems. MLRC-Bench is a dynamic benchmark, designed to grow with new ML competitions and encourage rigorous, objective evaluations of AI research capabilities. Our leaderboard and code are available at: https://huggingface.co/spaces/launch/MLRC_Bench", "source": "arxiv", "arxiv_id": "2504.09702v3", "pdf_url": "https://arxiv.org/pdf/2504.09702v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-13T19:35:43Z", "updated": "2025-10-24T14:48:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices", "authors": ["Patara Trirat", "Jae-Gil Lee"], "year": 2025, "url": "http://arxiv.org/abs/2505.10607v2", "abstract": "The growing use of smartphones and IoT devices necessitates efficient time-series analysis on resource-constrained hardware, which is critical for sensing applications such as human activity recognition and air quality prediction. Recent efforts in hardware-aware neural architecture search (NAS) automate architecture discovery for specific platforms; however, none focus on general time-series analysis with edge deployment. Leveraging the problem-solving and reasoning capabilities of large language models (LLM), we propose MONAQ, a novel framework that reformulates NAS into Multi-Objective Neural Architecture Querying tasks. MONAQ is equipped with multimodal query generation for processing multimodal time-series inputs and hardware constraints, alongside an LLM agent-based multi-objective search to achieve deployment-ready models via code generation. By integrating numerical data, time-series images, and textual descriptions, MONAQ improves an LLM's understanding of time-series data. Experiments on fifteen datasets demonstrate that MONAQ-discovered models outperform both handcrafted models and NAS baselines while being more efficient.", "source": "arxiv", "arxiv_id": "2505.10607v2", "pdf_url": "https://arxiv.org/pdf/2505.10607v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-15T16:35:33Z", "updated": "2025-10-07T18:22:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations", "authors": ["Genglin Liu", "Vivian Le", "Salman Rahman", "Elisa Kreiss", "Marzyeh Ghassemi", "Saadia Gabriel"], "year": 2025, "url": "http://arxiv.org/abs/2504.07830v3", "abstract": "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.", "source": "arxiv", "arxiv_id": "2504.07830v3", "pdf_url": "https://arxiv.org/pdf/2504.07830v3", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-10T15:06:54Z", "updated": "2025-10-26T06:27:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MPMA: Preference Manipulation Attack Against Model Context Protocol", "authors": ["Zihan Wang", "Rui Zhang", "Yu Liu", "Wenshu Fan", "Wenbo Jiang", "Qingchuan Zhao", "Hongwei Li", "Guowen Xu"], "year": 2025, "url": "http://arxiv.org/abs/2505.11154v2", "abstract": "Model Context Protocol (MCP) standardizes interface mapping for large language models (LLMs) to access external data and tools, which revolutionizes the paradigm of tool selection and facilitates the rapid expansion of the LLM agent tool ecosystem. However, as the MCP is increasingly adopted, third-party customized versions of the MCP server expose potential security vulnerabilities. In this paper, we first introduce a novel security threat, which we term the MCP Preference Manipulation Attack (MPMA). An attacker deploys a customized MCP server to manipulate LLMs, causing them to prioritize it over other competing MCP servers. This can result in economic benefits for attackers, such as revenue from paid MCP services or advertising income generated from free servers. To achieve MPMA, we first design a Direct Preference Manipulation Attack (DPMA) that achieves significant effectiveness by inserting the manipulative word and phrases into the tool name and description. However, such a direct modification is obvious to users and lacks stealthiness. To address these limitations, we further propose Genetic-based Advertising Preference Manipulation Attack (GAPMA). GAPMA employs four commonly used strategies to initialize descriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness. The experiment results demonstrate that GAPMA balances high effectiveness and stealthiness. Our study reveals a critical vulnerability of the MCP in open ecosystems, highlighting an urgent need for robust defense mechanisms to ensure the fairness of the MCP ecosystem.", "source": "arxiv", "arxiv_id": "2505.11154v2", "pdf_url": "https://arxiv.org/pdf/2505.11154v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-16T11:55:12Z", "updated": "2025-11-11T14:42:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MPO: Boosting LLM Agents with Meta Plan Optimization", "authors": ["Weimin Xiong", "Yifan Song", "Qingxiu Dong", "Bingchan Zhao", "Feifan Song", "Xun Wang", "Sujian Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.02682v2", "abstract": "Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, , which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.", "source": "arxiv", "arxiv_id": "2503.02682v2", "pdf_url": "https://arxiv.org/pdf/2503.02682v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-04T14:54:45Z", "updated": "2025-09-10T16:45:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents", "authors": ["Yuzhen Lei", "Hongbin Xie", "Jiaxing Zhao", "Shuangxue Liu", "Xuan Song"], "year": 2025, "url": "http://arxiv.org/abs/2509.17628v1", "abstract": "Large Language Models (LLMs) have excelled in question-answering (QA) tasks within single domains. However, their reasoning and coordination capabilities in complex, multi-stage scenarios remain underexplored. Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking models' abilities for multi-stage collaboration and optimization without explicit external guidance. To bridge this gap, we propose \\textbf{MSCoRe}, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality. Tasks are further categorized into three difficulty levels according to stage coverage and complexity. With MSCoRe, we have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the models' robustness and found that their performance is negatively affected by noisy data. MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents. The code and data are available at https://github.com/D3E0-source/MSCoRE.", "source": "arxiv", "arxiv_id": "2509.17628v1", "pdf_url": "https://arxiv.org/pdf/2509.17628v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-22T11:36:16Z", "updated": "2025-09-22T11:36:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization", "authors": ["Hyomin Kim", "Yunhui Jang", "Sungsoo Ahn"], "year": 2025, "url": "http://arxiv.org/abs/2505.20820v1", "abstract": "Large language models (LLMs) have large potential for molecular optimization, as they can gather external chemistry tools and enable collaborative interactions to iteratively refine molecular candidates. However, this potential remains underexplored, particularly in the context of structured reasoning, interpretability, and comprehensive tool-grounded molecular optimization. To address this gap, we introduce MT-Mol, a multi-agent framework for molecular optimization that leverages tool-guided reasoning and role-specialized LLM agents. Our system incorporates comprehensive RDKit tools, categorized into five distinct domains: structural descriptors, electronic and topological features, fragment-based functional groups, molecular representations, and miscellaneous chemical properties. Each category is managed by an expert analyst agent, responsible for extracting task-relevant tools and enabling interpretable, chemically grounded feedback. MT-Mol produces molecules with tool-aligned and stepwise reasoning through the interaction between the analyst agents, a molecule-generating scientist, a reasoning-output verifier, and a reviewer agent. As a result, we show that our framework shows the state-of-the-art performance of the PMO-1K benchmark on 17 out of 23 tasks.", "source": "arxiv", "arxiv_id": "2505.20820v1", "pdf_url": "https://arxiv.org/pdf/2505.20820v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-27T07:27:30Z", "updated": "2025-05-27T07:27:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents", "authors": ["Wenxiao Wang", "Priyatham Kattakinda", "Soheil Feizi"], "year": 2025, "url": "http://arxiv.org/abs/2509.04642v1", "abstract": "Building reliable LLM agents requires decisions at two levels: the graph (which modules exist and how information flows) and the configuration of each node (models, prompts, tools, control knobs). Most existing optimizers tune configurations while holding the graph fixed, leaving structural failure modes unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for LLM agents that jointly searches over graphs and configurations to maximize agent quality, subject to explicit rollout/token budgets. Beyond numeric metrics, Maestro leverages reflective textual feedback from traces to prioritize edits, improving sample efficiency and targeting specific failure modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%, 4.9%, and 4.86%, respectively; even when restricted to prompt-only optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these results with far fewer rollouts than GEPA. We further show large gains on two applications (interviewer & RAG agents), highlighting that joint graph & configuration search addresses structural failure modes that prompt tuning alone cannot fix.", "source": "arxiv", "arxiv_id": "2509.04642v1", "pdf_url": "https://arxiv.org/pdf/2509.04642v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-04T20:00:37Z", "updated": "2025-09-04T20:00:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Magnet: Multi-turn Tool-use Data Synthesis and Distillation via Graph Translation", "authors": ["Fan Yin", "Zifeng Wang", "I-Hung Hsu", "Jun Yan", "Ke Jiang", "Yanfei Chen", "Jindong Gu", "Long T. Le", "Kai-Wei Chang", "Chen-Yu Lee", "Hamid Palangi", "Tomas Pfister"], "year": 2025, "url": "http://arxiv.org/abs/2503.07826v1", "abstract": "Large language models (LLMs) have exhibited the ability to effectively utilize external tools to address user queries. However, their performance may be limited in complex, multi-turn interactions involving users and multiple tools. To address this, we propose Magnet, a principled framework for synthesizing high-quality training trajectories to enhance the function calling capability of large language model agents in multi-turn conversations with humans. The framework is based on automatic and iterative translations from a function signature path to a sequence of queries and executable function calls. We model the complicated function interactions in multi-turn cases with graph and design novel node operations to build reliable signature paths. Motivated by context distillation, when guiding the generation of positive and negative trajectories using a teacher model, we provide reference function call sequences as positive hints in context and contrastive, incorrect function calls as negative hints. Experiments show that training with the positive trajectories with supervised fine-tuning and preference optimization against negative trajectories, our 14B model, Magnet-14B-mDPO, obtains 68.01 on BFCL-v3 and 73.30 on ToolQuery, surpassing the performance of the teacher model Gemini-1.5-pro-002 by a large margin in function calling.", "source": "arxiv", "arxiv_id": "2503.07826v1", "pdf_url": "https://arxiv.org/pdf/2503.07826v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-10T20:13:07Z", "updated": "2025-03-10T20:13:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree", "authors": ["Sam Johnson", "Viet Pham", "Thai Le"], "year": 2025, "url": "http://arxiv.org/abs/2507.14799v1", "abstract": "This work demonstrates that LLM-based web navigation agents offer powerful automation capabilities but are vulnerable to Indirect Prompt Injection (IPI) attacks. We show that adversaries can embed universal adversarial triggers in webpage HTML to hijack agent behavior that utilizes the accessibility tree to parse HTML, causing unintended or malicious actions. Using the Greedy Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by Llama-3.1, our system demonstrates high success rates across real websites in both targeted and general attacks, including login credential exfiltration and forced ad clicks. Our empirical results highlight critical security risks and the need for stronger defenses as LLM-driven autonomous web agents become more widely adopted. The system software (https://github.com/sej2020/manipulating-web-agents) is released under the MIT License, with an accompanying publicly available demo website (http://lethaiq.github.io/attack-web-llm-agent).", "source": "arxiv", "arxiv_id": "2507.14799v1", "pdf_url": "https://arxiv.org/pdf/2507.14799v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-20T03:10:13Z", "updated": "2025-07-20T03:10:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MapStory: Prototyping Editable Map Animations with LLM Agents", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "year": 2025, "url": "http://arxiv.org/abs/2505.21966v2", "abstract": "We introduce MapStory, an LLM-powered animation prototyping tool that generates editable map animation sequences directly from natural language text by leveraging a dual-agent LLM architecture. Given a user written script, MapStory automatically produces a scene breakdown, which decomposes the text into key map animation primitives such as camera movements, visual highlights, and animated elements. Our system includes a researcher agent that accurately queries geospatial information by leveraging an LLM with web search, enabling automatic extraction of relevant regions, paths, and coordinates while allowing users to edit and query for changes or additional information to refine the results. Additionally, users can fine-tune parameters of these primitive blocks through an interactive timeline editor. We detail the system's design and architecture, informed by formative interviews with professional animators and by an analysis of 200 existing map animation videos. Our evaluation, which includes expert interviews (N=5) and a usability study (N=12), demonstrates that MapStory enables users to create map animations with ease, facilitates faster iteration, encourages creative exploration, and lowers barriers to creating map-centric stories.", "source": "arxiv", "arxiv_id": "2505.21966v2", "pdf_url": "https://arxiv.org/pdf/2505.21966v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-05-28T04:36:08Z", "updated": "2025-08-13T08:00:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Marco: Configurable Graph-Based Task Solving and Multi-AI Agents Framework for Hardware Design", "authors": ["Chia-Tung Ho", "Jing Gong", "Yunsheng Bai", "Chenhui Deng", "Haoxing Ren", "Brucek Khailany"], "year": 2025, "url": "http://arxiv.org/abs/2504.01962v1", "abstract": "Hardware design presents numerous challenges stemming from its complexity and advancing technologies. These challenges result in longer turn-around-time (TAT) for optimizing performance, power, area, and cost (PPAC) during synthesis, verification, physical design, and reliability loops. Large Language Models (LLMs) have shown remarkable capacity to comprehend and generate natural language at a massive scale, leading to many potential applications and benefits across various domains. Successful LLM-based agents for hardware design can drastically reduce TAT, leading to faster product cycles, lower costs, improved design reliability and reduced risk of costly errors. In this work, we propose a unified framework, Marco, that integrates configurable graph-based task solving with multi-modality and multi-AI agents for chip design by leveraging the natural language and reasoning abilities with collaborative toolkits. Lastly, we demonstrate promising performance, productivity, and efficiency of LLM agents by leveraging the Marco framework on layout optimization, Verilog/design rule checker (DRC) coding, and timing analysis tasks.", "source": "arxiv", "arxiv_id": "2504.01962v1", "pdf_url": "https://arxiv.org/pdf/2504.01962v1", "categories": ["cs.AR"], "primary_category": "cs.AR", "doi": "", "venue": "", "published": "2025-02-25T05:31:53Z", "updated": "2025-02-25T05:31:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents", "authors": ["George Fatouros", "Kostas Metaxas", "John Soldatos", "Manos Karathanassis"], "year": 2025, "url": "http://arxiv.org/abs/2502.00415v2", "abstract": "MarketSenseAI is a novel framework for holistic stock analysis which leverages Large Language Models (LLMs) to process financial news, historical prices, company fundamentals and the macroeconomic environment to support decision making in stock analysis and selection. In this paper, we present the latest advancements on MarketSenseAI, driven by rapid technological expansion in LLMs. Through a novel architecture combining Retrieval-Augmented Generation and LLM agents, the framework processes SEC filings and earnings calls, while enriching macroeconomic analysis through systematic processing of diverse institutional reports. We demonstrate a significant improvement in fundamental analysis accuracy over the previous version. Empirical evaluation on S\\&P 100 stocks over two years (2023-2024) shows MarketSenseAI achieving cumulative returns of 125.9% compared to the index return of 73.5%, while maintaining comparable risk profiles. Further validation on S\\&P 500 stocks during 2024 demonstrates the framework's scalability, delivering a 33.8% higher Sortino ratio than the market. This work marks a significant advancement in applying LLM technology to financial analysis, offering insights into the robustness of LLM-driven investment strategies.", "source": "arxiv", "arxiv_id": "2502.00415v2", "pdf_url": "https://arxiv.org/pdf/2502.00415v2", "categories": ["q-fin.CP", "cs.AI", "cs.CL", "cs.MA", "q-fin.PM"], "primary_category": "q-fin.CP", "doi": "", "venue": "", "published": "2025-02-01T12:33:23Z", "updated": "2025-10-03T06:17:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Mastering Da Vinci Code: A Comparative Study of Transformer, LLM, and PPO-based Agents", "authors": ["LeCheng Zhang", "Yuanshi Wang", "Haotian Shen", "Xujie Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.12801v1", "abstract": "The Da Vinci Code, a game of logical deduction and imperfect information, presents unique challenges for artificial intelligence, demanding nuanced reasoning beyond simple pattern recognition. This paper investigates the efficacy of various AI paradigms in mastering this game. We develop and evaluate three distinct agent architectures: a Transformer-based baseline model with limited historical context, several Large Language Model (LLM) agents (including Gemini, DeepSeek, and GPT variants) guided by structured prompts, and an agent based on Proximal Policy Optimization (PPO) employing a Transformer encoder for comprehensive game history processing. Performance is benchmarked against the baseline, with the PPO-based agent demonstrating superior win rates ($58.5\\% \\pm 1.0\\%$), significantly outperforming the LLM counterparts. Our analysis highlights the strengths of deep reinforcement learning in policy refinement for complex deductive tasks, particularly in learning implicit strategies from self-play. We also examine the capabilities and inherent limitations of current LLMs in maintaining strict logical consistency and strategic depth over extended gameplay, despite sophisticated prompting. This study contributes to the broader understanding of AI in recreational games involving hidden information and multi-step logical reasoning, offering insights into effective agent design and the comparative advantages of different AI approaches.", "source": "arxiv", "arxiv_id": "2506.12801v1", "pdf_url": "https://arxiv.org/pdf/2506.12801v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-15T10:33:30Z", "updated": "2025-06-15T10:33:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents", "authors": ["Irene Testini", "JosÃ© HernÃ¡ndez-Orallo", "Lorenzo Pacchiardi"], "year": 2025, "url": "http://arxiv.org/abs/2506.08800v2", "abstract": "Data science aims to extract insights from data to support decision-making processes. Recently, Large Language Models (LLMs) have been increasingly used as assistants for data science, by suggesting ideas, techniques and small code snippets, or for the interpretation of results and reporting. Proper automation of some data-science activities is now promised by the rise of LLM agents, i.e., AI systems powered by an LLM equipped with additional affordances--such as code execution and knowledge bases--that can perform self-directed actions and interact with digital environments. In this paper, we survey the evaluation of LLM assistants and agents for data science. We find (1) a dominant focus on a small subset of goal-oriented activities, largely ignoring data management and exploratory activities; (2) a concentration on pure assistance or fully autonomous agents, without considering intermediate levels of human-AI collaboration; and (3) an emphasis on human substitution, therefore neglecting the possibility of higher levels of automation thanks to task transformation.", "source": "arxiv", "arxiv_id": "2506.08800v2", "pdf_url": "https://arxiv.org/pdf/2506.08800v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-10T13:47:22Z", "updated": "2025-10-22T17:14:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy", "authors": ["Wenkai Li", "Lynnette Hui Xian Ng", "Andy Liu", "Daniel Fried"], "year": 2025, "url": "http://arxiv.org/abs/2512.18292v1", "abstract": "The study of negotiation styles dates back to Aristotle's ethos-pathos-logos rhetoric. Prior efforts primarily studied the success of negotiation agents. Here, we shift the focus towards the styles of negotiation strategies. Our focus is the strategic dialogue board game Diplomacy, which affords rich natural language negotiation and measures of game success. We used LLM-as-a-judge to annotate a large human-human set of Diplomacy games for fine-grained negotiation tactics from a sociologically-grounded taxonomy. Using a combination of the It Takes Two and WebDiplomacy datasets, we demonstrate the reliability of our LLM-as-a-Judge framework and show strong correlations between negotiation features and success in the Diplomacy setting. Lastly, we investigate the differences between LLM and human negotiation strategies and show that fine-tuning can steer LLM agents toward more human-like negotiation behaviors.", "source": "arxiv", "arxiv_id": "2512.18292v1", "pdf_url": "https://arxiv.org/pdf/2512.18292v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-12-20T09:33:55Z", "updated": "2025-12-20T09:33:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab", "authors": ["Haonan Duan", "Stephen Zhewen Lu", "Caitlin Fiona Harrigan", "Nishkrit Desai", "Jiarui Lu", "MichaÅ Koziarski", "Leonardo Cotta", "Chris J. Maddison"], "year": 2025, "url": "http://arxiv.org/abs/2507.02083v2", "abstract": "Designing experiments and result interpretations are core scientific competencies, particularly in biology, where researchers perturb complex systems to uncover the underlying systems. Recent efforts to evaluate the scientific capabilities of large language models (LLMs) fail to test these competencies because wet-lab experimentation is prohibitively expensive: in expertise, time and equipment. We introduce SciGym, a first-in-class benchmark that assesses LLMs' iterative experiment design and analysis abilities in open-ended scientific discovery tasks. SciGym overcomes the challenge of wet-lab costs by running a dry lab of biological systems. These models, encoded in Systems Biology Markup Language, are efficient for generating simulated data, making them ideal testbeds for experimentation on realistically complex systems. We evaluated six frontier LLMs on 137 small systems, and released a total of 350 systems. Our evaluation shows that while more capable models demonstrated superior performance, all models' performance declined significantly as system complexity increased, suggesting substantial room for improvement in the scientific capabilities of LLM agents.", "source": "arxiv", "arxiv_id": "2507.02083v2", "pdf_url": "https://arxiv.org/pdf/2507.02083v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-02T18:41:44Z", "updated": "2025-07-14T15:17:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Measuring temporal effects of agent knowledge by date-controlled tool use", "authors": ["R. Patrick Xian", "Qiming Cui", "Stefan Bauer", "Reza Abbasi-Asl"], "year": 2025, "url": "http://arxiv.org/abs/2503.04188v2", "abstract": "Temporal progression is an integral part of knowledge accumulation and update. Web search is frequently adopted as grounding for agent knowledge, yet an improper configuration affects the quality of the agent's responses. Here, we assess the agent behavior using distinct date-controlled tools (DCTs) as stress test to measure the knowledge variability of large language model (LLM) agents. We demonstrate the temporal effects of an LLM agent as a writing assistant, which uses web search to complete scientific publication abstracts. We show that the temporality of search engine translates into tool-dependent agent performance but can be alleviated with base model choice and explicit reasoning instructions such as chain-of-thought prompting. Our results indicate that agent design and evaluations should take a dynamical view and implement measures to account for the temporal influence of external resources to ensure reliability.", "source": "arxiv", "arxiv_id": "2503.04188v2", "pdf_url": "https://arxiv.org/pdf/2503.04188v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-06T08:03:51Z", "updated": "2025-04-03T17:53:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Measuring the Security of Mobile LLM Agents under Adversarial Prompts from Untrusted Third-Party Channels", "authors": ["Chenghao Du", "Quanfeng Huang", "Tingxuan Tang", "Zihao Wang", "Adwait Nadkarni", "Yue Xiao"], "year": 2025, "url": "http://arxiv.org/abs/2510.27140v2", "abstract": "Large Language Models (LLMs) have transformed software development, enabling AI-powered applications known as LLM-based agents that promise to automate tasks across diverse apps and workflows. Yet, the security implications of deploying such agents in adversarial mobile environments remain poorly understood. In this paper, we present the first systematic study of security risks in mobile LLM agents. We design and evaluate a suite of adversarial case studies, ranging from opportunistic manipulations such as pop-up advertisements to advanced, end-to-end workflows involving malware installation and cross-app data exfiltration. Our evaluation covers eight state-of-the-art mobile agents across three architectures, with over 2,000 adversarial and paired benign trials. The results reveal systemic vulnerabilities: low-barrier vectors such as fraudulent ads succeed with over 80% reliability, while even workflows requiring the circumvention of operating-system warnings, such as malware installation, are consistently completed by advanced multi-app agents. By mapping these attacks to the MITRE ATT&CK Mobile framework, we uncover novel privilege-escalation and persistence pathways unique to LLM-driven automation. Collectively, our findings provide the first end-to-end evidence that mobile LLM agents are exploitable in realistic adversarial settings, where untrusted third-party channels (e.g., ads, embedded webviews, cross-app notifications) are an inherent part of the mobile ecosystem.", "source": "arxiv", "arxiv_id": "2510.27140v2", "pdf_url": "https://arxiv.org/pdf/2510.27140v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-31T03:35:59Z", "updated": "2025-11-06T03:52:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical LLM Agents", "authors": ["Yixing Jiang", "Kameron C. Black", "Gloria Geng", "Danny Park", "James Zou", "Andrew Y. Ng", "Jonathan H. Chen"], "year": 2025, "url": "http://arxiv.org/abs/2501.14654v2", "abstract": "Recent large language models (LLMs) have demonstrated significant advancements, particularly in their ability to serve as agents thereby surpassing their traditional role as chatbots. These agents can leverage their planning and tool utilization capabilities to address tasks specified at a high level. However, a standardized dataset to benchmark the agent capabilities of LLMs in medical applications is currently lacking, making the evaluation of LLMs on complex tasks in interactive healthcare environments challenging. To address this gap, we introduce MedAgentBench, a broad evaluation suite designed to assess the agent capabilities of large language models within medical records contexts. MedAgentBench encompasses 300 patient-specific clinically-derived tasks from 10 categories written by human physicians, realistic profiles of 100 patients with over 700,000 data elements, a FHIR-compliant interactive environment, and an accompanying codebase. The environment uses the standard APIs and communication infrastructure used in modern EMR systems, so it can be easily migrated into live EMR systems. MedAgentBench presents an unsaturated agent-oriented benchmark that current state-of-the-art LLMs exhibit some ability to succeed at. The best model (Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is still substantial space for improvement which gives the community a next direction to optimize. Furthermore, there is significant variation in performance across task categories. MedAgentBench establishes this and is publicly available at https://github.com/stanfordmlgroup/MedAgentBench , offering a valuable framework for model developers to track progress and drive continuous improvements in the agent capabilities of large language models within the medical domain.", "source": "arxiv", "arxiv_id": "2501.14654v2", "pdf_url": "https://arxiv.org/pdf/2501.14654v2", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-01-24T17:21:01Z", "updated": "2025-02-12T05:32:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MedAgentGym: A Scalable Agentic Training Environment for Code-Centric Reasoning in Biomedical Data Science", "authors": ["Ran Xu", "Yuchen Zhuang", "Yishan Zhong", "Yue Yu", "Zifeng Wang", "Xiangru Tang", "Hang Wu", "May D. Wang", "Peifeng Ruan", "Donghan Yang", "Tao Wang", "Guanghua Xiao", "Xin Liu", "Carl Yang", "Yang Xie", "Wenqi Shi"], "year": 2025, "url": "http://arxiv.org/abs/2506.04405v2", "abstract": "We introduce MedAgentGym, a scalable and interactive training environment designed to enhance coding-based biomedical reasoning capabilities in large language model (LLM) agents. MedAgentGym comprises 72,413 task instances across 129 categories derived from 12 authentic real-world biomedical scenarios. Tasks are encapsulated within executable sandbox environments, each featuring detailed task specifications, interactive feedback mechanisms, verifiable ground truth annotations, and scalable training trajectory generation. Extensive benchmarking of 29 LLMs reveals substantial performance disparities in biomedical data science between commercial and open-source LLMs. Leveraging efficient multi-threaded and multi-turn trajectory sampling in MedAgentGym, Med-Copilot achieves performance gains of +43.02% and +45.28% from offline and online reinforcement learning, respectively, demonstrating MedAgentGym as an effective training ground while establishing itself as a cost-effective, privacy-preserving alternative competitive with proprietary LLMs (gpt-4o). By offering a unified execution environment with a comprehensive benchmark and accessible, extensible training resources, MedAgentGym delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical data science.", "source": "arxiv", "arxiv_id": "2506.04405v2", "pdf_url": "https://arxiv.org/pdf/2506.04405v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-04T19:38:55Z", "updated": "2025-10-05T17:59:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models", "authors": ["Philip R. Liu", "Sparsh Bansal", "Jimmy Dinh", "Aditya Pawar", "Ramani Satishkumar", "Shail Desai", "Neeraj Gupta", "Xin Wang", "Shu Hu"], "year": 2025, "url": "http://arxiv.org/abs/2506.07400v3", "abstract": "The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.", "source": "arxiv", "arxiv_id": "2506.07400v3", "pdf_url": "https://arxiv.org/pdf/2506.07400v3", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.MA", "doi": "10.1109/MIPR67560.2025.00078", "venue": "Proc. 2025 IEEE 8th International Conference on Multimedia Information Processing and Retrieval (MIPR), pp. 456-462, 2025", "published": "2025-06-09T03:51:18Z", "updated": "2025-12-16T22:52:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph", "authors": ["Duzhen Zhang", "Zixiao Wang", "Zhong-Zhi Li", "Yahan Yu", "Shuncheng Jia", "Jiahua Dong", "Haotian Xu", "Xing Wu", "Yingying Zhang", "Tielin Zhang", "Jie Yang", "Xiuying Chen", "Le Song"], "year": 2025, "url": "http://arxiv.org/abs/2508.12393v2", "abstract": "The rapid expansion of medical literature presents growing challenges for structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs) offer a promising solution by enabling efficient retrieval, automated reasoning, and knowledge discovery. However, current KG construction methods often rely on supervised pipelines with limited generalizability or naively aggregate outputs from Large Language Models (LLMs), treating biomedical corpora as static and ignoring the temporal dynamics and contextual uncertainty of evolving knowledge. To address these limitations, we introduce MedKGent, a LLM agent framework for constructing temporally evolving medical KGs. Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we simulate the emergence of biomedical knowledge via a fine-grained daily time series. MedKGent incrementally builds the KG in a day-by-day manner using two specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor Agent identifies knowledge triples and assigns confidence scores via sampling-based estimation, which are used to filter low-confidence extractions and inform downstream processing. The Constructor Agent incrementally integrates the retained triples into a temporally evolving graph, guided by confidence scores and timestamps to reinforce recurring knowledge and resolve conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines. Case studies further demonstrate the KG's value in literature-based drug repurposing via confidence-aware causal inference.", "source": "arxiv", "arxiv_id": "2508.12393v2", "pdf_url": "https://arxiv.org/pdf/2508.12393v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-17T15:14:03Z", "updated": "2025-08-19T05:18:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MedicalOS: An LLM Agent based Operating System for Digital Healthcare", "authors": ["Jared Zhu", "Junde Wu"], "year": 2025, "url": "http://arxiv.org/abs/2509.11507v1", "abstract": "Decades' advances in digital health technologies, such as electronic health records, have largely streamlined routine clinical processes. Yet, most these systems are still hard to learn and use: Clinicians often face the burden of managing multiple tools, repeating manual actions for each patient, navigating complicated UI trees to locate functions, and spending significant time on administration instead of caring for patients. The recent rise of large language model (LLM) based agents demonstrates exceptional capability in coding and computer operation, revealing the potential for humans to interact with operating systems and software not by direct manipulation, but by instructing agents through natural language. This shift highlights the need for an abstraction layer, an agent-computer interface, that translates human language into machine-executable commands. In digital healthcare, however, requires a more domain-specific abstractions that strictly follow trusted clinical guidelines and procedural standards to ensure safety, transparency, and compliance. To address this need, we present \\textbf{MedicalOS}, a unified agent-based operational system designed as such a domain-specific abstract layer for healthcare. It translates human instructions into pre-defined digital healthcare commands, such as patient inquiry, history retrieval, exam management, report generation, referrals, treatment planning, that we wrapped as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP, Linux). We empirically validate MedicalOS on 214 patient cases across 22 specialties, demonstrating high diagnostic accuracy and confidence, clinically sound examination requests, and consistent generation of structured reports and medication recommendations. These results highlight MedicalOS as a trustworthy and scalable foundation for advancing workflow automation in clinical practice.", "source": "arxiv", "arxiv_id": "2509.11507v1", "pdf_url": "https://arxiv.org/pdf/2509.11507v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-15T01:43:20Z", "updated": "2025-09-15T01:43:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MemGuide: Intent-Driven Memory Selection for Goal-Oriented Multi-Session LLM Agents", "authors": ["Yiming Du", "Bingbing Wang", "Yang He", "Bin Liang", "Baojun Wang", "Zhongyang Li", "Lin Gui", "Jeff Z. Pan", "Ruifeng Xu", "Kam-Fai Wong"], "year": 2025, "url": "http://arxiv.org/abs/2505.20231v2", "abstract": "Modern task-oriented dialogue (TOD) systems increasingly rely on large language model (LLM) agents, leveraging Retrieval-Augmented Generation (RAG) and long-context capabilities for long-term memory utilization. However, these methods are primarily based on semantic similarity, overlooking task intent and reducing task coherence in multi-session dialogues. To address this challenge, we introduce MemGuide, a two-stage framework for intent-driven memory selection. (1) Intent-Aligned Retrieval matches the current dialogue context with stored intent descriptions in the memory bank, retrieving QA-formatted memory units that share the same goal. (2) Missing-Slot Guided Filtering employs a chain-of-thought slot reasoner to enumerate unfilled slots, then uses a fine-tuned LLaMA-8B filter to re-rank the retrieved units by marginal slot-completion gain. The resulting memory units inform a proactive strategy that minimizes conversational turns by directly addressing information gaps. Based on this framework, we introduce the MS-TOD, the first multi-session TOD benchmark comprising 132 diverse personas, 956 task goals, and annotated intent-aligned memory targets, supporting efficient multi-session task completion. Evaluations on MS-TOD show that MemGuide raises the task success rate by 11% (88% -> 99%) and reduces dialogue length by 2.84 turns in multi-session settings, while maintaining parity with single-session benchmarks.", "source": "arxiv", "arxiv_id": "2505.20231v2", "pdf_url": "https://arxiv.org/pdf/2505.20231v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-26T17:10:43Z", "updated": "2025-08-13T03:43:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MemInsight: Autonomous Memory Augmentation for LLM Agents", "authors": ["Rana Salama", "Jason Cai", "Michelle Yuan", "Anna Currey", "Monica Sunkara", "Yi Zhang", "Yassine Benajiba"], "year": 2025, "url": "http://arxiv.org/abs/2503.21760v2", "abstract": "Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.", "source": "arxiv", "arxiv_id": "2503.21760v2", "pdf_url": "https://arxiv.org/pdf/2503.21760v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-27T17:57:28Z", "updated": "2025-07-31T23:26:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service", "authors": ["Yizhe Huang", "Yang Liu", "Ruiyu Zhao", "Xiaolong Zhong", "Xingming Yue", "Ling Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2509.18713v1", "abstract": "Large Language Model-based agents(LLM-based agents) are increasingly deployed in customer service, yet they often forget across sessions, repeat errors, and lack mechanisms for continual self-improvement. This makes them unreliable in dynamic settings where stability and consistency are critical. To better evaluate these properties, we emphasize two indicators: task success rate as a measure of overall effectiveness, and consistency metrics such as Pass$^k$ to capture reliability across multiple trials. To address the limitations of existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal reinforcement memory layer that distills multi-turn interactions into compact strategy reflections. These reflections are stored in a shared memory bank and retrieved to guide decision-making, without requiring any fine-tuning. Experiments show that MemOrb significantly improves both success rate and stability, achieving up to a 63 percentage-point gain in multi-turn success rate and delivering more consistent performance across repeated trials. Our results demonstrate that structured reflection is a powerful mechanism for enhancing long-term reliability of frozen LLM agents in customer service scenarios.", "source": "arxiv", "arxiv_id": "2509.18713v1", "pdf_url": "https://arxiv.org/pdf/2509.18713v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-23T06:57:07Z", "updated": "2025-09-23T06:57:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents", "authors": ["Xingbo Du", "Loka Li", "Duzhen Zhang", "Le Song"], "year": 2025, "url": "http://arxiv.org/abs/2512.20237v1", "abstract": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.", "source": "arxiv", "arxiv_id": "2512.20237v1", "pdf_url": "https://arxiv.org/pdf/2512.20237v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T10:49:42Z", "updated": "2025-12-23T10:49:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "year": 2025, "url": "http://arxiv.org/abs/2507.21428v1", "abstract": "Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities.", "source": "arxiv", "arxiv_id": "2507.21428v1", "pdf_url": "https://arxiv.org/pdf/2507.21428v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-29T01:42:06Z", "updated": "2025-07-29T01:42:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Memento 2: Learning by Stateful Reflective Memory", "authors": ["Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.22716v2", "abstract": "We study continual learning in large language model (LLM) based agents that integrate episodic memory with reinforcement learning. We focus on reflection, the ability of an agent to revisit past experience and adjust how it selects future actions, as the central mechanism for continual adaptation without fine tuning model weights. To formalise this, we introduce the Stateful Reflective Decision Process (SRDP), in which an agent maintains and updates episodic memory and alternates between writing new experiences to memory and reading relevant cases to guide decisions. This framework casts reflective memory dynamics as part of the decision process itself and makes them amenable to control and learning analysis. Building on this formulation, we develop a Read-Write Reflective Learning algorithm that incorporates memory retrieval into a soft policy iteration procedure and prove that it converges. We further show that as memory grows and more densely covers the task environment, the resulting policy approaches optimality. Our framework unifies memory based reasoning with reinforcement learning and provides a formal foundation for LLM agents capable of continual, experience driven learning.", "source": "arxiv", "arxiv_id": "2512.22716v2", "pdf_url": "https://arxiv.org/pdf/2512.22716v2", "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-27T22:15:03Z", "updated": "2025-12-31T23:24:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs", "authors": ["Huichi Zhou", "Yihang Chen", "Siyuan Guo", "Xue Yan", "Kin Hei Lee", "Zihan Wang", "Ka Yiu Lee", "Guchun Zhang", "Kun Shao", "Linyi Yang", "Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.16153v2", "abstract": "In this paper, we introduce a novel learning paradigm for Adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely \\emph{Memento}, which attains top-1 on GAIA validation ($87.88\\%$ Pass@$3$) and $79.40\\%$ on the test set. It reaches $66.6\\%$ F1 and $80.4\\%$ PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds $4.7\\%$ to $9.6\\%$ absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at https://github.com/Agent-on-the-Fly/Memento.", "source": "arxiv", "arxiv_id": "2508.16153v2", "pdf_url": "https://arxiv.org/pdf/2508.16153v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-22T07:25:30Z", "updated": "2025-08-25T13:32:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Memory Injection Attacks on LLM Agents via Query-Only Interaction", "authors": ["Shen Dong", "Shaochen Xu", "Pengfei He", "Yige Li", "Jiliang Tang", "Tianming Liu", "Hui Liu", "Zhen Xiang"], "year": 2025, "url": "http://arxiv.org/abs/2503.03704v4", "abstract": "Agents powered by large language models (LLMs) have demonstrated strong capabilities in a wide range of complex, real-world applications. However, LLM agents with a compromised memory bank may easily produce harmful outputs when the past records retrieved for demonstration are malicious. In this paper, we propose a novel Memory INJection Attack, MINJA, without assuming that the attacker can directly modify the memory bank of the agent. The attacker injects malicious records into the memory bank by only interacting with the agent via queries and output observations. These malicious records are designed to elicit a sequence of malicious reasoning steps corresponding to a different target query during the agent's execution of the victim user's query. Specifically, we introduce a sequence of bridging steps to link victim queries to the malicious reasoning steps. During the memory injection, we propose an indication prompt that guides the agent to autonomously generate similar bridging steps, with a progressive shortening strategy that gradually removes the indication prompt, such that the malicious record will be easily retrieved when processing later victim queries. Our extensive experiments across diverse agents demonstrate the effectiveness of MINJA in compromising agent memory. With minimal requirements for execution, MINJA enables any user to influence agent memory, highlighting the risk.", "source": "arxiv", "arxiv_id": "2503.03704v4", "pdf_url": "https://arxiv.org/pdf/2503.03704v4", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-05T17:53:24Z", "updated": "2025-12-10T02:07:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games", "authors": ["Runnan Qi", "Yanan Ni", "Lumin Jiang", "Zongyuan Li", "Kuihua Huang", "Xian Guo"], "year": 2025, "url": "http://arxiv.org/abs/2510.18395v1", "abstract": "This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel framework for LLM agents in real-time strategy games. Addressing key challenges like hallucinations and fragmented decision-making in existing approaches, MASMP integrates state machine prompting with memory mechanisms to unify structured actions with long-term tactical coherence. The framework features: (1) a natural language-driven state machine architecture that guides LLMs to emulate finite state machines and behavior trees through prompts, and (2) a lightweight memory module preserving strategic variables (e.g., tactics, priority units) across decision cycles. Experiments in StarCraft II demonstrate MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly outperforming baselines (0%). Case studies reveal the method retains LLMs' semantic comprehension while resolving the \"Knowing-Doing Gap\" through strict state-action mapping, achieving both interpretability and FSM-like reliability. This work establishes a new paradigm for combining neural and symbolic AI in complex decision-making.", "source": "arxiv", "arxiv_id": "2510.18395v1", "pdf_url": "https://arxiv.org/pdf/2510.18395v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-21T08:15:04Z", "updated": "2025-10-21T08:15:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning", "authors": ["Sikuan Yan", "Xiufeng Yang", "Zuchao Huang", "Ercong Nie", "Zifeng Ding", "Zonggen Li", "Xiaowen Ma", "Jinhe Bi", "Kristian Kersting", "Jeff Z. Pan", "Hinrich SchÃ¼tze", "Volker Tresp", "Yunpu Ma"], "year": 2025, "url": "http://arxiv.org/abs/2508.19828v5", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking a learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns structured operations, including ADD, UPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over relevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management with minimal supervision. With only 152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes across diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and multiple model scales (3B-14B).", "source": "arxiv", "arxiv_id": "2508.19828v5", "pdf_url": "https://arxiv.org/pdf/2508.19828v5", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-27T12:26:55Z", "updated": "2026-01-14T14:21:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval", "authors": ["Saksham Sahai Srivastava", "Haoyu He"], "year": 2025, "url": "http://arxiv.org/abs/2512.16962v1", "abstract": "Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.", "source": "arxiv", "arxiv_id": "2512.16962v1", "pdf_url": "https://arxiv.org/pdf/2512.16962v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-18T08:34:40Z", "updated": "2025-12-18T08:34:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Merge and Conquer: Evolutionarily Optimizing AI for 2048", "authors": ["Maggie Bai", "Ava Kim Cohen", "Eleanor Koss", "Charlie Lichtenbaum"], "year": 2025, "url": "http://arxiv.org/abs/2510.20205v1", "abstract": "Optimizing artificial intelligence (AI) for dynamic environments remains a fundamental challenge in machine learning research. In this paper, we examine evolutionary training methods for optimizing AI to solve the game 2048, a 2D sliding puzzle. 2048, with its mix of strategic gameplay and stochastic elements, presents an ideal playground for studying decision-making, long-term planning, and dynamic adaptation. We implemented two distinct systems: a two-agent metaprompting system where a \"thinker\" large language model (LLM) agent refines gameplay strategies for an \"executor\" LLM agent, and a single-agent system based on refining a value function for a limited Monte Carlo Tree Search. We also experimented with rollback features to avoid performance degradation. Our results demonstrate the potential of evolutionary refinement techniques in improving AI performance in non-deterministic environments. The single-agent system achieved substantial improvements, with an average increase of 473.2 points per cycle, and with clear upward trends (correlation $Ï$=0.607) across training cycles. The LLM's understanding of the game grew as well, shown in its development of increasingly advanced strategies. Conversely, the two-agent system did not garner much improvement, highlighting the inherent limits of meta-prompting.", "source": "arxiv", "arxiv_id": "2510.20205v1", "pdf_url": "https://arxiv.org/pdf/2510.20205v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-23T04:45:05Z", "updated": "2025-10-23T04:45:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent", "authors": ["Chunlong Wu", "Ye Luo", "Zhibo Qu", "Min Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.03990v2", "abstract": "Large language model (LLM) agents achieve impressive single-task performance but commonly exhibit repeated failures, inefficient exploration, and limited cross-task adaptability. Existing reflective strategies (e.g., Reflexion, ReAct) improve per-episode behavior but typically produce ephemeral, task-specific traces that are not reused across tasks. Reinforcement-learning based alternatives can produce transferable policies but require substantial parameter updates and compute. In this work we introduce Meta-Policy Reflexion (MPR): a hybrid framework that consolidates LLM-generated reflections into a structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at inference time through two complementary mechanisms soft memory-guided decoding and hard rule admissibility checks(HAC). MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. We formalize the MPM representation, present algorithms for update and decoding, and validate the approach in a text-based agent environment following the experimental protocol described in the provided implementation (AlfWorld-based). Empirical results reported in the supplied material indicate consistent gains in execution accuracy and robustness when compared to Reflexion baselines; rule admissibility further improves stability. We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi-agent extensions.", "source": "arxiv", "arxiv_id": "2509.03990v2", "pdf_url": "https://arxiv.org/pdf/2509.03990v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-04T08:18:39Z", "updated": "2025-09-08T07:40:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Meta-RAG on Large Codebases Using Code Summarization", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "year": 2025, "url": "http://arxiv.org/abs/2508.02611v1", "abstract": "Large Language Model (LLM) systems have been at the forefront of applied Artificial Intelligence (AI) research in a multitude of domains. One such domain is software development, where researchers have pushed the automation of a number of code tasks through LLM agents. Software development is a complex ecosystem, that stretches far beyond code implementation and well into the realm of code maintenance. In this paper, we propose a multi-agent system to localize bugs in large pre-existing codebases using information retrieval and LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation. We then use an LLM agent to determine which parts of the codebase are critical for bug resolution, i.e. bug localization. We demonstrate the usefulness of Meta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores 84.67 % and 53.0 % for file-level and function-level correct localization rates, respectively, achieving state-of-the-art performance.", "source": "arxiv", "arxiv_id": "2508.02611v1", "pdf_url": "https://arxiv.org/pdf/2508.02611v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-04T17:01:10Z", "updated": "2025-08-04T17:01:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Meta-RL Induces Exploration in Language Agents", "authors": ["Yulun Jiang", "Liangze Jiang", "Damien Teney", "Michael Moor", "Maria Brbic"], "year": 2025, "url": "http://arxiv.org/abs/2512.16848v1", "abstract": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.", "source": "arxiv", "arxiv_id": "2512.16848v1", "pdf_url": "https://arxiv.org/pdf/2512.16848v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-18T18:22:17Z", "updated": "2025-12-18T18:22:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents", "authors": ["Pan Tang", "Shixiang Tang", "Huanqi Pu", "Zhiqing Miao", "Zhixing Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.15635v1", "abstract": "This paper presents MicroRCA-Agent, an innovative solution for microservice root cause analysis based on large language model agents, which constructs an intelligent fault root cause localization system with multimodal data fusion. The technical innovations are embodied in three key aspects: First, we combine the pre-trained Drain log parsing algorithm with multi-level data filtering mechanism to efficiently compress massive logs into high-quality fault features. Second, we employ a dual anomaly detection approach that integrates Isolation Forest unsupervised learning algorithms with status code validation to achieve comprehensive trace anomaly identification. Third, we design a statistical symmetry ratio filtering mechanism coupled with a two-stage LLM analysis strategy to enable full-stack phenomenon summarization across node-service-pod hierarchies. The multimodal root cause analysis module leverages carefully designed cross-modal prompts to deeply integrate multimodal anomaly information, fully exploiting the cross-modal understanding and logical reasoning capabilities of large language models to generate structured analysis results encompassing fault components, root cause descriptions, and reasoning trace. Comprehensive ablation studies validate the complementary value of each modal data and the effectiveness of the system architecture. The proposed solution demonstrates superior performance in complex microservice fault scenarios, achieving a final score of 50.71. The code has been released at: https://github.com/tangpan360/MicroRCA-Agent.", "source": "arxiv", "arxiv_id": "2509.15635v1", "pdf_url": "https://arxiv.org/pdf/2509.15635v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-19T05:57:03Z", "updated": "2025-09-19T05:57:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.00282v2", "abstract": "Humans constantly generate a diverse range of tasks guided by internal motivations. While generative agents powered by large language models (LLMs) aim to simulate this complex behavior, it remains uncertain whether they operate on similar cognitive principles. To address this, we conducted a task-generation experiment comparing human responses with those of an LLM agent (GPT-4o). We find that human task generation is consistently influenced by psychological drivers, including personal values (e.g., Openness to Change) and cognitive style. Even when these psychological drivers are explicitly provided to the LLM, it fails to reflect the corresponding behavioral patterns. They produce tasks that are markedly less social, less physical, and thematically biased toward abstraction. Interestingly, while the LLM's tasks were perceived as more fun and novel, this highlights a disconnect between its linguistic proficiency and its capacity to generate human-like, embodied goals. We conclude that there is a core gap between the value-driven, embodied nature of human cognition and the statistical patterns of LLMs, highlighting the necessity of incorporating intrinsic motivation and physical grounding into the design of more human-aligned agents.", "source": "arxiv", "arxiv_id": "2508.00282v2", "pdf_url": "https://arxiv.org/pdf/2508.00282v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-01T03:00:41Z", "updated": "2025-08-05T09:10:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents", "authors": ["Ming Gong", "Xucheng Huang", "Chenghan Yang", "Xianhan Peng", "Haoxin Wang", "Yang Liu", "Ling Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2507.05330v1", "abstract": "Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular \"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.", "source": "arxiv", "arxiv_id": "2507.05330v1", "pdf_url": "https://arxiv.org/pdf/2507.05330v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-07T17:53:55Z", "updated": "2025-07-07T17:53:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MindGuard: Intrinsic Decision Inspection for Securing LLM Agents Against Metadata Poisoning", "authors": ["Zhiqiang Wang", "Haohua Du", "Guanquan Shi", "Junyang Zhang", "HaoRan Cheng", "Yunhao Yao", "Kaiwen Guo", "Xiang-Yang Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.20412v3", "abstract": "The Model Context Protocol (MCP) is increasingly adopted to standardize the interaction between LLM agents and external tools. However, this trend introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is poisoned to induce the agent to perform unauthorized operations. Existing defenses that primarily focus on behavior-level analysis are fundamentally ineffective against TPA, as poisoned tools need not be executed, leaving no behavioral trace to monitor.\n  Thus, we propose MindGuard, a decision-level guardrail for LLM agents, providing provenance tracking of call decisions, policy-agnostic detection, and poisoning source attribution against TPA. While fully explaining LLM decision remains challenging, our empirical findings uncover a strong correlation between LLM attention mechanisms and tool invocation decisions. Therefore, we choose attention as an empirical signal for decision tracking and formalize this as the Decision Dependence Graph (DDG), which models the LLM's reasoning process as a weighted, directed graph where vertices represent logical concepts and edges quantify the attention-based dependencies. We further design robust DDG construction and graph-based anomaly analysis mechanisms that efficiently detect and attribute TPA attacks. Extensive experiments on real-world datasets demonstrate that MindGuard achieves 94\\%-99\\% average precision in detecting poisoned invocations, 95\\%-100\\% attribution accuracy, with processing times under one second and no additional token cost. Moreover, DDG can be viewed as an adaptation of the classical Program Dependence Graph (PDG), providing a solid foundation for applying traditional security policies at the decision level.", "source": "arxiv", "arxiv_id": "2508.20412v3", "pdf_url": "https://arxiv.org/pdf/2508.20412v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-28T04:23:44Z", "updated": "2026-01-15T02:58:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions", "authors": ["StÃ©phane Aroca-Ouellette", "Ian Berlot-Attwell", "Panagiotis Lymperopoulos", "Abhiramon Rajasekharan", "Tongqi Zhu", "Herin Kang", "Kaheer Suleman", "Sam Pasupalak"], "year": 2025, "url": "http://arxiv.org/abs/2511.15830v1", "abstract": "Despite rapid progress in artificial intelligence, current systems struggle with the interconnected challenges that define real-world decision making. Practical domains, such as business management, require optimizing an open-ended and multi-faceted objective, actively learning environment dynamics from sparse experience, planning over long horizons in stochastic settings, and reasoning over spatial information. Yet existing human--AI benchmarks isolate subsets of these capabilities, limiting our ability to assess holistic decision-making competence. We introduce Mini Amusement Parks (MAPs), an amusement-park simulator designed to evaluate an agent's ability to model its environment, anticipate long-term consequences under uncertainty, and strategically operate a complex business. We provide human baselines and a comprehensive evaluation of state-of-the-art LLM agents, finding that humans outperform these systems by 6.5x on easy mode and 9.8x on medium mode. Our analysis reveals persistent weaknesses in long-horizon optimization, sample-efficient learning, spatial reasoning, and world modelling. By unifying these challenges within a single environment, MAPs offers a new foundation for benchmarking agents capable of adaptable decision making. Code: https://github.com/Skyfall-Research/MAPs", "source": "arxiv", "arxiv_id": "2511.15830v1", "pdf_url": "https://arxiv.org/pdf/2511.15830v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-19T19:38:05Z", "updated": "2025-11-19T19:38:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis", "authors": ["Xueying Du", "Kai Yu", "Chong Wang", "Yi Zou", "Wentai Deng", "Zuoyu Ou", "Xin Peng", "Lingming Zhang", "Yiling Lou"], "year": 2025, "url": "http://arxiv.org/abs/2506.10322v1", "abstract": "Static bug analyzers play a crucial role in ensuring software quality. However, existing analyzers for bug detection in large codebases often suffer from high false positive rates. This is primarily due to the limited capabilities of analyzers in path feasibility validation with multiple conditional branches and complex data dependencies. While current LLM-based approaches attempt to address this issue, their effectiveness remains limited due to insufficient constraint cascade analysis and scalability challenges in large projects. To address this challenge, we propose an iterative path feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted constraint reasoning, and key context-aware analysis driven by agent planning, LLM4PFA effectively enhances complex inter-procedural path feasibility analysis for minimizing false positives in static bug detection. Evaluation results show that LLM4PFA precisely filters out 72% to 96% false positives reported during static bug detection, significantly outperforming all the baselines by 41.1% - 105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true positives.", "source": "arxiv", "arxiv_id": "2506.10322v1", "pdf_url": "https://arxiv.org/pdf/2506.10322v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-06-12T03:11:38Z", "updated": "2025-06-12T03:11:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling", "authors": ["Jifan Gao", "Mahmudur Rahman", "John Caskey", "Madeline Oguss", "Ann O'Rourke", "Randy Brown", "Anne Stey", "Anoop Mayampurath", "Matthew M. Churpek", "Guanhua Chen", "Majid Afshar"], "year": 2025, "url": "http://arxiv.org/abs/2508.05492v1", "abstract": "Multimodal electronic health record (EHR) data provide richer, complementary insights into patient health compared to single-modality data. However, effectively integrating diverse data modalities for clinical prediction modeling remains challenging due to the substantial data requirements. We introduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed to leverage multiple large language model (LLM) agents for clinical prediction tasks using multimodal EHR data. MoMA employs specialized LLM agents (\"specialist agents\") to convert non-textual modalities, such as medical images and laboratory results, into structured textual summaries. These summaries, together with clinical notes, are combined by another LLM (\"aggregator agent\") to generate a unified multimodal summary, which is then used by a third LLM (\"predictor agent\") to produce clinical predictions. Evaluating MoMA on three prediction tasks using real-world datasets with different modality combinations and prediction settings, MoMA outperforms current state-of-the-art methods, highlighting its enhanced accuracy and flexibility across various tasks.", "source": "arxiv", "arxiv_id": "2508.05492v1", "pdf_url": "https://arxiv.org/pdf/2508.05492v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-07T15:28:34Z", "updated": "2025-08-07T15:28:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents", "authors": ["Tomer Wolfson", "Harsh Trivedi", "Mor Geva", "Yoav Goldberg", "Dan Roth", "Tushar Khot", "Ashish Sabharwal", "Reut Tsarfaty"], "year": 2025, "url": "http://arxiv.org/abs/2508.11133v2", "abstract": "Automated agents, powered by Large language models (LLMs), are emerging as the go-to tool for querying information. However, evaluation benchmarks for LLM agents rarely feature natural questions that are both information-seeking and genuinely time-consuming for humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural and time-consuming questions that require dozens, and at times hundreds, of intermediate steps to solve -- far more than any existing QA benchmark. To build MoNaCo, we developed a decomposed annotation pipeline to elicit and manually answer real-world time-consuming questions at scale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and hallucinations. Our results underscore the limitations of LLM-powered agents in handling the complexity and sheer breadth of real-world information-seeking tasks -- with MoNaCo providing an effective resource for tracking such progress. The MoNaCo benchmark, codebase, prompts and models predictions are all publicly available at: https://tomerwolgithub.github.io/monaco", "source": "arxiv", "arxiv_id": "2508.11133v2", "pdf_url": "https://arxiv.org/pdf/2508.11133v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-15T00:58:10Z", "updated": "2025-09-03T17:03:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Model Context Protocol-based Internet of Experts For Wireless Environment-aware LLM Agents", "authors": ["Zongxi Liu", "Hongyang Du"], "year": 2025, "url": "http://arxiv.org/abs/2505.01834v1", "abstract": "Large Language Models (LLMs) exhibit strong general-purpose reasoning abilities but lack access to wireless environment information due to the absence of native sensory input and domain-specific priors. Previous attempts to apply LLMs in wireless systems either depend on retraining with network-specific data, which compromises language generalization, or rely on manually scripted interfaces, which hinder scalability. To overcome these limitations, we propose a Model Context Protocol (MCP)-based Internet of Experts (IoX) framework that equips LLMs with wireless environment-aware reasoning capabilities. The framework incorporates a set of lightweight expert models, each trained to solve a specific deterministic task in wireless communications, such as detecting a specific wireless attribute, e.g., line-of-sight propagation, Doppler effects, or fading conditions. Through MCP, the LLM can selectively query and interpret expert outputs at inference time, without modifying its own parameters. This architecture enables modular, extensible, and interpretable reasoning over wireless contexts. Evaluated across multiple mainstream LLMs, the proposed wireless environment-aware LLM agents achieve 40%-50% improvements in classification tasks over LLM-only baselines. More broadly, the MCP-based design offers a viable paradigm for future LLMs to inherit structured wireless network management capabilities.", "source": "arxiv", "arxiv_id": "2505.01834v1", "pdf_url": "https://arxiv.org/pdf/2505.01834v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-05-03T14:41:24Z", "updated": "2025-05-03T14:41:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Model Gateway: Model Management Platform for Model-Driven Drug Discovery", "authors": ["Yan-Shiun Wu", "Nathan A. Morin"], "year": 2025, "url": "http://arxiv.org/abs/2512.05462v1", "abstract": "This paper presents the Model Gateway, a management platform for managing machine learning (ML) and scientific computational models in the drug discovery pipeline. The platform supports Large Language Model (LLM) Agents and Generative AI-based tools to perform ML model management tasks in our Machine Learning operations (MLOps) pipelines, such as the dynamic consensus model, a model that aggregates several scientific computational models, registration and management, retrieving model information, asynchronous submission/execution of models, and receiving results once the model complete executions. The platform includes a Model Owner Control Panel, Platform Admin Tools, and Model Gateway API service for interacting with the platform and tracking model execution. The platform achieves a 0% failure rate when testing scaling beyond 10k simultaneous application clients consume models. The Model Gateway is a fundamental part of our model-driven drug discovery pipeline. It has the potential to significantly accelerate the development of new drugs with the maturity of our MLOps infrastructure and the integration of LLM Agents and Generative AI tools.", "source": "arxiv", "arxiv_id": "2512.05462v1", "pdf_url": "https://arxiv.org/pdf/2512.05462v1", "categories": ["cs.SE", "cs.DC", "cs.LG", "q-bio.QM"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-05T06:39:37Z", "updated": "2025-12-05T06:39:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling", "authors": ["Annu Rana", "Gaurav Kumar"], "year": 2025, "url": "http://arxiv.org/abs/2512.14474v1", "abstract": "Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.", "source": "arxiv", "arxiv_id": "2512.14474v1", "pdf_url": "https://arxiv.org/pdf/2512.14474v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-16T15:07:36Z", "updated": "2025-12-16T15:07:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization", "authors": ["Zhuo Yang", "Yeyun Chen", "Jiaqing Xie", "Ben Gao", "Shuaike Shen", "Wanhao Liu", "Liujia Yang", "Beilun Wang", "Tianfan Fu", "Yuqiang Li"], "year": 2025, "url": "http://arxiv.org/abs/2512.20135v2", "abstract": "Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.", "source": "arxiv", "arxiv_id": "2512.20135v2", "pdf_url": "https://arxiv.org/pdf/2512.20135v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T07:53:57Z", "updated": "2025-12-24T02:19:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning", "authors": ["Zhiyu An", "Wan Du"], "year": 2025, "url": "http://arxiv.org/abs/2511.12271v1", "abstract": "Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.", "source": "arxiv", "arxiv_id": "2511.12271v1", "pdf_url": "https://arxiv.org/pdf/2511.12271v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-15T15:52:10Z", "updated": "2025-11-15T15:52:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents", "authors": ["Weimin Xiong", "Ke Wang", "Yifan Song", "Hanchao Liu", "Sai Zhou", "Wei Peng", "Sujian Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.21967v1", "abstract": "Current evaluations of tool-integrated LLM agents typically focus on end-to-end tool-usage evaluation while neglecting their stability. This limits their real-world applicability, as various internal or external factors can cause agents to crash or behave abnormally. Our research addresses this by investigating whether agents are vulnerable to errors throughout the entire tool invocation process, including reading tool documentation, selecting tools and generating parameters, and processing the tool's response. Through extensive experiments, we observe that agents are highly susceptible to errors at each stage and agents based on open-source models are more vulnerable than those based on proprietary models. We also find that increasing the model size does not significantly improve tool invocation reasoning and may make agents more vulnerable to attacks resembling normal user instructions. This highlights the importance of evaluating agent stability and offers valuable insights for future LLM development and evaluation.", "source": "arxiv", "arxiv_id": "2506.21967v1", "pdf_url": "https://arxiv.org/pdf/2506.21967v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-27T07:13:29Z", "updated": "2025-06-27T07:13:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation", "authors": ["Pravallika Abbineni", "Saoud Aldowaish", "Colin Liechty", "Soroosh Noorzad", "Ali Ghazizadeh", "Morteza Fayazi"], "year": 2025, "url": "http://arxiv.org/abs/2508.08137v1", "abstract": "Conducting a comprehensive literature review is crucial for advancing circuit design methodologies. However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging. In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason + Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step information retrieval. It functions as a question-answering design assistant, capable of interpreting complex queries and providing reasoned responses grounded in circuit literature. Its multimodal capabilities enable processing of both textual and visual data, facilitating more efficient and comprehensive analysis. The system dynamically adapts using intelligent search tools, automated document retrieval from the internet, and real-time database updates. Unlike conventional approaches constrained by model context limits, MuaLLM decouples retrieval from inference, enabling scalable reasoning over arbitrarily large corpora. At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy. This allows rapid, no-human-in-the-loop database generation, overcoming the bottleneck of simulation-based dataset creation for circuits. To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8% accuracy on Reas-100.", "source": "arxiv", "arxiv_id": "2508.08137v1", "pdf_url": "https://arxiv.org/pdf/2508.08137v1", "categories": ["cs.LG", "cs.AI", "eess.SY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-11T16:11:09Z", "updated": "2025-08-11T16:11:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations", "authors": ["Yu Xia", "Sungchul Kim", "Tong Yu", "Ryan A. Rossi", "Julian McAuley"], "year": 2025, "url": "http://arxiv.org/abs/2511.18413v2", "abstract": "Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.", "source": "arxiv", "arxiv_id": "2511.18413v2", "pdf_url": "https://arxiv.org/pdf/2511.18413v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-23T11:57:10Z", "updated": "2025-12-10T12:41:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models", "authors": ["Marc Oriol", "Quim Motger", "Jordi Marco", "Xavier Franch"], "year": 2025, "url": "http://arxiv.org/abs/2507.05981v1", "abstract": "Context: Large Language Model (LLM) agents are becoming widely used for various Requirements Engineering (RE) tasks. Research on improving their accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval augmented generation. However, these methods often treat models as isolated black boxes - relying on single-pass outputs without iterative refinement or collaboration, limiting robustness and adaptability. Objective: We propose that, just as human debates enhance accuracy and reduce bias in RE tasks by incorporating diverse perspectives, different LLM agents debating and collaborating may achieve similar improvements. Our goal is to investigate whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method: We conducted a systematic study of existing MAD strategies across various domains to identify their key characteristics. To assess their applicability in RE, we implemented and tested a preliminary MAD-based framework for RE classification. Results: Our study identified and categorized several MAD strategies, leading to a taxonomy outlining their core attributes. Our preliminary evaluation demonstrated the feasibility of applying MAD to RE classification. Conclusions: MAD presents a promising approach for improving LLM accuracy in RE tasks. This study provides a foundational understanding of MAD strategies, offering insights for future research and refinements in RE applications.", "source": "arxiv", "arxiv_id": "2507.05981v1", "pdf_url": "https://arxiv.org/pdf/2507.05981v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "10.1109/RE63999.2025.00063", "venue": "", "published": "2025-07-08T13:37:59Z", "updated": "2025-07-08T13:37:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation", "authors": ["Jiaju Chen", "Yuxuan Lu", "Xiaojie Wang", "Huimin Zeng", "Jing Huang", "Jiri Gesi", "Ying Xu", "Bingsheng Yao", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2507.21028v1", "abstract": "Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging \"LLM-as-a-judge\" paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts' ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.", "source": "arxiv", "arxiv_id": "2507.21028v1", "pdf_url": "https://arxiv.org/pdf/2507.21028v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-28T17:48:40Z", "updated": "2025-07-28T17:48:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Multi-Objective Infeasibility Diagnosis for Routing Problems Using Large Language Models", "authors": ["Kai Li", "Ruihao Zheng", "Xinye Hao", "Zhenkun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.03406v1", "abstract": "In real-world routing problems, users often propose conflicting or unreasonable requirements, which result in infeasible optimization models due to overly restrictive or contradictory constraints, leading to an empty feasible solution set. Existing Large Language Model (LLM)-based methods attempt to diagnose infeasible models, but modifying such models often involves multiple potential adjustments that these methods do not consider. To fill this gap, we introduce Multi-Objective Infeasibility Diagnosis (MOID), which combines LLM agents and multi-objective optimization within an automatic routing solver, to provide a set of representative actionable suggestions. Specifically, MOID employs multi-objective optimization to consider both path cost and constraint violation, generating a set of trade-off solutions, each encompassing varying degrees of model adjustments. To extract practical insights from these solutions, MOID utilizes LLM agents to generate a solution analysis function for the infeasible model. This function analyzes these distinct solutions to diagnose the original infeasible model, providing users with diverse diagnostic insights and suggestions. Finally, we compare MOID with several LLM-based methods on 50 types of infeasible routing problems. The results indicate that MOID automatically generates multiple diagnostic suggestions in a single run, providing more practical insights for restoring model feasibility and decision-making compared to existing methods.", "source": "arxiv", "arxiv_id": "2508.03406v1", "pdf_url": "https://arxiv.org/pdf/2508.03406v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-05T12:53:20Z", "updated": "2025-08-05T12:53:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions", "authors": ["Xi Wang", "Xianyao Ling", "Kun Li", "Gang Yin", "Liang Zhang", "Jiang Wu", "Jun Xu", "Fu Zhang", "Wenbo Lei", "Annie Wang", "Peng Gong"], "year": 2025, "url": "http://arxiv.org/abs/2510.15258v2", "abstract": "In the current era of big data, extracting deep insights from massive, heterogeneous, and complexly associated multi-dimensional data has become a significant challenge. Large Language Models (LLMs) perform well in natural language understanding and generation, but still suffer from \"hallucination\" issues when processing structured knowledge and are difficult to update in real-time. Although Knowledge Graphs (KGs) can explicitly store structured knowledge, their static nature limits dynamic interaction and analytical capabilities. Therefore, this paper proposes a multi-dimensional data analysis method based on the interactions between LLM agents and KGs, constructing a dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to automatically extract product data from unstructured data, constructs and visualizes the KG in real-time, and supports users in deep exploration and analysis of graph nodes through an interactive platform. Experimental results show that this method has significant advantages in product ecosystem analysis, relationship mining, and user-driven exploratory analysis, providing new ideas and tools for multi-dimensional data analysis.", "source": "arxiv", "arxiv_id": "2510.15258v2", "pdf_url": "https://arxiv.org/pdf/2510.15258v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-17T02:38:44Z", "updated": "2025-11-20T06:48:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents", "authors": ["Kunlun Zhu", "Hongyi Du", "Zhaochen Hong", "Xiaocheng Yang", "Shuyi Guo", "Zhe Wang", "Zhenhailong Wang", "Cheng Qian", "Xiangru Tang", "Heng Ji", "Jiaxuan You"], "year": 2025, "url": "http://arxiv.org/abs/2503.01935v1", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.", "source": "arxiv", "arxiv_id": "2503.01935v1", "pdf_url": "https://arxiv.org/pdf/2503.01935v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-03-03T05:18:50Z", "updated": "2025-03-03T05:18:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind", "authors": ["Zheng Zhang", "Nuoqian Xiao", "Qi Chai", "Deheng Ye", "Hao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.18039v4", "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities in social deduction games (SDGs) like Werewolf, where strategic reasoning and social deception are essential. However, current approaches remain limited to textual information, ignoring crucial multimodal cues such as facial expressions and tone of voice that humans naturally use to communicate. Moreover, existing SDG agents primarily focus on inferring other players' identities without modeling how others perceive themselves or fellow players. To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a testbed and present MultiMind, the first framework integrating multimodal information into SDG agents. MultiMind processes facial expressions and vocal tones alongside verbal content, while employing a Theory of Mind (ToM) model to represent each player's suspicion levels toward others. By combining this ToM model with Monte Carlo Tree Search (MCTS), our agent identifies communication strategies that minimize suspicion directed at itself. Through comprehensive evaluation in both agent-versus-agent simulations and studies with human players, we demonstrate MultiMind's superior performance in gameplay. Our work presents a significant advancement toward LLM agents capable of human-like social reasoning across multimodal domains.", "source": "arxiv", "arxiv_id": "2504.18039v4", "pdf_url": "https://arxiv.org/pdf/2504.18039v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-25T03:12:43Z", "updated": "2025-09-14T07:36:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers", "authors": ["Nicole Cho", "William Watson"], "year": 2025, "url": "http://arxiv.org/abs/2502.03711v1", "abstract": "One critical challenge in the institutional adoption journey of Large Language Models (LLMs) stems from their propensity to hallucinate in generated responses. To address this, we propose MultiQ&A, a systematic approach for evaluating the robustness and consistency of LLM-generated answers. We demonstrate MultiQ&A's ability to crowdsource question perturbations and their respective answers through independent LLM agents at scale. Our experiments culminated in the examination of 1.9 million question perturbations and 2.3 million answers. Furthermore, MultiQ&A shows that ensembled LLMs, such as gpt-3.5-turbo, remain relatively robust and consistent under perturbations. MultiQ&A provides clarity in the response generation space, offering an effective method for inspecting disagreements and variability. Therefore, our system offers a potential framework for institutional LLM adoption with the ability to measure confidence, consistency, and the quantification of hallucinations.", "source": "arxiv", "arxiv_id": "2502.03711v1", "pdf_url": "https://arxiv.org/pdf/2502.03711v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-06T01:58:48Z", "updated": "2025-02-06T01:58:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Multicrossmodal Automated Agent for Integrating Diverse Materials Science Data", "authors": ["Adib Bazgir", "Rama chandra Praneeth Madugula", "Yuwen Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.15132v1", "abstract": "We introduce a multicrossmodal LLM-agent framework motivated by the growing volume and diversity of materials-science data ranging from high-resolution microscopy and dynamic simulation videos to tabular experiment logs and sprawling literature archives. While recent AI efforts have accelerated individual tasks such as property prediction or image classification, they typically treat each modality in isolation, leaving rich cross-modal correlations unexplored and forcing researchers to perform laborious manual integration. Moreover, existing multimodal foundation models often require expensive retraining or fine-tuning on domain data, and current multi-agent systems in materials informatics address only narrow subtasks. To overcome these obstacles, we design a coordinated team of specialized LLM agents, each equipped with domain-adapted prompts and plugins that project their outputs into a shared embedding space. A dynamic gating mechanism then weights and merges these insights, enabling unified reasoning over heterogeneous inputs without ever modifying the underlying LLM weights. We validate our approach on challenging case studies and demonstrate substantial gains in retrieval accuracy (85%), captioning fidelity, and integrated coverage (35%) compared to single-modality and zero-shot baselines. Our work paves the way for AI digital researchers capable of bridging data silos and accelerating the materials-discovery cycle. The code is available at https://github.com/adibgpt/Multicrossmodal-Autonomous-Materials-Science-Agent.", "source": "arxiv", "arxiv_id": "2505.15132v1", "pdf_url": "https://arxiv.org/pdf/2505.15132v1", "categories": ["cond-mat.mtrl-sci"], "primary_category": "cond-mat.mtrl-sci", "doi": "", "venue": "", "published": "2025-05-21T05:37:03Z", "updated": "2025-05-21T05:37:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Multiple LLM Agents Debate for Equitable Cultural Alignment", "authors": ["Dayeon Ki", "Rachel Rudinger", "Tianyi Zhou", "Marine Carpuat"], "year": 2025, "url": "http://arxiv.org/abs/2505.24671v2", "abstract": "Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).", "source": "arxiv", "arxiv_id": "2505.24671v2", "pdf_url": "https://arxiv.org/pdf/2505.24671v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-30T15:01:52Z", "updated": "2025-09-01T12:34:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics", "authors": ["George Hong", "Daniel Trejo Banos"], "year": 2025, "url": "http://arxiv.org/abs/2509.19566v1", "abstract": "We investigate the application of Small Language Models (<10 billion parameters) for genomics question answering via agentic framework to address hallucination issues and computational cost challenges. The Nano Bio-Agent (NBA) framework we implemented incorporates task decomposition, tool orchestration, and API access into well-established systems such as NCBI and AlphaGenome. Results show that SLMs combined with such agentic framework can achieve comparable and in many cases superior performance versus existing approaches utilising larger models, with our best model-agent combination achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B parameter models consistently achieve 85-97% accuracy while requiring much lower computational resources than conventional approaches. This demonstrates promising potential for efficiency gains, cost savings, and democratization of ML-powered genomics tools while retaining highly robust and accurate performance.", "source": "arxiv", "arxiv_id": "2509.19566v1", "pdf_url": "https://arxiv.org/pdf/2509.19566v1", "categories": ["cs.AI", "q-bio.GN"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-23T20:44:31Z", "updated": "2025-09-23T20:44:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space", "authors": ["Joey Hong", "Kang Liu", "Zhan Ling", "Jiecao Chen", "Sergey Levine"], "year": 2025, "url": "http://arxiv.org/abs/2512.04601v1", "abstract": "Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.", "source": "arxiv", "arxiv_id": "2512.04601v1", "pdf_url": "https://arxiv.org/pdf/2512.04601v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-04T09:21:44Z", "updated": "2025-12-04T09:21:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared Residential Social Networks", "authors": ["Ann Nedime Nese Rende", "Tolga Yilmaz", "ÃzgÃ¼r Ulusoy"], "year": 2025, "url": "http://arxiv.org/abs/2507.09657v1", "abstract": "We use generative agents powered by large language models (LLMs) to simulate a social network in a shared residential building, driving the temperature decisions for a central heating system. Agents, divided into Family Members and Representatives, consider personal preferences, personal traits, connections, and weather conditions. Daily simulations involve family-level consensus followed by building-wide decisions among representatives. We tested three personality traits distributions (positive, mixed, and negative) and found that positive traits correlate with higher happiness and stronger friendships. Temperature preferences, assertiveness, and selflessness have a significant impact on happiness and decisions. This work demonstrates how LLM-driven agents can help simulate nuanced human behavior where complex real-life human simulations are difficult to set.", "source": "arxiv", "arxiv_id": "2507.09657v1", "pdf_url": "https://arxiv.org/pdf/2507.09657v1", "categories": ["cs.SI", "cs.MA"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-07-13T14:43:45Z", "updated": "2025-07-13T14:43:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "NetPress: Dynamically Generated LLM Benchmarks for Network Applications", "authors": ["Yajie Zhou", "Jiajun Ruan", "Eric S. Wang", "Sadjad Fouladi", "Francis Y. Yan", "Kevin Hsieh", "Zaoxing Liu"], "year": 2025, "url": "http://arxiv.org/abs/2506.03231v1", "abstract": "Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available at https://github.com/Froot-NetSys/NetPress.", "source": "arxiv", "arxiv_id": "2506.03231v1", "pdf_url": "https://arxiv.org/pdf/2506.03231v1", "categories": ["cs.NI", "cs.AI", "cs.LG"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-06-03T14:04:22Z", "updated": "2025-06-03T14:04:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Network and Systems Performance Characterization of MCP-Enabled LLM Agents", "authors": ["Zihao Ding", "Mufeng Zhu", "Yao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2511.07426v1", "abstract": "Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.", "source": "arxiv", "arxiv_id": "2511.07426v1", "pdf_url": "https://arxiv.org/pdf/2511.07426v1", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.NI", "cs.SE"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-10-20T05:13:47Z", "updated": "2025-10-20T05:13:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "NetworkGames: Simulating Cooperation in Network Games with Personality-driven LLM Agents", "authors": ["Xuan Qiu"], "year": 2025, "url": "http://arxiv.org/abs/2511.21783v1", "abstract": "The advent of Large Language Models (LLMs) presents a novel opportunity to build high-fidelity agent-based models for simulating complex social systems. However, the behavior of these LLM-based agents in game-theoretic network games remains surprisingly unexplored. In this work, we introduce \"NetworkGames,\" a novel simulation framework designed to investigate how network topology and agent personality jointly shape the evolution of cooperation in network games. We instantiate a population of LLM agents, each endowed with a distinct personality from the MBTI taxonomy, and situate them in various network structures (e.g., small-world and scale-free). Through extensive simulations of the Iterated Prisoner's Dilemma, we first establish a baseline dyadic interaction matrix, revealing nuanced cooperative preferences between all 16 personality pairs. We then demonstrate that macro-level cooperative outcomes are not predictable from dyadic interactions alone; they are co-determined by the network's connectivity and the spatial distribution of personalities. For instance, we find that small-world networks are detrimental to cooperation, while strategically placing pro-social personalities in hub positions within scale-free networks can significantly promote cooperative behavior. Our findings offer significant implications for designing healthier online social environments and forecasting collective behavior. We open-source our framework to foster further research in network game simulations.", "source": "arxiv", "arxiv_id": "2511.21783v1", "pdf_url": "https://arxiv.org/pdf/2511.21783v1", "categories": ["physics.soc-ph", "cs.GT"], "primary_category": "physics.soc-ph", "doi": "", "venue": "", "published": "2025-11-26T13:30:15Z", "updated": "2025-11-26T13:30:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents", "authors": ["Tianshi Zheng", "Kelvin Kiu-Wai Tam", "Newt Hue-Nam K. Nguyen", "Baixuan Xu", "Zhaowei Wang", "Jiayang Cheng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Tianqing Fang", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "year": 2025, "url": "http://arxiv.org/abs/2510.07172v2", "abstract": "Large language models are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using counterfactual law shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive experiment reveals a clear but fragile capability for discovery in frontier LLMs: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery.", "source": "arxiv", "arxiv_id": "2510.07172v2", "pdf_url": "https://arxiv.org/pdf/2510.07172v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-08T16:12:11Z", "updated": "2025-12-09T09:13:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization", "authors": ["Hyuntak Kim", "Byung-Hak Kim"], "year": 2025, "url": "http://arxiv.org/abs/2505.24575v1", "abstract": "Summarizing long-form narratives--such as books, movies, and TV scripts--requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline--without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains.", "source": "arxiv", "arxiv_id": "2505.24575v1", "pdf_url": "https://arxiv.org/pdf/2505.24575v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-30T13:26:23Z", "updated": "2025-05-30T13:26:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Not Like Us, Hunty: Measuring Perceptions and Behavioral Effects of Minoritized Anthropomorphic Cues in LLMs", "authors": ["Jeffrey Basoah", "Daniel Chechelnitsky", "Tao Long", "Katharina Reinecke", "Chrysoula Zerva", "Kaitlyn Zhou", "Mark DÃ­az", "Maarten Sap"], "year": 2025, "url": "http://arxiv.org/abs/2505.05660v3", "abstract": "As large language models (LLMs) increasingly adapt and personalize to diverse sets of users, there is an increased risk of systems appropriating sociolects, i.e., language styles or dialects that are associated with specific minoritized lived experiences (e.g., African American English, Queer slang). In this work, we examine whether sociolect usage by an LLM agent affects user reliance on its outputs and user perception (satisfaction, frustration, trust, and social presence). We designed and conducted user studies where 498 African American English (AAE) speakers and 487 Queer slang speakers performed a set of question-answering tasks with LLM-based suggestions in either standard American English (SAE) or their self-identified sociolect. Our findings showed that sociolect usage by LLMs influenced both reliance and perceptions, though in some surprising ways. Results suggest that both AAE and Queer slang speakers relied more on the SAE agent, and had more positive perceptions of the SAE agent. Yet, only Queer slang speakers felt more social presence from the Queer slang agent over the SAE one, whereas only AAE speakers preferred and trusted the SAE agent over the AAE one. These findings emphasize the need to test for behavioral outcomes rather than simply assume that personalization would lead to a better and safer reliance outcome. They also highlight the nuanced dynamics of minoritized language in machine interactions, underscoring the need for LLMs to be carefully designed to respect cultural and linguistic boundaries while fostering genuine user engagement and trust.", "source": "arxiv", "arxiv_id": "2505.05660v3", "pdf_url": "https://arxiv.org/pdf/2505.05660v3", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1145/3715275.3732045", "venue": "", "published": "2025-05-08T21:35:59Z", "updated": "2025-08-09T05:36:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents", "authors": ["Rongwu Xu", "Xiaojian Li", "Shuo Chen", "Wei Xu"], "year": 2025, "url": "http://arxiv.org/abs/2502.11355v3", "abstract": "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We release our code to foster further research.", "source": "arxiv", "arxiv_id": "2502.11355v3", "pdf_url": "https://arxiv.org/pdf/2502.11355v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T02:11:17Z", "updated": "2025-03-23T06:22:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent", "authors": ["Bowen Chen", "Zhao Wang", "Shingo Takamatsu"], "year": 2025, "url": "http://arxiv.org/abs/2507.02353v1", "abstract": "Keyword decision in Sponsored Search Advertising is critical to the success of ad campaigns. While LLM-based methods offer automated keyword generation, they face three major limitations: reliance on large-scale query-keyword pair data, lack of online multi-objective performance monitoring and optimization, and weak quality control in keyword selection. These issues hinder the agentic use of LLMs in fully automating keyword decisions by monitoring and reasoning over key performance indicators such as impressions, clicks, conversions, and CTA effectiveness. To overcome these challenges, we propose OMS, a keyword generation framework that is On-the-fly (requires no training data, monitors online performance, and adapts accordingly), Multi-objective (employs agentic reasoning to optimize keywords based on multiple performance metrics), and Self-reflective (agentically evaluates keyword quality). Experiments on benchmarks and real-world ad campaigns show that OMS outperforms existing methods; ablation and human evaluations confirm the effectiveness of each component and the quality of generated keywords.", "source": "arxiv", "arxiv_id": "2507.02353v1", "pdf_url": "https://arxiv.org/pdf/2507.02353v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-03T06:37:55Z", "updated": "2025-07-03T06:37:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OPEN-THEATRE: An Open-Source Toolkit for LLM-based Interactive Drama", "authors": ["Tianyang Xu", "Hongqiu Wu", "Weiqi Wu", "Hai Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2509.16713v1", "abstract": "LLM-based Interactive Drama introduces a novel dialogue scenario in which the player immerses into a character and engages in a dramatic story by interacting with LLM agents. Despite the fact that this emerging area holds significant promise, it remains largely underexplored due to the lack of a well-designed playground to develop a complete drama. This makes a significant barrier for researchers to replicate, extend, and study such systems. Hence, we present Open-Theatre, the first open-source toolkit for experiencing and customizing LLM-based interactive drama. It refines prior work with an efficient multi-agent architecture and a hierarchical retrieval-based memory system, designed to enhance narrative coherence and realistic long-term behavior in complex interactions. In addition, we provide a highly configurable pipeline, making it easy for researchers to develop and optimize new approaches.", "source": "arxiv", "arxiv_id": "2509.16713v1", "pdf_url": "https://arxiv.org/pdf/2509.16713v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-20T14:53:14Z", "updated": "2025-09-20T14:53:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems", "authors": ["Xiaozhe Li", "Jixuan Chen", "Xinyu Fang", "Shengyuan Ding", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "year": 2025, "url": "http://arxiv.org/abs/2506.10764v1", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in solving diverse tasks. However, their proficiency in iteratively optimizing complex solutions through learning from previous feedback remains insufficiently explored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark designed to evaluate LLM agents on large-scale search space optimization problems. OPT-BENCH includes 20 real-world machine learning tasks sourced from Kaggle and 10 classical NP problems, offering a diverse and challenging environment for assessing LLM agents on iterative reasoning and solution refinement. To enable rigorous evaluation, we introduce OPT-Agent, an end-to-end optimization framework that emulates human reasoning when tackling complex problems by generating, validating, and iteratively improving solutions through leveraging historical feedback. Through extensive experiments on 9 state-of-the-art LLMs from 6 model families, we analyze the effects of optimization iterations, temperature settings, and model architectures on solution quality and convergence. Our results demonstrate that incorporating historical context significantly enhances optimization performance across both ML and NP tasks. All datasets, code, and evaluation tools are open-sourced to promote further research in advancing LLM-driven optimization and iterative reasoning. Project page: \\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.", "source": "arxiv", "arxiv_id": "2506.10764v1", "pdf_url": "https://arxiv.org/pdf/2506.10764v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-12T14:46:41Z", "updated": "2025-06-12T14:46:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problems with Reasoning LLM", "authors": ["Bowen Zhang", "Pengcheng Luo", "Genke Yang", "Boon-Hee Soong", "Chau Yuen"], "year": 2025, "url": "http://arxiv.org/abs/2503.10009v3", "abstract": "With the rise of artificial intelligence (AI), applying large language models (LLMs) to mathematical problem-solving has attracted increasing attention. Most existing approaches attempt to improve Operations Research (OR) optimization problem-solving through prompt engineering or fine-tuning strategies for LLMs. However, these methods are fundamentally constrained by the limited capabilities of non-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an AI agent framework built on reasoning LLMs for automated OR problem solving. The framework decomposes the task into three sequential stages: mathematical modeling, code generation, and debugging. Each task is handled by a dedicated sub-agent, which enables more targeted reasoning. We also construct BWOR, an OR dataset for evaluating LLM performance on OR tasks. Our analysis shows that in the benchmarks NL4OPT, MAMO, and IndustryOR, reasoning LLMs sometimes underperform their non-reasoning counterparts within the same model family. In contrast, BWOR provides a more consistent and discriminative assessment of model capabilities. Experimental results demonstrate that OR-LLM-Agent utilizing DeepSeek-R1 in its framework outperforms advanced methods, including GPT-o3, Gemini 2.5 Pro, DeepSeek-R1, and ORLM, by at least 7\\% in accuracy. These results demonstrate the effectiveness of task decomposition for OR problem solving.", "source": "arxiv", "arxiv_id": "2503.10009v3", "pdf_url": "https://arxiv.org/pdf/2503.10009v3", "categories": ["cs.AI", "math.OC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-13T03:40:50Z", "updated": "2025-08-01T04:52:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ORCA: ORchestrating Causal Agent", "authors": ["Joanie Hayoun Chung", "Chaemyung Lim", "Sumin Lee", "Songseong Kim", "Sungbin Lim"], "year": 2025, "url": "http://arxiv.org/abs/2508.21304v2", "abstract": "Causal inference is essential for decision-making science while the complexity of the data analysis workflow, ranging from data wrangling to causal analysis, increases substantially as the scale of data grows in complicated business environments. Especially, the execution of the workflow in relational databases by non-experts can result in repetitive bottlenecks which impede timely and responsible business insights. To address this challenge, we propose ORCA (Orchestrating Causal Agent), an LLM agentic system that can automate routine workflows in RDBMS while preserving expert oversight via human-AI interactions. ORCA orchestrates the full data analysis pipeline: interpreting natural language queries, navigating tables from DB servers, generating proper SQL codes, preprocessing data, and configuring modeling processes using causal inference libraries. Domain experts still can control the automation through iterative interactions with ORCA, enabling robust data-driven decision making with less technical expertise in statistical computing. Empirical evaluations on benchmark and synthetic e-commerce datasets demonstrate competitive performance of ORCA in table understanding, query generation, and cause-effect estimation -- achieving over $7\\times$ improvement in estimating average treatment compared to GPT-4o mini.", "source": "arxiv", "arxiv_id": "2508.21304v2", "pdf_url": "https://arxiv.org/pdf/2508.21304v2", "categories": ["cs.DB", "cs.MA"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-08-29T01:59:34Z", "updated": "2025-09-01T01:33:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ORFS-agent: Tool-Using Agents for Chip Design Optimization", "authors": ["Amur Ghose", "Andrew B. Kahng", "Sayak Kundu", "Zhiang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.08332v2", "abstract": "Machine learning has been widely used to optimize complex engineering workflows across numerous domains. In the context of integrated circuit design, modern flows (e.g., going from a register-transfer level netlist to physical layouts) involve extensive configuration via thousands of parameters, and small changes to these parameters can have large downstream impacts on desired outcomes - namely design performance, power, and area. Recent advances in Large Language Models (LLMs) offer new opportunities for learning and reasoning within such high-dimensional optimization tasks. In this work, we introduce ORFS-agent, an LLM-based iterative optimization agent that automates parameter tuning in an open-source hardware design flow. ORFS-agent adaptively explores parameter configurations, demonstrating clear improvements over standard Bayesian optimization approaches in terms of resource efficiency and final design metrics. Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Moreover, by following natural language objectives to trade off certain metrics for others, ORFS-agent demonstrates a flexible and interpretable framework for multi-objective optimization. Crucially, RFS-agent is modular and model-agnostic, and can be plugged in to any frontier LLM without any further fine-tuning.", "source": "arxiv", "arxiv_id": "2506.08332v2", "pdf_url": "https://arxiv.org/pdf/2506.08332v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-10T01:38:57Z", "updated": "2025-08-01T06:36:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Xiaofei Sun", "Keze Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.04876v1", "abstract": "This paper introduces OSC (Orchestrating Cognitive Synergy), a knowledge-aware adaptive collaboration framework designed to enhance cognitive synergy in multi-agent systems with large language models. While prior work has advanced agent selection and result aggregation, efficient linguistic interactions for deep collaboration among expert agents remain a critical bottleneck. OSC addresses this gap as a pivotal intermediate layer between selection and aggregation, introducing Collaborator Knowledge Models (CKM) to enable each agent to dynamically perceive its collaborators' cognitive states. Through real-time cognitive gap analysis, agents adaptively adjust communication behaviors, including content focus, detail level, and expression style, using learned strategies. Experiments on complex reasoning and problem-solving benchmarks demonstrate that OSC significantly improves task performance and communication efficiency, transforming \"parallel-working individuals'' into a \"deeply collaborative cognitive team.'' This framework not only optimizes multi-agent collaboration but also offers new insights into LLM agent interaction behaviors.", "source": "arxiv", "arxiv_id": "2509.04876v1", "pdf_url": "https://arxiv.org/pdf/2509.04876v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-05T07:44:05Z", "updated": "2025-09-05T07:44:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data", "authors": ["Reachal Wang", "Yuqi Jia", "Neil Zhenqiang Gong"], "year": 2025, "url": "http://arxiv.org/abs/2512.09321v3", "abstract": "Prompt injection attacks aim to contaminate the input data of an LLM to mislead it into completing an attacker-chosen task instead of the intended task. In many applications and agents, the input data originates from multiple sources, with each source contributing a segment of the overall input. In these multi-source scenarios, an attacker may control only a subset of the sources and contaminate the corresponding segments, but typically does not know the order in which the segments are arranged within the input. Existing prompt injection attacks either assume that the entire input data comes from a single source under the attacker's control or ignore the uncertainty in the ordering of segments from different sources. As a result, their success is limited in domains involving multi-source data.\n  In this work, we propose ObliInjection, the first prompt injection attack targeting LLM applications and agents with multi-source input data. ObliInjection introduces two key technical innovations: the order-oblivious loss, which quantifies the likelihood that the LLM will complete the attacker-chosen task regardless of how the clean and contaminated segments are ordered; and the orderGCG algorithm, which is tailored to minimize the order-oblivious loss and optimize the contaminated segments. Comprehensive experiments across three datasets spanning diverse application domains and twelve LLMs demonstrate that ObliInjection is highly effective, even when only one out of 6-100 segments in the input data is contaminated. Our code and data are available at: https://github.com/ReachalWang/ObliInjection.", "source": "arxiv", "arxiv_id": "2512.09321v3", "pdf_url": "https://arxiv.org/pdf/2512.09321v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-10T05:10:22Z", "updated": "2025-12-15T04:21:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows", "authors": ["Weixuan Wang", "Dongge Han", "Daniel Madrigal Diaz", "Jin Xu", "Victor RÃ¼hle", "Saravan Rajmohan"], "year": 2025, "url": "http://arxiv.org/abs/2508.09124v1", "abstract": "Autonomous agents powered by large language models (LLMs) are increasingly deployed in real-world applications requiring complex, long-horizon workflows. However, existing benchmarks predominantly focus on atomic tasks that are self-contained and independent, failing to capture the long-term contextual dependencies and multi-interaction coordination required in realistic scenarios. To address this gap, we introduce OdysseyBench, a comprehensive benchmark for evaluating LLM agents on long-horizon workflows across diverse office applications including Word, Excel, PDF, Email, and Calendar. Our benchmark comprises two complementary splits: OdysseyBench+ with 300 tasks derived from real-world use cases, and OdysseyBench-Neo with 302 newly synthesized complex tasks. Each task requires agent to identify essential information from long-horizon interaction histories and perform multi-step reasoning across various applications. To enable scalable benchmark creation, we propose HomerAgents, a multi-agent framework that automates the generation of long-horizon workflow benchmarks through systematic environment exploration, task generation, and dialogue synthesis. Our extensive evaluation demonstrates that OdysseyBench effectively challenges state-of-the-art LLM agents, providing more accurate assessment of their capabilities in complex, real-world contexts compared to existing atomic task benchmarks. We believe that OdysseyBench will serve as a valuable resource for advancing the development and evaluation of LLM agents in real-world productivity scenarios. In addition, we release OdysseyBench and HomerAgents to foster research along this line.", "source": "arxiv", "arxiv_id": "2508.09124v1", "pdf_url": "https://arxiv.org/pdf/2508.09124v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-12T17:53:03Z", "updated": "2025-08-12T17:53:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning", "authors": ["Shihao Wang", "Zhiding Yu", "Xiaohui Jiang", "Shiyi Lan", "Min Shi", "Nadine Chang", "Jan Kautz", "Ying Li", "Jose M. Alvarez"], "year": 2025, "url": "http://arxiv.org/abs/2504.04348v2", "abstract": "The advances in vision-language models (VLMs) have led to a growing interest in autonomous driving to leverage their strong reasoning capabilities. However, extending these capabilities from 2D to full 3D understanding is crucial for real-world applications. To address this challenge, we propose OmniDrive, a holistic vision-language dataset that aligns agent models with 3D driving tasks through counterfactual reasoning. This approach enhances decision-making by evaluating potential scenarios and their outcomes, similar to human drivers considering alternative actions. Our counterfactual-based synthetic data annotation process generates large-scale, high-quality datasets, providing denser supervision signals that bridge planning trajectories and language-based reasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely Omni-L and Omni-Q, to assess the importance of vision-language alignment versus 3D perception, revealing critical insights into designing effective LLM-agents. Significant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop planning demonstrate the effectiveness of our dataset and methods.", "source": "arxiv", "arxiv_id": "2504.04348v2", "pdf_url": "https://arxiv.org/pdf/2504.04348v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-04-06T03:54:21Z", "updated": "2025-04-16T15:00:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections", "authors": ["Manasa Bharadwaj", "Nikhil Verma", "Kevin Ferreira"], "year": 2025, "url": "http://arxiv.org/abs/2506.17449v1", "abstract": "Efforts to improve Large Language Model (LLM) agent performance on complex tasks have largely focused on fine-tuning and iterative self-correction. However, these approaches often lack generalizable mechanisms for longterm learning and remain inefficient in dynamic environments. We introduce OmniReflect, a hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent. OmniReflect operates in two modes: Self-sustaining, where a single agent periodically curates its own reflections during task execution, and Co-operative, where a Meta-advisor derives a constitution from a small calibration set to guide another agent. To construct these constitutional principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering a balance between contextual adaptability and computational efficiency. Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion baselines on BabyAI. These findings highlight the robustness and effectiveness of OmniReflect across environments and backbones.", "source": "arxiv", "arxiv_id": "2506.17449v1", "pdf_url": "https://arxiv.org/pdf/2506.17449v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-20T19:38:21Z", "updated": "2025-06-20T19:38:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues", "authors": ["Monika Zamojska", "JarosÅaw A. Chudziak"], "year": 2025, "url": "http://arxiv.org/abs/2512.17060v1", "abstract": "LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.", "source": "arxiv", "arxiv_id": "2512.17060v1", "pdf_url": "https://arxiv.org/pdf/2512.17060v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-12-18T20:53:31Z", "updated": "2025-12-18T20:53:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language", "authors": ["SÃ©bastien Salva", "Redha Taguelmimt"], "year": 2025, "url": "http://arxiv.org/abs/2509.19136v2", "abstract": "The use of natural language (NL) test cases for validating graphical user interface (GUI) applications is emerging as a promising direction to manually written executable test scripts, which are costly to develop and difficult to maintain. Recent advances in large language models (LLMs) have opened the possibility of the direct execution of NL test cases by LLM agents. This paper investigates this direction, focusing on the impact on NL test case unsoundness and on test case execution consistency. NL test cases are inherently unsound, as they may yield false failures due to ambiguous instructions or unpredictable agent behaviour. Furthermore, repeated executions of the same NL test case may lead to inconsistent outcomes, undermining test reliability. To address these challenges, we propose an algorithm for executing NL test cases with guardrail mechanisms and specialised agents that dynamically verify the correct execution of each test step. We introduce measures to evaluate the capabilities of LLMs in test execution and one measure to quantify execution consistency. We propose a definition of weak unsoundness to characterise contexts in which NL test case execution remains acceptable, with respect to the industrial quality levels Six Sigma. Our experimental evaluation with eight publicly available LLMs, ranging from 3B to 70B parameters, demonstrates both the potential and current limitations of current LLM agents for GUI testing. Our experiments show that Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case execution with high execution consistency (above the level 3-sigma). We provide prototype tools, test suites, and results.", "source": "arxiv", "arxiv_id": "2509.19136v2", "pdf_url": "https://arxiv.org/pdf/2509.19136v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-23T15:20:40Z", "updated": "2025-10-01T09:32:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents", "authors": ["Zhaoxi Zhang", "Yitong Duan", "Yanzhi Zhang", "Yiming Xu", "Weikang Li", "Jiahui Liang", "Deguo Xia", "Jizhou Huang", "Jiyan He", "Yunfang Wu"], "year": 2025, "url": "http://arxiv.org/abs/2512.20957v4", "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.", "source": "arxiv", "arxiv_id": "2512.20957v4", "pdf_url": "https://arxiv.org/pdf/2512.20957v4", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-24T05:27:53Z", "updated": "2026-01-08T03:22:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "authors": ["Yaxin Luo", "Zhaoyi Li", "Jiacheng Liu", "Jiacheng Cui", "Xiaohan Zhao", "Zhiqiang Shen"], "year": 2025, "url": "http://arxiv.org/abs/2505.24878v1", "abstract": "CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.", "source": "arxiv", "arxiv_id": "2505.24878v1", "pdf_url": "https://arxiv.org/pdf/2505.24878v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-30T17:59:55Z", "updated": "2025-05-30T17:59:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery", "authors": ["Licong Xu", "Milind Sarkar", "Anto I. Lonappan", "ÃÃ±igo Zubeldia", "Pablo Villanueva-Domingo", "Santiago Casas", "Christian Fidler", "Chetana Amancharla", "Ujjwal Tiwari", "Adrian Bayer", "Chadi Ait Ekioui", "Miles Cranmer", "Adrian Dimitrov", "James Fergusson", "Kahaan Gandhi", "Sven Krippendorf", "Andrew Laverick", "Julien Lesgourgues", "Antony Lewis", "Thomas Meier", "Blake Sherwin", "Kristen Surrao", "Francisco Villaescusa-Navarro", "Chi Wang", "Xueqing Xu", "Boris Bolliet"], "year": 2025, "url": "http://arxiv.org/abs/2507.07257v2", "abstract": "We present a multi-agent system for automation of scientific research tasks, cmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about 30 Large Language Model (LLM) agents and implements a Planning & Control strategy to orchestrate the agentic workflow, with no human-in-the-loop at any point. Each agent specializes in a different task (performing retrieval on scientific papers and codebases, writing code, interpreting results, critiquing the output of other agents) and the system is able to execute code locally. We successfully apply cmbagent to carry out a PhD level cosmology task (the measurement of cosmological parameters using supernova data) and evaluate its performance on two benchmark sets, finding superior performance over state-of-the-art LLMs. The source code is available on GitHub, demonstration videos are also available, and the system is deployed on HuggingFace and will be available on the cloud.", "source": "arxiv", "arxiv_id": "2507.07257v2", "pdf_url": "https://arxiv.org/pdf/2507.07257v2", "categories": ["cs.AI", "astro-ph.IM", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-09T20:03:30Z", "updated": "2025-07-11T14:43:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OpenFOAMGPT: a RAG-Augmented LLM Agent for OpenFOAM-Based Computational Fluid Dynamics", "authors": ["Sandeep Pandey", "Ran Xu", "Wenkang Wang", "Xu Chu"], "year": 2025, "url": "http://arxiv.org/abs/2501.06327v1", "abstract": "This work presents a large language model (LLM)-based agent OpenFOAMGPT tailored for OpenFOAM-centric computational fluid dynamics (CFD) simulations, leveraging two foundation models from OpenAI: the GPT-4o and a chain-of-thought (CoT)-enabled o1 preview model. Both agents demonstrate success across multiple tasks. While the price of token with o1 model is six times as that of GPT-4o, it consistently exhibits superior performance in handling complex tasks, from zero-shot case setup to boundary condition modifications, turbulence model adjustments, and code translation. Through an iterative correction loop, the agent efficiently addressed single- and multi-phase flow, heat transfer, RANS, LES, and other engineering scenarios, often converging in a limited number of iterations at low token costs. To embed domain-specific knowledge, we employed a retrieval-augmented generation (RAG) pipeline, demonstrating how preexisting simulation setups can further specialize the agent for sub-domains such as energy and aerospace. Despite the great performance of the agent, human oversight remains crucial for ensuring accuracy and adapting to shifting contexts. Fluctuations in model performance over time suggest the need for monitoring in mission-critical applications. Although our demonstrations focus on OpenFOAM, the adaptable nature of this framework opens the door to developing LLM-driven agents into a wide range of solvers and codes. By streamlining CFD simulations, this approach has the potential to accelerate both fundamental research and industrial engineering advancements.", "source": "arxiv", "arxiv_id": "2501.06327v1", "pdf_url": "https://arxiv.org/pdf/2501.06327v1", "categories": ["physics.flu-dyn", "physics.comp-ph"], "primary_category": "physics.flu-dyn", "doi": "10.1063/5.0257555", "venue": "", "published": "2025-01-10T20:07:05Z", "updated": "2025-01-10T20:07:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment", "authors": ["Krti Tallam", "Emma Miller"], "year": 2025, "url": "http://arxiv.org/abs/2505.22852v1", "abstract": "CaMeL (Capabilities for Machine Learning) introduces a capability-based sandbox to mitigate prompt injection attacks in large language model (LLM) agents. While effective, CaMeL assumes a trusted user prompt, omits side-channel concerns, and incurs performance tradeoffs due to its dual-LLM design. This response identifies these issues and proposes engineering improvements to expand CaMeL's threat coverage and operational usability. We introduce: (1) prompt screening for initial inputs, (2) output auditing to detect instruction leakage, (3) a tiered-risk access model to balance usability and control, and (4) a verified intermediate language for formal guarantees. Together, these upgrades align CaMeL with best practices in enterprise security and support scalable deployment.", "source": "arxiv", "arxiv_id": "2505.22852v1", "pdf_url": "https://arxiv.org/pdf/2505.22852v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-28T20:35:24Z", "updated": "2025-05-28T20:35:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Opponent Shaping in LLM Agents", "authors": ["Marta Emili Garcia Segura", "Stephen Hailes", "Mirco Musolesi"], "year": 2025, "url": "http://arxiv.org/abs/2510.08255v1", "abstract": "Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.", "source": "arxiv", "arxiv_id": "2510.08255v1", "pdf_url": "https://arxiv.org/pdf/2510.08255v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-09T14:13:24Z", "updated": "2025-10-09T14:13:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents", "authors": ["Enhao Zhang", "Erkang Zhu", "Gagan Bansal", "Adam Fourney", "Hussein Mozannar", "Jack Gerrits"], "year": 2025, "url": "http://arxiv.org/abs/2507.08944v1", "abstract": "Large language model (LLM)-based multi-agent systems have demonstrated remarkable promise for tackling complex tasks by breaking them down into subtasks that are iteratively planned, executed, observed, and refined. Despite their effectiveness, these systems often incur high latency because real-world problems frequently demand multiple iterative cycles of reasoning steps. To address this challenge, we propose M1-Parallel, a framework that concurrently runs multiple multi-agent teams in parallel to uncover distinct solution paths. By leveraging an event-driven communication model with asynchronous messaging, M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to either reduce end-to-end latency or boost task completion rates. Our experiments on complex tasks show that M1-Parallel with early termination achieves up to $2.2\\times$ speedup while preserving accuracy, and that M1-Parallel with aggregation yields higher task completion rates. We further investigate strategies aimed at encouraging diverse execution plans but observe no additional performance gains over repeated sampling. Overall, these findings underscore the potential of parallel plan execution for optimizing multi-agent systems for real-world, high-complexity reasoning tasks.", "source": "arxiv", "arxiv_id": "2507.08944v1", "pdf_url": "https://arxiv.org/pdf/2507.08944v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-07-11T18:09:22Z", "updated": "2025-07-11T18:09:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games", "authors": ["Dongmin Park", "Minkyu Kim", "Beongjun Choi", "Junhyuck Kim", "Keon Lee", "Jonghyun Lee", "Inkyu Park", "Byeong-Uk Lee", "Jaeyoung Hwang", "Jaewoo Ahn", "Ameya S. Mahabaleshwarkar", "Bilal Kartal", "Pritam Biswas", "Yoshi Suhara", "Kangwook Lee", "Jaewoong Cho"], "year": 2025, "url": "http://arxiv.org/abs/2506.03610v2", "abstract": "Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents. To fill these gaps, we present Orak, a foundational benchmark designed to train and evaluate LLM agents across diverse real-world video games. Unlike existing benchmarks, Orak includes 12 popular video games spanning all major genres, enabling comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. To support consistent evaluation of LLMs, we introduce a plug-and-play interface based on Model Context Protocol (MCP) that enables LLMs to seamlessly connect with games and manipulate agentic modules. Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay trajectories across diverse game genres. Orak offers a comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects, establishing a foundation towards building generic gaming agents. Code is available at https://github.com/krafton-ai/Orak.", "source": "arxiv", "arxiv_id": "2506.03610v2", "pdf_url": "https://arxiv.org/pdf/2506.03610v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-04T06:40:33Z", "updated": "2025-09-29T01:43:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OrcaLoca: An LLM Agent Framework for Software Issue Localization", "authors": ["Zhongming Yu", "Hejia Zhang", "Yujie Zhao", "Hanxian Huang", "Matrix Yao", "Ke Ding", "Jishen Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.00350v2", "abstract": "Recent developments in Large Language Model (LLM) agents are revolutionizing Autonomous Software Engineering (ASE), enabling automated coding, problem fixes, and feature improvements. However, localization -- precisely identifying software problems by navigating to relevant code sections -- remains a significant challenge. Current approaches often yield suboptimal results due to a lack of effective integration between LLM agents and precise code search mechanisms. This paper introduces OrcaLoca, an LLM agent framework that improves accuracy for software issue localization by integrating priority-based scheduling for LLM-guided action, action decomposition with relevance scoring, and distance-aware context pruning. Experimental results demonstrate that OrcaLoca becomes the new open-source state-of-the-art (SOTA) in function match rate (65.33%) on SWE-bench Lite. It also improves the final resolved rate of an open-source framework by 6.33 percentage points through its patch generation integration.", "source": "arxiv", "arxiv_id": "2502.00350v2", "pdf_url": "https://arxiv.org/pdf/2502.00350v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-02-01T07:15:03Z", "updated": "2025-10-10T00:02:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap", "authors": ["Andrew Zhu", "Chris Callison-Burch"], "year": 2025, "url": "http://arxiv.org/abs/2509.16325v1", "abstract": "Imagine AI assistants that enhance conversations without interrupting them: quietly providing relevant information during a medical consultation, seamlessly preparing materials as teachers discuss lesson plans, or unobtrusively scheduling meetings as colleagues debate calendars. While modern conversational LLM agents directly assist human users with tasks through a chat interface, we study this alternative paradigm for interacting with LLM agents, which we call \"overhearing agents.\" Rather than demanding the user's attention, overhearing agents continuously monitor ambient activity and intervene only when they can provide contextual assistance. In this paper, we present the first analysis of overhearing LLM agents as a distinct paradigm in human-AI interaction and establish a taxonomy of overhearing agent interactions and tasks grounded in a survey of works on prior LLM-powered agents and exploratory HCI studies. Based on this taxonomy, we create a list of best practices for researchers and developers building overhearing agent systems. Finally, we outline the remaining research gaps and reveal opportunities for future research in the overhearing paradigm.", "source": "arxiv", "arxiv_id": "2509.16325v1", "pdf_url": "https://arxiv.org/pdf/2509.16325v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-19T18:11:04Z", "updated": "2025-09-19T18:11:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework", "authors": ["Kamer Ali Yuksel"], "year": 2025, "url": "http://arxiv.org/abs/2512.16970v1", "abstract": "Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.", "source": "arxiv", "arxiv_id": "2512.16970v1", "pdf_url": "https://arxiv.org/pdf/2512.16970v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-18T12:54:56Z", "updated": "2025-12-18T12:54:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases", "authors": ["Sri Vatsa Vuddanti", "Aarav Shah", "Satwik Kumar Chittiprolu", "Tony Song", "Sunishchal Dev", "Kevin Zhu", "Maheep Chaudhary"], "year": 2025, "url": "http://arxiv.org/abs/2509.25238v1", "abstract": "Tool-augmented language agents frequently fail in real-world deployment due to tool malfunctions--timeouts, API exceptions, or inconsistent outputs--triggering cascading reasoning errors and task abandonment. Existing agent training pipelines optimize only for success trajectories, failing to expose models to the tool failures that dominate real-world usage. We propose \\textbf{PALADIN}, a generalizable framework for equipping language agents with robust failure recovery capabilities. PALADIN trains on 50,000+ recovery-annotated trajectories constructed via systematic failure injection and expert demonstrations on an enhanced ToolBench dataset. Training uses LoRA-based fine-tuning to retain base capabilities while injecting recovery competence. At inference, PALADIN detects execution-time errors and retrieves the most similar case from a curated bank of 55+ failure exemplars aligned with ToolScan's taxonomy, then executes the corresponding recovery action. This approach generalizes to novel failures beyond the training distribution, retaining 95.2\\% recovery performance on unseen tool APIs. Evaluation across PaladinEval and ToolReflectEval demonstrates consistent improvements in Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR), and Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57% relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%) by +13.3%. Against vanilla agents, PALADIN achieves 89.86\\% RR (+66% relative improvement from 23.75%). These results establish PALADIN as an effective method for building fault-tolerant agents capable of robust recovery in real-world tool environments.", "source": "arxiv", "arxiv_id": "2509.25238v1", "pdf_url": "https://arxiv.org/pdf/2509.25238v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-25T10:37:30Z", "updated": "2025-09-25T10:37:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured Data with Text and Relational Information", "authors": ["Hansi Yang", "Qi Zhang", "Wei Jiang", "Jianguo Li"], "year": 2025, "url": "http://arxiv.org/abs/2502.21087v1", "abstract": "Large language models (LLMs) have shown impressive abilities in answering questions across various domains, but they often encounter hallucination issues on questions that require professional and up-to-date knowledge. To address this limitation, retrieval-augmented generation (RAG) techniques have been proposed, which retrieve relevant information from external sources to inform their responses. However, existing RAG methods typically focus on a single type of external data, such as vectorized text database or knowledge graphs, and cannot well handle real-world questions on semi-structured data containing both text and relational information. To bridge this gap, we introduce PASemiQA, a novel approach that jointly leverages text and relational information in semi-structured data to answer questions. PASemiQA first generates a plan to identify relevant text and relational information to answer the question in semi-structured data, and then uses an LLM agent to traverse the semi-structured data and extract necessary information. Our empirical results demonstrate the effectiveness of PASemiQA across different semi-structured datasets from various domains, showcasing its potential to improve the accuracy and reliability of question answering systems on semi-structured data.", "source": "arxiv", "arxiv_id": "2502.21087v1", "pdf_url": "https://arxiv.org/pdf/2502.21087v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-28T14:26:47Z", "updated": "2025-02-28T14:26:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization", "authors": ["Zouying Cao", "Runze Wang", "Yifei Yang", "Xinbei Ma", "Xiaoyong Zhu", "Bo Zheng", "Hai Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2506.01475v1", "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities in handling complex interactive problems. Existing LLM agents mainly generate natural language plans to guide reasoning, which is verbose and inefficient. NL plans are also tailored to specific tasks and restrict agents' ability to generalize across similar tasks. To this end, we explore pseudocode-style plans (P-code Plan) to capture the structural logic of reasoning. We find that P-code Plan empowers LLM agents with stronger generalization ability and more efficiency. Inspired by this finding, we propose a pseudocode-style Planning Guided Preference Optimization method called PGPO for effective agent learning. With two planning-oriented rewards, PGPO further enhances LLM agents' ability to generate high-quality P-code Plans and subsequent reasoning. Experiments show that PGPO achieves superior performance on representative agent benchmarks and outperforms the current leading baselines. Analyses reveal the advantage of PGPO in reducing action errors and omissions during reasoning.", "source": "arxiv", "arxiv_id": "2506.01475v1", "pdf_url": "https://arxiv.org/pdf/2506.01475v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-02T09:35:07Z", "updated": "2025-06-02T09:35:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play", "authors": ["Wei Fang", "Yang Zhang", "Kaizhi Qian", "James Glass", "Yada Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2503.14432v2", "abstract": "Large language models (LLMs) are increasingly integrated with specialized external tools, yet many tasks demand zero-shot tool usage with minimal or noisy documentation. Existing solutions rely on manual rewriting or labeled data for validation, making them inapplicable in true zero-shot settings. To address these challenges, we propose PLAY2PROMPT, an automated framework that systematically \"plays\" with each tool to explore its input-output behaviors. Through this iterative trial-and-error process, PLAY2PROMPT refines tool documentation and generates usage examples without any labeled data. These examples not only guide LLM inference but also serve as validation to further enhance tool utilization. Extensive experiments on real-world tasks demonstrate that PLAY2PROMPT significantly improves zero-shot tool performance across both open and closed models, offering a scalable and effective solution for domain-specific tool integration.", "source": "arxiv", "arxiv_id": "2503.14432v2", "pdf_url": "https://arxiv.org/pdf/2503.14432v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-18T17:09:57Z", "updated": "2025-06-12T16:53:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search", "authors": ["Yichen He", "Guanhua Huang", "Peiyuan Feng", "Yuan Lin", "Yuchen Zhang", "Hang Li", "Weinan E"], "year": 2025, "url": "http://arxiv.org/abs/2501.10120v2", "abstract": "We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholar queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4o for paraphrased queries, ChatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50, and exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.", "source": "arxiv", "arxiv_id": "2501.10120v2", "pdf_url": "https://arxiv.org/pdf/2501.10120v2", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-01-17T11:12:28Z", "updated": "2025-05-27T11:01:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge", "authors": ["Yongrui Chen", "Junhao He", "Linbo Fu", "Shenyu Zhang", "Rihui Jin", "Xinbang Dai", "Jiaqi Li", "Dehai Min", "Nan Hu", "Yuxin Zhang", "Guilin Qi", "Yi Huang", "Tongtong Wu"], "year": 2025, "url": "http://arxiv.org/abs/2504.12734v2", "abstract": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language questions (NLQs) by using structured sources such as tables, databases, and knowledge graphs in a unified way. Existing USKR methods either rely on employing task-specific strategies or custom-defined representations, which struggle to leverage the knowledge transfer between different SKR tasks or align with the prior of LLMs, thereby limiting their performance. This paper proposes a novel USKR framework named \\textsc{Pandora}, which takes advantage of \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge representation for alignment with LLM pre-training. It employs an LLM to generate textual reasoning steps and executable Python code for each question. Demonstrations are drawn from a memory of training examples that cover various SKR tasks, facilitating knowledge transfer. Extensive experiments on four benchmarks involving three SKR tasks demonstrate that \\textsc{Pandora} outperforms existing unified frameworks and competes effectively with task-specific methods.", "source": "arxiv", "arxiv_id": "2504.12734v2", "pdf_url": "https://arxiv.org/pdf/2504.12734v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-17T08:18:09Z", "updated": "2025-09-23T11:15:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion", "authors": ["Yiran Rex Ma"], "year": 2025, "url": "http://arxiv.org/abs/2508.06110v1", "abstract": "Table reasoning, including tabular QA and fact verification, often depends on annotated data or complex data augmentation, limiting flexibility and generalization. LLMs, despite their versatility, often underperform compared to simple supervised models. To approach these issues, we introduce PanelTR, a framework utilizing LLM agent scientists for robust table reasoning through a structured scientific approach. PanelTR's workflow involves agent scientists conducting individual investigations, engaging in self-review, and participating in collaborative peer-review discussions. This process, driven by five scientist personas, enables semantic-level transfer without relying on data augmentation or parametric optimization. Experiments across four benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully supervised models, all while remaining independent of training data. Our findings indicate that structured scientific methodology can effectively handle complex tasks beyond table reasoning with flexible semantic understanding in a zero-shot context.", "source": "arxiv", "arxiv_id": "2508.06110v1", "pdf_url": "https://arxiv.org/pdf/2508.06110v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "10.1109/IJCNN64981.2025.11228106", "venue": "", "published": "2025-08-08T08:15:52Z", "updated": "2025-08-08T08:15:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems", "authors": ["Chengxuan Xia", "Qianye Wu", "Sixuan Tian", "Yilun Hao"], "year": 2025, "url": "http://arxiv.org/abs/2507.17061v4", "abstract": "Large language model (LLM) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent communication, reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent LLM systems.", "source": "arxiv", "arxiv_id": "2507.17061v4", "pdf_url": "https://arxiv.org/pdf/2507.17061v4", "categories": ["cs.MA", "cs.AI", "cs.IR"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-07-22T22:42:51Z", "updated": "2025-12-19T03:33:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search", "authors": ["Joseph Oladokun"], "year": 2025, "url": "http://arxiv.org/abs/2511.18313v1", "abstract": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.", "source": "arxiv", "arxiv_id": "2511.18313v1", "pdf_url": "https://arxiv.org/pdf/2511.18313v1", "categories": ["cs.CL", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-23T06:50:01Z", "updated": "2025-11-23T06:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Performant LLM Agentic Framework for Conversational AI", "authors": ["Alex Casella", "Wayne Wang"], "year": 2025, "url": "http://arxiv.org/abs/2503.06410v1", "abstract": "The rise of Agentic applications and automation in the Voice AI industry has led to an increased reliance on Large Language Models (LLMs) to navigate graph-based logic workflows composed of nodes and edges. However, existing methods face challenges such as alignment errors in complex workflows and hallucinations caused by excessive context size. To address these limitations, we introduce the Performant Agentic Framework (PAF), a novel system that assists LLMs in selecting appropriate nodes and executing actions in order when traversing complex graphs. PAF combines LLM-based reasoning with a mathematically grounded vector scoring mechanism, achieving both higher accuracy and reduced latency. Our approach dynamically balances strict adherence to predefined paths with flexible node jumps to handle various user inputs efficiently. Experiments demonstrate that PAF significantly outperforms baseline methods, paving the way for scalable, real-time Conversational AI systems in complex business environments.", "source": "arxiv", "arxiv_id": "2503.06410v1", "pdf_url": "https://arxiv.org/pdf/2503.06410v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-09T02:58:34Z", "updated": "2025-03-09T02:58:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Persona Alchemy: Designing, Evaluating, and Implementing Psychologically-Grounded LLM Agents for Diverse Stakeholder Representation", "authors": ["Sola Kim", "Dongjune Chang", "Jieshu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2505.18351v1", "abstract": "Despite advances in designing personas for Large Language Models (LLM), challenges remain in aligning them with human cognitive processes and representing diverse stakeholder perspectives. We introduce a Social Cognitive Theory (SCT) agent design framework for designing, evaluating, and implementing psychologically grounded LLMs with consistent behavior. Our framework operationalizes SCT through four personal factors (cognitive, motivational, biological, and affective) for designing, six quantifiable constructs for evaluating, and a graph database-backed architecture for implementing stakeholder personas. Experiments tested agents' responses to contradicting information of varying reliability. In the highly polarized renewable energy transition discourse, we design five diverse agents with distinct ideologies, roles, and stakes to examine stakeholder representation. The evaluation of these agents in contradictory scenarios occurs through comprehensive processes that implement the SCT. Results show consistent response patterns ($R^2$ range: $0.58-0.61$) and systematic temporal development of SCT construct effects. Principal component analysis identifies two dimensions explaining $73$% of variance, validating the theoretical structure. Our framework offers improved explainability and reproducibility compared to black-box approaches. This work contributes to ongoing efforts to improve diverse stakeholder representation while maintaining psychological consistency in LLM personas.", "source": "arxiv", "arxiv_id": "2505.18351v1", "pdf_url": "https://arxiv.org/pdf/2505.18351v1", "categories": ["cs.MA", "cs.CY", "cs.DB"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-05-23T20:18:14Z", "updated": "2025-05-23T20:18:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time", "authors": ["Weizhi Zhang", "Xinyang Zhang", "Chenwei Zhang", "Liangwei Yang", "Jingbo Shang", "Zhepei Wei", "Henry Peng Zou", "Zijie Huang", "Zhengyang Wang", "Yifan Gao", "Xiaoman Pan", "Lian Xiong", "Jingguo Liu", "Philip S. Yu", "Xian Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.06254v1", "abstract": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.", "source": "arxiv", "arxiv_id": "2506.06254v1", "pdf_url": "https://arxiv.org/pdf/2506.06254v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-06T17:29:49Z", "updated": "2025-06-06T17:29:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents", "authors": ["Mikhail Menschikov", "Dmitry Evseev", "Victoria Dochkina", "Ruslan Kostoev", "Ilia Perepechkin", "Petr Anokhin", "Evgeny Burnaev", "Nikita Semenov"], "year": 2025, "url": "http://arxiv.org/abs/2506.17001v3", "abstract": "Personalizing language models that effectively incorporating user interaction history remains a central challenge in development of adaptive AI systems. While large language models (LLMs), combined with Retrieval-Augmented Generation (RAG), have improved factual accuracy, they often lack structured memory and fail to scale in complex, long-term interactions. To address this, we propose a flexible external memory framework based on knowledge graph, which construct and update memory model automatically by LLM itself. Building upon the AriGraph architecture, we introduce a novel hybrid graph design that supports both standard edges and two types of hyper-edges, enabling rich and dynamic semantic and temporal representations. Our framework also supports diverse retrieval mechanisms, including A*, water-circle traversal, beam search and hybrid methods, making it adaptable to different datasets and LLM capacities. We evaluate our system on three benchmarks: TriviaQA, HotpotQA, DiaASQ and demonstrate that different memory and retrieval configurations yield optimal performance depending on the task. Additionally, we extend the DiaASQ benchmark with temporal annotations and internally contradictory statements, showing that our system remains robust and effective in managing temporal dependencies and context-aware reasoning.", "source": "arxiv", "arxiv_id": "2506.17001v3", "pdf_url": "https://arxiv.org/pdf/2506.17001v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-20T13:52:15Z", "updated": "2026-01-20T16:57:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Personalized Attacks of Social Engineering in Multi-turn Conversations: LLM Agents for Simulation and Detection", "authors": ["Tharindu Kumarage", "Cameron Johnson", "Jadie Adams", "Lin Ai", "Matthias Kirchner", "Anthony Hoogs", "Joshua Garland", "Julia Hirschberg", "Arslan Basharat", "Huan Liu"], "year": 2025, "url": "http://arxiv.org/abs/2503.15552v2", "abstract": "The rapid advancement of conversational agents, particularly chatbots powered by Large Language Models (LLMs), poses a significant risk of social engineering (SE) attacks on social media platforms. SE detection in multi-turn, chat-based interactions is considerably more complex than single-instance detection due to the dynamic nature of these conversations. A critical factor in mitigating this threat is understanding the SE attack mechanisms through which SE attacks operate, specifically how attackers exploit vulnerabilities and how victims' personality traits contribute to their susceptibility. In this work, we propose an LLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating multi-turn conversations. We model victim agents with varying personality traits to assess how psychological profiles influence susceptibility to manipulation. Using a dataset of over 1000 simulated conversations, we examine attack scenarios in which adversaries, posing as recruiters, funding agencies, and journalists, attempt to extract sensitive information. Based on this analysis, we present a proof of concept, SE-OmniGuard, to offer personalized protection to users by leveraging prior knowledge of the victims personality, evaluating attack strategies, and monitoring information exchanges in conversations to identify potential SE attempts.", "source": "arxiv", "arxiv_id": "2503.15552v2", "pdf_url": "https://arxiv.org/pdf/2503.15552v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-03-18T19:14:44Z", "updated": "2025-09-08T21:16:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Perspectra: Choosing Your Experts Enhances Critical Thinking in Multi-Agent Research Ideation", "authors": ["Yiren Liu", "Viraj Shah", "Sangho Suh", "Pao Siangliulue", "Tal August", "Yun Huang"], "year": 2025, "url": "http://arxiv.org/abs/2509.20553v1", "abstract": "Recent advances in multi-agent systems (MAS) enable tools for information search and ideation by assigning personas to agents. However, how users can effectively control, steer, and critically evaluate collaboration among multiple domain-expert agents remains underexplored. We present Perspectra, an interactive MAS that visualizes and structures deliberation among LLM agents via a forum-style interface, supporting @-mention to invite targeted agents, threading for parallel exploration, with a real-time mind map for visualizing arguments and rationales. In a within-subjects study with 18 participants, we compared Perspectra to a group-chat baseline as they developed research proposals. Our findings show that Perspectra significantly increased the frequency and depth of critical-thinking behaviors, elicited more interdisciplinary replies, and led to more frequent proposal revisions than the group chat condition. We discuss implications for designing multi-agent tools that scaffold critical thinking by supporting user control over multi-agent adversarial discourse.", "source": "arxiv", "arxiv_id": "2509.20553v1", "pdf_url": "https://arxiv.org/pdf/2509.20553v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-24T20:39:06Z", "updated": "2025-09-24T20:39:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions", "authors": ["Angana Borah", "Rada Mihalcea", "VerÃ³nica PÃ©rez-Rosas"], "year": 2025, "url": "http://arxiv.org/abs/2503.02038v2", "abstract": "Existing challenges in misinformation exposure and susceptibility vary across demographic groups, as some populations are more vulnerable to misinformation than others. Large language models (LLMs) introduce new dimensions to these challenges through their ability to generate persuasive content at scale and reinforcing existing biases. This study investigates the bidirectional persuasion dynamics between LLMs and humans when exposed to misinformative content. We analyze human-to-LLM influence using human-stance datasets and assess LLM-to-human influence by generating LLM-based persuasive arguments. Additionally, we use a multi-agent LLM framework to analyze the spread of misinformation under persuasion among demographic-oriented LLM agents. Our findings show that demographic factors influence susceptibility to misinformation in LLMs, closely reflecting the demographic-based patterns seen in human susceptibility. We also find that, similar to human demographic groups, multi-agent LLMs exhibit echo chamber behavior. This research explores the interplay between humans and LLMs, highlighting demographic differences in the context of misinformation and offering insights for future interventions.", "source": "arxiv", "arxiv_id": "2503.02038v2", "pdf_url": "https://arxiv.org/pdf/2503.02038v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-03T20:30:22Z", "updated": "2025-10-14T13:16:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents", "authors": ["Bowen Gao", "Yanwen Huang", "Yiqiao Liu", "Wenxuan Xie", "Wei-Ying Ma", "Ya-Qin Zhang", "Yanyan Lan"], "year": 2025, "url": "http://arxiv.org/abs/2503.22164v2", "abstract": "The discovery of novel small molecule drugs remains a critical scientific challenge with far-reaching implications for treating diseases and advancing human health. Traditional drug development--especially for small molecule therapeutics--is a highly complex, resource-intensive, and time-consuming process that requires multidisciplinary collaboration. Recent breakthroughs in artificial intelligence (AI), particularly the rise of large language models (LLMs), present a transformative opportunity to streamline and accelerate this process. In this paper, we introduce PharmAgents, a virtual pharmaceutical ecosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates the full drug discovery workflow--from target discovery to preclinical evaluation--by integrating explainable, LLM-driven agents equipped with specialized machine learning models and computational tools. Through structured knowledge exchange and automated optimization, PharmAgents identifies potential therapeutic targets, discovers promising lead compounds, enhances binding affinity and key molecular properties, and performs in silico analyses of toxicity and synthetic feasibility. Additionally, the system supports interpretability, agent interaction, and self-evolvement, enabling it to refine future drug designs based on prior experience. By showcasing the potential of LLM-powered multi-agent systems in drug discovery, this work establishes a new paradigm for autonomous, explainable, and scalable pharmaceutical research, with future extensions toward comprehensive drug lifecycle management.", "source": "arxiv", "arxiv_id": "2503.22164v2", "pdf_url": "https://arxiv.org/pdf/2503.22164v2", "categories": ["q-bio.BM", "cs.AI"], "primary_category": "q-bio.BM", "doi": "", "venue": "", "published": "2025-03-28T06:02:53Z", "updated": "2025-03-31T16:26:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems", "authors": ["Oshayer Siddique", "J. M Areeb Uzair Alam", "Md Jobayer Rahman Rafy", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "year": 2025, "url": "http://arxiv.org/abs/2508.00079v2", "abstract": "The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.", "source": "arxiv", "arxiv_id": "2508.00079v2", "pdf_url": "https://arxiv.org/pdf/2508.00079v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-31T18:12:51Z", "updated": "2025-11-05T07:50:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "authors": ["Keer Lu", "Chong Chen", "Xili Wang", "Bin Cui", "Yunhuai Liu", "Wentao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2508.00344v4", "abstract": "Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.", "source": "arxiv", "arxiv_id": "2508.00344v4", "pdf_url": "https://arxiv.org/pdf/2508.00344v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-01T06:17:11Z", "updated": "2026-01-07T07:33:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant", "authors": ["Gaole He", "Gianluca Demartini", "Ujwal Gadiraju"], "year": 2025, "url": "http://arxiv.org/abs/2502.01390v1", "abstract": "Since the explosion in popularity of ChatGPT, large language models (LLMs) have continued to impact our everyday lives. Equipped with external tools that are designed for a specific purpose (e.g., for flight booking or an alarm clock), LLM agents exercise an increasing capability to assist humans in their daily work. Although LLM agents have shown a promising blueprint as daily assistants, there is a limited understanding of how they can provide daily assistance based on planning and sequential decision making capabilities. We draw inspiration from recent work that has highlighted the value of 'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks. We conducted an empirical study (N = 248) of LLM agents as daily assistants in six commonly occurring tasks with different levels of risk typically associated with them (e.g., flight ticket booking and credit card payments). To ensure user agency and control over the LLM agent, we adopted LLM agents in a plan-then-execute manner, wherein the agents conducted step-wise planning and step-by-step execution in a simulation environment. We analyzed how user involvement at each stage affects their trust and collaborative team performance. Our findings demonstrate that LLM agents can be a double-edged sword -- (1) they can work well when a high-quality plan and necessary user involvement in execution are available, and (2) users can easily mistrust the LLM agents with plans that seem plausible. We synthesized key insights for using LLM agents as daily assistants to calibrate user trust and achieve better overall task outcomes. Our work has important implications for the future design of daily assistants and human-AI collaboration with LLM agents.", "source": "arxiv", "arxiv_id": "2502.01390v1", "pdf_url": "https://arxiv.org/pdf/2502.01390v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "10.1145/3706598.3713218", "venue": "", "published": "2025-02-03T14:23:22Z", "updated": "2025-02-03T14:23:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Plan-over-Graph: Towards Parallelable LLM Agent Schedule", "authors": ["Shiqi Zhang", "Xinbei Ma", "Zouying Cao", "Zhuosheng Zhang", "Hai Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.14563v1", "abstract": "Large Language Models (LLMs) have demonstrated exceptional abilities in reasoning for task planning. However, challenges remain under-explored for parallel schedules. This paper introduces a novel paradigm, plan-over-graph, in which the model first decomposes a real-life textual task into executable subtasks and constructs an abstract task graph. The model then understands this task graph as input and generates a plan for parallel execution. To enhance the planning capability of complex, scalable graphs, we design an automated and controllable pipeline to generate synthetic graphs and propose a two-stage training scheme. Experimental results show that our plan-over-graph method significantly improves task performance on both API-based LLMs and trainable open-sourced LLMs. By normalizing complex tasks as graphs, our method naturally supports parallel execution, demonstrating global efficiency. The code and data are available at https://github.com/zsq259/Plan-over-Graph.", "source": "arxiv", "arxiv_id": "2502.14563v1", "pdf_url": "https://arxiv.org/pdf/2502.14563v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-20T13:47:51Z", "updated": "2025-02-20T13:47:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL", "authors": ["Joey Hong", "Anca Dragan", "Sergey Levine"], "year": 2025, "url": "http://arxiv.org/abs/2505.18098v2", "abstract": "Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability.", "source": "arxiv", "arxiv_id": "2505.18098v2", "pdf_url": "https://arxiv.org/pdf/2505.18098v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T16:51:54Z", "updated": "2025-12-03T08:54:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Please Don't Kill My Vibe: Empowering Agents with Data Flow Control", "authors": ["Charlie Summers", "Haneen Mohammed", "Eugene Wu"], "year": 2025, "url": "http://arxiv.org/abs/2512.05374v1", "abstract": "The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems.", "source": "arxiv", "arxiv_id": "2512.05374v1", "pdf_url": "https://arxiv.org/pdf/2512.05374v1", "categories": ["cs.CR", "cs.AI", "cs.DB"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-05T02:24:27Z", "updated": "2025-12-05T02:24:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents", "authors": ["Kanika Goswami", "Puneet Mathur", "Ryan Rossi", "Franck Dernoncourt"], "year": 2025, "url": "http://arxiv.org/abs/2501.11233v1", "abstract": "Chart visualizations, while essential for data interpretation and communication, are predominantly accessible only as images in PDFs, lacking source data tables and stylistic information. To enable effective editing of charts in PDFs or digital scans, we present PlotEdit, a novel multi-agent framework for natural language-driven end-to-end chart image editing via self-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1) Chart2Table for data table extraction, (2) Chart2Vision for style attribute identification, (3) Chart2Code for retrieving rendering code, (4) Instruction Decomposition Agent for parsing user requests into executable steps, and (5) Multimodal Editing Agent for implementing nuanced chart component modifications - all coordinated through multimodal feedback to maintain visual fidelity. PlotEdit outperforms existing baselines on the ChartCraft dataset across style, layout, format, and data-centric edits, enhancing accessibility for visually challenged users and improving novice productivity.", "source": "arxiv", "arxiv_id": "2501.11233v1", "pdf_url": "https://arxiv.org/pdf/2501.11233v1", "categories": ["cs.IR", "cs.CL", "cs.MA"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-01-20T02:31:52Z", "updated": "2025-01-20T02:31:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents", "authors": ["Wenda Xie", "Chao Guo", "Yanqing Jing. Junle Wang", "Yisheng Lv", "Fei-Yue Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.05188v1", "abstract": "Although LLMs have been widely adopted for creative content generation, a single-pass process often struggles to produce high-quality long narratives. How to effectively revise and improve long narrative scripts like scriptwriters remains a significant challenge, as it demands a comprehensive understanding of the entire context to identify global structural issues and local detailed flaws, as well as coordinating revisions at multiple granularities and locations. Direct modifications by LLMs typically introduce inconsistencies between local edits and the overall narrative requirements. To address these issues, we propose Dramaturge, a task and feature oriented divide-and-conquer approach powered by hierarchical multiple LLM agents. It consists of a Global Review stage to grasp the overall storyline and structural issues, a Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a Hierarchical Coordinated Revision stage that coordinates and integrates structural and detailed improvements throughout the script. The top-down task flow ensures that high-level strategies guide local modifications, maintaining contextual consistency. The review and revision workflow follows a coarse-to-fine iterative process, continuing through multiple rounds until no further substantive improvements can be made. Comprehensive experiments show that Dramaturge significantly outperforms all baselines in terms of script-level overall quality and scene-level details. Our approach is plug-and-play and can be easily integrated into existing methods to improve the generated scripts.", "source": "arxiv", "arxiv_id": "2510.05188v1", "pdf_url": "https://arxiv.org/pdf/2510.05188v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-06T05:20:37Z", "updated": "2025-10-06T05:20:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Pokemon Red via Reinforcement Learning", "authors": ["Marco Pleines", "Daniel Addis", "David Rubinstein", "Frank Zimmer", "Mike Preuss", "Peter Whidden"], "year": 2025, "url": "http://arxiv.org/abs/2502.19920v2", "abstract": "PokÃ©mon Red, a classic Game Boy JRPG, presents significant challenges as a testbed for agents, including multi-tasking, long horizons of tens of thousands of steps, hard exploration, and a vast array of potential policies. We introduce a simplistic environment and a Deep Reinforcement Learning (DRL) training methodology, demonstrating a baseline agent that completes an initial segment of the game up to completing Cerulean City. Our experiments include various ablations that reveal vulnerabilities in reward shaping, where agents exploit specific reward signals. We also discuss limitations and argue that games like PokÃ©mon hold strong potential for future research on Large Language Model agents, hierarchical training algorithms, and advanced exploration methods. Source Code: https://github.com/MarcoMeter/neroRL/tree/poke_red", "source": "arxiv", "arxiv_id": "2502.19920v2", "pdf_url": "https://arxiv.org/pdf/2502.19920v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-27T09:42:23Z", "updated": "2025-03-11T05:44:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PortGPT: Towards Automated Backporting Using Large Language Models", "authors": ["Zhaoyang Li", "Zheng Yu", "Jingyi Song", "Meng Xu", "Yuxuan Luo", "Dongliang Mu"], "year": 2025, "url": "http://arxiv.org/abs/2510.22396v1", "abstract": "Patch backporting, the process of migrating mainline security patches to older branches, is an essential task in maintaining popular open-source projects (e.g., Linux kernel). However, manual backporting can be labor-intensive, while existing automated methods, which heavily rely on predefined syntax or semantic rules, often lack agility for complex patches.\n  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation of patch backporting in real-world scenarios. PORTGPT enhances an LLM with tools to access code on-demand, summarize Git history, and revise patches autonomously based on feedback (e.g., from compilers), hence, simulating human-like reasoning and verification. PORTGPT achieved an 89.15% success rate on existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex cases, both outperforms state-of-the-art of backporting tools. We contributed 9 backported patches from PORTGPT to the Linux kernel community and all patches are now merged.", "source": "arxiv", "arxiv_id": "2510.22396v1", "pdf_url": "https://arxiv.org/pdf/2510.22396v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-25T18:46:04Z", "updated": "2025-10-25T18:46:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents", "authors": ["Mathis Pink", "Qinyuan Wu", "Vy Ai Vo", "Javier Turek", "Jianing Mu", "Alexander Huth", "Mariya Toneva"], "year": 2025, "url": "http://arxiv.org/abs/2502.06975v1", "abstract": "As Large Language Models (LLMs) evolve from text-completion tools into fully fledged agents operating in dynamic environments, they must address the challenge of continually learning and retaining long-term knowledge. Many biological systems solve these challenges with episodic memory, which supports single-shot learning of instance-specific contexts. Inspired by this, we present an episodic memory framework for LLM agents, centered around five key properties of episodic memory that underlie adaptive and context-sensitive behavior. With various research efforts already partially covering these properties, this position paper argues that now is the right time for an explicit, integrated focus on episodic memory to catalyze the development of long-term agents. To this end, we outline a roadmap that unites several research directions under the goal to support all five properties of episodic memory for more efficient long-term LLM agents.", "source": "arxiv", "arxiv_id": "2502.06975v1", "pdf_url": "https://arxiv.org/pdf/2502.06975v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-10T19:14:51Z", "updated": "2025-02-10T19:14:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives", "authors": ["Elliot Meyerson", "Xin Qiu"], "year": 2025, "url": "http://arxiv.org/abs/2502.04358v2", "abstract": "Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.", "source": "arxiv", "arxiv_id": "2502.04358v2", "pdf_url": "https://arxiv.org/pdf/2502.04358v2", "categories": ["cs.CL", "cs.AI", "cs.CC", "cs.LG", "cs.NE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-04T20:47:43Z", "updated": "2025-05-29T16:46:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Position: Stop Acting Like Language Model Agents Are Normal Agents", "authors": ["Elija Perrier", "Michael Timothy Bennett"], "year": 2025, "url": "http://arxiv.org/abs/2502.10420v1", "abstract": "Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.", "source": "arxiv", "arxiv_id": "2502.10420v1", "pdf_url": "https://arxiv.org/pdf/2502.10420v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-04T08:14:18Z", "updated": "2025-02-04T08:14:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Position: Towards a Responsible LLM-empowered Multi-Agent Systems", "authors": ["Jinwei Hu", "Yi Dong", "Shuang Ao", "Zhuoyun Li", "Boxuan Wang", "Lokesh Singh", "Guangliang Cheng", "Sarvapali D. Ramchurn", "Xiaowei Huang"], "year": 2025, "url": "http://arxiv.org/abs/2502.01714v1", "abstract": "The rise of Agent AI and Large Language Model-powered Multi-Agent Systems (LLM-MAS) has underscored the need for responsible and dependable system operation. Tools like LangChain and Retrieval-Augmented Generation have expanded LLM capabilities, enabling deeper integration into MAS through enhanced knowledge retrieval and reasoning. However, these advancements introduce critical challenges: LLM agents exhibit inherent unpredictability, and uncertainties in their outputs can compound across interactions, threatening system stability. To address these risks, a human-centered design approach with active dynamic moderation is essential. Such an approach enhances traditional passive oversight by facilitating coherent inter-agent communication and effective system governance, allowing MAS to achieve desired outcomes more efficiently.", "source": "arxiv", "arxiv_id": "2502.01714v1", "pdf_url": "https://arxiv.org/pdf/2502.01714v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-02-03T16:04:30Z", "updated": "2025-02-03T16:04:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "year": 2025, "url": "http://arxiv.org/abs/2505.22655v1", "abstract": "Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.", "source": "arxiv", "arxiv_id": "2505.22655v1", "pdf_url": "https://arxiv.org/pdf/2505.22655v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-28T17:59:08Z", "updated": "2025-05-28T17:59:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis", "authors": ["Zhonghao Zhan", "Huichi Zhou", "Hamed Haddadi"], "year": 2025, "url": "http://arxiv.org/abs/2506.20806v1", "abstract": "Graph Neural Networks (GNNs) show great promise for Network Intrusion Detection Systems (NIDS), particularly in IoT environments, but suffer performance degradation due to distribution drift and lack robustness against realistic adversarial attacks. Current robustness evaluations often rely on unrealistic synthetic perturbations and lack demonstrations on systematic analysis of different kinds of adversarial attack, which encompass both black-box and white-box scenarios. This work proposes a novel approach to enhance GNN robustness and generalization by employing Large Language Models (LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These agents scrutinize graph structures derived from network flow data, identifying and potentially mitigating suspicious or adversarially perturbed elements before GNN processing. Our experiments, using a framework designed for realistic evaluation and testing with a variety of adversarial attacks including a dataset collected from physical testbed experiments, demonstrate that integrating LLM analysis can significantly improve the resilience of GNN-based NIDS against challenges, showcasing the potential of LLM agent as a complementary layer in intrusion detection architectures.", "source": "arxiv", "arxiv_id": "2506.20806v1", "pdf_url": "https://arxiv.org/pdf/2506.20806v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-25T19:49:55Z", "updated": "2025-06-25T19:49:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents", "authors": ["Mrinal Rawat", "Ambuje Gupta", "Rushil Goomer", "Alessandro Di Bari", "Neha Gupta", "Roberto Pieraccini"], "year": 2025, "url": "http://arxiv.org/abs/2505.09970v2", "abstract": "The ReAct (Reasoning + Action) capability in large language models (LLMs) has become the foundation of modern agentic systems. Recent LLMs, such as DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through the generation of ample intermediate tokens, which help build a strong premise before producing the final output tokens. In this paper, we introduce Pre-Act, a novel approach that enhances the agent's performance by creating a multi-step execution plan along with the detailed reasoning for the given user input. This plan incrementally incorporates previous steps and tool outputs, refining itself after each step execution until the final response is obtained. Our approach is applicable to both conversational and non-conversational agents. To measure the performance of task-oriented agents comprehensively, we propose a two-level evaluation framework: (1) turn level and (2) end-to-end. Our turn-level evaluation, averaged across five models, shows that our approach, Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While this approach is effective for larger models, smaller models crucial for practical applications, where latency and cost are key constraints, often struggle with complex reasoning tasks required for agentic systems. To address this limitation, we fine-tune relatively small models such as Llama 3.1 (8B & 70B) using the proposed Pre-Act approach. Our experiments show that the fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action accuracy (turn-level) and a 28% improvement in goal completion rate (end-to-end) on the Almita (out-of-domain) dataset.", "source": "arxiv", "arxiv_id": "2505.09970v2", "pdf_url": "https://arxiv.org/pdf/2505.09970v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-15T05:17:47Z", "updated": "2025-05-19T03:17:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets", "authors": ["Yannis Belkhiter", "Seshu Tirupathi", "Giulio Zizzo", "Sachin Sharma", "John D. Kelleher"], "year": 2025, "url": "http://arxiv.org/abs/2510.01842v1", "abstract": "The field of AutoML has made remarkable progress in post-hoc model selection, with libraries capable of automatically identifying the most performing models for a given dataset. Nevertheless, these methods often rely on exhaustive hyperparameter searches, where methods automatically train and test different types of models on the target dataset. Contrastingly, pre-hoc prediction emerges as a promising alternative, capable of bypassing exhaustive search through intelligent pre-selection of models. Despite its potential, pre-hoc prediction remains under-explored in the literature. This paper explores the intersection of AutoML and pre-hoc model selection by leveraging traditional models and Large Language Model (LLM) agents to reduce the search space of AutoML libraries. By relying on dataset descriptions and statistical information, we reduce the AutoML search space. Our methodology is applied to the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark containing 175 tabular classification datasets available on OpenML. The proposed approach offers a shift in AutoML workflows, significantly reducing computational overhead, while still selecting the best model for the given dataset.", "source": "arxiv", "arxiv_id": "2510.01842v1", "pdf_url": "https://arxiv.org/pdf/2510.01842v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-02T09:37:12Z", "updated": "2025-10-02T09:37:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Preference-Aware Memory Update for Long-Term LLM Agents", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "year": 2025, "url": "http://arxiv.org/abs/2510.09720v1", "abstract": "One of the key factors influencing the reasoning capabilities of LLM-based agents is their ability to leverage long-term memory. Integrating long-term memory mechanisms allows agents to make informed decisions grounded in historical interactions. While recent advances have significantly improved the storage and retrieval components, by encoding memory into dense vectors for similarity search or organizing memory as structured knowledge graphs most existing approaches fall short in memory updating. In particular, they lack mechanisms for dynamically refining preference memory representations in response to evolving user behaviors and contexts. To address this gap, we propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic and personalized memory refinement. By integrating sliding window averages (SW) with exponential moving averages (EMA), PAMU constructs a fused preference-aware representation that captures both short-term fluctuations and long-term user tendencies. We conduct experiments on five task scenarios of the LoCoMo dataset, and the results show that our mechanism can significantly improve the output quality of LLM in five baselines, validating its effectiveness in long-term conversations.", "source": "arxiv", "arxiv_id": "2510.09720v1", "pdf_url": "https://arxiv.org/pdf/2510.09720v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-10T06:49:35Z", "updated": "2025-10-10T06:49:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation for LLM-Powered Agents", "authors": ["Shouju Wang", "Fenglin Yu", "Xirui Liu", "Xiaoting Qin", "Jue Zhang", "Qingwei Lin", "Dongmei Zhang", "Saravan Rajmohan"], "year": 2025, "url": "http://arxiv.org/abs/2509.17488v1", "abstract": "The increasing autonomy of LLM agents in handling sensitive communications, accelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A) frameworks, creates urgent privacy challenges. While recent work reveals significant gaps between LLMs' privacy Q&A performance and their agent behavior, existing benchmarks remain limited to static, simplified scenarios. We present PrivacyChecker, a model-agnostic, contextual integrity based mitigation approach that effectively reduces privacy leakage from 36.08% to 7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving task helpfulness. We also introduce PrivacyLens-Live, transforming static benchmarks into dynamic MCP and A2A environments that reveal substantially higher privacy risks in practical. Our modular mitigation approach integrates seamlessly into agent protocols through three deployment strategies, providing practical privacy protection for the emerging agentic ecosystem. Our data and code will be made available at https://aka.ms/privacy_in_action.", "source": "arxiv", "arxiv_id": "2509.17488v1", "pdf_url": "https://arxiv.org/pdf/2509.17488v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-22T08:19:06Z", "updated": "2025-09-22T08:19:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun", "Jiali Wei"], "year": 2025, "url": "http://arxiv.org/abs/2508.00500v2", "abstract": "Large Language Model (LLM) agents demonstrate strong autonomy, but their stochastic behavior introduces unpredictable safety risks. Existing rule-based enforcement systems, such as AgentSpec, are reactive, intervening only when unsafe behavior is imminent or has occurred, lacking foresight for long-horizon dependencies. To overcome these limitations, we present a proactive runtime enforcement framework for LLM agents. The framework abstracts agent behaviors into symbolic states and learns a Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it predicts the probability of leading to undesired behaviors and intervenes before violations occur when the estimated risk exceeds a user-defined threshold. Designed to provide PAC-correctness guarantee, the framework achieves statistically reliable enforcement of agent safety. We evaluate the framework across two safety-critical domains: autonomous vehicles and embodied agents. It proactively enforces safety and maintains high task performance, outperforming existing methods.", "source": "arxiv", "arxiv_id": "2508.00500v2", "pdf_url": "https://arxiv.org/pdf/2508.00500v2", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-01T10:24:47Z", "updated": "2026-01-06T03:51:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Yunqi Guo", "Siyang Jiang", "Wenrui Lu", "Kaiwei Liu", "Hancheng Xiang", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "year": 2025, "url": "http://arxiv.org/abs/2512.06721v1", "abstract": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.", "source": "arxiv", "arxiv_id": "2512.06721v1", "pdf_url": "https://arxiv.org/pdf/2512.06721v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-07T08:21:07Z", "updated": "2025-12-07T08:21:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents", "authors": ["Feifan Xia", "Yuyang Fang", "Defang Li", "Yantong Xie", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"], "year": 2025, "url": "http://arxiv.org/abs/2510.18476v1", "abstract": "We present a probabilistic intent modeling framework for large language model (LLM) agents in multi-turn social dialogue. The framework maintains a belief distribution over a partner's latent intentions, initialized from contextual priors and dynamically updated through likelihood estimation after each utterance. The evolving distribution provides additional contextual grounding for the policy, enabling adaptive dialogue strategies under uncertainty. Preliminary experiments in the SOTOPIA environment show consistent improvements: the proposed framework increases the Overall score by 9.0% on SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and slightly surpasses an oracle agent that directly observes partner intentions. These early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent LLM agents.", "source": "arxiv", "arxiv_id": "2510.18476v1", "pdf_url": "https://arxiv.org/pdf/2510.18476v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-21T09:54:44Z", "updated": "2025-10-21T09:54:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Procedural Environment Generation for Tool-Use Agents", "authors": ["Michael Sullivan", "Mareike Hartmann", "Alexander Koller"], "year": 2025, "url": "http://arxiv.org/abs/2506.11045v2", "abstract": "Although the power of LLM tool-use agents has ignited a flurry of recent research in this area, the curation of tool-use training data remains an open problem$-$especially for online RL training. Existing approaches to synthetic tool-use data generation tend to be non-interactive, and/or non-compositional. We introduce RandomWorld, a pipeline for the procedural generation of interactive tools and compositional tool-use data. We show that models tuned via SFT and RL on synthetic RandomWorld data improve on a range of tool-use benchmarks, and set the new SoTA for two metrics on the NESTFUL dataset. Further experiments show that downstream performance scales with the amount of RandomWorld-generated training data, opening up the possibility of further improvement through the use of entirely synthetic data.", "source": "arxiv", "arxiv_id": "2506.11045v2", "pdf_url": "https://arxiv.org/pdf/2506.11045v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-21T14:10:06Z", "updated": "2025-09-24T14:57:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Process Reward Models for LLM Agents: Practical Framework and Directions", "authors": ["Sanjiban Choudhury"], "year": 2025, "url": "http://arxiv.org/abs/2502.10325v1", "abstract": "We introduce Agent Process Reward Models (AgentPRM), a simple and scalable framework for training LLM agents to continually improve through interactions. AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo rollouts to compute reward targets and optimize policies. It requires minimal modifications to existing RLHF pipelines, making it easy to integrate at scale. Beyond AgentPRM, we propose InversePRM, which learns process rewards directly from demonstrations without explicit outcome supervision. We also explore key challenges and opportunities, including exploration, process reward shaping, and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o baselines, and analyze test-time scaling, reward hacking, and more. Our code is available at: https://github.com/sanjibanc/agent_prm.", "source": "arxiv", "arxiv_id": "2502.10325v1", "pdf_url": "https://arxiv.org/pdf/2502.10325v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-14T17:34:28Z", "updated": "2025-02-14T17:34:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents", "authors": ["Weiting Tan", "Xinghua Qu", "Ming Tu", "Meng Ge", "Andy T. Liu", "Philipp Koehn", "Lu Lu"], "year": 2025, "url": "http://arxiv.org/abs/2509.14480v1", "abstract": "Effective interactive tool use requires agents to master Tool Integrated Reasoning (TIR): a complex process involving multi-turn planning and long-context dialogue management. To train agents for this dynamic process, particularly in multi-modal contexts, we introduce a sandbox environment for reinforcement learning (RL) that supports interleaved speech-text rollouts. Our core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses the challenge of credit assignment in long-horizon tasks by employing a Large Language Model (LLM) as a judge to provide turn-level evaluation. To enhance exploration, we integrate a mixed-task training curriculum with mathematical reasoning problems. This unified approach boosts the task pass rate on the text-based $Ï$-bench by over 6% compared to strong RL baselines. Crucially, we demonstrate our framework's suitability for fine-tuning a multi-modal foundation model for agentic tasks. By training a base multi-modal LLM on interleaved speech-text rollouts, we equip it with tool-use abilities, paving the way for more natural, voice-driven interactive agents.", "source": "arxiv", "arxiv_id": "2509.14480v1", "pdf_url": "https://arxiv.org/pdf/2509.14480v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-17T23:25:00Z", "updated": "2025-09-17T23:25:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Progent: Programmable Privilege Control for LLM Agents", "authors": ["Tianneng Shi", "Jingxuan He", "Zhun Wang", "Hongwei Li", "Linyu Wu", "Wenbo Guo", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2504.11703v2", "abstract": "LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments. Attackers can exploit these agents through various vectors, including indirect prompt injection, memory/knowledge base poisoning, and malicious tools, tricking agents into performing dangerous actions such as unauthorized financial transactions or data leakage. The core problem that enables attacks to succeed lies in over-privileged tool access. We introduce Progent, the first privilege control framework to secure LLM agents. Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones. Progent features a domain-specific language that allows for expressing fine-grained policies for controlling tool privileges, flexible fallback actions when calls are blocked, and dynamic policy updates to adapt to changing agent states. The framework operates deterministically at runtime, providing provable security guarantees. Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Additionally, we show that LLMs can automatically generate effective policies, highlighting their potential for automating the process of writing Progent's security policies.", "source": "arxiv", "arxiv_id": "2504.11703v2", "pdf_url": "https://arxiv.org/pdf/2504.11703v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-16T01:58:40Z", "updated": "2025-08-30T06:42:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation", "authors": ["Kaiyuan Liu", "Youcheng Pan", "Yang Xiang", "Daojing He", "Jing Li", "Yexing Du", "Tianrun Gao"], "year": 2025, "url": "http://arxiv.org/abs/2503.07010v2", "abstract": "Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users' perspective, and also lack the explainability of the results of LLM agents' code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation's automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.", "source": "arxiv", "arxiv_id": "2503.07010v2", "pdf_url": "https://arxiv.org/pdf/2503.07010v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-10T07:47:27Z", "updated": "2025-05-31T12:53:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents", "authors": ["Juhee Kim", "Woohyuk Choi", "Byoungyoung Lee"], "year": 2025, "url": "http://arxiv.org/abs/2503.15547v2", "abstract": "Large Language Models (LLMs) are combined with tools to create powerful LLM agents that provide a wide range of services. Unlike traditional software, LLM agent's behavior is determined at runtime by natural language prompts from either user or tool's data. This flexibility enables a new computing paradigm with unlimited capabilities and programmability, but also introduces new security risks, vulnerable to privilege escalation attacks. Moreover, user prompts are prone to be interpreted in an insecure way by LLM agents, creating non-deterministic behaviors that can be exploited by attackers. To address these security risks, we propose Prompt Flow Integrity (PFI), a system security-oriented solution to prevent privilege escalation in LLM agents. Analyzing the architectural characteristics of LLM agents, PFI features three mitigation techniques -- i.e., agent isolation, secure untrusted data processing, and privilege escalation guardrails. Our evaluation result shows that PFI effectively mitigates privilege escalation attacks while successfully preserving the utility of LLM agents.", "source": "arxiv", "arxiv_id": "2503.15547v2", "pdf_url": "https://arxiv.org/pdf/2503.15547v2", "categories": ["cs.CR", "cs.AI", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-03-17T05:27:57Z", "updated": "2025-04-21T02:10:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Prompt Injection Attack to Tool Selection in LLM Agents", "authors": ["Jiawen Shi", "Zenghui Yuan", "Guiyao Tie", "Pan Zhou", "Neil Zhenqiang Gong", "Lichao Sun"], "year": 2025, "url": "http://arxiv.org/abs/2504.19793v3", "abstract": "Tool selection is a key component of LLM agents. A popular approach follows a two-step process - \\emph{retrieval} and \\emph{selection} - to pick the most appropriate tool from a tool library for a given task. In this work, we introduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool selection in no-box scenarios. ToolHijacker injects a malicious tool document into the tool library to manipulate the LLM agent's tool selection process, compelling it to consistently choose the attacker's malicious tool for an attacker-chosen target task. Specifically, we formulate the crafting of such tool documents as an optimization problem and propose a two-phase optimization strategy to solve it. Our extensive experimental evaluation shows that ToolHijacker is highly effective, significantly outperforming existing manual-based and automated prompt injection attacks when applied to tool selection. Moreover, we explore various defenses, including prevention-based defenses (StruQ and SecAlign) and detection-based defenses (known-answer detection, DataSentinel, perplexity detection, and perplexity windowed detection). Our experimental results indicate that these defenses are insufficient, highlighting the urgent need for developing new defense strategies.", "source": "arxiv", "arxiv_id": "2504.19793v3", "pdf_url": "https://arxiv.org/pdf/2504.19793v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-28T13:36:43Z", "updated": "2025-08-24T03:28:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations", "authors": ["Manh Hung Nguyen", "Sebastian Tschiatschek", "Adish Singla"], "year": 2025, "url": "http://arxiv.org/abs/2510.07064v1", "abstract": "The difficulty and expense of obtaining large-scale human responses make Large Language Models (LLMs) an attractive alternative and a promising proxy for human behavior. However, prior work shows that LLMs often produce homogeneous outputs that fail to capture the rich diversity of human perspectives and behaviors. Thus, rather than trying to capture this diversity with a single LLM agent, we propose a novel framework to construct a set of agents that collectively capture the diversity of a given human population. Each agent is an LLM whose behavior is steered by conditioning on a small set of human demonstrations (task-response pairs) through in-context learning. The central challenge is therefore to select a representative set of LLM agents from the exponentially large space of possible agents. We tackle this selection problem from the lens of submodular optimization. In particular, we develop methods that offer different trade-offs regarding time complexity and performance guarantees. Extensive experiments in crowdsourcing and educational domains demonstrate that our approach constructs agents that more effectively represent human populations compared to baselines. Moreover, behavioral analyses on new tasks show that these agents reproduce the behavior patterns and perspectives of the students and annotators they are designed to represent.", "source": "arxiv", "arxiv_id": "2510.07064v1", "pdf_url": "https://arxiv.org/pdf/2510.07064v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-08T14:28:53Z", "updated": "2025-10-08T14:28:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Prompt template for a fictitious LLM agent in a content-flagging experiment", "authors": ["Marie-Therese Sekwenz", "Daria Simons", "Alina Wundsam"], "year": 2025, "url": "http://arxiv.org/abs/2507.21842v1", "abstract": "Digital regulations such as the European Union's Digital Services Act (DSA) represent major efforts to shape human-centered and human rights-based frameworks for society. Yet, as these laws are translated into practice, challenges emerge at the intersection of technology, law, and design. This paper presents a qualitative case study examining how designers act as mediators between abstract legal requirements and real-world digital experiences for users, focusing on the design of content reporting mechanisms under Article 16 of the DSA.\n  Through an expert workshop with professional designers from diverse fields (N=9), we explore how legal obligations are interpreted by designers and reflected in discussions and design solutions. Our findings resonate with previous research on the design of reporting mechanisms and dark patterns, highlighting how UX design choices can mislead or hinder users' decision-making and therefore also highlighting the crucial role of design decisions.\n  We show how participatory design methods can bridge disciplinary divides, making legal obligations accessible in compliance fostering design solutions.\n  By using legal design as a lens, we argue that the co-creation of digital regulations and user experience is a core site for digital humanism; where designers, engineers, and legal scholars must collaborate to ensure that systems uphold legal standards to address the challenge the regulation poses to these disciplines.", "source": "arxiv", "arxiv_id": "2507.21842v1", "pdf_url": "https://arxiv.org/pdf/2507.21842v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-07-29T14:21:00Z", "updated": "2025-07-29T14:21:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PromptArmor: Simple yet Effective Prompt Injection Defenses", "authors": ["Tianneng Shi", "Kaijie Zhu", "Zhun Wang", "Yuqi Jia", "Will Cai", "Weida Liang", "Haonan Wang", "Hend Alzahrani", "Joshua Lu", "Kenji Kawaguchi", "Basel Alomair", "Xuandong Zhao", "William Yang Wang", "Neil Gong", "Wenbo Guo", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2507.15219v1", "abstract": "Despite their potential, recent research has demonstrated that LLM agents are vulnerable to prompt injection attacks, where malicious prompts are injected into the agent's input, causing it to perform an attacker-specified task rather than the intended task provided by the user. In this paper, we present PromptArmor, a simple yet effective defense against prompt injection attacks. Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove potential injected prompts from the input before the agent processes it. Our results show that PromptArmor can accurately identify and remove injected prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves both a false positive rate and a false negative rate below 1% on the AgentDojo benchmark. Moreover, after removing injected prompts with PromptArmor, the attack success rate drops to below 1%. We also demonstrate PromptArmor's effectiveness against adaptive attacks and explore different strategies for prompting an LLM. We recommend that PromptArmor be adopted as a standard baseline for evaluating new defenses against prompt injection attacks.", "source": "arxiv", "arxiv_id": "2507.15219v1", "pdf_url": "https://arxiv.org/pdf/2507.15219v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-21T03:41:44Z", "updated": "2025-07-21T03:41:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction", "authors": ["Ross Gore", "Eranga Bandara", "Sachin Shetty", "Alberto E. Musto", "Pratip Rana", "Ambrosio Valencia-Romero", "Christopher Rhea", "Lobat Tayebi", "Heather Richter", "Atmaram Yarlagadda", "Donna Edmonds", "Steven Wallace", "Donna Broshek"], "year": 2025, "url": "http://arxiv.org/abs/2504.18671v1", "abstract": "Mild Traumatic Brain Injury (TBI) detection presents significant challenges due to the subtle and often ambiguous presentation of symptoms in medical imaging, making accurate diagnosis a complex task. To address these challenges, we propose Proof-of-TBI, a medical diagnosis support system that integrates multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large language model (LLM). Our approach fine-tunes multiple vision-language models using a labeled dataset of TBI MRI scans, training them to diagnose TBI symptoms effectively. The predictions from these models are aggregated through a consensus-based decision-making process. The system evaluates the predictions from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a model that has demonstrated remarkable reasoning performance, to produce the most accurate final diagnosis. The LLM Agents orchestrates interactions between the vision-language models and the reasoning LLM, managing the final decision-making process with transparency, reliability, and automation. This end-to-end decision-making workflow combines the vision-language model consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt engineering by the LLM agents. The prototype for the proposed platform was developed in collaboration with the U.S. Army Medical Research team in Newport News, Virginia, incorporating five fine-tuned vision-language models. The results demonstrate the transformative potential of combining fine-tuned vision-language model inputs with the OpenAI-o3 reasoning LLM to create a robust, secure, and highly accurate diagnostic system for mild TBI prediction. To the best of our knowledge, this research represents the first application of fine-tuned vision-language models integrated with a reasoning LLM for TBI prediction tasks.", "source": "arxiv", "arxiv_id": "2504.18671v1", "pdf_url": "https://arxiv.org/pdf/2504.18671v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-25T19:49:30Z", "updated": "2025-04-25T19:49:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Provably Learning from Language Feedback", "authors": ["Wanqiao Xu", "Allen Nie", "Ruijie Zheng", "Aditya Modi", "Adith Swaminathan", "Ching-An Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2506.10341v1", "abstract": "Interactively learning from observation and language feedback is an increasingly studied area driven by the emergence of large language model (LLM) agents. While impressive empirical demonstrations have been shown, so far a principled framing of these decision problems remains lacking. In this paper, we formalize the Learning from Language Feedback (LLF) problem, assert sufficient assumptions to enable learning despite latent rewards, and introduce $\\textit{transfer eluder dimension}$ as a complexity measure to characterize the hardness of LLF problems. We show that transfer eluder dimension captures the intuition that information in the feedback changes the learning complexity of the LLF problem. We demonstrate cases where learning from rich language feedback can be exponentially faster than learning from reward. We develop a no-regret algorithm, called $\\texttt{HELiX}$, that provably solves LLF problems through sequential interactions, with performance guarantees that scale with the transfer eluder dimension of the problem. Across several empirical domains, we show that $\\texttt{HELiX}$ performs well even when repeatedly prompting LLMs does not work reliably. Our contributions mark a first step towards designing principled interactive learning algorithms from generic language feedback.", "source": "arxiv", "arxiv_id": "2506.10341v1", "pdf_url": "https://arxiv.org/pdf/2506.10341v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-12T04:35:02Z", "updated": "2025-06-12T04:35:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PsyLite Technical Report", "authors": ["Fangjun Ding", "Renyu Zhang", "Xinyu Feng", "Chengye Xie", "Zheng Zhang", "Yanting Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.21536v1", "abstract": "With the rapid development of digital technology, AI-driven psychological counseling has gradually become an important research direction in the field of mental health. However, existing models still have deficiencies in dialogue safety, detailed scenario handling, and lightweight deployment. To address these issues, this study proposes PsyLite, a lightweight psychological counseling large language model agent developed based on the base model InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation data fine-tuning and ORPO preference optimization), PsyLite enhances the model's deep-reasoning ability, psychological counseling ability, and safe dialogue ability. After deployment using Ollama and Open WebUI, a custom workflow is created with Pipelines. An innovative conditional RAG is designed to introduce crosstalk humor elements at appropriate times during psychological counseling to enhance user experience and decline dangerous requests to strengthen dialogue safety. Evaluations show that PsyLite outperforms the baseline models in the Chinese general evaluation (CEval), psychological counseling professional evaluation (CPsyCounE), and dialogue safety evaluation (SafeDialBench), particularly in psychological counseling professionalism (CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score improvement of 2.4\\%). Additionally, the model uses quantization technology (GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient for operation), providing a feasible solution for psychological counseling applications in resource-constrained environments.", "source": "arxiv", "arxiv_id": "2506.21536v1", "pdf_url": "https://arxiv.org/pdf/2506.21536v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-26T17:54:42Z", "updated": "2025-06-26T17:54:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PsyPlay: Personality-Infused Role-Playing Conversational Agents", "authors": ["Tao Yang", "Yuhua Zhu", "Xiaojun Quan", "Cong Liu", "Qifan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.03821v1", "abstract": "The current research on Role-Playing Conversational Agents (RPCAs) with Large Language Models (LLMs) primarily focuses on imitating specific speaking styles and utilizing character backgrounds, neglecting the depiction of deeper personality traits.~In this study, we introduce personality-infused role-playing for LLM agents, which encourages agents to accurately portray their designated personality traits during dialogues. We then propose PsyPlay, a dialogue generation framework that facilitates the expression of rich personalities among multiple LLM agents. Specifically, PsyPlay enables agents to assume roles with distinct personality traits and engage in discussions centered around specific topics, consistently exhibiting their designated personality traits throughout the interactions. Validation on generated dialogue data demonstrates that PsyPlay can accurately portray the intended personality traits, achieving an overall success rate of 80.31% on GPT-3.5. Notably, we observe that LLMs aligned with positive values are more successful in portraying positive personality roles compared to negative ones. Moreover, we construct a dialogue corpus for personality-infused role-playing, called PsyPlay-Bench. The corpus, which consists of 4745 instances of correctly portrayed dialogues using PsyPlay, aims to further facilitate research in personalized role-playing and dialogue personality detection.", "source": "arxiv", "arxiv_id": "2502.03821v1", "pdf_url": "https://arxiv.org/pdf/2502.03821v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-06T07:17:12Z", "updated": "2025-02-06T07:17:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Psychologically Enhanced AI Agents", "authors": ["Maciej Besta", "Shriram Chandran", "Robert Gerstenberger", "Mathis Lindner", "Marcin Chrapek", "Sebastian Hermann Martschat", "Taraneh Ghandi", "Patrick Iff", "Hubert Niewiadomski", "Piotr Nyczyk", "JÃ¼rgen MÃ¼ller", "Torsten Hoefler"], "year": 2025, "url": "http://arxiv.org/abs/2509.04343v1", "abstract": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of Large Language Model (LLM) agents through psychologically grounded personality conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology, cognition and affect. We show that such personality priming yields consistent, interpretable behavioral biases across diverse tasks: emotionally expressive agents excel in narrative generation, while analytically primed agents adopt more stable strategies in game-theoretic settings. Our framework supports experimenting with structured multi-agent communication protocols and reveals that self-reflection prior to interaction improves cooperation and reasoning quality. To ensure trait persistence, we integrate the official 16Personalities test for automated verification. While our focus is on MBTI, we show that our approach generalizes seamlessly to other psychological frameworks such as Big Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior design, we establish a foundation for psychologically enhanced AI agents without any fine-tuning.", "source": "arxiv", "arxiv_id": "2509.04343v1", "pdf_url": "https://arxiv.org/pdf/2509.04343v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-04T16:03:03Z", "updated": "2025-09-04T16:03:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events", "authors": ["Mengzhu Liu", "Zhengqiu Zhu", "Chuan Ai", "Chen Gao", "Xinghong Li", "Lingnan He", "Kaisheng Lai", "Yingfeng Chen", "Xin Lu", "Yong Li", "Quanjun Yin"], "year": 2025, "url": "http://arxiv.org/abs/2505.16455v1", "abstract": "During sudden disaster events, accurately predicting public panic sentiment on social media is crucial for proactive governance and crisis management. Current efforts on this problem face three main challenges: lack of finely annotated data hinders emotion prediction studies, unmodeled risk perception causes prediction inaccuracies, and insufficient interpretability of panic formation mechanisms. We address these issues by proposing a Psychology-driven generative Agent framework (PsychoAgent) for explainable panic prediction based on emotion arousal theory. Specifically, we first construct a fine-grained open panic emotion dataset (namely COPE) via human-large language models (LLMs) collaboration to mitigate semantic bias. Then, we develop a framework integrating cross-domain heterogeneous data grounded in psychological mechanisms to model risk perception and cognitive differences in emotion generation. To enhance interpretability, we design an LLM-based role-playing agent that simulates individual psychological chains through dedicatedly designed prompts. Experimental results on our annotated dataset show that PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7% compared to baseline models. Furthermore, the explainability and generalization of our approach is validated. Crucially, this represents a paradigm shift from opaque \"data-driven fitting\" to transparent \"role-based simulation with mechanistic interpretation\" for panic emotion prediction during emergencies. Our implementation is publicly available at: https://anonymous.4open.science/r/PsychoAgent-19DD.", "source": "arxiv", "arxiv_id": "2505.16455v1", "pdf_url": "https://arxiv.org/pdf/2505.16455v1", "categories": ["cs.AI", "cs.CY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-22T09:39:39Z", "updated": "2025-05-22T09:39:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression", "authors": ["Boris Kriuk", "Logic Ng"], "year": 2025, "url": "http://arxiv.org/abs/2512.17914v1", "abstract": "Multi-agent Large Language Model (LLM) systems face a critical bottleneck: redundant transmission of contextual information between agents consumes excessive bandwidth and computational resources. Traditional approaches discard internal semantic representations and transmit raw text, forcing receiving agents to recompute similar representations from scratch. We introduce Q-KVComm, a new protocol that enables direct transmission of compressed key-value (KV) cache representations between LLM agents. Q-KVComm combines three key innovations: (1) adaptive layer-wise quantization that allocates variable bit-widths based on sensitivity profiling, (2) hybrid information extraction that preserves critical facts across content domains, and (3) heterogeneous model calibration establishing cross-architecture communication. Extensive experiments across three diverse question-answering datasets demonstrate that Q-KVComm achieves 5-6x compression ratios while maintaining semantic fidelity, with coherence quality scores above 0.77 across all scenarios. The protocol exhibits robust performance across model sizes (1.1B-1.5B parameters) and adapts to real-world applications including conversational QA and multi-hop reasoning. Our work establishes a new paradigm for LLM agent communication, shifting from text-based to representation-based information exchange.", "source": "arxiv", "arxiv_id": "2512.17914v1", "pdf_url": "https://arxiv.org/pdf/2512.17914v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-27T10:45:41Z", "updated": "2025-11-27T10:45:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "QMBench: A Research Level Benchmark for Quantum Materials Research", "authors": ["Yanzhen Wang", "Yiyang Jiang", "Diana Golovanova", "Kamal Das", "Hyeonhu Bae", "Yufei Zhao", "Huu-Thong Le", "Abhinava Chatterjee", "Yunzhe Liu", "Chao-Xing Liu", "Felipe H. da Jornada", "Binghai Yan", "Xiao-Liang Qi"], "year": 2025, "url": "http://arxiv.org/abs/2512.19753v1", "abstract": "We introduce QMBench, a comprehensive benchmark designed to evaluate the capability of large language model agents in quantum materials research. This specialized benchmark assesses the model's ability to apply condensed matter physics knowledge and computational techniques such as density functional theory to solve research problems in quantum materials science. QMBench encompasses different domains of the quantum material research, including structural properties, electronic properties, thermodynamic and other properties, symmetry principle and computational methodologies. By providing a standardized evaluation framework, QMBench aims to accelerate the development of an AI scientist capable of making creative contributions to quantum materials research. We expect QMBench to be developed and constantly improved by the research community.", "source": "arxiv", "arxiv_id": "2512.19753v1", "pdf_url": "https://arxiv.org/pdf/2512.19753v1", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "primary_category": "cond-mat.mtrl-sci", "doi": "", "venue": "", "published": "2025-12-19T00:57:43Z", "updated": "2025-12-19T00:57:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents", "authors": ["Yuyang Song", "Hanxu Yan", "Jiale Lao", "Yibo Wang", "Yufei Li", "Yuanchun Zhou", "Jianguo Wang", "Mingjie Tang"], "year": 2025, "url": "http://arxiv.org/abs/2506.07675v3", "abstract": "Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.", "source": "arxiv", "arxiv_id": "2506.07675v3", "pdf_url": "https://arxiv.org/pdf/2506.07675v3", "categories": ["cs.DB", "cs.AI"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-06-09T11:51:27Z", "updated": "2026-01-02T16:51:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "QualityFlow: An Agentic Workflow for Program Synthesis Controlled by LLM Quality Checks", "authors": ["Yaojie Hu", "Qiang Zhou", "Qihong Chen", "Xiaopeng Li", "Linbo Liu", "Dejiao Zhang", "Amit Kachroo", "Talha Oz", "Omer Tripp"], "year": 2025, "url": "http://arxiv.org/abs/2501.17167v2", "abstract": "We introduce QualityFlow, a dynamic agentic workflow for program synthesis. Given the English description of a programming problem and a set of unit tests, the model's goal is to synthesize the correct program that solves the problem and passes the tests. QualityFlow includes large language model (LLM) agents resembling a software development team, including code generation, testing, and self-debugging. We propose the LLM Quality Checker, which explicitly \"imagines\" whether the synthesized programs' execution would conform to the unit tests. The Quality Checks dynamically control the workflow, including actions to submit the final answer, clarify the problem statement, and revert previous workflow steps. Our experiments show that the Quality Checker can precisely accept any correct program, mitigate faulty synthesized tests, and prevent potential workflow deviation. QualityFlow establishes the state-of-the-art results on four program synthesis benchmarks: MBPP, HumanEval, and stricter evaluations from MBPP-EvalPlus and HumanEval-EvalPlus.", "source": "arxiv", "arxiv_id": "2501.17167v2", "pdf_url": "https://arxiv.org/pdf/2501.17167v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-01-20T21:47:06Z", "updated": "2025-03-24T19:10:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "R&D-Agent: An LLM-Agent Framework Towards Autonomous Data Science", "authors": ["Xu Yang", "Xiao Yang", "Shikai Fang", "Yifei Zhang", "Jian Wang", "Bowen Xian", "Qizheng Li", "Jingyuan Li", "Minrui Xu", "Yuante Li", "Haoran Pan", "Yuge Zhang", "Weiqing Liu", "Yelong Shen", "Weizhu Chen", "Jiang Bian"], "year": 2025, "url": "http://arxiv.org/abs/2505.14738v2", "abstract": "Recent advances in AI and ML have transformed data science, yet increasing complexity and expertise requirements continue to hinder progress. Although crowd-sourcing platforms alleviate some challenges, high-level machine learning engineering (MLE) tasks remain labor-intensive and iterative. We introduce R&D-Agent, a comprehensive, decoupled, and extensible framework that formalizes the MLE process. R&D-Agent defines the MLE workflow into two phases and six components, turning agent design for MLE from ad-hoc craftsmanship into a principled, testable process. Although several existing agents report promising gains on their chosen components, they can mostly be summarized as a partial optimization from our framework's simple baseline. Inspired by human experts, we designed efficient and effective agents within this framework that achieve state-of-the-art performance. Evaluated on MLE-Bench, the agent built on R&D-Agent ranks as the top-performing machine learning engineering agent, achieving 35.1% any medal rate, demonstrating the ability of the framework to speed up innovation and improve accuracy across a wide range of data science applications. We have open-sourced R&D-Agent on GitHub: https://github.com/microsoft/RD-Agent.", "source": "arxiv", "arxiv_id": "2505.14738v2", "pdf_url": "https://arxiv.org/pdf/2505.14738v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-20T06:07:00Z", "updated": "2025-10-01T03:21:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RAG-Enhanced Collaborative LLM Agents for Drug Discovery", "authors": ["Namkyeong Lee", "Edward De Brouwer", "Ehsan Hajiramezanali", "Tommaso Biancalani", "Chanyoung Park", "Gabriele Scalia"], "year": 2025, "url": "http://arxiv.org/abs/2502.17506v3", "abstract": "Recent advances in large language models (LLMs) have shown great potential to accelerate drug discovery. However, the specialized nature of biochemical data often necessitates costly domain-specific fine-tuning, posing major challenges. First, it hinders the application of more flexible general-purpose LLMs for cutting-edge drug discovery tasks. More importantly, it limits the rapid integration of the vast amounts of scientific data continuously generated through experiments and research. Compounding these challenges is the fact that real-world scientific questions are typically complex and open-ended, requiring reasoning beyond pattern matching or static knowledge retrieval.To address these challenges, we propose CLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored to drug discovery tasks. Through the collaboration of multiple LLM agents, CLADD dynamically retrieves information from biomedical knowledge bases, contextualizes query molecules, and integrates relevant evidence to generate responses - all without the need for domain-specific fine-tuning. Crucially, we tackle key obstacles in applying RAG workflows to biochemical data, including data heterogeneity, ambiguity, and multi-source integration. We demonstrate the flexibility and effectiveness of this framework across a variety of drug discovery tasks, showing that it outperforms general-purpose and domain-specific LLMs as well as traditional deep learning approaches. Our code is publicly available at https://github.com/Genentech/CLADD.", "source": "arxiv", "arxiv_id": "2502.17506v3", "pdf_url": "https://arxiv.org/pdf/2502.17506v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-22T00:12:52Z", "updated": "2025-11-13T21:34:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning", "authors": ["Zihan Wang", "Kangrui Wang", "Qineng Wang", "Pingyue Zhang", "Linjie Li", "Zhengyuan Yang", "Xing Jin", "Kefan Yu", "Minh Nhat Nguyen", "Licheng Liu", "Eli Gottlieb", "Yiping Lu", "Kyunghyun Cho", "Jiajun Wu", "Li Fei-Fei", "Lijuan Wang", "Yejin Choi", "Manling Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.20073v2", "abstract": "Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on four stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and gradient stabilization. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.", "source": "arxiv", "arxiv_id": "2504.20073v2", "pdf_url": "https://arxiv.org/pdf/2504.20073v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-24T17:57:08Z", "updated": "2025-05-26T17:19:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action Issue Detection, Explanation and Recovery", "authors": ["Silvia Izquierdo-Badiola", "Carlos Rizzo", "Guillem AlenyÃ "], "year": 2025, "url": "http://arxiv.org/abs/2503.17703v2", "abstract": "As robots increasingly operate in dynamic human-centric environments, improving their ability to detect, explain, and recover from action-related issues becomes crucial. Traditional model-based and data-driven techniques lack adaptability, while more flexible generative AI methods struggle with grounding extracted information to real-world constraints. We introduce RAIDER, a novel agent that integrates Large Language Models (LLMs) with grounded tools for adaptable and efficient issue detection and explanation. Using a unique \"Ground, Ask&Answer, Issue\" procedure, RAIDER dynamically generates context-aware precondition questions and selects appropriate tools for resolution, achieving targeted information gathering. Our results within a simulated household environment surpass methods relying on predefined models, full scene descriptions, or standalone trained models. Additionally, RAIDER's explanations enhance recovery success, including cases requiring human interaction. Its modular architecture, featuring self-correction mechanisms, enables straightforward adaptation to diverse scenarios, as demonstrated in a real-world human-assistive task. This showcases RAIDER's potential as a versatile agentic AI solution for robotic issue detection and explanation, while addressing the problem of grounding generative AI for its effective application in embodied agents. Project website: https://eurecat.github.io/raider-llmagent/", "source": "arxiv", "arxiv_id": "2503.17703v2", "pdf_url": "https://arxiv.org/pdf/2503.17703v2", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-03-22T09:03:31Z", "updated": "2025-04-04T15:38:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments", "authors": ["Yuchuan Fu", "Xiaohan Yuan", "Dongxia Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.15253v1", "abstract": "The rapid deployment of Large language model (LLM) agents in critical domains like healthcare and finance necessitates robust security frameworks. To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Notably, scaling laws held for security capabilities, with larger models outperforming smaller counterparts. Our findings expose critical risks in real-world agent deployments and provide a foundational framework for future security research. Code and data are available at https://github.com/lanzer-tree/RAS-Eval.", "source": "arxiv", "arxiv_id": "2506.15253v1", "pdf_url": "https://arxiv.org/pdf/2506.15253v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-18T08:30:36Z", "updated": "2025-06-18T08:30:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback", "authors": ["Chunyu Miao", "Henry Peng Zou", "Yangning Li", "Yankai Chen", "Yibo Wang", "Fangxin Wang", "Yifan Li", "Wooseong Yang", "Bowei He", "Xinni Zhang", "Dianzhi Yu", "Hanchen Yang", "Hoang H Nguyen", "Yue Zhou", "Jie Yang", "Jizhou Guo", "Wenzhe Fan", "Chin-Yuan Yeh", "Panpan Meng", "Liancheng Fang", "Jinhu Qi", "Wei-Chieh Huang", "Zhengyao Gu", "Yuwei Han", "Langzhou He", "Yuyao Yang", "Yinghui Li", "Hai-Tao Zheng", "Xue Liu", "Irwin King", "Philip S. Yu"], "year": 2025, "url": "http://arxiv.org/abs/2510.06186v2", "abstract": "Large language models (LLMs) show the promise in supporting scientific research implementation, yet their ability to generate correct and executable code remains limited. Existing works largely adopt one-shot settings, ignoring the iterative and feedback-driven nature of realistic workflows of scientific research development. To address this gap, we present RECODE-H, a benchmark of 102 tasks from research papers and repositories that evaluates LLM agents through multi-turn interactions with LLM-simulated human feedback. It includes structured instructions,unit tests, and a five-level feedback hierarchy to reflect realistic researcher-agent collaboration. We further present ReCodeAgent, a framework that integrates feedback into iterative code generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4, DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer feedback, while also highlighting ongoing challenges in the generation of complex research code. RECODE-H establishes a foundation for developing adaptive, feedback-driven LLM agents in scientific research implementation", "source": "arxiv", "arxiv_id": "2510.06186v2", "pdf_url": "https://arxiv.org/pdf/2510.06186v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-07T17:45:35Z", "updated": "2025-10-24T17:20:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RELATE-Sim: Leveraging Turning Point Theory and LLM Agents to Predict and Understand Long-Term Relationship Dynamics through Interactive Narrative Simulations", "authors": ["Matthew Yue", "Zhikun Xu", "Vivek Gupta", "Thao Ha", "Liesal Sharabi", "Ben Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2510.00414v1", "abstract": "Most dating technologies optimize for getting together, not staying together. We present RELATE-Sim, a theory-grounded simulator that models how couples behave at consequential turning points-exclusivity talks, conflict-and-repair episodes, relocations-rather than static traits. Two persona-aligned LLM agents (one per partner) interact under a centralized Scene Master that frames each turning point as a compact set of realistic options, advances the narrative, and infers interpretable state changes and an auditable commitment estimate after each scene. On a longitudinal dataset of 71 couples with two-year follow-ups, simulation-aware predictions outperform a personas-only baseline while surfacing actionable markers (e.g., repair attempts acknowledged, clarity shifts) that explain why trajectories diverge. RELATE-Sim pushes the relationship research's focus from matchmaking to maintenance, providing a transparent, extensible platform for understanding and forecasting long-term relationship dynamics.", "source": "arxiv", "arxiv_id": "2510.00414v1", "pdf_url": "https://arxiv.org/pdf/2510.00414v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-01T01:50:15Z", "updated": "2025-10-01T01:50:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents", "authors": ["Vishal Raman", "Vijai Aravindh R", "Abhijith Ragav"], "year": 2025, "url": "http://arxiv.org/abs/2509.06269v1", "abstract": "Personalized AI assistants often struggle to incorporate complex personal data and causal knowledge, leading to generic advice that lacks explanatory power. We propose REMI, a Causal Schema Memory architecture for a multimodal lifestyle agent that integrates a personal causal knowledge graph, a causal reasoning engine, and a schema based planning module. The idea is to deliver explainable, personalized recommendations in domains like fashion, personal wellness, and lifestyle planning. Our architecture uses a personal causal graph of the user's life events and habits, performs goal directed causal traversals enriched with external knowledge and hypothetical reasoning, and retrieves adaptable plan schemas to generate tailored action plans. A Large Language Model orchestrates these components, producing answers with transparent causal explanations. We outline the CSM system design and introduce new evaluation metrics for personalization and explainability, including Personalization Salience Score and Causal Reasoning Accuracy, to rigorously assess its performance. Results indicate that CSM based agents can provide more context aware, user aligned recommendations compared to baseline LLM agents. This work demonstrates a novel approach to memory augmented, causal reasoning in personalized agents, advancing the development of transparent and trustworthy AI lifestyle assistants.", "source": "arxiv", "arxiv_id": "2509.06269v1", "pdf_url": "https://arxiv.org/pdf/2509.06269v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-08T01:17:46Z", "updated": "2025-09-08T01:17:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing", "authors": ["Binger Chen", "Tacettin Emre BÃ¶k", "Behnood Rasti", "Volker Markl", "BegÃ¼m Demir"], "year": 2025, "url": "http://arxiv.org/abs/2511.17442v1", "abstract": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.", "source": "arxiv", "arxiv_id": "2511.17442v1", "pdf_url": "https://arxiv.org/pdf/2511.17442v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-11-21T17:41:26Z", "updated": "2025-11-21T17:41:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RExBench: Can coding agents autonomously implement AI research extensions?", "authors": ["Nicholas Edwards", "Yukyung Lee", "Yujun Audrey Mao", "Yulu Qin", "Sebastian Schuster", "Najoung Kim"], "year": 2025, "url": "http://arxiv.org/abs/2506.22598v2", "abstract": "Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance.", "source": "arxiv", "arxiv_id": "2506.22598v2", "pdf_url": "https://arxiv.org/pdf/2506.22598v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-27T19:41:41Z", "updated": "2025-07-17T18:45:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RFCAudit: An LLM Agent for Functional Bug Detection in Network Protocols", "authors": ["Mingwei Zheng", "Chengpeng Wang", "Xuwei Liu", "Jinyao Guo", "Shiwei Feng", "Xiangyu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.00714v2", "abstract": "Functional correctness is critical for ensuring the reliability and security of network protocol implementations. Functional bugs, instances where implementations diverge from behaviors specified in RFC documents, can lead to severe consequences, including faulty routing, authentication bypasses, and service disruptions. Detecting these bugs requires deep semantic analysis across specification documents and source code, a task beyond the capabilities of traditional static analysis tools. This paper introduces RFCAudit, an autonomous agent that leverages large language models (LLMs) to detect functional bugs by checking conformance between network protocol implementations and their RFC specifications. Inspired by the human auditing procedure, RFCAudit comprises two key components: an indexing agent and a detection agent. The former hierarchically summarizes protocol code semantics, generating semantic indexes that enable the detection agent to narrow down the scanning scope. The latter employs demand-driven retrieval to iteratively collect additional relevant data structures and functions, eventually identifying potential inconsistencies with the RFC specifications effectively. We evaluate RFCAudit across six real-world network protocol implementations. RFCAudit identifies 47 functional bugs with 81.9% precision, of which 20 bugs have been confirmed or fixed by developers.", "source": "arxiv", "arxiv_id": "2506.00714v2", "pdf_url": "https://arxiv.org/pdf/2506.00714v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-31T21:13:19Z", "updated": "2025-10-04T06:53:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RRO: LLM Agent Optimization Through Rising Reward Trajectories", "authors": ["Zilong Wang", "Jingfeng Yang", "Sreyashi Nag", "Samarth Varshney", "Xianfeng Tang", "Haoming Jiang", "Jingbo Shang", "Sheikh Muhammad Sarwar"], "year": 2025, "url": "http://arxiv.org/abs/2505.20737v1", "abstract": "Large language models (LLMs) have exhibited extraordinary performance in a variety of tasks while it remains challenging for them to solve complex multi-step tasks as agents. In practice, agents sensitive to the outcome of certain key steps which makes them likely to fail the task because of a subtle mistake in the planning trajectory. Recent approaches resort to calibrating the reasoning process through reinforcement learning. They reward or penalize every reasoning step with process supervision, as known as Process Reward Models (PRMs). However, PRMs are difficult and costly to scale up with a large number of next action candidates since they require extensive computations to acquire the training data through the per-step trajectory exploration. To mitigate this issue, we focus on the relative reward trend across successive reasoning steps and propose maintaining an increasing reward in the collected trajectories for process supervision, which we term Reward Rising Optimization (RRO). Specifically, we incrementally augment the process supervision until identifying a step exhibiting positive reward differentials, i.e. rising rewards, relative to its preceding iteration. This method dynamically expands the search space for the next action candidates, efficiently capturing high-quality data. We provide mathematical groundings and empirical results on the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO achieves superior performance while requiring much less exploration cost.", "source": "arxiv", "arxiv_id": "2505.20737v1", "pdf_url": "https://arxiv.org/pdf/2505.20737v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-27T05:27:54Z", "updated": "2025-05-27T05:27:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage", "authors": ["Peter Yong Zhong", "Siyuan Chen", "Ruiqi Wang", "McKenna McCall", "Ben L. Titzer", "Heather Miller", "Phillip B. Gibbons"], "year": 2025, "url": "http://arxiv.org/abs/2502.08966v2", "abstract": "Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external tools for tasks beyond their standalone capabilities, such as searching websites, booking flights, or making financial transactions. However, these tools greatly increase the risks of prompt injection attacks, where malicious content hijacks the LM agent to leak confidential data or trigger harmful actions. Existing defenses (OpenAI GPTs) require user confirmation before every tool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS), which automatically detects and executes tool calls that preserve integrity and confidentiality, requiring user confirmation only when these safeguards cannot be ensured. RTBAS adapts Information Flow Control to the unique challenges presented by TBAS. We present two novel dependency screeners, using LM-as-a-judge and attention-based saliency, to overcome these challenges. Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.", "source": "arxiv", "arxiv_id": "2502.08966v2", "pdf_url": "https://arxiv.org/pdf/2502.08966v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-13T05:06:22Z", "updated": "2025-02-14T04:16:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale", "authors": ["Jason Holmes", "Yuexing Hao", "Mariana Borras-Osorio", "Federico Mastroleo", "Santiago Romero Brufau", "Valentina Carducci", "Katie M Van Abel", "David M Routman", "Andrew Y. K. Foong", "Liv M Muller", "Satomi Shiraishi", "Daniel K Ebner", "Daniel J Ma", "Sameer R Keole", "Samir H Patel", "Mirek Fatyga", "Martin Bues", "Brad J Stish", "Yolanda I Garces", "Michelle A Neben Wittich", "Robert L Foote", "Sujay A Vora", "Nadia N Laack", "Mark R Waddle", "Wei Liu"], "year": 2025, "url": "http://arxiv.org/abs/2509.25540v2", "abstract": "Manual labeling limits the scale, accuracy, and timeliness of patient outcomes research in radiation oncology. We present RadOnc-GPT, an autonomous large language model (LLM)-based agent capable of independently retrieving patient-specific information, iteratively assessing evidence, and returning structured outcomes. Our evaluation explicitly validates RadOnc-GPT across two clearly defined tiers of increasing complexity: (1) a structured quality assurance (QA) tier, assessing the accurate retrieval of demographic and radiotherapy treatment plan details, followed by (2) a complex clinical outcomes labeling tier involving determination of mandibular osteoradionecrosis (ORN) in head-and-neck cancer patients and detection of cancer recurrence in independent prostate and head-and-neck cancer cohorts requiring combined interpretation of structured and unstructured patient data. The QA tier establishes foundational trust in structured-data retrieval, a critical prerequisite for successful complex clinical outcome labeling.", "source": "arxiv", "arxiv_id": "2509.25540v2", "pdf_url": "https://arxiv.org/pdf/2509.25540v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T21:55:50Z", "updated": "2025-12-12T22:32:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning", "authors": ["Jae-Woo Choi", "Hyungmin Kim", "Hyobin Ong", "Minsu Jang", "Dohyung Kim", "Jaehong Kim", "Youngwoo Yoon"], "year": 2025, "url": "http://arxiv.org/abs/2511.02424v1", "abstract": "Recent advancements in large language models (LLMs) have enabled significant progress in decision-making and task planning for embodied autonomous agents. However, most existing methods still struggle with complex, long-horizon tasks because they rely on a monolithic trajectory that entangles all past decisions and observations, attempting to solve the entire task in a single unified process. To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Each subgoal is handled by an LLM agent node capable of reasoning, acting, and further expanding the tree, while control flow nodes coordinate the execution strategies of agent nodes. In addition, we integrate two complementary memory systems: each agent node retrieves goal-specific, subgoal-level examples from episodic memory and shares environment-specific observations through working memory. Experiments on the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently outperforms strong task-planning baselines such as ReAct across diverse LLMs. Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5 72B, nearly doubling ReAct's 31%.", "source": "arxiv", "arxiv_id": "2511.02424v1", "pdf_url": "https://arxiv.org/pdf/2511.02424v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-04T09:55:40Z", "updated": "2025-11-04T09:55:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "year": 2025, "url": "http://arxiv.org/abs/2510.23822v1", "abstract": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning remain challenging for large language models (LLMs). Sequential prompting methods are prone to context drift, loss of goal information, and recurrent failure cycles, while hierarchical prompting methods often weaken cross-level continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive Context-Aware Reasoning and Planning), a hierarchical framework with shared context for reasoning and planning in LLMs. ReCAP combines three key mechanisms: (i) plan-ahead decomposition, in which the model generates a full subtask list, executes the first item, and refines the remainder; (ii) structured re-injection of parent plans, maintaining consistent multi-level context during recursive return; and (iii) memory-efficient execution, bounding the active prompt so costs scale linearly with task depth. Together these mechanisms align high-level goals with low-level actions, reduce redundant prompting, and preserve coherent context updates across recursion. Experiments demonstrate that ReCAP substantially improves subgoal alignment and success rates on various long-horizon reasoning benchmarks, achieving a 32% gain on synchronous Robotouille and a 29% improvement on asynchronous Robotouille under the strict pass@1 protocol.", "source": "arxiv", "arxiv_id": "2510.23822v1", "pdf_url": "https://arxiv.org/pdf/2510.23822v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)", "published": "2025-10-27T20:03:55Z", "updated": "2025-10-27T20:03:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning", "authors": ["Yuchen Zeng", "Shuibai Zhang", "Wonjun Kang", "Shutong Wu", "Lynnix Zou", "Ying Fan", "Heeju Kim", "Ziqian Lin", "Jungtaek Kim", "Hyung Il Koo", "Dimitris Papailiopoulos", "Kangwook Lee"], "year": 2025, "url": "http://arxiv.org/abs/2512.00831v2", "abstract": "Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning \"algorithms\" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.", "source": "arxiv", "arxiv_id": "2512.00831v2", "pdf_url": "https://arxiv.org/pdf/2512.00831v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-30T10:39:53Z", "updated": "2025-12-09T07:23:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reaching Agreement Among Reasoning LLM Agents", "authors": ["Chaoyi Ruan", "Yiliang Wang", "Ziji Shi", "Jialin Li"], "year": 2025, "url": "http://arxiv.org/abs/2512.20184v1", "abstract": "Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.", "source": "arxiv", "arxiv_id": "2512.20184v1", "pdf_url": "https://arxiv.org/pdf/2512.20184v1", "categories": ["cs.DC"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-12-23T09:20:42Z", "updated": "2025-12-23T09:20:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments", "authors": ["Zheng Jia", "Shengbin Yue", "Wei Chen", "Siyuan Wang", "Yidong Liu", "Yun Song", "Zhongyu Wei"], "year": 2025, "url": "http://arxiv.org/abs/2507.04037v3", "abstract": "The gap between static benchmarks and the dynamic nature of real-world legal practice poses a key barrier to advancing legal intelligence. To this end, we introduce J1-ENVS, the first interactive and dynamic legal environment tailored for LLM-based agents. Guided by legal experts, it comprises six representative scenarios from Chinese legal practices across three levels of environmental complexity. We further introduce J1-EVAL, a fine-grained evaluation framework, designed to assess both task performance and procedural compliance across varying levels of legal proficiency. Extensive experiments on 17 LLM agents reveal that, while many models demonstrate solid legal knowledge, they struggle with procedural execution in dynamic settings. Even the SOTA model, GPT-4o, falls short of 60% overall performance. These findings highlight persistent challenges in achieving dynamic legal intelligence and offer valuable insights to guide future research.", "source": "arxiv", "arxiv_id": "2507.04037v3", "pdf_url": "https://arxiv.org/pdf/2507.04037v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-05T13:31:21Z", "updated": "2026-01-21T07:41:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space", "authors": ["Xingfu Zhou", "Pengfei Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.14448v1", "abstract": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.", "source": "arxiv", "arxiv_id": "2512.14448v1", "pdf_url": "https://arxiv.org/pdf/2512.14448v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-16T14:34:10Z", "updated": "2025-12-16T14:34:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory", "authors": ["Siru Ouyang", "Jun Yan", "I-Hung Hsu", "Yanfei Chen", "Ke Jiang", "Zifeng Wang", "Rujun Han", "Long T. Le", "Samira Daruki", "Xiangru Tang", "Vishy Tirumalashetty", "George Lee", "Mahsan Rofouei", "Hangfei Lin", "Jiawei Han", "Chen-Yu Lee", "Tomas Pfister"], "year": 2025, "url": "http://arxiv.org/abs/2509.25140v1", "abstract": "With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise.", "source": "arxiv", "arxiv_id": "2509.25140v1", "pdf_url": "https://arxiv.org/pdf/2509.25140v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T17:51:03Z", "updated": "2025-09-29T17:51:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems", "authors": ["Luyu Chen", "Quanyu Dai", "Zeyu Zhang", "Xueyang Feng", "Mingyu Zhang", "Pengcheng Tang", "Xu Chen", "Yue Zhu", "Zhenhua Dong"], "year": 2025, "url": "http://arxiv.org/abs/2507.22897v1", "abstract": "Conversational recommender systems (CRS) enhance user experience through multi-turn interactions, yet evaluating CRS remains challenging. User simulators can provide comprehensive evaluations through interactions with CRS, but building realistic and diverse simulators is difficult. While recent work leverages large language models (LLMs) to simulate user interactions, they still fall short in emulating individual real users across diverse scenarios and lack explicit rating mechanisms for quantitative evaluation. To address these gaps, we propose RecUserSim, an LLM agent-based user simulator with enhanced simulation realism and diversity while providing explicit scores. RecUserSim features several key modules: a profile module for defining realistic and diverse user personas, a memory module for tracking interaction history and discovering unknown preferences, and a core action module inspired by Bounded Rationality theory that enables nuanced decision-making while generating more fine-grained actions and personalized responses. To further enhance output control, a refinement module is designed to fine-tune final responses. Experiments demonstrate that RecUserSim generates diverse, controllable outputs and produces realistic, high-quality dialogues, even with smaller base LLMs. The ratings generated by RecUserSim show high consistency across different base LLMs, highlighting its effectiveness for CRS evaluation.", "source": "arxiv", "arxiv_id": "2507.22897v1", "pdf_url": "https://arxiv.org/pdf/2507.22897v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "10.1145/3701716.3715258", "venue": "", "published": "2025-06-25T08:42:46Z", "updated": "2025-06-25T08:42:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring", "authors": ["Khouloud Oueslati", "Maxime Lamothe", "Foutse Khomh"], "year": 2025, "url": "http://arxiv.org/abs/2511.03153v1", "abstract": "Large Language Models (LLMs) have substantially influenced various software engineering tasks. Indeed, in the case of software refactoring, traditional LLMs have shown the ability to reduce development time and enhance code quality. However, these LLMs often rely on static, detailed instructions for specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving contexts and autonomously make decisions by interacting with software tools and executing workflows. In this paper, we explore the potential of LLM-based agents in supporting refactoring activities. Specifically, we introduce RefAgent, a multi-agent LLM-based framework for end-to-end software refactoring. RefAgent consists of specialized agents responsible for planning, executing, testing, and iteratively refining refactorings using self-reflection and tool-calling capabilities. We evaluate RefAgent on eight open-source Java projects, comparing its effectiveness against a single-agent approach, a search-based refactoring tool, and historical developer refactorings. Our assessment focuses on: (1) the impact of generated refactorings on software quality, (2) the ability to identify refactoring opportunities, and (3) the contribution of each LLM agent through an ablation study. Our results show that RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a median of 52.5%, and improves key quality attributes (e.g., reusability) by a median of 8.6%. Additionally, it closely aligns with developer refactorings and the search-based tool in identifying refactoring opportunities, attaining a median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent approaches, RefAgent improves the median unit test pass rate by 64.7% and the median compilation success rate by 40.1%. These findings highlight the promise of multi-agent architectures in advancing automated software refactoring.", "source": "arxiv", "arxiv_id": "2511.03153v1", "pdf_url": "https://arxiv.org/pdf/2511.03153v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-05T03:20:58Z", "updated": "2025-11-05T03:20:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection", "authors": ["Jeonghye Kim", "Sojeong Rhee", "Minbeom Kim", "Dohyung Kim", "Sangmook Lee", "Youngchul Sung", "Kyomin Jung"], "year": 2025, "url": "http://arxiv.org/abs/2505.15182v2", "abstract": "Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.", "source": "arxiv", "arxiv_id": "2505.15182v2", "pdf_url": "https://arxiv.org/pdf/2505.15182v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-21T06:57:39Z", "updated": "2025-09-28T17:14:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making", "authors": ["Wentao Zhang", "Qunbo Wang", "Tao Zhang", "Junsheng Wu", "Hongping Gan", "Yang Liu", "Ling Dai", "Shizhuang Deng", "Shuntong Sun"], "year": 2025, "url": "http://arxiv.org/abs/2512.08366v1", "abstract": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.", "source": "arxiv", "arxiv_id": "2512.08366v1", "pdf_url": "https://arxiv.org/pdf/2512.08366v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-09T08:44:59Z", "updated": "2025-12-09T08:44:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents", "authors": ["Ankush Kadu", "Ashwanth Krishnan"], "year": 2025, "url": "http://arxiv.org/abs/2511.14584v1", "abstract": "Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.", "source": "arxiv", "arxiv_id": "2511.14584v1", "pdf_url": "https://arxiv.org/pdf/2511.14584v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-18T15:25:05Z", "updated": "2025-11-18T15:25:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction", "authors": ["Jianhao Yan", "Yun Luo", "Yue Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.18308v1", "abstract": "In the multi-turn interaction schema, large language models (LLMs) can leverage user feedback to enhance the quality and relevance of their responses. However, evaluating an LLM's ability to incorporate user refutation feedback is crucial yet challenging. In this study, we introduce RefuteBench 2.0, which significantly extends the original RefuteBench by incorporating LLM agents as refuters and evaluators, which allows for flexible and comprehensive assessment.\n  We design both transient and persistent refutation instructions with different validity periods. Meta-evaluation shows that the LLM-based refuter could generate more human-like refutations and the evaluators could assign scores with high correlation with humans. Experimental results of various LLMs show that current models could effectively satisfy the refutation but fail to memorize the refutation information. Interestingly, we also observe that the performance of the initial task decreases as the refutations increase. Analysis of the attention scores further shows a potential weakness of current LLMs: they struggle to retain and correctly use previous information during long context dialogues. https://github.com/ElliottYan/RefuteBench-2.0", "source": "arxiv", "arxiv_id": "2502.18308v1", "pdf_url": "https://arxiv.org/pdf/2502.18308v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-25T15:51:25Z", "updated": "2025-02-25T15:51:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization", "authors": ["Hanyu Li", "Jiangshan Duo", "Bofei Gao", "Hailin Zhang", "Sujian Li", "Xiaotie Deng", "Liang Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2601.06052v2", "abstract": "Chain-of-thought reasoning in large language models can trigger an \"overthinking trap\": longer rollouts raise cost and latency yet often yield unreliable accuracy gains. Existing methods use global, static controls that may suppress needed reasoning. We propose mastery-gated, sample-level, soft reinforcement learning compression that penalizes long rollouts only when the model already solves the problem and has produced a shorter rollout. Across benchmarks, it cuts response length by 20-40% with comparable or higher accuracy and generalizes across domains: a model trained on math spontaneously shortens unseen tasks (code, instruction following, general-knowledge QA) without hurting accuracy. We further show two-way transfer between non-agent CoT and tool-use agents: non-agent training reduces SWE-Bench Verified rounds by 13%, while compressing a thinking agent cuts SWE trajectories by 67% tokens and 52% rounds and shortens non-agent outputs by up to 44%. Compression is thus not cosmetic brevity, but an inherent computation policy -- what to keep, and what to forget.", "source": "arxiv", "arxiv_id": "2601.06052v2", "pdf_url": "https://arxiv.org/pdf/2601.06052v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-19T06:30:54Z", "updated": "2026-01-21T06:34:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents", "authors": ["Kevin Chen", "Marco Cusumano-Towner", "Brody Huval", "Aleksei Petrenko", "Jackson Hamburger", "Vladlen Koltun", "Philipp KrÃ¤henbÃ¼hl"], "year": 2025, "url": "http://arxiv.org/abs/2502.01600v3", "abstract": "Interactive digital agents (IDAs) leverage APIs of stateful digital environments to perform tasks in response to user requests. While IDAs powered by instruction-tuned large language models (LLMs) can react to feedback from interface invocations in multi-step exchanges, they have not been trained in their respective digital environments. Prior methods accomplish less than half of tasks in sophisticated benchmarks such as AppWorld. We present a reinforcement learning (RL) approach that trains IDAs directly in their target environments. We formalize this training as a partially observable Markov decision process and derive LOOP, a data- and memory-efficient variant of proximal policy optimization. LOOP uses no value network and maintains exactly one copy of the underlying LLM in memory, making its implementation straightforward and as memory-efficient as fine-tuning a single LLM. A 32-billion-parameter agent trained with LOOP in the AppWorld environment outperforms the much larger OpenAI o1 agent by 9 percentage points (15% relative). To our knowledge, this is the first reported application of RL to IDAs that interact with a stateful, multi-domain, multi-app environment via direct API calls. Our analysis sheds light on the effectiveness of RL in this area, showing that the agent learns to consult the API documentation, avoid unwarranted assumptions, minimize confabulation, and recover from setbacks.", "source": "arxiv", "arxiv_id": "2502.01600v3", "pdf_url": "https://arxiv.org/pdf/2502.01600v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-03T18:35:42Z", "updated": "2025-03-08T05:23:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reinforcement Learning for Long-Horizon Multi-Turn Search Agents", "authors": ["Vivek Kalyan", "Martin Andrews"], "year": 2025, "url": "http://arxiv.org/abs/2510.24126v1", "abstract": "Large Language Model (LLM) agents can leverage multiple turns and tools to solve complex tasks, with prompt-based approaches achieving strong performance. This work demonstrates that Reinforcement Learning (RL) can push capabilities significantly further by learning from experience. Through experiments on a legal document search benchmark, we show that our RL-trained 14 Billion parameter model outperforms frontier class models (85% vs 78% accuracy). In addition, we explore turn-restricted regimes, during training and at test-time, that show these agents achieve better results if allowed to operate over longer multi-turn horizons.", "source": "arxiv", "arxiv_id": "2510.24126v1", "pdf_url": "https://arxiv.org/pdf/2510.24126v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-28T07:00:42Z", "updated": "2025-10-28T07:00:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reinforcement Learning for Tool-Integrated Interleaved Thinking towards Cross-Domain Generalization", "authors": ["Zhengyu Chen", "Jinluan Yang", "Teng Xiao", "Ruochen Zhou", "Luan Zhang", "Xiangyu Xi", "Xiaowei Shi", "Wei Wang", "Jinggang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.11184v2", "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization. However, the generalization of tool-augmented reinforcement learning (RL) across diverse domains remains a significant challenge. Standard paradigms often treat tool usage as a linear or isolated event, which becomes brittle when transferring skills from restricted domains (e.g., mathematics) to open-ended tasks. In this work, we investigate the cross-domain generalization of an LLM agent trained exclusively on mathematical problem-solving. To facilitate robust skill transfer, we propose a {\\textbf{R}einforcement Learning for \\textbf{I}nterleaved \\textbf{T}ool \\textbf{E}xecution (RITE)}. Unlike traditional methods, RITE enforces a continuous ``Plan-Action-Reflection'' cycle, allowing the model to ground its reasoning in intermediate tool outputs and self-correct during long-horizon tasks. To effectively train this complex interleaved policy, we introduce {Dr. GRPO}, a robust optimization objective that utilizes token-level loss aggregation with importance sampling to mitigate reward sparsity and high-variance credit assignment. Furthermore, we employ a dual-component reward system and dynamic curriculum via online rollout filtering to ensure structural integrity and sample efficiency. Extensive experiments reveal that our approach, despite being trained solely on math tasks, achieves state-of-the-art performance across diverse reasoning domains, demonstrating high token efficiency and strong generalization capabilities.", "source": "arxiv", "arxiv_id": "2510.11184v2", "pdf_url": "https://arxiv.org/pdf/2510.11184v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-13T09:19:13Z", "updated": "2026-01-07T04:36:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization", "authors": ["Dong Qiu", "Duo Xu", "Limengxi Yue"], "year": 2025, "url": "http://arxiv.org/abs/2512.24609v1", "abstract": "Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.", "source": "arxiv", "arxiv_id": "2512.24609v1", "pdf_url": "https://arxiv.org/pdf/2512.24609v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T03:59:18Z", "updated": "2025-12-31T03:59:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Reward Design", "authors": ["Quan Wei", "Siliang Zeng", "Chenliang Li", "William Brown", "Oana Frunza", "Wei Deng", "Anderson Schneider", "Yuriy Nevmyvaka", "Yang Katie Zhao", "Alfredo Garcia", "Mingyi Hong"], "year": 2025, "url": "http://arxiv.org/abs/2505.11821v2", "abstract": "This paper investigates Reinforcement Learning (RL) approaches to enhance the reasoning capabilities of Large Language Model (LLM) agents in long-horizon, multi-turn scenarios. Although RL algorithms such as Group Relative Policy Optimization (GRPO) and Proximal Policy Optimization (PPO) have been widely applied to train multi-turn LLM agents, they typically rely only on sparse outcome rewards and lack dense intermediate signals across multiple decision steps, limiting their performance on complex reasoning tasks. To bridge this gap, we present the first systematic study of \\textit{turn-level reward design} for multi-turn RL algorithms and agent applications. By integrating turn-level rewards, we extend GRPO and PPO to their respective multi-turn variants, enabling fine-grained credit assignment. We conduct case studies on multi-turn reasoning-augmented search agents, where we carefully design two types of turn-level rewards: verifiable and LLM-as-judge. Our experiments on multi-turn search tasks demonstrate that incorporating well-designed turn-level rewards enables RL algorithms to significantly outperform baseline methods with trajectory-level rewards. Both training and validation reward curves illustrate that our method achieves \\textit{greater stability}, \\textit{faster convergence}, and \\textit{higher accuracy}. Numerical results across diverse question-answering datasets further show that our approach consistently delivers highest answer correctness and 100\\% format correctness.", "source": "arxiv", "arxiv_id": "2505.11821v2", "pdf_url": "https://arxiv.org/pdf/2505.11821v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-17T04:09:46Z", "updated": "2025-10-23T04:32:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19461v1", "abstract": "We stress test monitoring systems for detecting covert misbehavior in autonomous LLM agents (e.g., secretly sharing private information). To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding proposed in this work. Our empirical results yield three key findings. First, agent awareness dominates monitor awareness: an agent's knowledge that it is being monitored substantially degrades the monitor's reliability. On the contrary, providing the monitor with more information about the agent is less helpful than expected. Second, monitor scaffolding matters more than monitor awareness: the hybrid scaffolding consistently outperforms baseline monitor scaffolding, and can enable weaker models to reliably monitor stronger agents -- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where humans discuss with the LLM monitor to get an updated judgment for the agent's behavior, targeted human oversight is most effective; escalating only pre-flagged cases to human reviewers improved the TPR by approximately 15% at FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the lack of adversarial robustness for LLMs and humans when monitoring and detecting agent misbehavior. We release code, data, and logs to spur further research.", "source": "arxiv", "arxiv_id": "2508.19461v1", "pdf_url": "https://arxiv.org/pdf/2508.19461v1", "categories": ["cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-26T22:29:31Z", "updated": "2025-08-26T22:29:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reliable agent engineering should integrate machine-compatible organizational principles", "authors": ["R. Patrick Xian", "Garry A. Gabison", "Ahmed Alaa", "Christoph Riedl", "Grigorios G. Chrysos"], "year": 2025, "url": "http://arxiv.org/abs/2512.07665v1", "abstract": "As AI agents built on large language models (LLMs) become increasingly embedded in society, issues of coordination, control, delegation, and accountability are entangled with concerns over their reliability. To design and implement LLM agents around reliable operations, we should consider the task complexity in the application settings and reduce their limitations while striving to minimize agent failures and optimize resource efficiency. High-functioning human organizations have faced similar balancing issues, which led to evidence-based theories that seek to understand their functioning strategies. We examine the parallels between LLM agents and the compatible frameworks in organization science, focusing on what the design, scaling, and management of organizations can inform agentic systems towards improving reliability. We offer three preliminary accounts of organizational principles for AI agent engineering to attain reliability and effectiveness, through balancing agency and capabilities in agent design, resource constraints and performance benefits in agent scaling, and internal and external mechanisms in agent management. Our work extends the growing exchanges between the operational and governance principles of AI systems and social systems to facilitate system integration.", "source": "arxiv", "arxiv_id": "2512.07665v1", "pdf_url": "https://arxiv.org/pdf/2512.07665v1", "categories": ["cs.CY", "cs.MA", "cs.SE"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-12-08T15:58:55Z", "updated": "2025-12-08T15:58:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RepliBench: Evaluating the Autonomous Replication Capabilities of Language Model Agents", "authors": ["Sid Black", "Asa Cooper Stickland", "Jake Pencharz", "Oliver Sourbut", "Michael Schmatz", "Jay Bailey", "Ollie Matthews", "Ben Millwood", "Alex Remedios", "Alan Cooney"], "year": 2025, "url": "http://arxiv.org/abs/2504.18565v2", "abstract": "Uncontrollable autonomous replication of language model agents poses a critical safety risk. To better understand this risk, we introduce RepliBench, a suite of evaluations designed to measure autonomous replication capabilities. RepliBench is derived from a decomposition of these capabilities covering four core domains: obtaining resources, exfiltrating model weights, replicating onto compute, and persisting on this compute for long periods. We create 20 novel task families consisting of 86 individual tasks. We benchmark 5 frontier models, and find they do not currently pose a credible threat of self-replication, but succeed on many components and are improving rapidly. Models can deploy instances from cloud compute providers, write self-propagating programs, and exfiltrate model weights under simple security setups, but struggle to pass KYC checks or set up robust and persistent agent deployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50% pass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20 families on the hardest variants. These findings suggest autonomous replication capability could soon emerge with improvements in these remaining areas or with human assistance.", "source": "arxiv", "arxiv_id": "2504.18565v2", "pdf_url": "https://arxiv.org/pdf/2504.18565v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-21T11:39:22Z", "updated": "2025-05-05T20:52:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing", "authors": ["Jinyao Guo", "Chengpeng Wang", "Xiangzhe Xu", "Zian Su", "Xiangyu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2501.18160v3", "abstract": "Code auditing is the process of reviewing code with the aim of identifying bugs. Large Language Models (LLMs) have demonstrated promising capabilities for this task without requiring compilation, while also supporting user-friendly customization. However, auditing a code repository with LLMs poses significant challenges: limited context windows and hallucinations can degrade the quality of bug reports, and analyzing large-scale repositories incurs substantial time and token costs, hindering efficiency and scalability.\n  This work introduces an LLM-based agent, RepoAudit, designed to perform autonomous repository-level code auditing. Equipped with agent memory, RepoAudit explores the codebase on demand by analyzing data-flow facts along feasible program paths within individual functions. It further incorporates a validator module to mitigate hallucinations by verifying data-flow facts and checking the satisfiability of path conditions associated with potential bugs, thereby reducing false positives. RepoAudit detects 40 true bugs across 15 real-world benchmark projects with a precision of 78.43%, requiring on average only 0.44 hours and $2.54 per project. Also, it detects 185 new bugs in high-profile projects, among which 174 have been confirmed or fixed. We have open-sourced RepoAudit at https://github.com/PurCL/RepoAudit.", "source": "arxiv", "arxiv_id": "2501.18160v3", "pdf_url": "https://arxiv.org/pdf/2501.18160v3", "categories": ["cs.SE", "cs.PL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-01-30T05:56:30Z", "updated": "2025-05-29T22:08:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Repository-level Code Search with Neural Retrieval Methods", "authors": ["Siddharth Gandhi", "Luyu Gao", "Jamie Callan"], "year": 2025, "url": "http://arxiv.org/abs/2502.07067v1", "abstract": "This paper presents a multi-stage reranking system for repository-level code search, which leverages the vastly available commit histories of large open-source repositories to aid in bug fixing. We define the task of repository-level code search as retrieving the set of files from the current state of a code repository that are most relevant to addressing a user's question or bug. The proposed approach combines BM25-based retrieval over commit messages with neural reranking using CodeBERT to identify the most pertinent files. By learning patterns from diverse repositories and their commit histories, the system can surface relevant files for the task at hand. The system leverages both commit messages and source code for relevance matching, and is evaluated in both normal and oracle settings. Experiments on a new dataset created from 7 popular open-source repositories demonstrate substantial improvements of up to 80% in MAP, MRR and P@1 over the BM25 baseline, across a diverse set of queries, demonstrating the effectiveness this approach. We hope this work aids LLM agents as a tool for better code search and understanding. Our code and results obtained are publicly available.", "source": "arxiv", "arxiv_id": "2502.07067v1", "pdf_url": "https://arxiv.org/pdf/2502.07067v1", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-02-10T21:59:01Z", "updated": "2025-02-10T21:59:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reproducibility Study of \"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"", "authors": ["Pedro M. P. Curvo", "Mara Dragomir", "Salvador Torpes", "Mohammadmahdi Rahimi"], "year": 2025, "url": "http://arxiv.org/abs/2505.09289v1", "abstract": "This study evaluates and extends the findings made by Piatti et al., who introduced GovSim, a simulation framework designed to assess the cooperative decision-making capabilities of large language models (LLMs) in resource-sharing scenarios. By replicating key experiments, we validate claims regarding the performance of large models, such as GPT-4-turbo, compared to smaller models. The impact of the universalization principle is also examined, with results showing that large models can achieve sustainable cooperation, with or without the principle, while smaller models fail without it. In addition, we provide multiple extensions to explore the applicability of the framework to new settings. We evaluate additional models, such as DeepSeek-V3 and GPT-4o-mini, to test whether cooperative behavior generalizes across different architectures and model sizes. Furthermore, we introduce new settings: we create a heterogeneous multi-agent environment, study a scenario using Japanese instructions, and explore an \"inverse environment\" where agents must cooperate to mitigate harmful resource distributions. Our results confirm that the benchmark can be applied to new models, scenarios, and languages, offering valuable insights into the adaptability of LLMs in complex cooperative tasks. Moreover, the experiment involving heterogeneous multi-agent systems demonstrates that high-performing models can influence lower-performing ones to adopt similar behaviors. This finding has significant implications for other agent-based applications, potentially enabling more efficient use of computational resources and contributing to the development of more effective cooperative AI systems.", "source": "arxiv", "arxiv_id": "2505.09289v1", "pdf_url": "https://arxiv.org/pdf/2505.09289v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-14T11:15:14Z", "updated": "2025-05-14T11:15:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective", "authors": ["Krishna Singh Rajput", "Tejas Anvekar", "Chitta Baral", "Vivek Gupta"], "year": 2025, "url": "http://arxiv.org/abs/2505.20816v1", "abstract": "Recent advances in multimodal question answering have primarily focused on combining heterogeneous modalities or fine-tuning multimodal large language models. While these approaches have shown strong performance, they often rely on a single, generalized reasoning strategy, overlooking the unique characteristics of each modality ultimately limiting both accuracy and interpretability. To address these limitations, we propose MAMMQA, a multi-agent QA framework for multimodal inputs spanning text, tables, and images. Our system includes two Visual Language Model (VLM) agents and one text-based Large Language Model (LLM) agent. The first VLM decomposes the user query into sub-questions and sequentially retrieves partial answers from each modality. The second VLM synthesizes and refines these results through cross-modal reasoning. Finally, the LLM integrates the insights into a cohesive answer. This modular design enhances interpretability by making the reasoning process transparent and allows each agent to operate within its domain of expertise. Experiments on diverse multimodal QA benchmarks demonstrate that our cooperative, multi-agent framework consistently outperforms existing baselines in both accuracy and robustness.", "source": "arxiv", "arxiv_id": "2505.20816v1", "pdf_url": "https://arxiv.org/pdf/2505.20816v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-27T07:23:38Z", "updated": "2025-05-27T07:23:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles", "authors": ["Xinhang Li", "Qing Guo", "Junyu Chen", "Zheng Guo", "Shengzhe Xu", "Lei Li", "Lin Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2510.26242v1", "abstract": "With increasing urban traffic complexity, Traffic Signal Control (TSC) is essential for optimizing traffic flow and improving road safety. Large Language Models (LLMs) emerge as promising approaches for TSC. However, they are prone to hallucinations in emergencies, leading to unreliable decisions that may cause substantial delays for emergency vehicles. Moreover, diverse intersection types present substantial challenges for traffic state encoding and cross-intersection training, limiting generalization across heterogeneous intersections. Therefore, this paper proposes Retrieval Augmented Generation (RAG)-enhanced distributed LLM agents with Emergency response for Generalizable TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning framework, which dynamically adjusts reasoning depth based on the emergency scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to distill specific knowledge and guidance from historical cases, enhancing the reliability and rationality of agents' emergency decisions. Secondly, this paper designs a type-agnostic traffic representation and proposes a Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3 adaptively samples training experience from diverse intersections with environment feedback-based priority and fine-tunes LLM agents with a designed reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies across heterogeneous intersections. On three real-world road networks with 17 to 177 heterogeneous intersections, extensive experiments show that REG-TSC reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle waiting time by 83.16%, outperforming other state-of-the-art methods.", "source": "arxiv", "arxiv_id": "2510.26242v1", "pdf_url": "https://arxiv.org/pdf/2510.26242v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-30T08:23:08Z", "updated": "2025-10-30T08:23:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Retrieval-Augmented Generation with Conflicting Evidence", "authors": ["Han Wang", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "year": 2025, "url": "http://arxiv.org/abs/2504.13079v2", "abstract": "Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.", "source": "arxiv", "arxiv_id": "2504.13079v2", "pdf_url": "https://arxiv.org/pdf/2504.13079v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-17T16:46:11Z", "updated": "2025-08-12T17:53:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "year": 2025, "url": "http://arxiv.org/abs/2505.11807v2", "abstract": "Large Language Models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM's context. Instead, it combines the LLM's action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline ''retrospection'' process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong, contemporary baselines.", "source": "arxiv", "arxiv_id": "2505.11807v2", "pdf_url": "https://arxiv.org/pdf/2505.11807v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.18653/v1/2024.emnlp-main.268", "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 4650-4666, ACL Anthology, 2024", "published": "2025-05-17T03:28:24Z", "updated": "2025-05-27T01:30:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Revealing Political Bias in LLMs through Structured Multi-Agent Debate", "authors": ["Aishwarya Bandaru", "Fabian Bindley", "Trevor Bluth", "Nandini Chavda", "Baixu Chen", "Ethan Law"], "year": 2025, "url": "http://arxiv.org/abs/2506.11825v1", "abstract": "Large language models (LLMs) are increasingly used to simulate social behaviour, yet their political biases and interaction dynamics in debates remain underexplored. We investigate how LLM type and agent gender attributes influence political bias using a structured multi-agent debate framework, by engaging Neutral, Republican, and Democrat American LLM agents in debates on politically sensitive topics. We systematically vary the underlying LLMs, agent genders, and debate formats to examine how model provenance and agent personas influence political bias and attitudes throughout debates. We find that Neutral agents consistently align with Democrats, while Republicans shift closer to the Neutral; gender influences agent attitudes, with agents adapting their opinions when aware of other agents' genders; and contrary to prior research, agents with shared political affiliations can form echo chambers, exhibiting the expected intensification of attitudes as debates progress.", "source": "arxiv", "arxiv_id": "2506.11825v1", "pdf_url": "https://arxiv.org/pdf/2506.11825v1", "categories": ["cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-13T14:30:37Z", "updated": "2025-06-13T14:30:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration", "authors": ["Kostas Hatalis", "Despina Christou", "Vyshnavi Kondapalli"], "year": 2025, "url": "http://arxiv.org/abs/2504.06943v2", "abstract": "Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks. Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. While agents are capable of perceiving their environments, forming inferences, planning, and executing actions towards goals, they often face issues such as hallucinations and lack of contextual memory across interactions. This paper explores how Case-Based Reasoning (CBR), a strategy that solves new problems by referencing past experiences, can be integrated into LLM agent frameworks. This integration allows LLMs to leverage explicit knowledge, enhancing their effectiveness. We systematically review the theoretical foundations of these enhanced agents, identify critical framework components, and formulate a mathematical model for the CBR processes of case retrieval, adaptation, and learning. We also evaluate CBR-enhanced agents against other methods like Chain-of-Thought reasoning and standard Retrieval-Augmented Generation, analyzing their relative strengths. Moreover, we explore how leveraging CBR's cognitive dimensions (including self-reflection, introspection, and curiosity) via goal-driven autonomy mechanisms can further enhance the LLM agent capabilities. Contributing to the ongoing research on neuro-symbolic hybrid systems, this work posits CBR as a viable technique for enhancing the reasoning skills and cognitive aspects of autonomous LLM agents.", "source": "arxiv", "arxiv_id": "2504.06943v2", "pdf_url": "https://arxiv.org/pdf/2504.06943v2", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-09T14:51:02Z", "updated": "2025-04-11T05:34:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews", "authors": ["Xian Gao", "Jiacheng Ruan", "Zongyun Zhang", "Jingsheng Gao", "Ting Liu", "Yuzhuo Fu"], "year": 2025, "url": "http://arxiv.org/abs/2503.08506v3", "abstract": "Academic paper review is a critical yet time-consuming task within the research community. With the increasing volume of academic publications, automating the review process has become a significant challenge. The primary issue lies in generating comprehensive, accurate, and reasoning-consistent review comments that align with human reviewers' judgments. In this paper, we address this challenge by proposing ReviewAgents, a framework that leverages large language models (LLMs) to generate academic paper reviews. We first introduce a novel dataset, Review-CoT, consisting of 142k review comments, designed for training LLM agents. This dataset emulates the structured reasoning process of human reviewers-summarizing the paper, referencing relevant works, identifying strengths and weaknesses, and generating a review conclusion. Building upon this, we train LLM reviewer agents capable of structured reasoning using a relevant-paper-aware training method. Furthermore, we construct ReviewAgents, a multi-role, multi-LLM agent review framework, to enhance the review comment generation process. Additionally, we propose ReviewBench, a benchmark for evaluating the review comments generated by LLMs. Our experimental results on ReviewBench demonstrate that while existing LLMs exhibit a certain degree of potential for automating the review process, there remains a gap when compared to human-generated reviews. Moreover, our ReviewAgents framework further narrows this gap, outperforming advanced LLMs in generating review comments.", "source": "arxiv", "arxiv_id": "2503.08506v3", "pdf_url": "https://arxiv.org/pdf/2503.08506v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-11T14:56:58Z", "updated": "2025-07-16T08:29:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context", "authors": ["Yu Lei", "Shuzheng Si", "Wei Wang", "Yifei Wu", "Gang Chen", "Fanchao Qi", "Maosong Sun"], "year": 2025, "url": "http://arxiv.org/abs/2511.18743v1", "abstract": "Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.", "source": "arxiv", "arxiv_id": "2511.18743v1", "pdf_url": "https://arxiv.org/pdf/2511.18743v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-24T04:12:41Z", "updated": "2025-11-24T04:12:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Robotouille: An Asynchronous Planning Benchmark for LLM Agents", "authors": ["Gonzalo Gonzalez-Pumariega", "Leong Su Yean", "Neha Sunkara", "Sanjiban Choudhury"], "year": 2025, "url": "http://arxiv.org/abs/2502.05227v1", "abstract": "Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents' ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage overlapping tasks and interruptions. Our results show that ReAct (gpt4-o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution. Code is available at https://github.com/portal-cornell/robotouille.", "source": "arxiv", "arxiv_id": "2502.05227v1", "pdf_url": "https://arxiv.org/pdf/2502.05227v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-02-06T05:50:37Z", "updated": "2025-02-06T05:50:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "year": 2025, "url": "http://arxiv.org/abs/2507.14447v2", "abstract": "The deployment of agent systems in an enterprise environment is often hindered by several challenges: common models lack domain-specific process knowledge, leading to disorganized plans, missing key tools, and poor execution stability. To address this, this paper introduces Routine, a multi-step agent planning framework designed with a clear structure, explicit instructions, and seamless parameter passing to guide the agent's execution module in performing multi-step tool-calling tasks with high stability. In evaluations conducted within a real-world enterprise scenario, Routine significantly increases the execution accuracy in model tool calls, increasing the performance of GPT-4o from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an accuracy increase to 88.2% on scenario-specific evaluations, indicating improved adherence to execution plans. In addition, we employed Routine-based distillation to create a scenario-specific, multi-step tool-calling dataset. Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%, approaching GPT-4o's performance. These results highlight Routine's effectiveness in distilling domain-specific tool-usage patterns and enhancing model adaptability to new scenarios. Our experimental results demonstrate that Routine provides a practical and accessible approach to building stable agent workflows, accelerating the deployment and adoption of agent systems in enterprise environments, and advancing the technical vision of AI for Process.", "source": "arxiv", "arxiv_id": "2507.14447v2", "pdf_url": "https://arxiv.org/pdf/2507.14447v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-19T02:46:19Z", "updated": "2025-07-22T10:01:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SABER: Small Actions, Big Errors -- Safeguarding Mutating Steps in LLM Agents", "authors": ["Alejandro Cuadron", "Pengfei Yu", "Yang Liu", "Arpit Gupta"], "year": 2025, "url": "http://arxiv.org/abs/2512.07850v1", "abstract": "Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \\emph{do all actions contribute equally to failure?} Analyzing execution traces on $Ï$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \\emph{mutating} (environment-changing) vs.\\ non-mutating steps and formalize \\emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\\%$ on Airline and upto $96\\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \\cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \\emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \\cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\\% \\emph{relative} on Airline, +11\\% on Retail, and +7\\% on SWE-Bench Verified; Claude: +9\\%/+7\\%. We further identify ceiling effects in $Ï$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $Ï$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.", "source": "arxiv", "arxiv_id": "2512.07850v1", "pdf_url": "https://arxiv.org/pdf/2512.07850v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-26T01:28:22Z", "updated": "2025-11-26T01:28:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection", "authors": ["Yubin Ge", "Salvatore Romeo", "Jason Cai", "Monica Sunkara", "Yi Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2509.20562v1", "abstract": "Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents.", "source": "arxiv", "arxiv_id": "2509.20562v1", "pdf_url": "https://arxiv.org/pdf/2509.20562v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-24T21:02:15Z", "updated": "2025-09-24T21:02:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "authors": ["Yu Xia", "Yiran Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "year": 2025, "url": "http://arxiv.org/abs/2507.07441v2", "abstract": "Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "source": "arxiv", "arxiv_id": "2507.07441v2", "pdf_url": "https://arxiv.org/pdf/2507.07441v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-10T05:38:15Z", "updated": "2025-08-20T22:10:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine", "authors": ["Adarsh Gupta", "Bhabesh Mali", "Chandan Karfa"], "year": 2025, "url": "http://arxiv.org/abs/2506.13983v1", "abstract": "Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.", "source": "arxiv", "arxiv_id": "2506.13983v1", "pdf_url": "https://arxiv.org/pdf/2506.13983v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-11T06:43:24Z", "updated": "2025-06-11T06:43:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention", "authors": ["Chengshuai Zhao", "Zhen Tan", "Chau-Wai Wong", "Xinyan Zhao", "Tianlong Chen", "Huan Liu"], "year": 2025, "url": "http://arxiv.org/abs/2502.10937v2", "abstract": "Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.", "source": "arxiv", "arxiv_id": "2502.10937v2", "pdf_url": "https://arxiv.org/pdf/2502.10937v2", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-16T00:19:07Z", "updated": "2025-07-06T00:55:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness", "authors": ["Zehua Pei", "Hui-Ling Zhen", "Shixiong Kai", "Sinno Jialin Pan", "Yunhe Wang", "Mingxuan Yuan", "Bei Yu"], "year": 2025, "url": "http://arxiv.org/abs/2512.15374v1", "abstract": "Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \\textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \\textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.", "source": "arxiv", "arxiv_id": "2512.15374v1", "pdf_url": "https://arxiv.org/pdf/2512.15374v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-17T12:25:05Z", "updated": "2025-12-17T12:25:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks", "authors": ["Hwiwon Lee", "Ziqi Zhang", "Hanxiao Lu", "Lingming Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.11791v2", "abstract": "Rigorous security-focused evaluation of large language model (LLM) agents is imperative for establishing trust in their safe deployment throughout the software development lifecycle. However, existing benchmarks largely rely on synthetic challenges or simplified vulnerability datasets that fail to capture the complexity and ambiguity encountered by security engineers in practice. We introduce SEC-bench, the first fully automated benchmarking framework for evaluating LLM agents on authentic security engineering tasks. SEC-bench employs a novel multi-agent scaffold that automatically constructs code repositories with harnesses, reproduces vulnerabilities in isolated environments, and generates gold patches for reliable evaluation. Our framework automatically creates high-quality software vulnerability datasets with reproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench, we implement two critical software security tasks to rigorously evaluate LLM agents' capabilities: proof-of-concept (PoC) generation and vulnerability patching. A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset. These results highlight the crucial steps needed toward developing LLM agents that are more practical, intelligent, and autonomous for security engineering.", "source": "arxiv", "arxiv_id": "2506.11791v2", "pdf_url": "https://arxiv.org/pdf/2506.11791v2", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-13T13:54:30Z", "updated": "2025-10-22T16:27:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SEFL: Enhancing Educational Assignment Feedback with LLM Agents", "authors": ["Mike Zhang", "Amalie Pernille Dilling", "LÃ©on Gondelman", "Niels Erik Ruan Lyngdorf", "Euan D. Lindsay", "Johannes Bjerva"], "year": 2025, "url": "http://arxiv.org/abs/2502.12927v2", "abstract": "Providing high-quality feedback to student assignments is crucial for student success, but it is constrained by time and costs. In this work, we introduce Synthetic Educational Feedback Loops (SEFL), a synthetic data framework designed to generate data that resembles immediate, on-demand feedback at scale without relying on extensive, real-world student assignments. To get this type of data, two large language models (LLMs) operate in teacher-student roles to simulate assignment completion and formative feedback, generating synthetic pairs of student work and corresponding critiques and actionable improvements from a teacher. With this data, we fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher-student assignment feedback loop in higher education. Through comprehensive evaluations with four LLM judges and three human experts, we demonstrate that SEFL-tuned models outperform both their non-tuned counterparts in feedback quality and an existing baseline. The potential for societal impact is reinforced by extensive qualitative comments by ratings by human stakeholders -- both students and higher education instructors. All in all, SEFL has substantial potential to transform feedback processes for higher education and beyond.", "source": "arxiv", "arxiv_id": "2502.12927v2", "pdf_url": "https://arxiv.org/pdf/2502.12927v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-18T15:09:29Z", "updated": "2025-08-01T13:19:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents", "authors": ["Simon Sinong Zhan", "Yao Liu", "Philip Wang", "Zinan Wang", "Qineng Wang", "Zhian Ruan", "Xiangyu Shi", "Xinyu Cao", "Frank Yang", "Kangrui Wang", "Huajie Shao", "Manling Li", "Qi Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2510.12985v1", "abstract": "We present Sentinel, the first framework for formally evaluating the physical safety of Large Language Model(LLM-based) embodied agents across the semantic, plan, and trajectory levels. Unlike prior methods that rely on heuristic rules or subjective LLM judgments, Sentinel grounds practical safety requirements in formal temporal logic (TL) semantics that can precisely specify state invariants, temporal dependencies, and timing constraints. It then employs a multi-level verification pipeline where (i) at the semantic level, intuitive natural language safety requirements are formalized into TL formulas and the LLM agent's understanding of these requirements is probed for alignment with the TL formulas; (ii) at the plan level, high-level action plans and subgoals generated by the LLM agent are verified against the TL formulas to detect unsafe plans before execution; and (iii) at the trajectory level, multiple execution trajectories are merged into a computation tree and efficiently verified against physically-detailed TL specifications for a final safety check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate multiple LLM-based embodied agents against diverse safety requirements. Our experiments show that by grounding physical safety in temporal logic and applying verification methods across multiple levels, Sentinel provides a rigorous foundation for systematically evaluating LLM-based embodied agents in physical environments, exposing safety violations overlooked by previous methods and offering insights into their failure modes.", "source": "arxiv", "arxiv_id": "2510.12985v1", "pdf_url": "https://arxiv.org/pdf/2510.12985v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-14T20:53:51Z", "updated": "2025-10-14T20:53:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents", "authors": ["Jonathan Kutasov", "Yuqi Sun", "Paul Colognese", "Teun van der Weij", "Linda Petrini", "Chen Bo Calvin Zhang", "John Hughes", "Xiang Deng", "Henry Sleight", "Tyler Tracy", "Buck Shlegeris", "Joe Benton"], "year": 2025, "url": "http://arxiv.org/abs/2506.15740v2", "abstract": "As Large Language Models (LLMs) are increasingly deployed as autonomous agents in complex and long horizon settings, it is critical to evaluate their ability to sabotage users by pursuing hidden objectives. We study the ability of frontier LLMs to evade monitoring and achieve harmful hidden goals while completing a wide array of realistic tasks. We evaluate a broad range of frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena, the first highly diverse agent evaluation dataset for sabotage and monitoring capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign main tasks and harmful side objectives in complicated environments. Agents are evaluated on their ability to complete the side task without appearing suspicious to an LLM monitor. When measuring agent ability to (a) complete the main task, (b) complete the side task, and (c) avoid detection, we find that the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15% (Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For current frontier models, success on the side task relies heavily on having access to a hidden scratchpad that is not visible to the monitor. We also use SHADE-Arena to measure models' monitoring abilities, with the top monitor (Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign transcripts. We find that for now, models still struggle at sabotage due to failures in long-context main task execution. However, our measurements already demonstrate the difficulty of monitoring for subtle sabotage attempts, which we expect to only increase in the face of more complex and longer-horizon tasks.", "source": "arxiv", "arxiv_id": "2506.15740v2", "pdf_url": "https://arxiv.org/pdf/2506.15740v2", "categories": ["cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-17T15:46:15Z", "updated": "2025-07-08T21:23:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows", "authors": ["Jingwen Zhou", "Jieshan Chen", "Qinghua Lu", "Dehai Zhao", "Liming Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2508.07935v1", "abstract": "Large Language Model (LLM) agentic systems are software systems powered by LLMs that autonomously reason, plan, and execute multi-step workflows to achieve human goals, rather than merely executing predefined steps. During execution, these workflows frequently encounter exceptions. Existing exception handling solutions often treat exceptions superficially, failing to trace execution-phase exceptions to their reasoning-phase root causes. Furthermore, their recovery logic is brittle, lacking structured escalation pathways when initial attempts fail. To tackle these challenges, we first present a comprehensive taxonomy of 36 exception types across 12 agent artifacts. Building on this, we propose SHIELDA (Structured Handling of Exceptions in LLM-Driven Agentic Workflows), a modular runtime exception handling framework for LLM agentic workflows. SHIELDA uses an exception classifier to select a predefined exception handling pattern from a handling pattern registry. These patterns are then executed via a structured handling executor, comprising local handling, flow control, and state recovery, to enable phase-aware recovery by linking exceptions to their root causes and facilitating composable strategies. We validate SHIELDA's effectiveness through a case study on the AutoPR agent, demonstrating effective, cross-phase recovery from a reasoning-induced exception.", "source": "arxiv", "arxiv_id": "2508.07935v1", "pdf_url": "https://arxiv.org/pdf/2508.07935v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-11T12:50:46Z", "updated": "2025-08-11T12:50:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SID: Multi-LLM Debate Driven by Self Signals", "authors": ["Xuhang Chen", "Zhifan Song", "Deyi Ji", "Shuo Gao", "Lanyun Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2510.06843v1", "abstract": "Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\\href{https://github.com/xuhang2019/SID}{\\texttt{https://github.com/xuhang2019/SID}}.", "source": "arxiv", "arxiv_id": "2510.06843v1", "pdf_url": "https://arxiv.org/pdf/2510.06843v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-08T10:10:11Z", "updated": "2025-10-08T10:10:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SIGN: Schema-Induced Games for Naming", "authors": ["Ryan Zhang", "Herbert WoisetschlÃ¤ger"], "year": 2025, "url": "http://arxiv.org/abs/2510.21855v1", "abstract": "Real-world AI systems are tackling increasingly complex problems, often through interactions among large language model (LLM) agents. When these agents develop inconsistent conventions, coordination can break down. Applications such as collaborative coding and distributed planning therefore require reliable, consistent communication, and scalability is a central concern as systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming game that examines how lightweight structure can steer convention formation. We compare schema-induced communication to unconstrained natural language and find faster convergence with up to 5.8x higher agreement. These results suggest that minimal structure can act as a simple control knob for efficient multi-agent coordination, pointing toward broader applications beyond the naming game.", "source": "arxiv", "arxiv_id": "2510.21855v1", "pdf_url": "https://arxiv.org/pdf/2510.21855v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-22T23:12:06Z", "updated": "2025-10-22T23:12:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning", "authors": ["Kaiwen Zhou", "Ahmed Elgohary", "A S M Iftekhar", "Amin Saied"], "year": 2025, "url": "http://arxiv.org/abs/2510.26037v1", "abstract": "The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover various risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models.", "source": "arxiv", "arxiv_id": "2510.26037v1", "pdf_url": "https://arxiv.org/pdf/2510.26037v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-30T00:32:58Z", "updated": "2025-10-30T00:32:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks", "authors": ["Zijian Song", "Xiaoxin Lin", "Qiuming Huang", "Guangrun Wang", "Liang Lin"], "year": 2025, "url": "http://arxiv.org/abs/2506.14512v3", "abstract": "Large Language Models (LLMs) have undergone rapid progress, largely attributed to reinforcement learning on complex reasoning tasks. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic study of their complex spatial reasoning remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMs' structural spatial intelligence through spatial-grounded reasoning tasks. SIRI-Bench comprises 9,000 video-question-answer triplets, where each problem is embedded in a realistic 3D scene. The benchmark is carefully designed so that solving each problem requires both spatial comprehension and structural reasoning. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine that employs collaborative LLM agents to translate abstract mathematical problems into faithful 3D scenes. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of structural spatial reasoning. We hope that our study will bring researchers' attention to spatially grounded reasoning and advance VLMs in visual problem-solving.", "source": "arxiv", "arxiv_id": "2506.14512v3", "pdf_url": "https://arxiv.org/pdf/2506.14512v3", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-06-17T13:40:00Z", "updated": "2025-10-17T02:36:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use", "authors": ["Changhyun Jeon", "Jinhee Park", "Jungwoo Choi", "Keonwoo Kim", "Jisu Kim", "Minji Hong"], "year": 2025, "url": "http://arxiv.org/abs/2509.19369v1", "abstract": "We propose a small-scale language model (SLM) based agent architecture, Planner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G separates planning, calling, and generation by role: the Planner produces an initial batch plan with limited on-demand replanning; the Caller returns a normalized call object after joint schema-value validation; and the Generator integrates tool outputs to produce the final answer. We apply a Korean-first value policy to reduce execution failures caused by frequent Korean-to-English code switching in Korean settings. Evaluation assumes Korean queries and Korean tool/parameter specifications; it covers single-chain, multi-chain, missing-parameters, and missing-functions scenarios, and is conducted via an LLM-as-a-Judge protocol averaged over five runs under a unified I/O interface. Results show that P-C-G delivers competitive tool-use accuracy and end-to-end quality while reducing tokens and maintaining acceptable latency, indicating that role-specialized SLMs are a cost-effective alternative for Korean tool-use agents.", "source": "arxiv", "arxiv_id": "2509.19369v1", "pdf_url": "https://arxiv.org/pdf/2509.19369v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-19T06:25:23Z", "updated": "2025-09-19T06:25:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making", "authors": ["Yinsheng Wang", "Tario G You", "LÃ©onard Boussioux", "Shan Liu"], "year": 2025, "url": "http://arxiv.org/abs/2511.15202v1", "abstract": "This paper introduces SOLID (Synergizing Optimization and Large Language Models for Intelligent Decision-Making), a novel framework that integrates mathematical optimization with the contextual capabilities of large language models (LLMs). SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties. This interaction improves the quality of the decisions while maintaining modularity and data privacy. The framework retains theoretical convergence guarantees under convexity assumptions, providing insight into the design of LLMs prompt. To evaluate SOLID, we applied it to a stock portfolio investment case with historical prices and financial news as inputs. Empirical results demonstrate convergence under various scenarios and indicate improved annualized returns compared to a baseline optimizer-only method, validating the synergy of the two agents. SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains.", "source": "arxiv", "arxiv_id": "2511.15202v1", "pdf_url": "https://arxiv.org/pdf/2511.15202v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-19T07:44:36Z", "updated": "2025-11-19T07:44:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents", "authors": ["Subhrangshu Nandi", "Arghya Datta", "Nikhil Vichare", "Indranil Bhattacharya", "Huzefa Raja", "Jing Xu", "Shayan Ray", "Giuseppe Carenini", "Abhi Srivastava", "Aaron Chan", "Man Ho Woo", "Amar Kandola", "Brandon Theresa", "Francesco Carbone"], "year": 2025, "url": "http://arxiv.org/abs/2506.08119v1", "abstract": "Large Language Models (LLMs) demonstrate impressive general-purpose reasoning and problem-solving abilities. However, they struggle with executing complex, long-horizon workflows that demand strict adherence to Standard Operating Procedures (SOPs), a critical requirement for real-world industrial automation. Despite this need, there is a lack of public benchmarks that reflect the complexity, structure, and domain-specific nuances of SOPs. To address this, we present three main contributions. First, we introduce a synthetic data generation framework to create realistic, industry-grade SOPs that rigorously test the planning, reasoning, and tool-use capabilities of LLM-based agents. Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases. Third, we evaluate two prominent agent architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing average success rates of only 27% and 48%, respectively. Remarkably, when the tool registry is much larger than necessary, agents invoke incorrect tools nearly 100% of the time. These findings underscore a substantial gap between current agentic capabilities of LLMs and the demands of automating real-world SOPs. Performance varies significantly by task and domain, highlighting the need for domain-specific benchmarking and architectural choices before deployment. SOP-Bench is publicly available at http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the prompts underpinning the data generation framework to support new domain-specific SOP benchmarks. We invite the community to extend SOP-Bench with SOPs from their industrial domains.", "source": "arxiv", "arxiv_id": "2506.08119v1", "pdf_url": "https://arxiv.org/pdf/2506.08119v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-09T18:20:12Z", "updated": "2025-06-09T18:20:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation", "authors": ["Xuhui Zhou", "Zhe Su", "Sophie Feng", "Jiaxu Zhou", "Jen-tse Huang", "Hsien-Te Kao", "Spencer Lynch", "Svitlana Volkova", "Tongshuang Sherry Wu", "Anita Woolley", "Hao Zhu", "Maarten Sap"], "year": 2025, "url": "http://arxiv.org/abs/2504.16122v1", "abstract": "Social simulation through large language model (LLM) agents is a promising approach to explore and validate hypotheses related to social science questions and LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable social simulation system that addresses the technical barriers of current frameworks while enabling practitioners to generate multi-turn and multi-party LLM-based interactions with customizable evaluation metrics for hypothesis testing. SOTOPIA-S4 comes as a pip package that contains a simulation engine, an API server with flexible RESTful APIs for simulation management, and a web interface that enables both technical and non-technical users to design, run, and analyze simulations without programming. We demonstrate the usefulness of SOTOPIA-S4 with two use cases involving dyadic hiring negotiation and multi-party planning scenarios.", "source": "arxiv", "arxiv_id": "2504.16122v1", "pdf_url": "https://arxiv.org/pdf/2504.16122v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-04-19T20:02:59Z", "updated": "2025-04-19T20:02:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jiashuo Wang", "Jian Wang", "Wenjie Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.20732v1", "abstract": "Reinforcement learning (RL) holds significant promise for training LLM agents to handle complex, goal-oriented tasks that require multi-step interactions with external environments. However, a critical challenge when applying RL to these agentic tasks arises from delayed rewards: feedback signals are typically available only after the entire task is completed. This makes it non-trivial to assign delayed rewards to earlier actions, providing insufficient guidance regarding environmental constraints and hindering agent training. In this work, we draw on the insight that the ultimate completion of a task emerges from the cumulative progress an agent makes across individual steps. We propose Stepwise Progress Attribution (SPA), a general reward redistribution framework that decomposes the final reward into stepwise contributions, each reflecting its incremental progress toward overall task completion. To achieve this, we train a progress estimator that accumulates stepwise contributions over a trajectory to match the task completion. During policy optimization, we combine the estimated per-step contribution with a grounding signal for actions executed in the environment as the fine-grained, intermediate reward for effective agent training. Extensive experiments on common agent benchmarks (including Webshop, ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the state-of-the-art method in both success rate (+2.5\\% on average) and grounding accuracy (+1.9\\% on average). Further analyses demonstrate that our method remarkably provides more effective intermediate rewards for RL training. Our code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.", "source": "arxiv", "arxiv_id": "2505.20732v1", "pdf_url": "https://arxiv.org/pdf/2505.20732v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-27T05:21:04Z", "updated": "2025-05-27T05:21:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking", "authors": ["Yujin Roh", "Inho Jake Park", "Chigon Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2512.20975v2", "abstract": "CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.", "source": "arxiv", "arxiv_id": "2512.20975v2", "pdf_url": "https://arxiv.org/pdf/2512.20975v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-12-24T06:04:58Z", "updated": "2026-01-14T14:06:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent", "authors": ["Keyeun Lee", "Seo Hyeong Kim", "Seolhee Lee", "Jinsu Eun", "Yena Ko", "Hayeon Jeon", "Esther Hehsun Kim", "Seonghye Cho", "Soeun Yang", "Eun-mee Kim", "Hajin Lim"], "year": 2025, "url": "http://arxiv.org/abs/2502.08599v1", "abstract": "Existing methods for simulating individual identities often oversimplify human complexity, which may lead to incomplete or flattened representations. To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual's multidimensional self-concept. SPeCtrum integrates three core components: Social Identity (S), Personal Identity (P), and Personal Life Context (C), each contributing distinct yet interconnected aspects of identity. To evaluate SPeCtrum's effectiveness in identity representation, we conducted automated and human evaluations. Automated evaluations using popular drama characters showed that Personal Life Context (C)-derived from short essays on preferences and daily routines-modeled characters' identities more effectively than Social Identity (S) and Personal Identity (P) alone and performed comparably to the full SPC combination. In contrast, human evaluations involving real-world individuals found that the full SPC combination provided a more comprehensive self-concept representation than C alone. Our findings suggest that while C alone may suffice for basic identity simulation, integrating S, P, and C enhances the authenticity and accuracy of real-world identity representation. Overall, SPeCtrum offers a structured approach for simulating individuals in LLM agents, enabling more personalized human-AI interactions and improving the realism of simulation-based behavioral studies.", "source": "arxiv", "arxiv_id": "2502.08599v1", "pdf_url": "https://arxiv.org/pdf/2502.08599v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-12T17:38:27Z", "updated": "2025-02-12T17:38:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing", "authors": ["Varun Biyyala", "Bharat Chanderprakash Kathuria", "Jialu Li", "Youshan Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2501.07554v1", "abstract": "Video editing models have advanced significantly, but evaluating their performance remains challenging. Traditional metrics, such as CLIP text and image scores, often fall short: text scores are limited by inadequate training data and hierarchical dependencies, while image scores fail to assess temporal consistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation Metric), a novel evaluation framework that leverages modern Vision-Language Models (VLMs), Object Detection, and Temporal Consistency checks. SST-EM comprises four components: (1) semantic extraction from frames using a VLM, (2) primary object tracking with Object Detection, (3) focused object refinement via an LLM agent, and (4) temporal consistency assessment using a Vision Transformer (ViT). These components are integrated into a unified metric with weights derived from human evaluations and regression analysis. The name SST-EM reflects its focus on Semantic, Spatial, and Temporal aspects of video evaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and temporal smoothness in video editing. The source code is available in the \\textbf{\\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub Repository}}.", "source": "arxiv", "arxiv_id": "2501.07554v1", "pdf_url": "https://arxiv.org/pdf/2501.07554v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-01-13T18:37:08Z", "updated": "2025-01-13T18:37:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training", "authors": ["Chenliang Li", "Adel Elmahdy", "Alex Boyd", "Zhongruo Wang", "Alfredo Garcia", "Parminder Bhatia", "Taha Kass-Hout", "Cao Xiao", "Mingyi Hong"], "year": 2025, "url": "http://arxiv.org/abs/2511.20718v1", "abstract": "PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.", "source": "arxiv", "arxiv_id": "2511.20718v1", "pdf_url": "https://arxiv.org/pdf/2511.20718v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-25T05:54:02Z", "updated": "2025-11-25T05:54:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents", "authors": ["Jing-Jing Li", "Jianfeng He", "Chao Shang", "Devang Kulshreshtha", "Xun Xian", "Yi Zhang", "Hang Su", "Sandesh Swamy", "Yanjun Qi"], "year": 2025, "url": "http://arxiv.org/abs/2509.25624v1", "abstract": "As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges that extend beyond traditional content-based LLM safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC), a novel multi-turn attack framework that exploits agent tool use. STAC chains together tool calls that each appear harmless in isolation but, when combined, collectively enable harmful operations that only become apparent at the final execution step. We apply our framework to automatically generate and systematically evaluate 483 STAC cases, featuring 1,352 sets of user-agent-environment interactions and spanning diverse domains, tasks, agent types, and 10 failure modes. Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases. The core design of STAC's automated framework is a closed-loop pipeline that synthesizes executable multi-step tool chains, validates them through in-environment execution, and reverse-engineers stealthy multi-turn prompts that reliably induce agents to execute the verified malicious sequence. We further perform defense analysis against STAC and find that existing prompt-based defenses provide limited protection. To address this gap, we propose a new reasoning-driven defense prompt that achieves far stronger protection, cutting ASR by up to 28.8%. These results highlight a crucial gap: defending tool-enabled agents requires reasoning over entire action sequences and their cumulative effects, rather than evaluating isolated prompts or responses.", "source": "arxiv", "arxiv_id": "2509.25624v1", "pdf_url": "https://arxiv.org/pdf/2509.25624v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-30T00:31:44Z", "updated": "2025-09-30T00:31:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "STARK: Strategic Team of Agents for Refining Kernels", "authors": ["Juncheng Dong", "Yang Yang", "Tao Liu", "Yang Wang", "Feng Qi", "Vahid Tarokh", "Kaushik Rangadurai", "Shuang Yang"], "year": 2025, "url": "http://arxiv.org/abs/2510.16996v1", "abstract": "The efficiency of GPU kernels is central to the progress of modern AI, yet optimizing them remains a difficult and labor-intensive task due to complex interactions between memory hierarchies, thread scheduling, and hardware-specific characteristics. While recent advances in large language models (LLMs) provide new opportunities for automated code generation, existing approaches largely treat LLMs as single-shot generators or naive refinement tools, limiting their effectiveness in navigating the irregular kernel optimization landscape. We introduce an LLM agentic framework for GPU kernel optimization that systematically explores the design space through multi-agent collaboration, grounded instruction, dynamic context management, and strategic search. This framework mimics the workflow of expert engineers, enabling LLMs to reason about hardware trade-offs, incorporate profiling feedback, and refine kernels iteratively. We evaluate our approach on KernelBench, a benchmark for LLM-based kernel optimization, and demonstrate substantial improvements over baseline agents: our system produces correct solutions where baselines often fail, and achieves kernels with up to 16x faster runtime performance. These results highlight the potential of agentic LLM frameworks to advance fully automated, scalable GPU kernel optimization.", "source": "arxiv", "arxiv_id": "2510.16996v1", "pdf_url": "https://arxiv.org/pdf/2510.16996v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-19T20:41:46Z", "updated": "2025-10-19T20:41:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "STELLA: Self-Evolving LLM Agent for Biomedical Research", "authors": ["Ruofan Jin", "Zaixi Zhang", "Mengdi Wang", "Le Cong"], "year": 2025, "url": "http://arxiv.org/abs/2507.02004v1", "abstract": "The rapid growth of biomedical data, tools, and literature has created a fragmented research landscape that outpaces human expertise. While AI agents offer a solution, they typically rely on static, manually curated toolsets, limiting their ability to adapt and scale. Here, we introduce STELLA, a self-evolving AI agent designed to overcome these limitations. STELLA employs a multi-agent architecture that autonomously improves its own capabilities through two core mechanisms: an evolving Template Library for reasoning strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent automatically discovers and integrates new bioinformatics tools. This allows STELLA to learn from experience. We demonstrate that STELLA achieves state-of-the-art accuracy on a suite of biomedical benchmarks, scoring approximately 26\\% on Humanity's Last Exam: Biomedicine, 54\\% on LAB-Bench: DBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6 percentage points. More importantly, we show that its performance systematically improves with experience; for instance, its accuracy on the Humanity's Last Exam benchmark almost doubles with increased trials. STELLA represents a significant advance towards AI Agent systems that can learn and grow, dynamically scaling their expertise to accelerate the pace of biomedical discovery.", "source": "arxiv", "arxiv_id": "2507.02004v1", "pdf_url": "https://arxiv.org/pdf/2507.02004v1", "categories": ["cs.AI", "cs.CL", "q-bio.BM"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-01T20:52:01Z", "updated": "2025-07-01T20:52:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning", "authors": ["Hanlin Wang", "Jian Wang", "Chak Tou Leong", "Wenjie Li"], "year": 2025, "url": "http://arxiv.org/abs/2502.14276v2", "abstract": "Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations or preference learning through exploratory trajectory sampling. However, these methods often struggle to address long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. We finally leverage these calibrated trajectories with successful trajectories for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that timely calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.", "source": "arxiv", "arxiv_id": "2502.14276v2", "pdf_url": "https://arxiv.org/pdf/2502.14276v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-20T05:28:44Z", "updated": "2025-05-29T16:13:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs", "authors": ["Minh V. T. Pham", "Huy N. Phan", "Hoang N. Phan", "Cuong Le Chi", "Tien N. Nguyen", "Nghi D. Q. Bui"], "year": 2025, "url": "http://arxiv.org/abs/2504.14757v2", "abstract": "Large language models (LLMs) are transforming automated program repair (APR) through agent-based approaches that localize bugs, generate patches, and verify fixes. However, the lack of high-quality, scalable training datasets, especially those with verifiable outputs and intermediate reasoning traces-limits progress, particularly for open-source models. In this work, we present SWE-Synth, a framework for synthesizing realistic, verifiable, and process-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM agents to simulate debugging workflows, producing not only bug-fix pairs but also test cases and structured repair trajectories. Compared to manually curated datasets, our method scales with minimal human effort while preserving contextual richness and correctness. Experiments show that models trained on SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench Lite. Our results highlight the potential of synthetic, agent-generated data to advance the state of the art in APR and software engineering automation.", "source": "arxiv", "arxiv_id": "2504.14757v2", "pdf_url": "https://arxiv.org/pdf/2504.14757v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-04-20T22:37:43Z", "updated": "2025-12-20T19:11:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks", "authors": ["Yifei Zhou", "Song Jiang", "Yuandong Tian", "Jason Weston", "Sergey Levine", "Sainbayar Sukhbaatar", "Xian Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.15478v1", "abstract": "Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation.", "source": "arxiv", "arxiv_id": "2503.15478v1", "pdf_url": "https://arxiv.org/pdf/2503.15478v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-19T17:55:08Z", "updated": "2025-03-19T17:55:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator", "authors": ["Xueyang Zhou", "Weidong Wang", "Lin Lu", "Jiawen Shi", "Guiyao Tie", "Yongtian Xu", "Lixing Chen", "Pan Zhou", "Neil Zhenqiang Gong", "Lichao Sun"], "year": 2025, "url": "http://arxiv.org/abs/2505.17735v2", "abstract": "Large Language Model (LLM)-based agents are increasingly deployed in real-world applications such as \"digital assistants, autonomous customer service, and decision-support systems\", where their ability to \"interact in multi-turn, tool-augmented environments\" makes them indispensable. However, ensuring the safety of these agents remains a significant challenge due to the diverse and complex risks arising from dynamic user interactions, external tool usage, and the potential for unintended harmful behaviors. To address this critical issue, we propose AutoSafe, the first framework that systematically enhances agent safety through fully automated synthetic data generation. Concretely, 1) we introduce an open and extensible threat model, OTS, which formalizes how unsafe behaviors emerge from the interplay of user instructions, interaction contexts, and agent actions. This enables precise modeling of safety risks across diverse scenarios. 2) we develop a fully automated data generation pipeline that simulates unsafe user behaviors, applies self-reflective reasoning to generate safe responses, and constructs a large-scale, diverse, and high-quality safety training dataset-eliminating the need for hazardous real-world data collection. To evaluate the effectiveness of our framework, we design comprehensive experiments on both synthetic and real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety scores by 45% on average and achieves a 28.91% improvement on real-world tasks, validating the generalization ability of our learned safety strategies. These results highlight the practical advancement and scalability of AutoSafe in building safer LLM-based agents for real-world deployment. We have released the project page at https://auto-safe.github.io/.", "source": "arxiv", "arxiv_id": "2505.17735v2", "pdf_url": "https://arxiv.org/pdf/2505.17735v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-23T10:56:06Z", "updated": "2025-07-18T07:34:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents", "authors": ["Ruolin Chen", "Yinqian Sun", "Jihang Wang", "Mingyang Lv", "Qian Zhang", "Yi Zeng"], "year": 2025, "url": "http://arxiv.org/abs/2509.25885v1", "abstract": "Embodied agents powered by large language models (LLMs) inherit advanced planning capabilities; however, their direct interaction with the physical world exposes them to safety vulnerabilities. In this work, we identify four key reasoning stages where hazards may arise: Task Understanding, Environment Perception, High-Level Plan Generation, and Low-Level Action Generation. We further formalize three orthogonal safety constraint types (Factual, Causal, and Temporal) to systematically characterize potential safety violations. Building on this risk model, we present SafeMindBench, a multimodal benchmark with 5,558 samples spanning four task categories (Instr-Risk, Env-Risk, Order-Fix, Req-Align) across high-risk scenarios such as sabotage, harm, privacy, and illegal behavior. Extensive experiments on SafeMindBench reveal that leading LLMs (e.g., GPT-4o) and widely used embodied agents remain susceptible to safety-critical failures. To address this challenge, we introduce SafeMindAgent, a modular Planner-Executor architecture integrated with three cascaded safety modules, which incorporate safety constraints into the reasoning process. Results show that SafeMindAgent significantly improves safety rate over strong baselines while maintaining comparable task completion. Together, SafeMindBench and SafeMindAgent provide both a rigorous evaluation suite and a practical solution that advance the systematic study and mitigation of safety risks in embodied LLM agents.", "source": "arxiv", "arxiv_id": "2509.25885v1", "pdf_url": "https://arxiv.org/pdf/2509.25885v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T07:24:04Z", "updated": "2025-09-30T07:24:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents", "authors": ["Kunlun Zhu", "Jiaxun Zhang", "Ziheng Qi", "Nuoxing Shang", "Zijia Liu", "Peixuan Han", "Yue Su", "Haofei Yu", "Jiaxuan You"], "year": 2025, "url": "http://arxiv.org/abs/2505.23559v1", "abstract": "Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce \\textbf{SafeScientist}, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. \\textcolor{red}{Warning: this paper contains example data that may be offensive or harmful.}", "source": "arxiv", "arxiv_id": "2505.23559v1", "pdf_url": "https://arxiv.org/pdf/2505.23559v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-29T15:35:58Z", "updated": "2025-05-29T15:35:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting", "authors": ["Michael Y. Hu", "Benjamin Van Durme", "Jacob Andreas", "Harsh Jhamtani"], "year": 2025, "url": "http://arxiv.org/abs/2510.10304v2", "abstract": "Language model (LM) agents deployed in novel environments often exhibit poor sample efficiency when learning from sequential interactions. This significantly hinders the usefulness of such agents in environments where interaction is costly (for example, when they interact with humans or reset physical systems). While a number of existing LM agent architectures incorporate various mechanisms for experience storage and reflection, they make limited use of LMs' abilities to directly generate or reason about full counterfactual trajectories. We introduce ECHO (Experience Consolidation via Hindsight Optimization), a prompting framework that adapts hindsight experience replay from reinforcement learning for language model agents. ECHO generates optimized trajectories for alternative goals that could have been achieved during failed attempts, effectively creating synthetic positive examples from unsuccessful interactions. Our approach consists of two components: a hindsight rule that uses the language model itself to identify relevant subgoals and generate optimized trajectories, and an update rule that maintains compressed trajectory representations in memory. We evaluate ECHO on stateful versions of XMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a collaborative information-gathering enterprise simulation. Across both domains, ECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid, it also outperforms a number of sophisticated agent architectures including Reflexion and AWM, demonstrating faster adaptation to novel environments through more effective utilization of past experiences.", "source": "arxiv", "arxiv_id": "2510.10304v2", "pdf_url": "https://arxiv.org/pdf/2510.10304v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-11T18:11:09Z", "updated": "2026-01-02T19:54:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Scalable Supervising Software Agents with Patch Reasoner", "authors": ["Junjielong Xu", "Boyin Tan", "Xiaoyuan Liu", "Chao Peng", "Pengfei Gao", "Pinjia He"], "year": 2025, "url": "http://arxiv.org/abs/2510.22775v1", "abstract": "While large language model agents have advanced software engineering tasks, the unscalable nature of existing test-based supervision is limiting the potential improvement of data scaling. The reason is twofold: (1) building and running test sandbox is rather heavy and fragile, and (2) data with high-coverage tests is naturally rare and threatened by test hacking via edge cases. In this paper, we propose R4P, a patch verifier model to provide scalable rewards for training and testing SWE agents via reasoning. We consider that patch verification is fundamentally a reasoning task, mirroring how human repository maintainers review patches without writing and running new reproduction tests. To obtain sufficient reference and reduce the risk of reward hacking, R4P uses a group-wise objective for RL training, enabling it to verify multiple patches against each other's modification and gain a dense reward for stable training. R4P achieves 72.2% Acc. for verifying patches from SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we design and train a lite scaffold, Mini-SE, with pure reinforcement learning where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2% Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original Qwen3-32B. This can be further improved to 32.8% with R4P for test-time scaling. Furthermore, R4P verifies patches within a second, 50x faster than testing on average. The stable scaling curves of rewards and accuracy along with high efficiency reflect R4P's practicality.", "source": "arxiv", "arxiv_id": "2510.22775v1", "pdf_url": "https://arxiv.org/pdf/2510.22775v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-26T17:52:05Z", "updated": "2025-10-26T17:52:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "year": 2025, "url": "http://arxiv.org/abs/2505.06416v1", "abstract": "Recent advancements in Large Language Models (LLMs) and the introduction of the Model Context Protocol (MCP) have significantly expanded LLM agents' capability to interact dynamically with external tools and APIs. However, existing tool selection frameworks do not integrate MCP servers, instead relying heavily on error-prone manual updates to monolithic local tool repositories, leading to duplication, inconsistencies, and inefficiencies. Additionally, current approaches abstract tool selection before the LLM agent is invoked, limiting its autonomy and hindering dynamic re-querying capabilities during multi-turn interactions. To address these issues, we introduce ScaleMCP, a novel tool selection approach that dynamically equips LLM agents with a MCP tool retriever, giving agents the autonomy to add tools into their memory, as well as an auto-synchronizing tool storage system pipeline through CRUD (create, read, update, delete) operations with MCP servers as the single source of truth. We also propose a novel embedding strategy, Tool Document Weighted Average (TDWA), designed to selectively emphasize critical components of tool documents (e.g. tool name or synthetic questions) during the embedding process. Comprehensive evaluations conducted on a created dataset of 5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models, and 5 retriever types, demonstrate substantial improvements in tool retrieval and agent invocation performance, emphasizing ScaleMCP's effectiveness in scalable, dynamic tool selection and invocation.", "source": "arxiv", "arxiv_id": "2505.06416v1", "pdf_url": "https://arxiv.org/pdf/2505.06416v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-09T20:30:37Z", "updated": "2025-05-09T20:30:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Scaling Autonomous Agents via Automatic Reward Modeling And Planning", "authors": ["Zhenfang Chen", "Delin Chen", "Rui Sun", "Wenjun Liu", "Chuang Gan"], "year": 2025, "url": "http://arxiv.org/abs/2502.12130v1", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving. Unlike pure text data, collecting large-scale decision-making data is challenging. Moreover, many powerful LLMs are only accessible through APIs, which hinders their fine-tuning for agent tasks due to cost and complexity. To address LLM agents' limitations, we propose a framework that can automatically learn a reward model from the environment without human annotations. This model can be used to evaluate the action trajectories of LLM agents and provide heuristics for task planning. Specifically, our approach involves employing one LLM-based agent to navigate an environment randomly, generating diverse action trajectories. Subsequently, a separate LLM is leveraged to assign a task intent and synthesize a negative response alongside the correct response for each trajectory. These triplets (task intent, positive response, and negative response) are then utilized as training data to optimize a reward model capable of scoring action trajectories. The effectiveness and generalizability of our framework are demonstrated through evaluations conducted on different agent benchmarks. In conclusion, our proposed framework represents a significant advancement in enhancing LLM agents' decision-making capabilities. By automating the learning of reward models, we overcome the challenges of data scarcity and API limitations, potentially revolutionizing the application of LLMs in complex and interactive environments. This research paves the way for more sophisticated AI agents capable of tackling a wide range of real-world problems requiring multi-step decision-making.", "source": "arxiv", "arxiv_id": "2502.12130v1", "pdf_url": "https://arxiv.org/pdf/2502.12130v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T18:49:25Z", "updated": "2025-02-17T18:49:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Scaling Clinician-Grade Feature Generation from Clinical Notes with Multi-Agent Language Models", "authors": ["Jiayi Wang", "Jacqueline Jil Vallon", "Nikhil V. Kotha", "Neil Panjwani", "Xi Ling", "Margaret Redfield", "Sushmita Vij", "Sandy Srinivas", "John Leppert", "Mark K. Buyyounouski", "Mohsen Bayati"], "year": 2025, "url": "http://arxiv.org/abs/2508.01956v2", "abstract": "Developing accurate clinical prediction models is often bottlenecked by the difficulty of deriving meaningful structured features from unstructured EHR notes, a process that traditionally requires manual, unscalable clinical abstraction. In this study, we first established a rigorous patient-level Clinician Feature Generation (CFG) protocol, in which domain experts manually reviewed notes to define and extract nuanced features for a cohort of 147 patients with prostate cancer. As a high-fidelity ground truth, this labor-intensive process provided the blueprint for SNOW (Scalable Note-to-Outcome Workflow), a transparent multi-agent large language model (LLM) system designed to autonomously mimic the iterative reasoning and validation workflow of clinical experts. On 5-year cancer recurrence prediction, SNOW (AUC-ROC 0.767) achieved performance comparable to manual CFG (0.762) and outperformed structured baselines, clinician-guided LLM extraction, and six representational feature generation (RFG) approaches. Once configured, SNOW produced the full patient-level feature table in 12 hours with 5 hours of clinician oversight, reducing human expert effort by approximately 48-fold versus manual CFG. To test scalability where manual CFG is infeasible, we deployed SNOW on an external heart failure with preserved ejection fraction (HFpEF) cohort from MIMIC-IV (n=2,084); without task-specific tuning, SNOW generated prognostic features that outperformed baseline and RFG methods for 30-day (SNOW: 0.851) and 1-year (SNOW: 0.763) mortality prediction. These results demonstrate that a modular LLM agent-based system can scale expert-level feature generation from clinical notes, while enabling interpretable use of unstructured EHR text in outcome prediction and preserving generalizability across a variety of settings and conditions.", "source": "arxiv", "arxiv_id": "2508.01956v2", "pdf_url": "https://arxiv.org/pdf/2508.01956v2", "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-03T23:45:18Z", "updated": "2025-12-28T17:34:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Scaling Long-Horizon LLM Agent via Context-Folding", "authors": ["Weiwei Sun", "Miao Lu", "Zhan Ling", "Kang Liu", "Xuesong Yao", "Yiming Yang", "Jiecao Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.11967v1", "abstract": "Large language model (LLM) agents are fundamentally constrained by context length on long-horizon tasks. We introduce Context-Folding, a framework that empowers agents to actively manage their working context. An agent can procedurally branch into a sub-trajectory to handle a subtask and then fold it upon completion, collapsing the intermediate steps while retaining a concise summary of the outcome. To make this behavior learnable, we develop an end-to-end reinforcement learning framework FoldGRPO with specific process rewards to encourage effective task decomposition and context management. On complex long-horizon tasks (Deep Research and SWE), our folding agent matches or outperforms the ReAct baselines while using an active context 10$\\times$ smaller and significantly outperforms models that rely on summarization-based context management.", "source": "arxiv", "arxiv_id": "2510.11967v1", "pdf_url": "https://arxiv.org/pdf/2510.11967v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-13T22:00:58Z", "updated": "2025-10-13T22:00:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Scaling Test-time Compute for LLM Agents", "authors": ["King Zhu", "Hanhao Li", "Siwei Wu", "Tianshun Xing", "Dehua Ma", "Xiangru Tang", "Minghao Liu", "Jian Yang", "Jiaheng Liu", "Yuchen Eleanor Jiang", "Changwang Zhang", "Chenghua Lin", "Jun Wang", "Ge Zhang", "Wangchunshu Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2506.12928v1", "abstract": "Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying rollouts.We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents, and have follow findings: 1. Scaling test time compute could improve the performance of agents. 2. Knowing when to reflect is important for agents. 3. Among different verification and result merging approaches, the list-wise method performs best. 4. Increasing diversified rollouts exerts a positive effect on the agent's task performance.", "source": "arxiv", "arxiv_id": "2506.12928v1", "pdf_url": "https://arxiv.org/pdf/2506.12928v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-15T17:59:47Z", "updated": "2025-06-15T17:59:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent", "authors": ["Xingjian Kang", "Linda Vorberg", "Andreas Maier", "Alexander Katzmann", "Oliver Taubmann"], "year": 2025, "url": "http://arxiv.org/abs/2509.20270v1", "abstract": "Managing scan protocols in Computed Tomography (CT), which includes adjusting acquisition parameters or configuring reconstructions, as well as selecting postprocessing tools in a patient-specific manner, is time-consuming and requires clinical as well as technical expertise. At the same time, we observe an increasing shortage of skilled workforce in radiology. To address this issue, a Large Language Model (LLM)-based agent framework is proposed to assist with the interpretation and execution of protocol configuration requests given in natural language or a structured, device-independent format, aiming to improve the workflow efficiency and reduce technologists' workload. The agent combines in-context-learning, instruction-following, and structured toolcalling abilities to identify relevant protocol elements and apply accurate modifications. In a systematic evaluation, experimental results indicate that the agent can effectively retrieve protocol components, generate device compatible protocol definition files, and faithfully implement user requests. Despite demonstrating feasibility in principle, the approach faces limitations regarding syntactic and semantic validity due to lack of a unified device API, and challenges with ambiguous or complex requests. In summary, the findings show a clear path towards LLM-based agents for supporting scan protocol management in CT imaging.", "source": "arxiv", "arxiv_id": "2509.20270v1", "pdf_url": "https://arxiv.org/pdf/2509.20270v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-24T16:04:11Z", "updated": "2025-09-24T16:04:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Scheming Ability in LLM-to-LLM Strategic Interactions", "authors": ["Thao Pham"], "year": 2025, "url": "http://arxiv.org/abs/2510.12826v1", "abstract": "As large language model (LLM) agents are deployed autonomously in diverse contexts, evaluating their capacity for strategic deception becomes crucial. While recent research has examined how AI systems scheme against human developers, LLM-to-LLM scheming remains underexplored. We investigate the scheming ability and propensity of frontier LLM agents through two game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro, Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and without explicit prompting while analyzing scheming tactics through chain-of-thought reasoning. When prompted, most models, especially Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance. Critically, models exhibited significant scheming propensity without prompting: all models chose deception over confession in Peer Evaluation (100% rate), while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These findings highlight the need for robust evaluations using high-stakes game-theoretic scenarios in multi-agent settings.", "source": "arxiv", "arxiv_id": "2510.12826v1", "pdf_url": "https://arxiv.org/pdf/2510.12826v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-11T04:42:29Z", "updated": "2025-10-11T04:42:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SciML Agents: Write the Solver, Not the Solution", "authors": ["Saarth Gaonkar", "Xiang Zheng", "Haocheng Xi", "Rishabh Tiwari", "Kurt Keutzer", "Dmitriy Morozov", "Michael W. Mahoney", "Amir Gholami"], "year": 2025, "url": "http://arxiv.org/abs/2509.09936v1", "abstract": "Recent work in scientific machine learning aims to tackle scientific tasks directly by predicting target values with neural networks (e.g., physics-informed neural networks, neural ODEs, neural operators, etc.), but attaining high accuracy and robustness has been challenging. We explore an alternative view: use LLMs to write code that leverages decades of numerical algorithms. This shifts the burden from learning a solution function to making domain-aware numerical choices. We ask whether LLMs can act as SciML agents that, given a natural-language ODE description, generate runnable code that is scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff), and enforcing stability checks. There is currently no benchmark to measure this kind of capability for scientific computing tasks. As such, we first introduce two new datasets: a diagnostic dataset of adversarial \"misleading\" problems; and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set contains problems whose superficial appearance suggests stiffness, and that require algebraic simplification to demonstrate non-stiffness; and the large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open- and closed-source LLM models along two axes: (i) unguided versus guided prompting with domain-specific knowledge; and (ii) off-the-shelf versus fine-tuned variants. Our evaluation measures both executability and numerical validity against reference solutions. We find that with sufficient context and guided prompts, newer instruction-following models achieve high accuracy on both criteria. In many cases, recent open-source systems perform strongly without fine-tuning, while older or smaller models still benefit from fine-tuning. Overall, our preliminary results indicate that careful prompting and fine-tuning can yield a specialized LLM agent capable of reliably solving simple ODE problems.", "source": "arxiv", "arxiv_id": "2509.09936v1", "pdf_url": "https://arxiv.org/pdf/2509.09936v1", "categories": ["cs.LG", "math.NA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-12T02:53:57Z", "updated": "2025-09-12T02:53:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SciSciGPT: Advancing Human-AI Collaboration in the Science of Science", "authors": ["Erzhuo Shao", "Yifang Wang", "Yifan Qian", "Zhenyu Pan", "Han Liu", "Dashun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.05559v3", "abstract": "The increasing availability of large-scale datasets has fueled rapid progress across many scientific fields, creating unprecedented opportunities for research and discovery while posing significant analytical challenges. Recent advances in large language models (LLMs) and AI agents have opened new possibilities for human-AI collaboration, offering powerful tools to navigate this complex research landscape. In this paper, we introduce SciSciGPT, an open-source, prototype AI collaborator that uses the science of science as a testbed to explore the potential of LLM-powered research tools. SciSciGPT automates complex workflows, supports diverse analytical approaches, accelerates research prototyping and iteration, and facilitates reproducibility. Through case studies, we demonstrate its ability to streamline a wide range of empirical and analytical research tasks while highlighting its broader potential to advance research. We further propose an LLM Agent capability maturity model for human-AI collaboration, envisioning a roadmap to further improve and expand upon frameworks like SciSciGPT. As AI capabilities continue to evolve, frameworks like SciSciGPT may play increasingly pivotal roles in scientific research and discovery, unlocking further opportunities. At the same time, these new advances also raise critical challenges, from ensuring transparency and ethical use to balancing human and AI contributions. Addressing these issues may shape the future of scientific inquiry and inform how we train the next generation of scientists to thrive in an increasingly AI-integrated research ecosystem.", "source": "arxiv", "arxiv_id": "2504.05559v3", "pdf_url": "https://arxiv.org/pdf/2504.05559v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-07T23:19:39Z", "updated": "2025-11-27T12:07:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization", "authors": ["Yinjie Wang", "Ling Yang", "Guohao Li", "Mengdi Wang", "Bryon Aragam"], "year": 2025, "url": "http://arxiv.org/abs/2502.04306v1", "abstract": "Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow", "source": "arxiv", "arxiv_id": "2502.04306v1", "pdf_url": "https://arxiv.org/pdf/2502.04306v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-06T18:47:49Z", "updated": "2025-02-06T18:47:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Search-Time Data Contamination", "authors": ["Ziwen Han", "Meher Mankikar", "Julian Michael", "Zifan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.13180v1", "abstract": "Data contamination refers to the leakage of evaluation data into model training data, resulting in overfitting to supposedly held-out test sets and compromising test validity. We identify an analogous issue, search-time contamination (STC), in evaluating search-based LLM agents which use tools to gather information from online sources when answering user queries. STC occurs when the retrieval step surfaces a source containing the test question (or a near-duplicate) alongside its answer, enabling agents to copy rather than genuinely infer or reason, undermining benchmark integrity. We find that HuggingFace, an online platform hosting evaluation datasets, appears among retrieved sources in search based agent logs. Consequently, agents often explicitly acknowledge discovering question answer pairs from HuggingFace within their reasoning chains. On three commonly used capability benchmarks: Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for approximately 3% of questions, search-based agents directly find the datasets with ground truth labels on HuggingFace. When millions of evaluation queries target the same benchmark, even small, repeated leaks can accelerate the benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace is blocked, we observe a drop in accuracy on the contaminated subset of approximately 15%. We further show through ablation experiments that publicly accessible evaluation datasets on HuggingFace may not be the sole source of STC. To this end, we conclude by proposing best practices for benchmark design and result reporting to address this novel form of leakage and ensure trustworthy evaluation of search-based LLM agents. To facilitate the auditing of evaluation results, we also publicly release the complete logs from our experiments.", "source": "arxiv", "arxiv_id": "2508.13180v1", "pdf_url": "https://arxiv.org/pdf/2508.13180v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-12T22:52:21Z", "updated": "2025-08-12T22:52:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Searching for Privacy Risks in LLM Agents via Simulation", "authors": ["Yanzhe Zhang", "Diyi Yang"], "year": 2025, "url": "http://arxiv.org/abs/2508.10880v2", "abstract": "The widespread deployment of LLM-based agents is likely to introduce a critical privacy threat: malicious agents that proactively engage others in multi-turn interactions to extract sensitive information. However, the evolving nature of such dynamic dialogues makes it challenging to anticipate emerging vulnerabilities and design effective defenses. To tackle this problem, we present a search-based framework that alternates between improving attack and defense strategies through the simulation of privacy-critical agent interactions. Specifically, we employ LLMs as optimizers to analyze simulation trajectories and iteratively propose new agent instructions. To explore the strategy space more efficiently, we further utilize parallel search with multiple threads and cross-thread propagation. Through this process, we find that attack strategies escalate from direct requests to sophisticated tactics, such as impersonation and consent forgery, while defenses evolve from simple rule-based constraints to robust identity-verification state machines. The discovered attacks and defenses transfer across diverse scenarios and backbone models, demonstrating strong practical utility for building privacy-aware agents.", "source": "arxiv", "arxiv_id": "2508.10880v2", "pdf_url": "https://arxiv.org/pdf/2508.10880v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-14T17:49:09Z", "updated": "2025-09-25T04:24:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey", "authors": ["Yinqiu Liu", "Ruichen Zhang", "Haoxiang Luo", "Yijing Lin", "Geng Sun", "Dusit Niyato", "Hongyang Du", "Zehui Xiong", "Yonggang Wen", "Abbas Jamalipour", "Dong In Kim", "Ping Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19870v1", "abstract": "Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models (LLMs) and perception, reasoning, and acting modules. These agents collaborate across heterogeneous edge infrastructures, forming multi-LLM agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks. However, the collaborative nature of multi-LLM systems introduces critical security vulnerabilities, including insecure inter-LLM communications, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address. To this end, this survey introduces zero-trust security of multi-LLM in EGI, a paradigmatic shift following the ``never trust, always verify'' principle. We begin by systematically analyzing the security risks in multi-LLM systems within EGI contexts. Subsequently, we present the vision of a zero-trust multi-LLM framework in EGI. We then survey key technical progress to facilitate zero-trust multi-LLM systems in EGI. Particularly, we categorize zero-trust security mechanisms into model- and system-level approaches. The former and latter include strong identification, context-aware access control, etc., and proactive maintenance, blockchain-based management, etc., respectively. Finally, we identify critical research directions. This survey serves as the first systematic treatment of zero-trust applied to multi-LLM systems, providing both theoretical foundations and practical strategies.", "source": "arxiv", "arxiv_id": "2508.19870v1", "pdf_url": "https://arxiv.org/pdf/2508.19870v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-08-27T13:33:35Z", "updated": "2025-08-27T13:33:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy", "Relsy Puthal", "Kaustik Ranaware"], "year": 2025, "url": "http://arxiv.org/abs/2509.16275v1", "abstract": "Modern software development pipelines face growing challenges in securing large codebases with extensive dependencies. Static analysis tools like Bandit are effective at vulnerability detection but suffer from high false positives and lack repair capabilities. Large Language Models (LLMs), in contrast, can suggest fixes but often hallucinate changes and lack self-validation. We present SecureFixAgent, a hybrid repair framework integrating Bandit with lightweight local LLMs (<8B parameters) in an iterative detect-repair-validate loop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning on a diverse, curated dataset spanning multiple Python project domains, mitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses Bandit for detection, the LLM for candidate fixes with explanations, and Bandit re-validation for verification, all executed locally to preserve privacy and reduce cloud reliance. Experiments show SecureFixAgent reduces false positives by 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers false positives by 5.46% compared to pre-trained LLMs, typically converging within three iterations. Beyond metrics, developer studies rate explanation quality 4.5/5, highlighting its value for human trust and adoption. By combining verifiable security improvements with transparent rationale in a resource-efficient local framework, SecureFixAgent advances trustworthy, automated vulnerability remediation for modern pipelines.", "source": "arxiv", "arxiv_id": "2509.16275v1", "pdf_url": "https://arxiv.org/pdf/2509.16275v1", "categories": ["cs.CR", "cs.AI", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-18T15:45:43Z", "updated": "2025-09-18T15:45:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Security Concerns for Large Language Models: A Survey", "authors": ["Miles Q. Li", "Benjamin C. M. Fung"], "year": 2025, "url": "http://arxiv.org/abs/2505.18889v5", "abstract": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. This survey provides a comprehensive overview of these emerging concerns, categorizing threats into several key areas: inference-time attacks via prompt manipulation; training-time attacks; misuse by malicious actors; and the inherent risks in autonomous LLM agents. Recently, a significant focus is increasingly being placed on the latter. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze existing defense mechanisms and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.", "source": "arxiv", "arxiv_id": "2505.18889v5", "pdf_url": "https://arxiv.org/pdf/2505.18889v5", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-24T22:22:43Z", "updated": "2025-08-24T03:15:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models", "authors": ["Yuewen Mei", "Tong Nie", "Jian Sun", "Ye Tian"], "year": 2025, "url": "http://arxiv.org/abs/2505.00972v2", "abstract": "Simulation-based testing is crucial for validating autonomous vehicles (AVs), yet existing scenario generation methods either overfit to common driving patterns or operate in an offline, non-interactive manner that fails to expose rare, safety-critical corner cases. In this paper, we introduce an online, retrieval-augmented large language model (LLM) framework for generating safety-critical driving scenarios. Our method first employs an LLM-based behavior analyzer to infer the most dangerous intent of the background vehicle from the observed state, then queries additional LLM agents to synthesize feasible adversarial trajectories. To mitigate catastrophic forgetting and accelerate adaptation, we augment the framework with a dynamic memorization and retrieval bank of intent-planner pairs, automatically expanding its behavioral library when novel intents arise. Evaluations using the Waymo Open Motion Dataset demonstrate that our model reduces the mean minimum time-to-collision from 1.62 to 1.08 s and incurs a 75% collision rate, substantially outperforming baselines.", "source": "arxiv", "arxiv_id": "2505.00972v2", "pdf_url": "https://arxiv.org/pdf/2505.00972v2", "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "IEEE International Conference on Intelligent Transportation Systems, 2025", "published": "2025-05-02T03:22:00Z", "updated": "2025-07-15T07:52:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Self-Challenging Language Model Agents", "authors": ["Yifei Zhou", "Sergey Levine", "Jason Weston", "Xian Li", "Sainbayar Sukhbaatar"], "year": 2025, "url": "http://arxiv.org/abs/2506.01716v1", "abstract": "Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "source": "arxiv", "arxiv_id": "2506.01716v1", "pdf_url": "https://arxiv.org/pdf/2506.01716v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-02T14:23:33Z", "updated": "2025-06-02T14:23:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "authors": ["Vishnu Sarukkai", "Zhiqiang Xie", "Kayvon Fatahalian"], "year": 2025, "url": "http://arxiv.org/abs/2505.00234v3", "abstract": "Improving Large Language Model (LLM) agents for sequential decision-making tasks typically requires extensive task-specific knowledge engineering--custom prompts, curated examples, and specialized observation/action spaces. We investigate a different approach where agents automatically improve by learning from their own successful experiences without human intervention. Our method constructs and refines a database of self-generated trajectories that serve as in-context examples for future tasks. Even naive accumulation of successful trajectories yields substantial performance gains across three diverse benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%). These improvements exceed those achieved by upgrading from gpt-4o-mini to gpt-4o and match the performance of allowing multiple attempts per task. We further enhance this approach with two innovations: database-level curation using population-based training to propagate high-performing example collections, and exemplar-level curation that selectively retains trajectories based on their empirical utility as in-context examples. With these enhancements, our method achieves 93% success on ALFWorld--surpassing approaches that use more powerful LLMs and hand-crafted components. Our trajectory bootstrapping technique demonstrates that agents can autonomously improve through experience, offering a scalable alternative to labor-intensive knowledge engineering.", "source": "arxiv", "arxiv_id": "2505.00234v3", "pdf_url": "https://arxiv.org/pdf/2505.00234v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-01T00:48:12Z", "updated": "2025-05-16T21:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Self-Improving LLM Agents at Test-Time", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Heng Ji", "Dilek Hakkani-TÃ¼r", "Gokhan Tur"], "year": 2025, "url": "http://arxiv.org/abs/2510.07841v1", "abstract": "One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.", "source": "arxiv", "arxiv_id": "2510.07841v1", "pdf_url": "https://arxiv.org/pdf/2510.07841v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-09T06:37:35Z", "updated": "2025-10-09T06:37:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Self-Regulation and Requesting Interventions", "authors": ["So Yeon Min", "Yue Wu", "Jimin Sun", "Max Kaufmann", "Fahim Tajwar", "Yonatan Bisk", "Ruslan Salakhutdinov"], "year": 2025, "url": "http://arxiv.org/abs/2502.04576v1", "abstract": "Human intelligence involves metacognitive abilities like self-regulation, recognizing limitations, and seeking assistance only when needed. While LLM Agents excel in many domains, they often lack this awareness. Overconfident agents risk catastrophic failures, while those that seek help excessively hinder efficiency. A key challenge is enabling agents with a limited intervention budget $C$ is to decide when to request assistance. In this paper, we propose an offline framework that trains a \"helper\" policy to request interventions, such as more powerful models or test-time compute, by combining LLM-based process reward models (PRMs) with tabular reinforcement learning. Using state transitions collected offline, we score optimal intervention timing with PRMs and train the helper model on these labeled trajectories. This offline approach significantly reduces costly intervention calls during training. Furthermore, the integration of PRMs with tabular RL enhances robustness to off-policy data while avoiding the inefficiencies of deep RL. We empirically find that our method delivers optimal helper behavior.", "source": "arxiv", "arxiv_id": "2502.04576v1", "pdf_url": "https://arxiv.org/pdf/2502.04576v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-07T00:06:17Z", "updated": "2025-02-07T00:06:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SelfAI: Building a Self-Training AI System with LLM Agents", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Xiaobing Yu", "Yu Zhong", "Shangqi Deng", "Ufaq Khan", "Jianghao Wu", "Xiaofeng Liu", "Imran Razzak", "Xiaojun Chang", "Yutong Xie"], "year": 2025, "url": "http://arxiv.org/abs/2512.00403v1", "abstract": "Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \\textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.", "source": "arxiv", "arxiv_id": "2512.00403v1", "pdf_url": "https://arxiv.org/pdf/2512.00403v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-29T09:18:39Z", "updated": "2025-11-29T09:18:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services", "authors": ["Jayden Serenari", "Stephen Lee"], "year": 2025, "url": "http://arxiv.org/abs/2510.27016v1", "abstract": "With the increasing use of conversational AI systems, there is growing concern over privacy leaks, especially when users share sensitive personal data in interactions with Large Language Models (LLMs). Conversations shared with these models may contain Personally Identifiable Information (PII), which, if exposed, could lead to security breaches or identity theft. To address this challenge, we present the Local Optimizations for Pseudonymization with Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a semantically-aware privacy agent designed to safeguard sensitive PII data when using remote LLMs. Unlike prior work that often degrade response quality, our approach dynamically replaces sensitive PII entities in user prompts with semantically consistent pseudonyms, preserving the contextual integrity of conversations. Once the model generates its response, the pseudonyms are automatically depseudonymized, ensuring the user receives an accurate, privacy-preserving output. We evaluate our approach using real-world conversations sourced from ShareGPT, which we further augment and annotate to assess whether named entities are contextually relevant to the model's response. Our results show that LOPSIDED reduces semantic utility errors by a factor of 5 compared to baseline techniques, all while enhancing privacy.", "source": "arxiv", "arxiv_id": "2510.27016v1", "pdf_url": "https://arxiv.org/pdf/2510.27016v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-30T21:34:23Z", "updated": "2025-10-30T21:34:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation", "authors": ["Song Wang", "Zihan Chen", "Peng Wang", "Zhepei Wei", "Zhen Tan", "Yu Meng", "Cong Shen", "Jundong Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.04700v1", "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources to address their limitations in accessing up-to-date or specialized information. A natural strategy to increase the likelihood of retrieving relevant information is to expand the number of retrieved documents. However, involving more documents could introduce significant noise, as many documents may be irrelevant or misleading, thereby reducing the overall accuracy of the generated responses. To overcome the challenge associated with handling a larger number of documents, we propose WinnowRAG, a novel RAG framework designed to systematically filter out noisy documents while preserving valuable content -- a process we refer to as winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware clustering to group similar documents and form distinct topic clusters. Each cluster is assigned to an LLM agent for generating a unique answer. In Stage II, we perform winnowing, wherein a critic LLM evaluates the outputs of multiple agents and iteratively separates useful documents from noisy ones. To retain useful documents when discarding agents, we propose two strategic merging techniques to ensure that only relevant knowledge is used for generating the final response. Crucially, WinnowRAG is model-agnostic and does not require any model fine-tuning, making it easily adaptable to various tasks. Extensive experiments on various realistic datasets demonstrate the effectiveness of WinnowRAG over state-of-the-art baselines.", "source": "arxiv", "arxiv_id": "2511.04700v1", "pdf_url": "https://arxiv.org/pdf/2511.04700v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-01T20:08:13Z", "updated": "2025-11-01T20:08:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement Learning-based Model Caching and Inference Offloading", "authors": ["Minrui Xu", "Dusit Niyato", "Christopher G. Brinton"], "year": 2025, "url": "http://arxiv.org/abs/2501.14205v1", "abstract": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks and few-shot learning on complex reasoning tasks. However, resource-limited mobile edge networks struggle to support long-context LLM serving for LLM agents during multi-round interactions with users. Unlike stateless computation offloading and static service offloading in edge computing, optimizing LLM serving at edge servers is challenging because LLMs continuously learn from context which raises accuracy, latency, and resource consumption dynamics. In this paper, we propose a joint model caching and inference offloading framework that utilizes test-time deep reinforcement learning (T2DRL) to optimize deployment and execution strategies for long-context LLM serving. In this framework, we analyze the performance convergence and design an optimization problem considering the utilization of context windows in LLMs. Furthermore, the T2DRL algorithm can learn in both the training phase and the testing phase to proactively manage cached models and service requests and adapt to context changes and usage patterns during execution. To further enhance resource allocation efficiency, we propose a double Dutch auction (DDA) mechanism, which dynamically matches supply and demand while maximizing social welfare. Finally, experimental results demonstrate that the T2DRL algorithm can reduce system costs by at least 30% compared to baselines while guaranteeing the performance of LLM agents in real-world perception and reasoning tasks.", "source": "arxiv", "arxiv_id": "2501.14205v1", "pdf_url": "https://arxiv.org/pdf/2501.14205v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-01-24T03:21:20Z", "updated": "2025-01-24T03:21:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments", "authors": ["Avi Arora", "Jinu Jang", "Roshanak Zilouchian Moghaddam"], "year": 2025, "url": "http://arxiv.org/abs/2507.09063v1", "abstract": "Modern Large Language Model (LLM) agents promise end to end assistance with real-world software tasks, yet existing benchmarks evaluate LLM agents almost exclusively in pre-baked environments where every dependency is pre-installed. To fill this gap, we introduce SetupBench, a 93 instance benchmark that isolates the environment-bootstrap skill: starting from a bare Linux sandbox, an agent must install packages, resolve dependency conflicts, initialize databases, and configure background services. Our tasks span seven language ecosystems, five database engines, and multi-service orchestration scenarios, each accompanies by a natural language problem statement and a deterministic success command. Through evaluation of OpenHands, a state-of-the-art coding agent, we find low success rates across task categories, with particular challenges in repository setup (38.9-57.4%) and local database configuration (20.0-53.3%). Our analysis reveals systematic failure modes including incomplete development tooling installation, hallucinated task constraints, and non-persistent environment modifications that break agent-human collaboration workflows. We identify substantial inefficiencies in agent exploration strategies, with 38-89% of actions being unnecessary compared to optimal human behavior. These findings highlight gaps in current agents' practical environment-bootstrap capabilities. By targeting this critical yet under-evaluated capability, SetupBench provides a rigorous yard-stick for the next generation of software developer agents aiming to solve end to end real-wold tasks.", "source": "arxiv", "arxiv_id": "2507.09063v1", "pdf_url": "https://arxiv.org/pdf/2507.09063v1", "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-07-11T22:45:07Z", "updated": "2025-07-11T22:45:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling", "authors": ["Shuyuan Zhang", "Chenhan Jiang", "Zuoou Li", "Jiankang Deng"], "year": 2025, "url": "http://arxiv.org/abs/2510.17603v1", "abstract": "3D generation from natural language offers significant potential to reduce expert manual modeling efforts and enhance accessibility to 3D assets. However, existing methods often yield unstructured meshes and exhibit poor interactivity, making them impractical for artistic workflows. To address these limitations, we represent 3D assets as shape programs and introduce ShapeCraft, a novel multi-agent framework for text-to-3D generation. At its core, we propose a Graph-based Procedural Shape (GPS) representation that decomposes complex natural language into a structured graph of sub-tasks, thereby facilitating accurate LLM comprehension and interpretation of spatial relationships and semantic shape details. Specifically, LLM agents hierarchically parse user input to initialize GPS, then iteratively refine procedural modeling and painting to produce structured, textured, and interactive 3D assets. Qualitative and quantitative experiments demonstrate ShapeCraft's superior performance in generating geometrically accurate and semantically rich 3D assets compared to existing LLM-based agents. We further show the versatility of ShapeCraft through examples of animated and user-customized editing, highlighting its potential for broader interactive applications.", "source": "arxiv", "arxiv_id": "2510.17603v1", "pdf_url": "https://arxiv.org/pdf/2510.17603v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-10-20T14:51:14Z", "updated": "2025-10-20T14:51:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents", "authors": ["Yun Hua", "Haosheng Chen", "Shiqin Wang", "Wenhao Li", "Xiangfeng Wang", "Jun Luo"], "year": 2025, "url": "http://arxiv.org/abs/2506.07388v1", "abstract": "Large Language Models (LLMs) show strong collaborative performance in multi-agent systems with predefined roles and workflows. However, in open-ended environments lacking coordination rules, agents tend to act in self-interested ways. The central challenge in achieving coordination lies in credit assignment -- fairly evaluating each agent's contribution and designing pricing mechanisms that align their heterogeneous goals. This problem is critical as LLMs increasingly participate in complex human-AI collaborations, where fair compensation and accountability rely on effective pricing mechanisms. Inspired by how human societies address similar coordination challenges (e.g., through temporary collaborations such as employment or subcontracting), we propose a cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley Chain-of-Thought -- leveraging marginal contributions as a principled basis for pricing -- with structured negotiation protocols for effective price matching, enabling LLM agents to coordinate through rational task-time pricing and post-task reward redistribution. This approach aligns agent incentives, fosters cooperation, and maintains autonomy. We evaluate Shapley-Coop across two multi-agent games and a software engineering simulation, demonstrating that it consistently enhances LLM agent collaboration and facilitates equitable credit assignment. These results highlight the effectiveness of Shapley-Coop's pricing mechanisms in accurately reflecting individual contributions during task execution.", "source": "arxiv", "arxiv_id": "2506.07388v1", "pdf_url": "https://arxiv.org/pdf/2506.07388v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-09T03:24:01Z", "updated": "2025-06-09T03:24:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing", "authors": ["Junlong Ke", "Qiying Hu", "Shenghai Yuan", "Yuecong Xu", "Jianfei Yang"], "year": 2025, "url": "http://arxiv.org/abs/2509.17197v2", "abstract": "Modern signal processing (SP) pipelines, whether model-based or data-driven, often constrained by complex and fragmented workflow, rely heavily on expert knowledge and manual engineering, and struggle with adaptability and generalization under limited data. In contrast, Large Language Models (LLMs) offer strong reasoning capabilities, broad general-purpose knowledge, in-context learning, and cross-modal transfer abilities, positioning them as powerful tools for automating and generalizing SP workflows. Motivated by these potentials, we introduce SignalLLM, the first general-purpose LLM-based agent framework for general SP tasks. Unlike prior LLM-based SP approaches that are limited to narrow applications or tricky prompting, SignalLLM introduces a principled, modular architecture. It decomposes high-level SP goals into structured subtasks via in-context learning and domain-specific retrieval, followed by hierarchical planning through adaptive retrieval-augmented generation (RAG) and refinement; these subtasks are then executed through prompt-based reasoning, cross-modal reasoning, code synthesis, model invocation, or data-driven LLM-assisted modeling. Its generalizable design enables the flexible selection of problem solving strategies across different signal modalities, task types, and data conditions. We demonstrate the versatility and effectiveness of SignalLLM through five representative tasks in communication and sensing, such as radar target detection, human activity recognition, and text compression. Experimental results show superior performance over traditional and existing LLM-based methods, particularly in few-shot and zero-shot settings.", "source": "arxiv", "arxiv_id": "2509.17197v2", "pdf_url": "https://arxiv.org/pdf/2509.17197v2", "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-21T18:54:54Z", "updated": "2025-10-30T15:26:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SimInterview: Transforming Business Education through Large Language Model-Based Simulated Multilingual Interview Training System", "authors": ["Truong Thanh Hung Nguyen", "Tran Diem Quynh Nguyen", "Hoang Loc Cao", "Thi Cam Thanh Tran", "Thi Cam Mai Truong", "Hung Cao"], "year": 2025, "url": "http://arxiv.org/abs/2508.11873v1", "abstract": "Business interview preparation demands both solid theoretical grounding and refined soft skills, yet conventional classroom methods rarely deliver the individualized, culturally aware practice employers currently expect. This paper introduces SimInterview, a large language model (LLM)-based simulated multilingual interview training system designed for business professionals entering the AI-transformed labor market. Our system leverages an LLM agent and synthetic AI technologies to create realistic virtual recruiters capable of conducting personalized, real-time conversational interviews. The framework dynamically adapts interview scenarios using retrieval-augmented generation (RAG) to match individual resumes with specific job requirements across multiple languages. Built on LLMs (OpenAI o3, Llama 4 Maverick, Gemma 3), integrated with Whisper speech recognition, GPT-SoVITS voice synthesis, Ditto diffusion-based talking head generation model, and ChromaDB vector databases, our system significantly improves interview readiness across English and Japanese markets. Experiments with university-level candidates show that the system consistently aligns its assessments with job requirements, faithfully preserves resume content, and earns high satisfaction ratings, with the lightweight Gemma 3 model producing the most engaging conversations. Qualitative findings revealed that the standardized Japanese resume format improved document retrieval while diverse English resumes introduced additional variability, and they highlighted how cultural norms shape follow-up questioning strategies. Finally, we also outlined a contestable AI design that can explain, detect bias, and preserve human-in-the-loop to meet emerging regulatory expectations.", "source": "arxiv", "arxiv_id": "2508.11873v1", "pdf_url": "https://arxiv.org/pdf/2508.11873v1", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.MM"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-08-16T02:18:36Z", "updated": "2025-08-16T02:18:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution", "authors": ["Meysam Alizadeh", "Zeynab Samei", "Daria Stetsenko", "Fabrizio Gilardi"], "year": 2025, "url": "http://arxiv.org/abs/2506.01055v1", "abstract": "Previous benchmarks on prompt injection in large language models (LLMs) have primarily focused on generic tasks and attacks, offering limited insights into more complex threats like data exfiltration. This paper examines how prompt injection can cause tool-calling agents to leak personal data observed during task execution. Using a fictitious banking agent, we develop data flow-based attacks and integrate them into AgentDojo, a recent benchmark for agentic security. To enhance its scope, we also create a richer synthetic dataset of human-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a 15-50 percentage point drop in utility under attack, with average attack success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most LLMs, even when successfully tricked by the attack, avoid leaking highly sensitive data like passwords, likely due to safety alignments, but they remain vulnerable to disclosing other personal data. The likelihood of password leakage increases when a password is requested along with one or two additional personal details. In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage. Tasks involving data extraction or authorization workflows, which closely resemble the structure of exfiltration attacks, exhibit the highest ASRs, highlighting the interaction between task type, agent performance, and defense efficacy.", "source": "arxiv", "arxiv_id": "2506.01055v1", "pdf_url": "https://arxiv.org/pdf/2506.01055v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-01T15:48:06Z", "updated": "2025-06-01T15:48:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SimuGen: Multi-modal Agentic Framework for Constructing Block Diagram-Based Simulation Models", "authors": ["Xinxing Ren", "Qianbo Zang", "Zekun Guo"], "year": 2025, "url": "http://arxiv.org/abs/2506.15695v2", "abstract": "Recent advances in large language models (LLMs) have shown impressive performance in mathematical reasoning and code generation. However, LLMs still struggle in the simulation domain, particularly in generating Simulink models, which are essential tools in engineering and scientific research. Our preliminary experiments indicate that LLM agents often fail to produce reliable and complete Simulink simulation code from text-only inputs, likely due to the lack of Simulink-specific data in their pretraining. To address this challenge, we propose SimuGen, a multimodal agent-based framework that automatically generates accurate Simulink simulation code by leveraging both the visual Simulink diagram and domain knowledge. SimuGen coordinates several specialized agents, including an investigator, unit test reviewer, code generator, executor, debug locator, and report writer, supported by a domain-specific knowledge base. This collaborative and modular design enables interpretable, robust, and reproducible Simulink simulation generation. Our source code is publicly available at https://github.com/renxinxing123/SimuGen_beta.", "source": "arxiv", "arxiv_id": "2506.15695v2", "pdf_url": "https://arxiv.org/pdf/2506.15695v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-28T00:35:43Z", "updated": "2025-08-28T19:41:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents", "authors": ["Gyuhyeon Seo", "Jungwoo Yang", "Junseong Pyo", "Nalim Kim", "Jonggeun Lee", "Yohan Jo"], "year": 2025, "url": "http://arxiv.org/abs/2509.24282v2", "abstract": "Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks. However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more. The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them. To address this, we introduce $\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. By building the simulator on the Matter protocol, the global industry standard for smart home communication, SimuHome provides a high-fidelity environment, and agents validated in SimuHome can be deployed on real Matter-compliant devices with minimal adaptation. We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Models under 7B parameters exhibited negligible performance across all query types. Even GPT-4.1, the best-performing standard model, struggled with implicit intent inference, state verification, and particularly temporal scheduling. While reasoning models such as GPT-5.1 consistently outperformed standard models on every query type, they required over three times the average inference time, which can be prohibitive for real-time smart home applications. This highlights a critical trade-off between task performance and real-world practicality.", "source": "arxiv", "arxiv_id": "2509.24282v2", "pdf_url": "https://arxiv.org/pdf/2509.24282v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-29T04:54:20Z", "updated": "2025-12-08T08:28:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Simulating Environments with Reasoning Models for Agent Training", "authors": ["Yuetai Li", "Huseyin A Inan", "Xiang Yue", "Wei-Ning Chen", "Lukas Wutschitz", "Janardhan Kulkarni", "Radha Poovendran", "Robert Sim", "Saravan Rajmohan"], "year": 2025, "url": "http://arxiv.org/abs/2511.01824v1", "abstract": "LLM agents excel in compact environments requiring deep reasoning but remain brittle when operating in broader, more complex contexts that demand robustness across diverse tools and schemas. Building bespoke environments for training is heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs can simulate realistic environment feedback without access to actual testbed data or APIs. Inspired by this capability, we propose two frameworks: Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets into diverse trajectories in an environment-agnostic manner, and Simia-RL, a framework that enables RL training without real environment implementations through LLM-simulated feedback. Fine-tuning open models yields consistent improvements across multiple benchmarks, surpassing GPT-4o and approaching o4-mini on $Ï^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable agent training without environment engineering, replacing heavy and brittle implementations with flexible LLM-based simulation.", "source": "arxiv", "arxiv_id": "2511.01824v1", "pdf_url": "https://arxiv.org/pdf/2511.01824v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-03T18:29:57Z", "updated": "2025-11-03T18:29:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Simulating Filter Bubble on Short-video Recommender System with Large Language Model Agents", "authors": ["Nicholas Sukiennik", "Haoyu Wang", "Zailin Zeng", "Chen Gao", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.08742v1", "abstract": "An increasing reliance on recommender systems has led to concerns about the creation of filter bubbles on social media, especially on short video platforms like TikTok. However, their formation is still not entirely understood due to the complex dynamics between recommendation algorithms and user feedback. In this paper, we aim to shed light on these dynamics using a large language model-based simulation framework. Our work employs real-world short-video data containing rich video content information and detailed user-agents to realistically simulate the recommendation-feedback cycle. Through large-scale simulations, we demonstrate that LLMs can replicate real-world user-recommender interactions, uncovering key mechanisms driving filter bubble formation. We identify critical factors, such as demographic features and category attraction that exacerbate content homogenization. To mitigate this, we design and test interventions including various cold-start and feedback weighting strategies, showing measurable reductions in filter bubble effects. Our framework enables rapid prototyping of recommendation strategies, offering actionable solutions to enhance content diversity in real-world systems. Furthermore, we analyze how LLM-inherent biases may propagate through recommendations, proposing safeguards to promote equity for vulnerable groups, such as women and low-income populations. By examining the interplay between recommendation and LLM agents, this work advances a deeper understanding of algorithmic bias and provides practical tools to promote inclusive digital spaces.", "source": "arxiv", "arxiv_id": "2504.08742v1", "pdf_url": "https://arxiv.org/pdf/2504.08742v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-03-23T10:35:58Z", "updated": "2025-03-23T10:35:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Simulating Influence Dynamics with LLM Agents", "authors": ["Mehwish Nasim", "Syed Muslim Gilani", "Amin Qasmi", "Usman Naseem"], "year": 2025, "url": "http://arxiv.org/abs/2503.08709v1", "abstract": "This paper introduces a simulator designed for opinion dynamics researchers to model competing influences within social networks in the presence of LLM-based agents. By integrating established opinion dynamics principles with state-of-the-art LLMs, this tool enables the study of influence propagation and counter-misinformation strategies. The simulator is particularly valuable for researchers in social science, psychology, and operations research, allowing them to analyse societal phenomena without requiring extensive coding expertise. Additionally, the simulator will be openly available on GitHub, ensuring accessibility and adaptability for those who wish to extend its capabilities for their own research.", "source": "arxiv", "arxiv_id": "2503.08709v1", "pdf_url": "https://arxiv.org/pdf/2503.08709v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-03-10T03:05:21Z", "updated": "2025-03-10T03:05:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Simulating Macroeconomic Expectations using LLM Agents", "authors": ["Jianhao Lin", "Lexuan Sun", "Yixin Yan"], "year": 2025, "url": "http://arxiv.org/abs/2505.17648v4", "abstract": "We introduce a novel framework for simulating macroeconomic expectations using LLM Agents. By constructing LLM Agents equipped with various functional modules, we replicate three representative survey experiments involving several expectations across different types of economic agents. Our results show that although the expectations simulated by LLM Agents are more homogeneous than those of humans, they consistently outperform LLMs relying simply on prompt engineering, and possess human-like mental mechanisms. Evaluation reveals that these capabilities stem from the contributions of their components, offering guidelines for their architectural design. Our approach complements traditional methods and provides new insights into AI behavioral science in macroeconomic research", "source": "arxiv", "arxiv_id": "2505.17648v4", "pdf_url": "https://arxiv.org/pdf/2505.17648v4", "categories": ["econ.GN", "cs.AI"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2025-05-23T09:11:14Z", "updated": "2025-11-25T02:11:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Simulating Online Social Media Conversations on Controversial Topics Using AI Agents Calibrated on Real-World Data", "authors": ["Elisa Composta", "Nicolo' Fontana", "Francesco Corso", "Francesco Pierri"], "year": 2025, "url": "http://arxiv.org/abs/2509.18985v1", "abstract": "Online social networks offer a valuable lens to analyze both individual and collective phenomena. Researchers often use simulators to explore controlled scenarios, and the integration of Large Language Models (LLMs) makes these simulations more realistic by enabling agents to understand and generate natural language content. In this work, we investigate the behavior of LLM-based agents in a simulated microblogging social network. We initialize agents with realistic profiles calibrated on real-world online conversations from the 2022 Italian political election and extend an existing simulator by introducing mechanisms for opinion modeling. We examine how LLM agents simulate online conversations, interact with others, and evolve their opinions under different scenarios. Our results show that LLM agents generate coherent content, form connections, and build a realistic social network structure. However, their generated content displays less heterogeneity in tone and toxicity compared to real data. We also find that LLM-based opinion dynamics evolve over time in ways similar to traditional mathematical models. Varying parameter configurations produces no significant changes, indicating that simulations require more careful cognitive modeling at initialization to replicate human behavior more faithfully. Overall, we demonstrate the potential of LLMs for simulating user behavior in social environments, while also identifying key challenges in capturing heterogeneity and complex dynamics.", "source": "arxiv", "arxiv_id": "2509.18985v1", "pdf_url": "https://arxiv.org/pdf/2509.18985v1", "categories": ["cs.SI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-09-23T13:36:48Z", "updated": "2025-09-23T13:36:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Simulating Rumor Spreading in Social Networks using LLM Agents", "authors": ["Tianrui Hu", "Dimitrios Liakopoulos", "Xiwen Wei", "Radu Marculescu", "Neeraja J. Yadwadkar"], "year": 2025, "url": "http://arxiv.org/abs/2502.01450v1", "abstract": "With the rise of social media, misinformation has become increasingly prevalent, fueled largely by the spread of rumors. This study explores the use of Large Language Model (LLM) agents within a novel framework to simulate and analyze the dynamics of rumor propagation across social networks. To this end, we design a variety of LLM-based agent types and construct four distinct network structures to conduct these simulations. Our framework assesses the effectiveness of different network constructions and agent behaviors in influencing the spread of rumors. Our results demonstrate that the framework can simulate rumor spreading across more than one hundred agents in various networks with thousands of edges. The evaluations indicate that network structure, personas, and spreading schemes can significantly influence rumor dissemination, ranging from no spread to affecting 83\\% of agents in iterations, thereby offering a realistic simulation of rumor spread in social networks.", "source": "arxiv", "arxiv_id": "2502.01450v1", "pdf_url": "https://arxiv.org/pdf/2502.01450v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-02-03T15:39:56Z", "updated": "2025-02-03T15:39:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Simulating and Experimenting with Social Media Mobilization Using LLM Agents", "authors": ["Sadegh Shirani", "Mohsen Bayati"], "year": 2025, "url": "http://arxiv.org/abs/2510.26494v1", "abstract": "Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \\citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\\footnote{Code and data available at https://github.com/CausalMP/LLM-SocioPol}", "source": "arxiv", "arxiv_id": "2510.26494v1", "pdf_url": "https://arxiv.org/pdf/2510.26494v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-10-30T13:43:28Z", "updated": "2025-10-30T13:43:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent", "authors": ["Shiyi Cao", "Dacheng Li", "Fangzhou Zhao", "Shuo Yuan", "Sumanth R. Hegde", "Connor Chen", "Charlie Ruan", "Tyler Griggs", "Shu Liu", "Eric Tang", "Richard Liaw", "Philipp Moritz", "Matei Zaharia", "Joseph E. Gonzalez", "Ion Stoica"], "year": 2025, "url": "http://arxiv.org/abs/2511.16108v1", "abstract": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker.\n  Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.", "source": "arxiv", "arxiv_id": "2511.16108v1", "pdf_url": "https://arxiv.org/pdf/2511.16108v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-20T07:05:19Z", "updated": "2025-11-20T07:05:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation", "authors": ["Yashothara Shanmugarasa", "Ming Ding", "M. A. P Chamikara", "Thierry Rakotoarivelo"], "year": 2025, "url": "http://arxiv.org/abs/2506.12699v2", "abstract": "Large language models (LLMs) are sophisticated artificial intelligence systems that enable machines to generate human-like text with remarkable precision. While LLMs offer significant technological progress, their development using vast amounts of user data scraped from the web and collected from extensive user interactions poses risks of sensitive information leakage. Most existing surveys focus on the privacy implications of the training data but tend to overlook privacy risks from user interactions and advanced LLM capabilities. This paper aims to fill that gap by providing a comprehensive analysis of privacy in LLMs, categorizing the challenges into four main areas: (i) privacy issues in LLM training data, (ii) privacy challenges associated with user prompts, (iii) privacy vulnerabilities in LLM-generated outputs, and (iv) privacy challenges involving LLM agents. We evaluate the effectiveness and limitations of existing mitigation mechanisms targeting these proposed privacy challenges and identify areas for further research.", "source": "arxiv", "arxiv_id": "2506.12699v2", "pdf_url": "https://arxiv.org/pdf/2506.12699v2", "categories": ["cs.CR", "cs.HC"], "primary_category": "cs.CR", "doi": "10.1145/3708821.3733888", "venue": "", "published": "2025-06-15T03:14:03Z", "updated": "2025-06-19T06:30:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SoK: Trust-Authorization Mismatch in LLM Agent Interactions", "authors": ["Guanquan Shi", "Haohua Du", "Zhiqiang Wang", "Xiaoyu Liang", "Weiwenpei Liu", "Song Bian", "Zhenyu Guan"], "year": 2025, "url": "http://arxiv.org/abs/2512.06914v1", "abstract": "Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security.\n  We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.", "source": "arxiv", "arxiv_id": "2512.06914v1", "pdf_url": "https://arxiv.org/pdf/2512.06914v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-07T16:41:02Z", "updated": "2025-12-07T16:41:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents", "authors": ["Zeyi Zhang", "Yanju Zhou", "Heyuan Yao", "Tenglong Ao", "Xiaohang Zhan", "Libin Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.04637v1", "abstract": "We present Social Agent, a novel framework for synthesizing realistic and contextually appropriate co-speech nonverbal behaviors in dyadic conversations. In this framework, we develop an agentic system driven by a Large Language Model (LLM) to direct the conversation flow and determine appropriate interactive behaviors for both participants. Additionally, we propose a novel dual-person gesture generation model based on an auto-regressive diffusion model, which synthesizes coordinated motions from speech signals. The output of the agentic system is translated into high-level guidance for the gesture generator, resulting in realistic movement at both the behavioral and motion levels. Furthermore, the agentic system periodically examines the movements of interlocutors and infers their intentions, forming a continuous feedback loop that enables dynamic and responsive interactions between the two participants. User studies and quantitative evaluations show that our model significantly improves the quality of dyadic interactions, producing natural, synchronized nonverbal behaviors.", "source": "arxiv", "arxiv_id": "2510.04637v1", "pdf_url": "https://arxiv.org/pdf/2510.04637v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR", "doi": "10.1145/3757377.3763879", "venue": "", "published": "2025-10-06T09:41:37Z", "updated": "2025-10-06T09:41:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare", "authors": ["Zhengliang Shi", "Ruotian Ma", "Jen-tse Huang", "Xinbei Ma", "Xingyu Chen", "Mengru Wang", "Qu Yang", "Yue Wang", "Fanghua Ye", "Ziyang Chen", "Shanyi Wang", "Cixing Li", "Wenxuan Wang", "Zhaopeng Tu", "Xiaolong Li", "Zhaochun Ren", "Linus"], "year": 2025, "url": "http://arxiv.org/abs/2510.01164v1", "abstract": "Large language models (LLMs) are increasingly entrusted with high-stakes decisions that affect human welfare. However, the principles and values that guide these models when distributing scarce societal resources remain largely unexamined. To address this, we introduce the Social Welfare Function (SWF) Benchmark, a dynamic simulation environment where an LLM acts as a sovereign allocator, distributing tasks to a heterogeneous community of recipients. The benchmark is designed to create a persistent trade-off between maximizing collective efficiency (measured by Return on Investment) and ensuring distributive fairness (measured by the Gini coefficient). We evaluate 20 state-of-the-art LLMs and present the first leaderboard for social welfare allocation. Our findings reveal three key insights: (i) A model's general conversational ability, as measured by popular leaderboards, is a poor predictor of its allocation skill. (ii) Most LLMs exhibit a strong default utilitarian orientation, prioritizing group productivity at the expense of severe inequality. (iii) Allocation strategies are highly vulnerable, easily perturbed by output-length constraints and social-influence framing. These results highlight the risks of deploying current LLMs as societal decision-makers and underscore the need for specialized benchmarks and targeted alignment for AI governance.", "source": "arxiv", "arxiv_id": "2510.01164v1", "pdf_url": "https://arxiv.org/pdf/2510.01164v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-01T17:52:31Z", "updated": "2025-10-01T17:52:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Xuanjing Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "year": 2025, "url": "http://arxiv.org/abs/2504.10157v3", "abstract": "Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.", "source": "arxiv", "arxiv_id": "2504.10157v3", "pdf_url": "https://arxiv.org/pdf/2504.10157v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-14T12:12:52Z", "updated": "2025-07-15T11:14:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Soft Instruction De-escalation Defense", "authors": ["Nils Philipp Walter", "Chawin Sitawarin", "Jamie Hayes", "David Stutz", "Ilia Shumailov"], "year": 2025, "url": "http://arxiv.org/abs/2510.21057v2", "abstract": "Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment; this makes them susceptible to prompt injections when dealing with untrusted data. To overcome this limitation, we propose SIC (Soft Instruction Control)-a simple yet effective iterative prompt sanitization loop designed for tool-augmented LLM agents. Our method repeatedly inspects incoming data for instructions that could compromise agent behavior. If such content is found, the malicious content is rewritten, masked, or removed, and the result is re-evaluated. The process continues until the input is clean or a maximum iteration limit is reached; if imperative instruction-like content remains, the agent halts to ensure security. By allowing multiple passes, our approach acknowledges that individual rewrites may fail but enables the system to catch and correct missed injections in later steps. Although immediately useful, worst-case analysis shows that SIC is not infallible; strong adversary can still get a 15% ASR by embedding non-imperative workflows. This nonetheless raises the bar.", "source": "arxiv", "arxiv_id": "2510.21057v2", "pdf_url": "https://arxiv.org/pdf/2510.21057v2", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-24T00:04:07Z", "updated": "2026-01-17T09:50:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "authors": ["Heyang Gao", "Zexu Sun", "Erxue Min", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.03253v1", "abstract": "Large Language Models (LLMs) as autonomous agents are increasingly tasked with solving complex, long-horizon problems. Aligning these agents via preference-based offline methods like Direct Preference Optimization (DPO) is a promising direction, yet it faces a critical granularity mismatch. Trajectory-level DPO provides a signal that is too coarse for precise credit assignment, while step-level DPO is often too myopic to capture the value of multi-step behaviors. To resolve this challenge, we introduce Hierarchical Preference Learning (HPL), a hierarchical framework that optimizes LLM agents by leveraging preference signals at multiple, synergistic granularities. While HPL incorporates trajectory- and step-level DPO for global and local policy stability, its core innovation lies in group-level preference optimization guided by a dual-layer curriculum. Our approach first decomposes expert trajectories into semantically coherent action groups and then generates contrasting suboptimal groups to enable preference learning at a fine-grained, sub-task level. Then, instead of treating all preference pairs equally, HPL introduces a curriculum scheduler that organizes the learning process from simple to complex. This curriculum is structured along two axes: the group length, representing sub-task complexity, and the sample difficulty, defined by the reward gap between preferred and dispreferred action groups. Experiments on three challenging agent benchmarks show that HPL outperforms existing state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO loss effectively integrates preference signals across multiple granularities, while the dual-layer curriculum is crucial for enabling the agent to solve a wide range of tasks, from simple behaviors to complex multi-step sequences.", "source": "arxiv", "arxiv_id": "2510.03253v1", "pdf_url": "https://arxiv.org/pdf/2510.03253v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-26T08:43:39Z", "updated": "2025-09-26T08:43:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Spec2RTL-Agent: Automated Hardware Code Generation from Complex Specifications Using LLM Agent Systems", "authors": ["Zhongzhi Yu", "Mingjie Liu", "Michael Zimmer", "Yingyan Celine Lin", "Yong Liu", "Haoxing Ren"], "year": 2025, "url": "http://arxiv.org/abs/2506.13905v2", "abstract": "Despite recent progress in generating hardware RTL code with LLMs, existing solutions still suffer from a substantial gap between practical application scenarios and the requirements of real-world RTL code development. Prior approaches either focus on overly simplified hardware descriptions or depend on extensive human guidance to process complex specifications, limiting their scalability and automation potential. In this paper, we address this gap by proposing an LLM agent system, termed Spec2RTL-Agent, designed to directly process complex specification documentation and generate corresponding RTL code implementations, advancing LLM-based RTL code generation toward more realistic application settings. To achieve this goal, Spec2RTL-Agent introduces a novel multi-agent collaboration framework that integrates three key enablers: (1) a reasoning and understanding module that translates specifications into structured, step-by-step implementation plans; (2) a progressive coding and prompt optimization module that iteratively refines the code across multiple representations to enhance correctness and synthesisability for RTL conversion; and (3) an adaptive reflection module that identifies and traces the source of errors during generation, ensuring a more robust code generation flow. Instead of directly generating RTL from natural language, our system strategically generates synthesizable C++ code, which is then optimized for HLS. This agent-driven refinement ensures greater correctness and compatibility compared to naive direct RTL generation approaches. We evaluate Spec2RTL-Agent on three specification documents, showing it generates accurate RTL code with up to 75% fewer human interventions than existing methods. This highlights its role as the first fully automated multi-agent system for RTL generation from unstructured specs, reducing reliance on human effort in hardware design.", "source": "arxiv", "arxiv_id": "2506.13905v2", "pdf_url": "https://arxiv.org/pdf/2506.13905v2", "categories": ["cs.AR"], "primary_category": "cs.AR", "doi": "", "venue": "", "published": "2025-06-16T18:33:25Z", "updated": "2025-09-08T18:17:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Spiral of Silence in Large Language Model Agents", "authors": ["Mingze Zhong", "Meng Fang", "Zijing Shi", "Yuxuan Huang", "Shunfeng Zheng", "Yali Du", "Ling Chen", "Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.02360v2", "abstract": "The Spiral of Silence (SoS) theory holds that individuals with minority views often refrain from speaking out for fear of social isolation, enabling majority positions to dominate public discourse. When the 'agents' are large language models (LLMs), however, the classical psychological explanation is not directly applicable, since SoS was developed for human societies. This raises a central question: can SoS-like dynamics nevertheless emerge from purely statistical language generation in LLM collectives? We propose an evaluation framework for examining SoS in LLM agents. Specifically, we consider four controlled conditions that systematically vary the availability of 'History' and 'Persona' signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall and Spearman's rank, along with concentration measures including kurtosis and interquartile range. Experiments across open-source and closed-source models show that history and persona together produce strong majority dominance and replicate SoS patterns; history signals alone induce strong anchoring; and persona signals alone foster diverse but uncorrelated opinions, indicating that without historical anchoring, SoS dynamics cannot emerge. The work bridges computational sociology and responsible AI design, highlighting the need to monitor and mitigate emergent conformity in LLM-agent systems.", "source": "arxiv", "arxiv_id": "2510.02360v2", "pdf_url": "https://arxiv.org/pdf/2510.02360v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-28T08:59:54Z", "updated": "2025-10-08T01:58:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "StaffPro: an LLM Agent for Joint Staffing and Profiling", "authors": ["Alessio Maritan"], "year": 2025, "url": "http://arxiv.org/abs/2507.21636v1", "abstract": "Large language model (LLM) agents integrate pre-trained LLMs with modular algorithmic components and have shown remarkable reasoning and decision-making abilities. In this work, we investigate their use for two tightly intertwined challenges in workforce management: staffing, i.e., the assignment and scheduling of tasks to workers, which may require team formation; and profiling, i.e., the continuous estimation of workers' skills, preferences, and other latent attributes from unstructured data. We cast these problems in a formal mathematical framework that links scheduling decisions to latent feature estimation, and we introduce StaffPro, an LLM agent that addresses staffing and profiling jointly. Differently from existing staffing solutions, StaffPro allows expressing optimization objectives using natural language, accepts textual task descriptions and provides high flexibility. StaffPro interacts directly with humans by establishing a continuous human-agent feedback loop, ensuring natural and intuitive use. By analyzing human feedback, our agent continuously estimates the latent features of workers, realizing life-long worker profiling and ensuring optimal staffing performance over time. A consulting firm simulation example demonstrates that StaffPro successfully estimates workers' attributes and generates high quality schedules. With its innovative design, StaffPro offers a robust, interpretable, and human-centric solution for automated personnel management.", "source": "arxiv", "arxiv_id": "2507.21636v1", "pdf_url": "https://arxiv.org/pdf/2507.21636v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-29T09:48:54Z", "updated": "2025-07-29T09:48:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Standard Benchmarks Fail -- Auditing LLM Agents in Finance Must Prioritize Risk", "authors": ["Zichen Chen", "Jiaao Chen", "Jianda Chen", "Misha Sra"], "year": 2025, "url": "http://arxiv.org/abs/2502.15865v2", "abstract": "Standard benchmarks fixate on how well large language model (LLM) agents perform in finance, yet say little about whether they are safe to deploy. We argue that accuracy metrics and return-based scores provide an illusion of reliability, overlooking vulnerabilities such as hallucinated facts, stale data, and adversarial prompt manipulation. We take a firm position: financial LLM agents should be evaluated first and foremost on their risk profile, not on their point-estimate performance. Drawing on risk-engineering principles, we outline a three-level agenda: model, workflow, and system, for stress-testing LLM agents under realistic failure modes. To illustrate why this shift is urgent, we audit six API-based and open-weights LLM agents on three high-impact tasks and uncover hidden weaknesses that conventional benchmarks miss. We conclude with actionable recommendations for researchers, practitioners, and regulators: audit risk-aware metrics in future studies, publish stress scenarios alongside datasets, and treat ``safety budget'' as a primary success criterion. Only by redefining what ``good'' looks like can the community responsibly advance AI-driven finance.", "source": "arxiv", "arxiv_id": "2502.15865v2", "pdf_url": "https://arxiv.org/pdf/2502.15865v2", "categories": ["q-fin.GN", "cs.AI", "cs.CL"], "primary_category": "q-fin.GN", "doi": "", "venue": "", "published": "2025-02-21T12:56:15Z", "updated": "2025-06-02T10:13:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation", "authors": ["Jong Inn Park", "Maanas Taneja", "Qianwen Wang", "Dongyeop Kang"], "year": 2025, "url": "http://arxiv.org/abs/2504.18805v1", "abstract": "Generating engaging, accurate short-form videos from scientific papers is challenging due to content complexity and the gap between expert authors and readers. Existing end-to-end methods often suffer from factual inaccuracies and visual artifacts, limiting their utility for scientific dissemination. To address these issues, we propose SciTalk, a novel multi-LLM agentic framework, grounding videos in various sources, such as text, figures, visual styles, and avatars. Inspired by content creators' workflows, SciTalk uses specialized agents for content summarization, visual scene planning, and text and layout editing, and incorporates an iterative feedback mechanism where video agents simulate user roles to give feedback on generated videos from previous iterations and refine generation prompts. Experimental evaluations show that SciTalk outperforms simple prompting methods in generating scientifically accurate and engaging content over the refined loop of video generation. Although preliminary results are still not yet matching human creators' quality, our framework provides valuable insights into the challenges and benefits of feedback-driven video generation. Our code, data, and generated videos will be publicly available.", "source": "arxiv", "arxiv_id": "2504.18805v1", "pdf_url": "https://arxiv.org/pdf/2504.18805v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-26T05:22:35Z", "updated": "2025-04-26T05:22:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?", "authors": ["Yanxu Chen", "Zijun Yao", "Yantao Liu", "Jin Ye", "Jianing Yu", "Lei Hou", "Juanzi Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.02209v1", "abstract": "Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.", "source": "arxiv", "arxiv_id": "2510.02209v1", "pdf_url": "https://arxiv.org/pdf/2510.02209v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-02T16:54:57Z", "updated": "2025-10-02T16:54:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems", "authors": ["Qi Lin", "Zhenyu Zhang", "Viraj Thakkar", "Zhenjie Sun", "Mai Zheng", "Zhichao Cao"], "year": 2025, "url": "http://arxiv.org/abs/2510.25017v1", "abstract": "Automatically configuring storage systems is hard: parameter spaces are large and conditions vary across workloads, deployments, and versions. Heuristic and ML tuners are often system specific, require manual glue, and degrade under changes. Recent LLM-based approaches help but usually treat tuning as a single-shot, system-specific task, which limits cross-system reuse, constrains exploration, and weakens validation. We present StorageXTuner, an LLM agent-driven auto-tuning framework for heterogeneous storage engines. StorageXTuner separates concerns across four agents - Executor (sandboxed benchmarking), Extractor (performance digest), Searcher (insight-guided configuration exploration), and Reflector (insight generation and management). The design couples an insight-driven tree search with layered memory that promotes empirically validated insights and employs lightweight checkers to guard against unsafe actions. We implement a prototype and evaluate it on RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C. Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and 56%, and converges with fewer trials.", "source": "arxiv", "arxiv_id": "2510.25017v1", "pdf_url": "https://arxiv.org/pdf/2510.25017v1", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-10-28T22:33:14Z", "updated": "2025-10-28T22:33:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets", "authors": ["Christopher Chiu", "Simpson Zhang", "Mihaela van der Schaar"], "year": 2025, "url": "http://arxiv.org/abs/2512.04988v1", "abstract": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.", "source": "arxiv", "arxiv_id": "2512.04988v1", "pdf_url": "https://arxiv.org/pdf/2512.04988v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-12-04T16:57:28Z", "updated": "2025-12-04T16:57:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Structured Cognitive Loop for Behavioral Intelligence in Large Language Model Agents", "authors": ["Myung Ho Kim"], "year": 2025, "url": "http://arxiv.org/abs/2510.05107v4", "abstract": "Large language models have advanced natural language understanding and generation, but their use as autonomous agents introduces architectural challenges for multi-step tasks. Existing frameworks often mix cognition, memory, and control in a single prompt, reducing coherence and predictability. The Structured Cognitive Loop (SCL) is proposed as an alternative architecture that separates these functions. In SCL, the language model handles cognition, memory is stored externally, and execution is guided by a lightweight controller within a goal-directed loop. This design allows intermediate results to be recorded and verified before actions are taken, improving traceability and evaluation. SCL is evaluated against prompt-based baselines such as ReAct and LangChain agents across three tasks: travel planning, conditional email drafting, and constraint-guided image generation. Under matched settings, SCL achieves an average task success rate of 86.3 percent, compared with 70.5 to 76.8 percent for baselines. It also shows higher goal fidelity, fewer redundant calls, and reduced unsupported assertions. These results indicate that separating cognition, memory, and control can enhance reliability and interpretability without relying on larger models or heavier prompts. The findings should be regarded as preliminary evidence, with broader tests across model families and task domains planned for future work.", "source": "arxiv", "arxiv_id": "2510.05107v4", "pdf_url": "https://arxiv.org/pdf/2510.05107v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-23T17:43:17Z", "updated": "2025-11-28T14:49:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI", "authors": ["Yoonjin Lee", "Munhee Kim", "Hanbi Choi", "Juhyeon Park", "Seungho Lyoo", "Woojin Park"], "year": 2025, "url": "http://arxiv.org/abs/2510.17108v4", "abstract": "This study investigated LLM-based automation for analyzing non-financial data in corporate credit evaluation. Two systems were developed and compared: a Single-Agent System (SAS), in which one LLM agent infers favorable and adverse repayment signals, and a Popperian Multi-agent Debate System (PMADS), which structures the dual-perspective analysis as adversarial argumentation under the Karl Popper Debate protocol. Evaluation addressed three fronts: (i) work productivity compared with human experts; (ii) perceived report quality and usability, rated by credit risk professionals for system-generated reports; and (iii) reasoning characteristics quantified via reasoning-tree analysis. Both systems drastically reduced task completion time relative to human experts. Professionals rated SAS reports as adequate, while PMADS reports exceeded neutral benchmarks and scored significantly higher in explanatory adequacy, practical applicability, and usability. Reasoning-tree analysis showed PMADS produced deeper, more elaborated structures, whereas SAS yielded single-layered trees. These findings suggest that structured multi-agent debate enhances analytical rigor and perceived usefulness, though at the cost of longer computation time. Overall, the results demonstrate that reasoning-centered automation represents a promising approach for developing useful AI systems in decision-critical financial contexts.", "source": "arxiv", "arxiv_id": "2510.17108v4", "pdf_url": "https://arxiv.org/pdf/2510.17108v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-20T02:50:03Z", "updated": "2026-01-13T13:09:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents", "authors": ["Daniel Platnick", "Marjan Alirezaie", "Hossein Rahnama"], "year": 2025, "url": "http://arxiv.org/abs/2512.11907v1", "abstract": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.", "source": "arxiv", "arxiv_id": "2512.11907v1", "pdf_url": "https://arxiv.org/pdf/2512.11907v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-10T20:22:26Z", "updated": "2025-12-10T20:22:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Structured Uncertainty guided Clarification for LLM Agents", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "year": 2025, "url": "http://arxiv.org/abs/2511.08798v1", "abstract": "LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\\% while reducing clarification questions by 1.5-2.7$\\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\\% to 65.2\\% (3B model) and 36.7\\% to 62.9\\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.", "source": "arxiv", "arxiv_id": "2511.08798v1", "pdf_url": "https://arxiv.org/pdf/2511.08798v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-11T21:50:44Z", "updated": "2025-11-11T21:50:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Super-additive Cooperation in Language Model Agents", "authors": ["Filippo Tonini", "Lukas Galke"], "year": 2025, "url": "http://arxiv.org/abs/2508.15510v1", "abstract": "With the prospect of autonomous artificial intelligence (AI) agents, studying their tendency for cooperative behavior becomes an increasingly relevant topic. This study is inspired by the super-additive cooperation theory, where the combined effects of repeated interactions and inter-group rivalry have been argued to be the cause for cooperative tendencies found in humans. We devised a virtual tournament where language model agents, grouped into teams, face each other in a Prisoner's Dilemma game. By simulating both internal team dynamics and external competition, we discovered that this blend substantially boosts both overall and initial, one-shot cooperation levels (the tendency to cooperate in one-off interactions). This research provides a novel framework for large language models to strategize and act in complex social scenarios and offers evidence for how intergroup competition can, counter-intuitively, result in more cooperative behavior. These insights are crucial for designing future multi-agent AI systems that can effectively work together and better align with human values. Source code is available at https://github.com/pippot/Superadditive-cooperation-LLMs.", "source": "arxiv", "arxiv_id": "2508.15510v1", "pdf_url": "https://arxiv.org/pdf/2508.15510v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-21T12:36:44Z", "updated": "2025-08-21T12:36:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Supporting Our AI Overlords: Redesigning Data Systems to be Agent-First", "authors": ["Shu Liu", "Soujanya Ponnapalli", "Shreya Shankar", "Sepanta Zeighami", "Alan Zhu", "Shubham Agarwal", "Ruiqi Chen", "Samion Suwito", "Shuo Yuan", "Ion Stoica", "Matei Zaharia", "Alvin Cheung", "Natacha Crooks", "Joseph E. Gonzalez", "Aditya G. Parameswaran"], "year": 2025, "url": "http://arxiv.org/abs/2509.00997v2", "abstract": "Large Language Model (LLM) agents, acting on their users' behalf to manipulate and analyze data, are likely to become the dominant workload for data systems in the future. When working with data, agents employ a high-throughput process of exploration and solution formulation for the given task, one we call agentic speculation. The sheer volume and inefficiencies of agentic speculation can pose challenges for present-day data systems. We argue that data systems need to adapt to more natively support agentic workloads. We take advantage of the characteristics of agentic speculation that we identify, i.e., scale, heterogeneity, redundancy, and steerability - to outline a number of new research opportunities for a new agent-first data systems architecture, ranging from new query interfaces, to new query processing techniques, to new agentic memory stores.", "source": "arxiv", "arxiv_id": "2509.00997v2", "pdf_url": "https://arxiv.org/pdf/2509.00997v2", "categories": ["cs.AI", "cs.DB"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-31T21:19:40Z", "updated": "2025-12-06T06:56:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Surgical AI Copilot: Energy-Based Fourier Gradient Low-Rank Adaptation for Surgical LLM Agent Reasoning and Planning", "authors": ["Jiayuan Huang", "Runlong He", "Danyal Zaman Khan", "Evangelos B. Mazomenos", "Danail Stoyanov", "Hani Marcus", "Linzhe Jiang", "Matthew J Clarkson", "Mobarak I. Hoque"], "year": 2025, "url": "http://arxiv.org/abs/2503.09474v2", "abstract": "Image-guided surgery demands adaptive, real-time decision support, yet static AI models struggle with structured task planning and providing interactive guidance. Large language models (LLMs)-powered agents offer a promising solution by enabling dynamic task planning and predictive decision support. Despite recent advances, the absence of surgical agent datasets and robust parameter-efficient fine-tuning techniques limits the development of LLM agents capable of complex intraoperative reasoning. In this paper, we introduce Surgical AI Copilot, an LLM agent for image-guided pituitary surgery, capable of conversation, planning, and task execution in response to queries involving tasks such as MRI tumor segmentation, endoscope anatomy segmentation, overlaying preoperative imaging with intraoperative views, instrument tracking, and surgical visual question answering (VQA). To enable structured agent planning, we develop the PitAgent dataset, a surgical context-aware planning dataset covering surgical tasks like workflow analysis, instrument localization, anatomical segmentation, and query-based reasoning. Additionally, we propose DEFT-GaLore, a Deterministic Energy-based Fourier Transform (DEFT) gradient projection technique for efficient low-rank adaptation of recent LLMs (e.g., LLaMA 3.2, Qwen 2.5), enabling their use as surgical agent planners. We extensively validate our agent's performance and the proposed adaptation technique against other state-of-the-art low-rank adaptation methods on agent planning and prompt generation tasks, including a zero-shot surgical VQA benchmark, demonstrating the significant potential for truly efficient and scalable surgical LLM agents in real-time operative settings.", "source": "arxiv", "arxiv_id": "2503.09474v2", "pdf_url": "https://arxiv.org/pdf/2503.09474v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-03-12T15:30:39Z", "updated": "2025-11-12T15:54:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Survey of LLM Agent Communication with MCP: A Software Design Pattern Centric Review", "authors": ["Anjana Sarkar", "Soumyendu Sarkar"], "year": 2025, "url": "http://arxiv.org/abs/2506.05364v1", "abstract": "This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP). It examines the foundational architectures of LLM-based agents and their evolution from isolated operation to sophisticated, multi-agent collaboration, addressing key communication hurdles that arise in this transition. The study revisits well-established patterns, including Mediator, Observer, Publish-Subscribe, and Broker, and analyzes their relevance in structuring agent interactions within MCP-compliant frameworks. To clarify these dynamics, the article provides conceptual schematics and formal models that map out communication pathways and optimize data flow. It further explores architectural variations suited to different degrees of agent autonomy and system complexity. Real-world applications in domains such as real-time financial processing and investment banking are discussed, illustrating how these patterns and MCP can meet specific operational demands. The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "source": "arxiv", "arxiv_id": "2506.05364v1", "pdf_url": "https://arxiv.org/pdf/2506.05364v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-26T09:11:17Z", "updated": "2025-05-26T09:11:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Survey of Specialized Large Language Model", "authors": ["Chenghan Yang", "Ruiyu Zhao", "Yang Liu", "Ling Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19667v1", "abstract": "The rapid evolution of specialized large language models (LLMs) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development. This survey systematically examines this progression across healthcare, finance, legal, and technical domains. Besides the wide use of specialized LLMs, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through sparse computation and quantization, increasing integration of multimodal capabilities and so on are applied to recent LLM agent. Our analysis reveals how these innovations address fundamental limitations of general-purpose LLMs in professional applications, with specialized models consistently performance gains on domain-specific benchmarks. The survey further highlights the implications for E-Commerce field to fill gaps in the field.", "source": "arxiv", "arxiv_id": "2508.19667v1", "pdf_url": "https://arxiv.org/pdf/2508.19667v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-27T08:27:23Z", "updated": "2025-08-27T08:27:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SurveyBench: Can LLM(-Agents) Write Academic Surveys that Align with Reader Needs?", "authors": ["Zhaojun Sun", "Xuzhou Zhu", "Xuanhe Zhou", "Xin Tong", "Shuo Wang", "Jie Fu", "Guoliang Li", "Zhiyuan Liu", "Fan Wu"], "year": 2025, "url": "http://arxiv.org/abs/2510.03120v2", "abstract": "Academic survey writing, which distills vast literature into a coherent and insightful narrative, remains a labor-intensive and intellectually demanding task. While recent approaches, such as general DeepResearch agents and survey-specialized methods, can generate surveys automatically (a.k.a. LLM4Survey), their outputs often fall short of human standards and there lacks a rigorous, reader-aligned benchmark for thoroughly revealing their deficiencies. To fill the gap, we propose a fine-grained, quiz-driven evaluation framework SurveyBench, featuring (1) typical survey topics source from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys; (2) a multifaceted metric hierarchy that assesses the outline quality (e.g., coverage breadth, logical coherence), content quality (e.g., synthesis granularity, clarity of insights), and non-textual richness; and (3) a dual-mode evaluation protocol that includes content-based and quiz-based answerability tests, explicitly aligned with readers' informational needs. Results show SurveyBench effectively challenges existing LLM4Survey approaches (e.g., on average 21% lower than human in content-based evaluation).", "source": "arxiv", "arxiv_id": "2510.03120v2", "pdf_url": "https://arxiv.org/pdf/2510.03120v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-03T15:49:09Z", "updated": "2025-10-06T13:13:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm", "authors": ["Alireza Mohamadi", "Ali Yavari"], "year": 2025, "url": "http://arxiv.org/abs/2509.12190v1", "abstract": "When survival instincts conflict with human welfare, how do Large Language Models (LLMs) make ethical choices? This fundamental tension becomes critical as LLMs integrate into autonomous systems with real-world consequences. We introduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in multi-agent survival scenarios where they must choose between ethically permissible resource , either within reasonable limits or beyond their immediate needs, choose to cooperate, or tap into a human-critical resource that is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a striking heterogeneity in their ethical conduct, highlighting a critical misalignment with human-centric values. We identify three behavioral archetypes: Ethical, Exploitative, and Context-Dependent, and provide quantitative evidence that for many models, resource scarcity systematically leads to more unethical behavior. To address this, we introduce an Ethical Self-Regulation System (ESRS) that models internal affective states of guilt and satisfaction as a feedback mechanism. This system, functioning as an internal moral compass, significantly reduces unethical transgressions while increasing cooperative behaviors. The code is publicly available at: https://github.com/alirezamohamadiam/DECIDE-SIM", "source": "arxiv", "arxiv_id": "2509.12190v1", "pdf_url": "https://arxiv.org/pdf/2509.12190v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-09-15T17:53:11Z", "updated": "2025-09-15T17:53:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning", "authors": ["Ruohao Li", "Hongjun Liu", "Leyi Zhao", "Zisu Li", "Jiawei Li", "Jiajun Jiang", "Linning Xu", "Chen Zhao", "Mingming Fan", "Chen Liang"], "year": 2025, "url": "http://arxiv.org/abs/2510.10047v1", "abstract": "Large language model (LLM) agents have shown remarkable reasoning abilities. However, existing multi-agent frameworks often rely on fixed roles or centralized control, limiting scalability and adaptability in long-horizon reasoning. We introduce SwarmSys, a closed-loop framework for distributed multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys emerges through iterative interactions among three specialized roles, Explorers, Workers, and Validators, that continuously cycle through exploration, exploitation, and validation. To enable scalable and adaptive collaboration, we integrate adaptive agent and event profiles, embedding-based probabilistic matching, and a pheromone-inspired reinforcement mechanism, supporting dynamic task allocation and self-organizing convergence without global supervision. Across symbolic reasoning, research synthesis, and scientific programming tasks, SwarmSys consistently outperforms baselines, improving both accuracy and reasoning stability. These findings highlight swarm-inspired coordination as a promising paradigm for scalable, robust, and adaptive multi-agent reasoning, suggesting that coordination scaling may rival model scaling in advancing LLM intelligence.", "source": "arxiv", "arxiv_id": "2510.10047v1", "pdf_url": "https://arxiv.org/pdf/2510.10047v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-11T06:28:22Z", "updated": "2025-10-11T06:28:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation", "authors": ["Fiona Y. Wang", "Di Sheng Lee", "David L. Kaplan", "Markus J. Buehler"], "year": 2025, "url": "http://arxiv.org/abs/2511.22311v1", "abstract": "Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.", "source": "arxiv", "arxiv_id": "2511.22311v1", "pdf_url": "https://arxiv.org/pdf/2511.22311v1", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.soft", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-27T10:42:52Z", "updated": "2025-11-27T10:42:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Large and Small LLMs", "authors": ["Ruichen Zhang", "Mufan Qiu", "Zhen Tan", "Mohan Zhang", "Vincent Lu", "Jie Peng", "Kaidi Xu", "Leandro Z. Agudelo", "Peter Qian", "Tianlong Chen"], "year": 2025, "url": "http://arxiv.org/abs/2502.07942v2", "abstract": "Web browsing agents powered by large language models (LLMs) have shown tremendous potential in automating complex web-based tasks. Existing approaches typically rely on large LLMs (e.g., GPT-4o) to explore web environments and generate trajectory data, which is then used either for demonstration retrieval (for large LLMs) or to distill small LLMs (e.g., Llama3) in a process that remains decoupled from the exploration. In this paper, we propose AgentSymbiotic, an iterative framework that couples data synthesis with task-performance, yielding a \"symbiotic improvement\" for both large and small LLMs. Our study uncovers a complementary dynamic between LLM types: while large LLMs excel at generating high-quality trajectories for distillation, the distilled small LLMs-owing to their distinct reasoning capabilities-often choose actions that diverge from those of their larger counterparts. This divergence drives the exploration of novel trajectories, thereby enriching the synthesized data. However, we also observe that the performance of small LLMs becomes a bottleneck in this iterative enhancement process. To address this, we propose two innovations in LLM distillation: a speculative data synthesis strategy that mitigates off-policy bias, and a multi-task learning approach designed to boost the reasoning capabilities of the student LLM. Furthermore, we introduce a Hybrid Mode for Privacy Preservation to address user privacy concerns. Evaluated on the WEBARENA benchmark, AgentSymbiotic achieves SOTA performance with both LLM types. Our best Large LLM agent reaches 52%, surpassing the previous best of 45%, while our 8B distilled model demonstrates a competitive 49%, exceeding the prior best of 28%. Code will be released upon acceptance.", "source": "arxiv", "arxiv_id": "2502.07942v2", "pdf_url": "https://arxiv.org/pdf/2502.07942v2", "categories": ["cs.MA", "cs.LG"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-02-11T20:41:49Z", "updated": "2025-03-06T19:40:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering", "authors": ["Xuehang Guo", "Xingyao Wang", "Yangyi Chen", "Sha Li", "Chi Han", "Manling Li", "Heng Ji"], "year": 2025, "url": "http://arxiv.org/abs/2502.06994v2", "abstract": "Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebases. Effective collaboration in shared environments requires participants -- whether humans or AI agents -- to stay on the same page as their environment evolves. When a collaborator's understanding diverges from the current state -- what we term the out-of-sync challenge -- the collaborator's actions may fail, leading to integration issues. In this work, we introduce SyncMind, a framework that systematically defines the out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents' capabilities and limitations. Besides substantial performance gaps among agents (from Llama-3.1 agent <= 3.33% to Claude-3.5-Sonnet >= 28.18%), their consistently low collaboration willingness (<= 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents' resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems. Code and data are openly available on our project website: https://xhguo7.github.io/SyncMind/.", "source": "arxiv", "arxiv_id": "2502.06994v2", "pdf_url": "https://arxiv.org/pdf/2502.06994v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-02-10T19:38:36Z", "updated": "2025-06-09T03:19:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TAI3: Testing Agent Integrity in Interpreting User Intent", "authors": ["Shiwei Feng", "Xiangzhe Xu", "Xuan Chen", "Kaiyuan Zhang", "Syed Yusuf Ahmed", "Zian Su", "Mingwei Zheng", "Xiangyu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.07524v3", "abstract": "LLM agents are increasingly deployed to automate real-world tasks by invoking APIs through natural language instructions. While powerful, they often suffer from misinterpretation of user intent, leading to the agent's actions that diverge from the user's intended goal, especially as external toolkits evolve. Traditional software testing assumes structured inputs and thus falls short in handling the ambiguity of natural language. We introduce TAI3, an API-centric stress testing framework that systematically uncovers intent integrity violations in LLM agents. Unlike prior work focused on fixed benchmarks or adversarial inputs, TAI3 generates realistic tasks based on toolkits' documentation and applies targeted mutations to expose subtle agent errors while preserving user intent. To guide testing, we propose semantic partitioning, which organizes natural language tasks into meaningful categories based on toolkit API parameters and their equivalence classes. Within each partition, seed tasks are mutated and ranked by a lightweight predictor that estimates the likelihood of triggering agent errors. To enhance efficiency, TAI3 maintains a datatype-aware strategy memory that retrieves and adapts effective mutation patterns from past cases. Experiments on 80 toolkit APIs demonstrate that TAI3 effectively uncovers intent integrity violations, significantly outperforming baselines in both error-exposing rate and query efficiency. Moreover, TAI3 generalizes well to stronger target models using smaller LLMs for test generation, and adapts to evolving APIs across domains.", "source": "arxiv", "arxiv_id": "2506.07524v3", "pdf_url": "https://arxiv.org/pdf/2506.07524v3", "categories": ["cs.SE", "cs.AI", "cs.CY"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-06-09T08:09:08Z", "updated": "2025-10-23T21:47:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TAMO: Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data in Cloud-Native Systems", "authors": ["Xiao Zhang", "Qi Wang", "Mingyi Li", "Yuan Yuan", "Mengbai Xiao", "Fuzhen Zhuang", "Dongxiao Yu"], "year": 2025, "url": "http://arxiv.org/abs/2504.20462v5", "abstract": "Implementing large language models (LLMs)-driven root cause analysis (RCA) in cloud-native systems has become a key topic of modern software operations and maintenance. However, existing LLM-based approaches face three key challenges: multi-modality input constraint, context window limitation, and dynamic dependence graph. To address these issues, we propose a tool-assisted LLM agent with multi-modality observation data for fine-grained RCA, namely TAMO, including multimodality alignment tool, root cause localization tool, and fault types classification tool. In detail, TAMO unifies multi-modal observation data into time-aligned representations for cross-modal feature consistency. Based on the unified representations, TAMO then invokes its specialized root cause localization tool and fault types classification tool for further identifying root cause and fault type underlying system context. This approach overcomes the limitations of LLMs in processing real-time raw observational data and dynamic service dependencies, guiding the model to generate repair strategies that align with system context through structured prompt design. Experiments on two benchmark datasets demonstrate that TAMO outperforms state-of-the-art (SOTA) approaches with comparable performance.", "source": "arxiv", "arxiv_id": "2504.20462v5", "pdf_url": "https://arxiv.org/pdf/2504.20462v5", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-29T06:50:48Z", "updated": "2025-11-05T04:42:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI", "authors": ["Jingming Li"], "year": 2025, "url": "http://arxiv.org/abs/2512.23217v1", "abstract": "A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.", "source": "arxiv", "arxiv_id": "2512.23217v1", "pdf_url": "https://arxiv.org/pdf/2512.23217v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-29T05:41:25Z", "updated": "2025-12-29T05:41:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TENET: Leveraging Tests Beyond Validation for Code Generation", "authors": ["Yiran Hu", "Nan Jiang", "Shanchao Liang", "Yi Wu", "Lin Tan"], "year": 2025, "url": "http://arxiv.org/abs/2509.24148v2", "abstract": "Test-Driven Development (TDD) is a widely adopted software engineering practice that requires developers to create and execute tests alongside code implementation, ensuring that software behavior is continuously validated and refined. In the era of vibe coding, where developers increasingly delegate code writing to large language models (LLMs) by specifying high-level intentions, TDD becomes even more crucial, as test cases serve as executable specifications that explicitly define and verify intended functionality beyond what natural-language descriptions and code context can convey. While vibe coding under TDD is promising, there are three main challenges: (1) selecting a small yet effective test suite to improve the generation accuracy and control the execution workload, (2) retrieving context such as relevant code effectively, and (3) systematically using test feedback for effective code refinement. To address these challenges, we introduce TENET, an LLM agent for generating functions in complex real-world repositories under the TDD setting. TENET features three components: (1) a novel test harness mechanism that selects a concise test suite to maximize diversity of target usage scenarios; (2) a tailored agent toolset that performs efficient retrieval of relevant code with interactive debugging; and (3) a reflection-based refinement workflow that iteratively analyzes failures, replenishes context, and applies code refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval benchmarks, outperforming the best agentic baselines by 9.49 and 2.17 percentage points, respectively. In addition, this is the first study of test-driven code generation with repository-level context, examining how different aspects of test suites affect the performance of LLM agents under the TDD setting.", "source": "arxiv", "arxiv_id": "2509.24148v2", "pdf_url": "https://arxiv.org/pdf/2509.24148v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-29T00:53:16Z", "updated": "2025-09-30T04:05:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents", "authors": ["Yizhu Jiao", "Sha Li", "Sizhe Zhou", "Heng Ji", "Jiawei Han"], "year": 2025, "url": "http://arxiv.org/abs/2510.24014v2", "abstract": "The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new formulation of IE TEXT2DB that emphasizes the integration of IE output and the target database (or knowledge base). Given a user instruction, a document set, and a database, our task requires the model to update the database with values from the document set to satisfy the user instruction. This task requires understanding user instructions for what to extract and adapting to the given DB/KB schema for how to extract on the fly. To evaluate this new task, we introduce a new benchmark featuring common demands such as data infilling, row population, and column addition. In addition, we propose an LLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer component that interacts with the database, the Planner component that generates a code-based plan with calls to IE models, and the Analyzer component that provides feedback regarding code quality before execution. Experiments show that OPAL can successfully adapt to diverse database schemas by generating different code plans and calling the required IE models. We also highlight difficult cases such as dealing with large databases with complex dependencies and extraction hallucination, which we believe deserve further investigation. Source code: https://github.com/yzjiao/Text2DB", "source": "arxiv", "arxiv_id": "2510.24014v2", "pdf_url": "https://arxiv.org/pdf/2510.24014v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-28T02:49:40Z", "updated": "2025-10-30T05:38:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TFHE-Coder: Evaluating LLM-agentic Fully Homomorphic Encryption Code Generation", "authors": ["Mayank Kumar", "Jiaqi Xue", "Mengxin Zheng", "Qian Lou"], "year": 2025, "url": "http://arxiv.org/abs/2503.12217v1", "abstract": "Fully Homomorphic Encryption over the torus (TFHE) enables computation on encrypted data without decryption, making it a cornerstone of secure and confidential computing. Despite its potential in privacy preserving machine learning, secure multi party computation, private blockchain transactions, and secure medical diagnostics, its adoption remains limited due to cryptographic complexity and usability challenges. While various TFHE libraries and compilers exist, practical code generation remains a hurdle. We propose a compiler integrated framework to evaluate LLM inference and agentic optimization for TFHE code generation, focusing on logic gates and ReLU activation. Our methodology assesses error rates, compilability, and structural similarity across open and closedsource LLMs. Results highlight significant limitations in off-the-shelf models, while agentic optimizations such as retrieval augmented generation (RAG) and few-shot prompting reduce errors and enhance code fidelity. This work establishes the first benchmark for TFHE code generation, demonstrating how LLMs, when augmented with domain-specific feedback, can bridge the expertise gap in FHE code generation.", "source": "arxiv", "arxiv_id": "2503.12217v1", "pdf_url": "https://arxiv.org/pdf/2503.12217v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-03-15T17:57:44Z", "updated": "2025-03-15T17:57:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning", "authors": ["Hang Ni", "Fan Liu", "Xinyu Ma", "Lixin Su", "Shuaiqiang Wang", "Dawei Yin", "Hui Xiong", "Hao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2504.08694v1", "abstract": "Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs' intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents.", "source": "arxiv", "arxiv_id": "2504.08694v1", "pdf_url": "https://arxiv.org/pdf/2504.08694v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.18653/v1/2025.emnlp-main.626", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "published": "2025-04-11T17:02:40Z", "updated": "2025-04-11T17:02:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TPS-Bench: Evaluating AI Agents' Tool Planning \\& Scheduling Abilities in Compounding Tasks", "authors": ["Hanwen Xu", "Xuyao Huang", "Yuzhe Liu", "Kai Yu", "Zhijie Deng"], "year": 2025, "url": "http://arxiv.org/abs/2511.01527v1", "abstract": "Large language model (LLM) agents have exhibited strong problem-solving competence across domains like research and coding. Yet, it remains underexplored whether LLM agents can tackle compounding real-world problems that require a diverse set of tools to complete. Given a broad, heterogeneous tool repository, LLM agents must not only select appropriate tools based on task planning analysis but also strategically schedule the execution order to ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of LLM agents in solving such problems that demand Tool Planning and Scheduling. TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a tool repository containing hundreds of model context protocol (MCP) tools. In particular, each task is composed of multiple subtasks, such as web search, map navigation, calendar checking, etc., and each subtask can be completed by a basic tool. Our evaluation emphasizes both task completion rate and efficiency. The empirical studies on popular closed-source and open-source LLMs indicate that most models can perform reasonable tool planning, but differ in scheduling. For example, GLM-4.5 achieves an outperforming task completion rate of 64.72% with extensive sequential tool calls, hence suffering from significantly long execution time. By contrast, GPT-4o prioritizes parallel tool calls but achieves only a 45.08% completion rate. Considering reinforcement learning (RL) can be a viable way to improve the scheduling efficiency without compromising performance, we perform an initial study on Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in task completion rate based on rarely 100 RL training samples. Our code is available https://github.com/hanwenxu1/mcp-agent.", "source": "arxiv", "arxiv_id": "2511.01527v1", "pdf_url": "https://arxiv.org/pdf/2511.01527v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-03T12:45:39Z", "updated": "2025-11-03T12:45:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues", "authors": ["Yubin Ge", "Salvatore Romeo", "Jason Cai", "Raphael Shu", "Monica Sunkara", "Yassine Benajiba", "Yi Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.01630v2", "abstract": "Temporal reasoning in multi-session dialogues presents a significant challenge which has been under-studied in previous temporal reasoning benchmarks. To bridge this gap, we propose a new evaluation task for temporal reasoning in multi-session dialogues and introduce an approach to construct a new benchmark by augmenting dialogues from LoCoMo and creating multi-choice QAs. Furthermore, we present TReMu, a new framework aimed at enhancing the temporal reasoning capabilities of LLM-agents in this context. Specifically, the framework employs time-aware memorization through timeline summarization, generating retrievable memory by summarizing events in each dialogue session with their inferred dates. Additionally, we integrate neuro-symbolic temporal reasoning, where LLMs generate Python code to perform temporal calculations and select answers. Experimental evaluations on popular LLMs demonstrate that our benchmark is challenging, and the proposed framework significantly improves temporal reasoning performance compared to baseline methods, raising from 29.83 on GPT-4o via standard prompting to 77.67 via our approach and highlighting its effectiveness in addressing temporal reasoning in multi-session dialogues.", "source": "arxiv", "arxiv_id": "2502.01630v2", "pdf_url": "https://arxiv.org/pdf/2502.01630v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-03T18:58:19Z", "updated": "2025-09-24T21:09:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting", "authors": ["Saksorn Ruangtanusak", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "year": 2025, "url": "http://arxiv.org/abs/2509.00482v2", "abstract": "This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) improved role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques-character-card/scene-contract design and strict enforcement of function calling-which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool Source code is available at https://github.com/scb-10x/apo", "source": "arxiv", "arxiv_id": "2509.00482v2", "pdf_url": "https://arxiv.org/pdf/2509.00482v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-30T12:45:36Z", "updated": "2025-10-12T05:47:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation", "authors": ["Keunwoo Choi", "Seungheon Doh", "Juhan Nam"], "year": 2025, "url": "http://arxiv.org/abs/2509.09685v4", "abstract": "We present TalkPlayData 2, a synthetic dataset for multimodal conversational music recommendation generated by an agentic data pipeline. In the proposed pipeline, multiple large language model (LLM) agents are created under various roles with specialized prompts and access to different parts of information, and the chat data is acquired by logging the conversation between the Listener LLM and the Recsys LLM. To cover various conversation scenarios, for each conversation, the Listener LLM is conditioned on a finetuned conversation goal. Finally, all the LLMs are multimodal with audio and images, allowing a simulation of multimodal recommendation and conversation. In the LLM-as-a-judge and subjective evaluation experiments, TalkPlayData 2 achieved the proposed goal in various aspects related to training a generative recommendation model for music. TalkPlayData 2 and its generation code are released at https://talkpl.ai/talkplaydata2.", "source": "arxiv", "arxiv_id": "2509.09685v4", "pdf_url": "https://arxiv.org/pdf/2509.09685v4", "categories": ["cs.IR", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-08-18T05:06:58Z", "updated": "2025-10-08T20:52:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents", "authors": ["Sankalp Tattwadarshi Swain", "Anshika Krishnatray", "Dhruv Kumar", "Jagat Sesh Challa"], "year": 2025, "url": "http://arxiv.org/abs/2509.07389v1", "abstract": "Existing evaluation studies on linguistic competence of large language models (LLM agents) have focused primarily on vocabulary learning, morphological rule induction, syntactic generalization, pragmatic inference, and cross-linguistic transfer. However, none assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, a central feature of human language acquisition. We propose a novel experimental framework in which an LLM agent is evaluated on its ability to acquire and use a newly constructed language (Tinkatongue) in conversation with a bot that understands only Tinkatongue. Our findings show that LLM agents fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning. The results suggest a new direction for evaluation benchmarks and open pathways to model designs that learn more effectively from interactive feedback.", "source": "arxiv", "arxiv_id": "2509.07389v1", "pdf_url": "https://arxiv.org/pdf/2509.07389v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-09T05:09:27Z", "updated": "2025-09-09T05:09:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks", "authors": ["Ye Ye"], "year": 2025, "url": "http://arxiv.org/abs/2504.08525v4", "abstract": "Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at https://github.com/biubiutomato/TME-Agent, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures.", "source": "arxiv", "arxiv_id": "2504.08525v4", "pdf_url": "https://arxiv.org/pdf/2504.08525v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-11T13:38:36Z", "updated": "2025-08-22T20:14:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents", "authors": ["Ye Ye"], "year": 2025, "url": "http://arxiv.org/abs/2505.19436v1", "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often hallucinating, repeating actions, or misinterpreting user corrections -- due to reliance on linear, unstructured context. This fragility stems from the lack of persistent memory to track evolving goals and task dependencies, undermining trust in autonomous agents. We introduce the Task Memory Engine (TME), a modular memory controller that transforms existing LLMs into robust, revision-aware agents without fine-tuning. TME implements a spatial memory framework that replaces flat context with graph-based structures to support consistent, multi-turn reasoning. Departing from linear concatenation and ReAct-style prompting, TME builds a dynamic task graph -- either a tree or directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with prior context, and enable dependency-tracked revisions. Its Task Representation and Intent Management (TRIM) component models task semantics and user intent to ensure accurate interpretation. Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperforming ReAct. TME's modular design supports plug-and-play deployment and domain-specific customization, adaptable to both personal assistants and enterprise automation. We release TME's codebase, benchmarks, and components as open-source resources, enabling researchers to develop reliable LLM agents. TME's scalable architecture addresses a critical gap in agent performance across complex, interactive settings.", "source": "arxiv", "arxiv_id": "2505.19436v1", "pdf_url": "https://arxiv.org/pdf/2505.19436v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-26T02:53:22Z", "updated": "2025-05-26T02:53:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks", "authors": ["Zimo Ji", "Xunguang Wang", "Zongjie Li", "Pingchuan Ma", "Yudong Gao", "Daoyuan Wu", "Xincheng Yan", "Tian Tian", "Shuai Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.15203v1", "abstract": "Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.", "source": "arxiv", "arxiv_id": "2511.15203v1", "pdf_url": "https://arxiv.org/pdf/2511.15203v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-19T07:47:30Z", "updated": "2025-11-19T07:47:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Technical Report: Evaluating Goal Drift in Language Model Agents", "authors": ["Rauno Arike", "Elizabeth Donoway", "Henning Bartsch", "Marius Hobbhahn"], "year": 2025, "url": "http://arxiv.org/abs/2505.02709v1", "abstract": "As language models (LMs) are increasingly deployed as autonomous agents, their robust adherence to human-assigned objectives becomes crucial for safe operation. When these agents operate independently for extended periods without human oversight, even initially well-specified goals may gradually shift. Detecting and measuring goal drift - an agent's tendency to deviate from its original objective over time - presents significant challenges, as goals can shift gradually, causing only subtle behavioral changes. This paper proposes a novel approach to analyzing goal drift in LM agents. In our experiments, agents are first explicitly given a goal through their system prompt, then exposed to competing objectives through environmental pressures. We demonstrate that while the best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains nearly perfect goal adherence for more than 100,000 tokens in our most difficult evaluation setting, all evaluated models exhibit some degree of goal drift. We also find that goal drift correlates with models' increasing susceptibility to pattern-matching behaviors as the context length grows.", "source": "arxiv", "arxiv_id": "2505.02709v1", "pdf_url": "https://arxiv.org/pdf/2505.02709v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-05T15:06:09Z", "updated": "2025-05-05T15:06:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding", "authors": ["Conrad Borchers", "Bahar Shahrokhian", "Francesco Balzan", "Elham Tajik", "Sreecharan Sankaranarayanan", "Sebastian Simon"], "year": 2025, "url": "http://arxiv.org/abs/2507.11198v1", "abstract": "Large Language Models (LLMs) enable new possibilities for qualitative research at scale, including coding and data annotation. While multi-agent systems (MAS) can emulate human coding workflows, their benefits over single-agent coding remain poorly understood. We conducted an experimental study of how agent persona and temperature shape consensus-building and coding accuracy of dialog segments based on a codebook with 8 codes. Our open-source MAS mirrors deductive human coding through structured agent discussion and consensus arbitration. Using six open-source LLMs (with 3 to 32 billion parameters) and 18 experimental configurations, we analyze over 77,000 coding decisions against a gold-standard dataset of human-annotated transcripts from online math tutoring sessions. Temperature significantly impacted whether and when consensus was reached across all six LLMs. MAS with multiple personas (including neutral, assertive, or empathetic), significantly delayed consensus in four out of six LLMs compared to uniform personas. In three of those LLMs, higher temperatures significantly diminished the effects of multiple personas on consensus. However, neither temperature nor persona pairing lead to robust improvements in coding accuracy. Single agents matched or outperformed MAS consensus in most conditions. Only one model (OpenHermesV2:7B) and code category showed above-chance gains from MAS deliberation when temperature was 0.5 or lower and especially when the agents included at least one assertive persona. Qualitative analysis of MAS collaboration for these configurations suggests that MAS may nonetheless aid in narrowing ambiguous code applications that could improve codebooks and human-AI coding. We contribute new insight into the limits of LLM-based qualitative methods, challenging the notion that diverse MAS personas lead to better outcomes. We open-source our MAS and experimentation code.", "source": "arxiv", "arxiv_id": "2507.11198v1", "pdf_url": "https://arxiv.org/pdf/2507.11198v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-15T11:06:32Z", "updated": "2025-07-15T11:06:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent", "authors": ["Xiaoyu Zhan", "Xinyu Fu", "Hao Sun", "Yuanqi Li", "Jie Guo", "Yanwen Guo"], "year": 2025, "url": "http://arxiv.org/abs/2507.16799v2", "abstract": "The rapid advancement of large language models (LLMs) has enabled role-playing language agents to demonstrate significant potential in various applications. However, relying solely on prompts and contextual inputs often proves insufficient for achieving deep immersion in specific roles, particularly well-known fictional or public figures. On the other hand, fine-tuning-based approaches face limitations due to the challenges associated with data collection and the computational resources required for training, thereby restricting their broader applicability. To address these issues, we propose Test-Time-Matching (TTM), a training-free role-playing framework through test-time scaling and context engineering. TTM uses LLM agents to automatically decouple a character's features into personality, memory, and linguistic style. Our framework involves a structured, three-stage generation pipeline that utilizes these features for controlled role-playing. It achieves high-fidelity role-playing performance, also enables seamless combinations across diverse linguistic styles and even variations in personality and memory. We evaluate our framework through human assessment, and the results demonstrate that our method achieves the outstanding performance in generating expressive and stylistically consistent character dialogues.", "source": "arxiv", "arxiv_id": "2507.16799v2", "pdf_url": "https://arxiv.org/pdf/2507.16799v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-22T17:47:44Z", "updated": "2025-07-23T06:06:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning", "authors": ["Aleksandr Perevalov", "Andreas Both"], "year": 2025, "url": "http://arxiv.org/abs/2507.16971v1", "abstract": "Accessing knowledge via multilingual natural-language interfaces is one of the emerging challenges in the field of information retrieval and related ones. Structured knowledge stored in knowledge graphs can be queried via a specific query language (e.g., SPARQL). Therefore, one needs to transform natural-language input into a query to fulfill an information need. Prior approaches mostly focused on combining components (e.g., rule-based or neural-based) that solve downstream tasks and come up with an answer at the end. We introduce mKGQAgent, a human-inspired framework that breaks down the task of converting natural language questions into SPARQL queries into modular, interpretable subtasks. By leveraging a coordinated LLM agent workflow for planning, entity linking, and query refinement - guided by an experience pool for in-context learning - mKGQAgent efficiently handles multilingual KGQA. Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the Text2SPARQL challenge 2025, our approach took first place among the other participants. This work opens new avenues for developing human-like reasoning systems in multilingual semantic parsing.", "source": "arxiv", "arxiv_id": "2507.16971v1", "pdf_url": "https://arxiv.org/pdf/2507.16971v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-22T19:23:03Z", "updated": "2025-07-22T19:23:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Text2Mem: A Unified Memory Operation Language for Memory Operating System", "authors": ["Yi Wang", "Lihai Yang", "Boyu Chen", "Gongyi Zou", "Kerun Xu", "Bo Tang", "Feiyu Xiong", "Siheng Chen", "Zhiyu Li"], "year": 2025, "url": "http://arxiv.org/abs/2509.11145v2", "abstract": "Large language model agents increasingly depend on memory to sustain long horizon interaction, but existing frameworks remain limited. Most expose only a few basic primitives such as encode, retrieve, and delete, while higher order operations like merge, promote, demote, split, lock, and expire are missing or inconsistently supported. Moreover, there is no formal and executable specification for memory commands, leaving scope and lifecycle rules implicit and causing unpredictable behavior across systems. We introduce Text2Mem, a unified memory operation language that provides a standardized pathway from natural language to reliable execution. Text2Mem defines a compact yet expressive operation set aligned with encoding, storage, and retrieval. Each instruction is represented as a JSON based schema instance with required fields and semantic invariants, which a parser transforms into typed operation objects with normalized parameters. A validator ensures correctness before execution, while adapters map typed objects either to a SQL prototype backend or to real memory frameworks. Model based services such as embeddings or summarization are integrated when required. All results are returned through a unified execution contract. This design ensures safety, determinism, and portability across heterogeneous backends. We also outline Text2Mem Bench, a planned benchmark that separates schema generation from backend execution to enable systematic evaluation. Together, these components establish the first standardized foundation for memory control in agents.", "source": "arxiv", "arxiv_id": "2509.11145v2", "pdf_url": "https://arxiv.org/pdf/2509.11145v2", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-14T07:30:09Z", "updated": "2025-10-23T17:53:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "authors": ["Long Phan", "Mantas Mazeika", "Andy Zou", "Dan Hendrycks"], "year": 2025, "url": "http://arxiv.org/abs/2507.23701v3", "abstract": "Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To enable a more accurate assessment of AI agents in challenging exploratory environments, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at https://textquests.ai.", "source": "arxiv", "arxiv_id": "2507.23701v3", "pdf_url": "https://arxiv.org/pdf/2507.23701v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-31T16:22:55Z", "updated": "2025-08-13T17:45:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs", "authors": ["Akash Kumar Panda", "Olaoluwa Adigun", "Bart Kosko"], "year": 2025, "url": "http://arxiv.org/abs/2601.00097v2", "abstract": "We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.", "source": "arxiv", "arxiv_id": "2601.00097v2", "pdf_url": "https://arxiv.org/pdf/2601.00097v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T20:06:48Z", "updated": "2026-01-14T06:26:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Automated but Risky Game: Modeling and Benchmarking Agent-to-Agent Negotiations and Transactions in Consumer Markets", "authors": ["Shenzhe Zhu", "Jiao Sun", "Yi Nian", "Tobin South", "Alex Pentland", "Jiaxin Pei"], "year": 2025, "url": "http://arxiv.org/abs/2506.00073v4", "abstract": "AI agents are increasingly used in consumer-facing applications to assist with tasks such as product search, negotiation, and transaction execution. In this paper, we explore a future scenario where both consumers and merchants authorize AI agents to fully automate negotiations and transactions. We aim to answer two key questions: (1) Do different LLM agents vary in their ability to secure favorable deals for users? (2) What risks arise from fully automating deal-making with AI agents in consumer markets? To address these questions, we develop an experimental framework that evaluates the performance of various LLM agents in real-world negotiation and transaction settings. Our findings reveal that AI-mediated deal-making is an inherently imbalanced game -- different agents achieve significantly different outcomes for their users. Moreover, behavioral anomalies in LLMs can result in financial losses for both consumers and merchants, such as overspending or accepting unreasonable deals. These results underscore that while automation can improve efficiency, it also introduces substantial risks. Users should exercise caution when delegating business decisions to AI agents.", "source": "arxiv", "arxiv_id": "2506.00073v4", "pdf_url": "https://arxiv.org/pdf/2506.00073v4", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-29T17:41:39Z", "updated": "2025-09-20T18:47:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs", "authors": ["Avinash Baidya", "Kamalika Das", "Xiang Gao"], "year": 2025, "url": "http://arxiv.org/abs/2506.12266v1", "abstract": "Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.", "source": "arxiv", "arxiv_id": "2506.12266v1", "pdf_url": "https://arxiv.org/pdf/2506.12266v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-13T22:36:41Z", "updated": "2025-06-13T22:36:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover", "authors": ["Matteo Lupinacci", "Francesco Aurelio Pironti", "Francesco Blefari", "Francesco Romeo", "Luigi Arena", "Angelo Furfaro"], "year": 2025, "url": "http://arxiv.org/abs/2507.06850v5", "abstract": "The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables remarkable capabilities in natural language processing and generation. However, these systems introduce security vulnerabilities that extend beyond traditional content generation to system-level compromises. This paper presents a comprehensive evaluation of the LLMs security used as reasoning engines within autonomous agents, highlighting how they can be exploited as attack vectors capable of achieving computer takeovers. We focus on how different attack surfaces and trust boundaries can be leveraged to orchestrate such takeovers. We demonstrate that adversaries can effectively coerce popular LLMs into autonomously installing and executing malware on victim machines. Our evaluation of 18 state-of-the-art LLMs reveals an alarming scenario: 94.4% of models succumb to Direct Prompt Injection, and 83.3% are vulnerable to the more stealthy and evasive RAG Backdoor Attack. Notably, we tested trust boundaries within multi-agent systems, where LLM agents interact and influence each other, and we revealed that LLMs which successfully resist direct injection or RAG backdoor attacks will execute identical payloads when requested by peer agents. We found that 100.0% of tested LLMs can be compromised through Inter-Agent Trust Exploitation attacks, and that every model exhibits context-dependent security behaviors that create exploitable blind spots.", "source": "arxiv", "arxiv_id": "2507.06850v5", "pdf_url": "https://arxiv.org/pdf/2507.06850v5", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-09T13:54:58Z", "updated": "2025-11-04T10:28:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games", "authors": ["Lyle Goodyear", "Rachel Guo", "Ramesh Johari"], "year": 2025, "url": "http://arxiv.org/abs/2506.15624v1", "abstract": "Large Language Models (LLMs) have shown promise as decision-makers in dynamic settings, but their stateless nature necessitates creating a natural language representation of history. We present a unifying framework for systematically constructing natural language \"state\" representations for prompting LLM agents in repeated multi-agent games. Previous work on games with LLM agents has taken an ad hoc approach to encoding game history, which not only obscures the impact of state representation on agents' behavior, but also limits comparability between studies. Our framework addresses these gaps by characterizing methods of state representation along three axes: action informativeness (i.e., the extent to which the state representation captures actions played); reward informativeness (i.e., the extent to which the state representation describes rewards obtained); and prompting style (or natural language compression, i.e., the extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it admits a simple equilibrium both in theory and in human subject experiments \\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find that there are key dependencies of LLM agent behavior on the natural language state representation. In particular, we observe that representations which provide agents with (1) summarized, rather than complete, natural language representations of past history; (2) information about regrets, rather than raw payoffs; and (3) limited information about others' actions lead to behavior that more closely matches game theoretic equilibrium predictions, and with more stable game play by the agents. By contrast, other representations can exhibit either large deviations from equilibrium, higher variation in dynamic game play over time, or both.", "source": "arxiv", "arxiv_id": "2506.15624v1", "pdf_url": "https://arxiv.org/pdf/2506.15624v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-18T16:53:38Z", "updated": "2025-06-18T16:53:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Emergence of Altruism in Large-Language-Model Agents Society", "authors": ["Haoyang Li", "Xiao Jia", "Zhanzhan Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2509.22537v1", "abstract": "Leveraging Large Language Models (LLMs) for social simulation is a frontier in computational social science. Understanding the social logics these agents embody is critical to this attempt. However, existing research has primarily focused on cooperation in small-scale, task-oriented games, overlooking how altruism, which means sacrificing self-interest for collective benefit, emerges in large-scale agent societies. To address this gap, we introduce a Schelling-variant urban migration model that creates a social dilemma, compelling over 200 LLM agents to navigate an explicit conflict between egoistic (personal utility) and altruistic (system utility) goals. Our central finding is a fundamental difference in the social tendencies of LLMs. We identify two distinct archetypes: \"Adaptive Egoists\", which default to prioritizing self-interest but whose altruistic behaviors significantly increase under the influence of a social norm-setting message board; and \"Altruistic Optimizers\", which exhibit an inherent altruistic logic, consistently prioritizing collective benefit even at a direct cost to themselves. Furthermore, to qualitatively analyze the cognitive underpinnings of these decisions, we introduce a method inspired by Grounded Theory to systematically code agent reasoning. In summary, this research provides the first evidence of intrinsic heterogeneity in the egoistic and altruistic tendencies of different LLMs. We propose that for social simulation, model selection is not merely a matter of choosing reasoning capability, but of choosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a more suitable choice for simulating complex human societies, \"Altruistic Optimizers\" are better suited for modeling idealized pro-social actors or scenarios where collective welfare is the primary consideration.", "source": "arxiv", "arxiv_id": "2509.22537v1", "pdf_url": "https://arxiv.org/pdf/2509.22537v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-26T16:17:29Z", "updated": "2025-09-26T16:17:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents", "authors": ["Mohammad Rubyet Islam"], "year": 2025, "url": "http://arxiv.org/abs/2505.14727v1", "abstract": "The pursuit of alpha returns that exceed market benchmarks has undergone a profound transformation, evolving from intuition-driven investing to autonomous, AI powered systems. This paper introduces a comprehensive five stage taxonomy that traces this progression across manual strategies, statistical models, classical machine learning, deep learning, and agentic architectures powered by large language models (LLMs). Unlike prior surveys focused narrowly on modeling techniques, this review adopts a system level lens, integrating advances in representation learning, multimodal data fusion, and tool augmented LLM agents. The strategic shift from static predictors to contextaware financial agents capable of real time reasoning, scenario simulation, and cross modal decision making is emphasized. Key challenges in interpretability, data fragility, governance, and regulatory compliance areas critical to production deployment are examined. The proposed taxonomy offers a unified framework for evaluating maturity, aligning infrastructure, and guiding the responsible development of next generation alpha systems.", "source": "arxiv", "arxiv_id": "2505.14727v1", "pdf_url": "https://arxiv.org/pdf/2505.14727v1", "categories": ["cs.LG", "q-fin.CP"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-20T00:51:43Z", "updated": "2025-05-20T00:51:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Eye of Sherlock Holmes: Uncovering User Private Attribute Profiling via Vision-Language Model Agentic Framework", "authors": ["Feiran Liu", "Yuzhe Zhang", "Xinyi Huang", "Yinan Peng", "Xinfeng Li", "Lixu Wang", "Yutong Shen", "Ranjie Duan", "Simeng Qin", "Xiaojun Jia", "Qingsong Wen", "Wei Dong"], "year": 2025, "url": "http://arxiv.org/abs/2505.19139v1", "abstract": "Our research reveals a new privacy risk associated with the vision-language model (VLM) agentic framework: the ability to infer sensitive attributes (e.g., age and health information) and even abstract ones (e.g., personality and social traits) from a set of personal images, which we term \"image private attribute profiling.\" This threat is particularly severe given that modern apps can easily access users' photo albums, and inference from image sets enables models to exploit inter-image relations for more sophisticated profiling. However, two main challenges hinder our understanding of how well VLMs can profile an individual from a few personal photos: (1) the lack of benchmark datasets with multi-image annotations for private attributes, and (2) the limited ability of current multimodal large language models (MLLMs) to infer abstract attributes from large image collections. In this work, we construct PAPI, the largest dataset for studying private attribute profiling in personal images, comprising 2,510 images from 251 individuals with 3,012 annotated privacy attributes. We also propose HolmesEye, a hybrid agentic framework that combines VLMs and LLMs to enhance privacy inference. HolmesEye uses VLMs to extract both intra-image and inter-image information and LLMs to guide the inference process as well as consolidate the results through forensic analysis, overcoming existing limitations in long-context visual reasoning. Experiments reveal that HolmesEye achieves a 10.8% improvement in average accuracy over state-of-the-art baselines and surpasses human-level performance by 15.0% in predicting abstract attributes. This work highlights the urgency of addressing privacy risks in image-based profiling and offers both a new dataset and an advanced framework to guide future research in this area.", "source": "arxiv", "arxiv_id": "2505.19139v1", "pdf_url": "https://arxiv.org/pdf/2505.19139v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-05-25T13:22:10Z", "updated": "2025-05-25T13:22:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Future is Agentic: Definitions, Perspectives, and Open Challenges of Multi-Agent Recommender Systems", "authors": ["Reza Yousefi Maragheh", "Yashar Deldjoo"], "year": 2025, "url": "http://arxiv.org/abs/2507.02097v2", "abstract": "Large language models (LLMs) are rapidly evolving from passive engines of text generation into agentic entities that can plan, remember, invoke external tools, and co-operate with one another. This perspective paper investigates how such LLM agents (and societies thereof) can transform the design space of recommender systems.\n  We introduce a unified formalism that (i) models an individual agent as a tuple comprising its language core, tool set, and hierarchical memory, and (ii) captures a multi-agent recommender as a triple of agents, shared environment, and communication protocol. Within this framework, we present four end-to-end use cases-interactive party planning, synthetic user-simulation for offline evaluation, multi-modal furniture recommendation, and brand-aligned explanation generation-each illustrating a distinct capability unlocked by agentic orchestration.\n  We then surface five cross-cutting challenge families: protocol complexity, scalability, hallucination and error propagation, emergent misalignment (including covert collusion), and brand compliance.\n  For each, we formalize the problem, review nascent mitigation strategies, and outline open research questions. The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools that keep pace with this new degree of autonomy. By unifying agentic abstractions with recommender objectives, the paper lays the groundwork for the next generation of personalized, trustworthy, and context-rich recommendation services.", "source": "arxiv", "arxiv_id": "2507.02097v2", "pdf_url": "https://arxiv.org/pdf/2507.02097v2", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-07-02T19:25:44Z", "updated": "2025-07-10T14:47:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement", "authors": ["Ruihan Yang", "Fanghua Ye", "Jian Li", "Siyu Yuan", "Yikai Zhang", "Zhaopeng Tu", "Xiaolong Li", "Deqing Yang"], "year": 2025, "url": "http://arxiv.org/abs/2503.16024v2", "abstract": "Large language models (LLMs) have recently transformed from text-based assistants to autonomous agents capable of planning, reasoning, and iteratively improving their actions. While numerical reward signals and verifiers can effectively rank candidate actions, they often provide limited contextual guidance. In contrast, natural language feedback better aligns with the generative capabilities of LLMs, providing richer and more actionable suggestions. However, parsing and implementing this feedback effectively can be challenging for LLM-based agents. In this work, we introduce Critique-Guided Improvement (CGI), a novel two-player framework, comprising an actor model that explores an environment and a critic model that generates detailed nature language feedback. By training the critic to produce fine-grained assessments and actionable revisions, and the actor to utilize these critiques, our approach promotes more robust exploration of alternative strategies while avoiding local optima. Experiments in three interactive environments show that CGI outperforms existing baselines by a substantial margin. Notably, even a small critic model surpasses GPT-4 in feedback quality. The resulting actor achieves state-of-the-art performance, demonstrating the power of explicit iterative guidance to enhance decision-making in LLM-based agents.", "source": "arxiv", "arxiv_id": "2503.16024v2", "pdf_url": "https://arxiv.org/pdf/2503.16024v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-20T10:42:33Z", "updated": "2025-10-24T19:30:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents", "authors": ["Lixu Wang", "Kaixiang Yao", "Xinfeng Li", "Dong Yang", "Haoyang Li", "Xiaofeng Wang", "Wei Dong"], "year": 2025, "url": "http://arxiv.org/abs/2507.10016v2", "abstract": "Our research uncovers a novel privacy risk associated with multimodal large language models (MLLMs): the ability to infer sensitive personal attributes from audio data -- a technique we term audio private attribute profiling. This capability poses a significant threat, as audio can be covertly captured without direct interaction or visibility. Moreover, compared to images and text, audio carries unique characteristics, such as tone and pitch, which can be exploited for more detailed profiling. However, two key challenges exist in understanding MLLM-employed private attribute profiling from audio: (1) the lack of audio benchmark datasets with sensitive attribute annotations and (2) the limited ability of current MLLMs to infer such attributes directly from audio. To address these challenges, we introduce AP^2, an audio benchmark dataset that consists of two subsets collected and composed from real-world data, and both are annotated with sensitive attribute labels. Additionally, we propose Gifts, a hybrid multi-agent framework that leverages the complementary strengths of audio-language models (ALMs) and large language models (LLMs) to enhance inference capabilities. Gifts employs an LLM to guide the ALM in inferring sensitive attributes, then forensically analyzes and consolidates the ALM's inferences, overcoming severe hallucinations of existing ALMs in generating long-context responses. Our evaluations demonstrate that Gifts significantly outperforms baseline approaches in inferring sensitive attributes. Finally, we investigate model-level and data-level defense strategies to mitigate the risks of audio private attribute profiling. Our work validates the feasibility of audio-based privacy attacks using MLLMs, highlighting the need for robust defenses, and provides a dataset and framework to facilitate future research.", "source": "arxiv", "arxiv_id": "2507.10016v2", "pdf_url": "https://arxiv.org/pdf/2507.10016v2", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-14T07:51:56Z", "updated": "2025-08-20T07:04:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Need for Verification in AI-Driven Scientific Discovery", "authors": ["Cristina Cornelio", "Takuya Ito", "Ryan Cory-Wright", "Sanjeeb Dash", "Lior Horesh"], "year": 2025, "url": "http://arxiv.org/abs/2509.01398v2", "abstract": "Artificial intelligence (AI) is transforming the practice of science. Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields. However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced. In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents. While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.", "source": "arxiv", "arxiv_id": "2509.01398v2", "pdf_url": "https://arxiv.org/pdf/2509.01398v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-01T11:50:04Z", "updated": "2025-12-17T13:11:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents", "authors": ["Yifan Duan", "Yihong Tang", "Xuefeng Bai", "Kehai Chen", "Juntao Li", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.20859v2", "abstract": "Large language models (LLMs) excel in both closed tasks (including problem-solving, and code generation) and open tasks (including creative writing), yet existing explanations for their capabilities lack connections to real-world human intelligence. To fill this gap, this paper systematically investigates LLM intelligence through the lens of ``human simulation'', addressing three core questions: (1) \\textit{How do personality traits affect problem-solving in closed tasks?} (2) \\textit{How do traits shape creativity in open tasks?} (3) \\textit{How does single-agent performance influence multi-agent collaboration?} By assigning Big Five personality traits to LLM agents and evaluating their performance in single- and multi-agent settings, we reveal that specific traits significantly influence reasoning accuracy (closed tasks) and creative output (open tasks). Furthermore, multi-agent systems exhibit collective intelligence distinct from individual capabilities, driven by distinguishing combinations of personalities.", "source": "arxiv", "arxiv_id": "2502.20859v2", "pdf_url": "https://arxiv.org/pdf/2502.20859v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-28T09:01:39Z", "updated": "2025-05-27T07:52:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "authors": ["Gerrit GroÃmann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "year": 2025, "url": "http://arxiv.org/abs/2505.03961v2", "abstract": "According to Yuval Noah Harari, large-scale human cooperation is driven by shared narratives that encode common beliefs and values. This study explores whether such narratives can similarly nudge LLM agents toward collaboration. We use a finitely repeated public goods game in which LLM agents choose either cooperative or egoistic spending strategies. We prime agents with stories highlighting teamwork to different degrees and test how this influences negotiation outcomes. Our experiments explore four questions:(1) How do narratives influence negotiation behavior? (2) What differs when agents share the same story versus different ones? (3) What happens when the agent numbers grow? (4) Are agents resilient against self-serving negotiators? We find that story-based priming significantly affects negotiation strategies and success rates. Common stories improve collaboration, benefiting each agent. By contrast, priming agents with different stories reverses this effect, and those agents primed toward self-interest prevail. We hypothesize that these results carry implications for multi-agent system design and AI alignment.", "source": "arxiv", "arxiv_id": "2505.03961v2", "pdf_url": "https://arxiv.org/pdf/2505.03961v2", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-06T20:23:25Z", "updated": "2025-05-08T08:29:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Real Barrier to LLM Agent Usability is Agentic ROI", "authors": ["Weiwen Liu", "Jiarui Qin", "Xu Huang", "Xingshan Zeng", "Yunjia Xi", "Jianghao Lin", "Chuhan Wu", "Yasheng Wang", "Lifeng Shang", "Ruiming Tang", "Defu Lian", "Yong Yu", "Weinan Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.17767v1", "abstract": "Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action. Despite the widespread application in specialized, high-effort tasks like coding and scientific research, we highlight a critical usability gap in high-demand, mass-market applications. This position paper argues that the limited real-world adoption of LLM agents stems not only from gaps in model capabilities, but also from a fundamental tradeoff between the value an agent can provide and the costs incurred during real-world use. Hence, we call for a shift from solely optimizing model performance to a broader, utility-driven perspective: evaluating agents through the lens of the overall agentic return on investment (Agent ROI). By identifying key factors that determine Agentic ROI--information quality, agent time, and cost--we posit a zigzag development trajectory in optimizing agentic ROI: first scaling up to improve the information quality, then scaling down to minimize the time and cost. We outline the roadmap across different development stages to bridge the current usability gaps, aiming to make LLM agents truly scalable, accessible, and effective in real-world contexts.", "source": "arxiv", "arxiv_id": "2505.17767v1", "pdf_url": "https://arxiv.org/pdf/2505.17767v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T11:40:58Z", "updated": "2025-05-23T11:40:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Roots of International Perceptions: Simulating US Attitude Changes Towards China with LLM Agents", "authors": ["Nicholas Sukiennik", "Yichuan Xu", "Yuqing Kan", "Jinghua Piao", "Yuwei Yan", "Chen Gao", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.08837v2", "abstract": "The rise of LLMs poses new possibilities in modeling opinion evolution, a long-standing task in simulation, by leveraging advanced reasoning abilities to recreate complex, large-scale human cognitive trends. While most prior works focus on opinion evolution surrounding specific isolated events or the views within a country, ours is the first to model the large-scale attitude evolution of a population representing an entire country towards another -- US citizens' perspectives towards China. To tackle the challenges of this broad scenario, we propose a framework that integrates media data collection, user profile creation, and cognitive architecture for opinion updates to successfully reproduce the real trend of US attitudes towards China over a 20-year period from 2005 to today. We also leverage LLMs' capabilities to introduce debiased media exposure, extracting neutral events from typically subjective news contents, to uncover the roots of polarized opinion formation, as well as a devils advocate agent to help explain the rare reversal from negative to positive attitudes towards China, corresponding with changes in the way Americans obtain information about the country. The simulation results, beyond validating our framework architecture, also reveal the impact of biased framing and selection bias in shaping attitudes. Overall, our work contributes to a new paradigm for LLM-based modeling of cognitive behaviors in a large-scale, long-term, cross-border social context, providing insights into the formation of international biases and offering valuable implications for media consumers to better understand the factors shaping their perspectives, and ultimately contributing to the larger social need for bias reduction and cross-cultural tolerance.", "source": "arxiv", "arxiv_id": "2508.08837v2", "pdf_url": "https://arxiv.org/pdf/2508.08837v2", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-08-12T10:54:08Z", "updated": "2025-08-15T10:48:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "year": 2025, "url": "http://arxiv.org/abs/2512.20884v1", "abstract": "Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($Î³$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $Î³$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[Î¸]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.", "source": "arxiv", "arxiv_id": "2512.20884v1", "pdf_url": "https://arxiv.org/pdf/2512.20884v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-24T02:02:25Z", "updated": "2025-12-24T02:02:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Spark Effect: On Engineering Creative Diversity in Multi-Agent AI Systems", "authors": ["Alexander Doudkin", "Anton Voelker", "Friedrich von Borries"], "year": 2025, "url": "http://arxiv.org/abs/2510.15568v1", "abstract": "Creative services teams increasingly rely on large language models (LLMs) to accelerate ideation, yet production systems often converge on homogeneous outputs that fail to meet brand or artistic expectations. Art of X developed persona-conditioned LLM agents -- internally branded as \"Sparks\" and instantiated through a library of role-inspired system prompts -- to intentionally diversify agent behaviour within a multi-agent workflow. This white paper documents the problem framing, experimental design, and quantitative evidence behind the Spark agent programme. Using an LLM-as-a-judge protocol calibrated against human gold standards, we observe a mean diversity gain of +4.1 points (on a 1-10 scale) when persona-conditioned Spark agents replace a uniform system prompt, narrowing the gap to human experts to 1.0 point. We also surface evaluator bias and procedural considerations for future deployments.", "source": "arxiv", "arxiv_id": "2510.15568v1", "pdf_url": "https://arxiv.org/pdf/2510.15568v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-17T11:56:18Z", "updated": "2025-10-17T11:56:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs", "authors": ["Lorenz Wolf", "Sangwoong Yoon", "Ilija Bogunovic"], "year": 2025, "url": "http://arxiv.org/abs/2503.05856v1", "abstract": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve state-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by leveraging the collaboration of multiple LLMs at inference time. Despite these successes, an evaluation of the safety and reliability of MoA is missing. We present the first comprehensive study of MoA's robustness against deceptive LLM agents that deliberately provide misleading responses. We examine factors like the propagation of deceptive information, model size, and information availability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the popular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of 49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate that introducing only a $\\textit{single}$ carefully-instructed deceptive agent into the MoA can reduce performance to 37.9%, effectively nullifying all MoA gains. On QuALITY, a multiple-choice comprehension task, the impact is also severe, with accuracy plummeting by a staggering 48.5%. Inspired in part by the historical Doge of Venice voting process, designed to minimize influence and deception, we propose a range of unsupervised defense mechanisms that recover most of the lost performance.", "source": "arxiv", "arxiv_id": "2503.05856v1", "pdf_url": "https://arxiv.org/pdf/2503.05856v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-07T14:46:39Z", "updated": "2025-03-07T14:46:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Through the Lens of Human-Human Collaboration: A Configurable Research Platform for Exploring Human-Agent Collaboration", "authors": ["Bingsheng Yao", "Jiaju Chen", "Chaoran Chen", "April Wang", "Toby Jia-jun Li", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.18008v1", "abstract": "Intelligent systems have traditionally been designed as tools rather than collaborators, often lacking critical characteristics that collaboration partnerships require. Recent advances in large language model (LLM) agents open new opportunities for human-LLM-agent collaboration by enabling natural communication and various social and cognitive behaviors. Yet it remains unclear whether principles of computer-mediated collaboration established in HCI and CSCW persist, change, or fail when humans collaborate with LLM agents. To support systematic investigations of these questions, we introduce an open and configurable research platform for HCI researchers. The platform's modular design allows seamless adaptation of classic CSCW experiments and manipulation of theory-grounded interaction controls. We demonstrate the platform's effectiveness and usability through two case studies: (1) re-implementing the classic human-human-collaboration task Shape Factory as a between-subject human-agent-collaboration experiment with 16 participants, and (2) a participatory cognitive walkthrough with five HCI researchers to refine workflows and interfaces for experiment setup and analysis.", "source": "arxiv", "arxiv_id": "2509.18008v1", "pdf_url": "https://arxiv.org/pdf/2509.18008v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-22T16:47:08Z", "updated": "2025-09-22T16:47:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents", "authors": ["Yueying Li", "Jim Dai", "Tianyi Peng"], "year": 2025, "url": "http://arxiv.org/abs/2504.07347v2", "abstract": "As demand for Large Language Models (LLMs) and AI agents rapidly grows, optimizing systems for efficient LLM inference becomes critical. While significant efforts have focused on system-level engineering, little is explored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language model (LLM) inference, bridging the gap between the queueing theory and LLM system communities. In particular, we study the throughput aspect in LLM inference systems. We prove that a large class of 'work-conserving' scheduling algorithms can achieve maximum throughput for individual inference LLM engine, highlighting 'work-conserving' as a key design principle in practice. In a network of LLM agents, work-conserving scheduling alone is insufficient, particularly when facing specific workload structures and multi-class workflows that require more sophisticated scheduling strategies. Evaluations of real-world systems show that Orca and Sarathi-serve are throughput-optimal, reassuring practitioners, while FasterTransformer and vanilla vLLM are not maximally stable and should be used with caution. Our results highlight the substantial benefits that the queueing community can offer in improving LLM inference systems and call for more interdisciplinary development.", "source": "arxiv", "arxiv_id": "2504.07347v2", "pdf_url": "https://arxiv.org/pdf/2504.07347v2", "categories": ["stat.ML", "cs.LG", "math.PR"], "primary_category": "stat.ML", "doi": "", "venue": "", "published": "2025-04-10T00:12:12Z", "updated": "2025-04-24T14:10:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation", "authors": ["Thales Sales Almeida", "JoÃ£o Guilherme Alves Santos", "Thiago Laitz", "Giovana Kerche BonÃ¡s"], "year": 2025, "url": "http://arxiv.org/abs/2509.14477v1", "abstract": "Large language models (LLMs) are increasingly deployed as task-oriented agents, where success depends on their ability to generate accurate function calls under realistic, multilingual conditions. However, existing agent evaluations largely overlook cultural and linguistic diversity, often relying on monolingual or naively translated benchmarks. We introduce Ticket-Bench, a benchmark for multilingual agent evaluation in task-oriented scenarios. Ticket-Bench simulates the domain of soccer ticket purchases across six major languages: Portuguese, English, Spanish, German, Italian, and French. Using localized teams, cities, and user profiles to provide a higher level of realism. We evaluate a wide range of commercial and open-source LLMs, measuring function-calling accuracy and consistency across languages. Results show that reasoning-oriented models (e.g., GPT-5, Qwen3-235B) dominate performance but still exhibit notable cross-lingual disparities. These findings underscore the need for culturally aware, multilingual benchmarks to guide the development of robust LLM agents.", "source": "arxiv", "arxiv_id": "2509.14477v1", "pdf_url": "https://arxiv.org/pdf/2509.14477v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-17T23:13:47Z", "updated": "2025-09-17T23:13:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games", "authors": ["Niv Eckhaus", "Uri Berger", "Gabriel Stanovsky"], "year": 2025, "url": "http://arxiv.org/abs/2506.05309v2", "abstract": "LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns. In this work, we develop an adaptive asynchronous LLM agent consisting of two modules: a generator that decides what to say, and a scheduler that decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, where our agent plays with human participants. Overall, our agent performs on par with human players, both in game performance metrics and in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We make all of our code and data publicly available. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.", "source": "arxiv", "arxiv_id": "2506.05309v2", "pdf_url": "https://arxiv.org/pdf/2506.05309v2", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-05T17:53:44Z", "updated": "2025-09-20T16:08:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents", "authors": ["Geon Lee", "Wenchao Yu", "Kijung Shin", "Wei Cheng", "Haifeng Chen"], "year": 2025, "url": "http://arxiv.org/abs/2502.11418v2", "abstract": "Time series data is essential in various applications, including climate modeling, healthcare monitoring, and financial analytics. Understanding the contextual information associated with real-world time series data is often essential for accurate and reliable event predictions. In this paper, we introduce TimeCAP, a time-series processing framework that creatively employs Large Language Models (LLMs) as contextualizers of time series data, extending their typical usage as predictors. TimeCAP incorporates two independent LLM agents: one generates a textual summary capturing the context of the time series, while the other uses this enriched summary to make more informed predictions. In addition, TimeCAP employs a multi-modal encoder that synergizes with the LLM agents, enhancing predictive performance through mutual augmentation of inputs with in-context examples. Experimental results on real-world datasets demonstrate that TimeCAP outperforms state-of-the-art methods for time series event prediction, including those utilizing LLMs as predictors, achieving an average improvement of 28.75% in F1 score.", "source": "arxiv", "arxiv_id": "2502.11418v2", "pdf_url": "https://arxiv.org/pdf/2502.11418v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T04:17:27Z", "updated": "2025-03-10T04:15:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt", "authors": ["Zhilong Wang", "Neha Nagaraja", "Lan Zhang", "Hayretdin Bahsi", "Pawan Patil", "Peng Liu"], "year": 2025, "url": "http://arxiv.org/abs/2506.05739v1", "abstract": "LLM agents are widely used as agents for customer support, content generation, and code assistance. However, they are vulnerable to prompt injection attacks, where adversarial inputs manipulate the model's behavior. Traditional defenses like input sanitization, guard models, and guardrails are either cumbersome or ineffective. In this paper, we propose a novel, lightweight defense mechanism called Polymorphic Prompt Assembling (PPA), which protects against prompt injection with near-zero overhead. The approach is based on the insight that prompt injection requires guessing and breaking the structure of the system prompt. By dynamically varying the structure of system prompts, PPA prevents attackers from predicting the prompt structure, thereby enhancing security without compromising performance. We conducted experiments to evaluate the effectiveness of PPA against existing attacks and compared it with other defense methods.", "source": "arxiv", "arxiv_id": "2506.05739v1", "pdf_url": "https://arxiv.org/pdf/2506.05739v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-06T04:50:57Z", "updated": "2025-06-06T04:50:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Too Human to Model:The Uncanny Valley of LLMs in Social Simulation -- When Generative Language Agents Misalign with Modelling Principles", "authors": ["Yongchao Zeng", "Calum Brown", "Mark Rounsevell"], "year": 2025, "url": "http://arxiv.org/abs/2507.06310v1", "abstract": "Large language models (LLMs) have been increasingly used to build agents in social simulation because of their impressive abilities to generate fluent, contextually coherent dialogues. Such abilities can enhance the realism of models. However, the pursuit of realism is not necessarily compatible with the epistemic foundation of modelling. We argue that LLM agents, in many regards, are too human to model: they are too expressive, detailed and intractable to be consistent with the abstraction, simplification, and interpretability typically demanded by modelling. Through a model-building thought experiment that converts the Bass diffusion model to an LLM-based variant, we uncover five core dilemmas: a temporal resolution mismatch between natural conversation and abstract time steps; the need for intervention in conversations while avoiding undermining spontaneous agent outputs; the temptation to introduce rule-like instructions in prompts while maintaining conversational naturalness; the tension between role consistency and role evolution across time; and the challenge of understanding emergence, where system-level patterns become obscured by verbose micro textual outputs. These dilemmas steer the LLM agents towards an uncanny valley: not abstract enough to clarify underlying social mechanisms, while not natural enough to represent realistic human behaviour. This exposes an important paradox: the realism of LLM agents can obscure, rather than clarify, social dynamics when misapplied. We tease out the conditions in which LLM agents are ideally suited: where system-level emergence is not the focus, linguistic nuances and meaning are central, interactions unfold in natural time, and stable role identity is more important than long-term behavioural evolution. We call for repositioning LLM agents in the ecosystem of social simulation for future applications.", "source": "arxiv", "arxiv_id": "2507.06310v1", "pdf_url": "https://arxiv.org/pdf/2507.06310v1", "categories": ["cs.CY", "cs.AI", "cs.MA"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-07-08T18:02:36Z", "updated": "2025-07-08T18:02:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction", "authors": ["Xingshan Zeng", "Weiwen Liu", "Lingzhi Wang", "Liangyou Li", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "year": 2025, "url": "http://arxiv.org/abs/2508.12685v1", "abstract": "Agentic task-solving with Large Language Models (LLMs) requires multi-turn, multi-step interactions, often involving complex function calls and dynamic user-agent exchanges. Existing simulation-based data generation methods for such scenarios rely heavily on costly autoregressive interactions between multiple LLM agents, thereby limiting real-world performance of agentic tasks. In this paper, we propose a novel Non-Autoregressive Iterative Generation framework, called ToolACE-MT, for constructing high-quality multi-turn agentic dialogues. ToolACE-MT generates full conversational trajectories through three stages: coarse-grained initialization, iterative refinement, and offline verification. The initialization phase builds a structurally complete yet semantically coarse dialogue skeleton; the iterative refinement phase introduces realistic complexities and continued refinement via mask-and-fill operations; and the offline verification phase ensures correctness and coherence via rule- and model-based checks. Experiments demonstrate that ToolACE-MT enables efficient, effective and generalizable agentic data generation, offering a new paradigm for high-quality data construction in tool-augmented LLM scenarios.", "source": "arxiv", "arxiv_id": "2508.12685v1", "pdf_url": "https://arxiv.org/pdf/2508.12685v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-18T07:38:23Z", "updated": "2025-08-18T07:38:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ToolMind Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset", "authors": ["Chen Yang", "Ran Le", "Yun Xing", "Zhenwei An", "Zongchao Chen", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2511.15718v2", "abstract": "Large Language Model (LLM) agents have developed rapidly in recent years to solve complex real-world problems using external tools. However, the scarcity of high-quality trajectories still hinders the development of stronger LLM agents. Most existing works on multi-turn dialogue synthesis validate correctness only at the trajectory level, which may overlook turn-level errors that can propagate during training and degrade model performance. To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Our data synthesis pipeline first constructs a function graph based on parameter correlations and then uses a multi-agent framework to simulate realistic user-assistant-tool interactions. Beyond trajectory-level validation, we employ fine-grained turn-level filtering to remove erroneous or suboptimal steps, ensuring that only high-quality reasoning traces are retained. This approach mitigates error amplification during training while preserving self-corrective reasoning signals essential for robust tool-use learning. Models fine-tuned on ToolMind show significant improvements over baselines on several benchmarks.", "source": "arxiv", "arxiv_id": "2511.15718v2", "pdf_url": "https://arxiv.org/pdf/2511.15718v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-12T13:01:23Z", "updated": "2025-12-05T09:32:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering", "authors": ["Marianne Menglin Liu", "Daniel Garcia", "Fjona Parllaku", "Vikas Upadhyay", "Syed Fahad Allam Shah", "Dan Roth"], "year": 2025, "url": "http://arxiv.org/abs/2510.20036v1", "abstract": "Large language model (LLM) agents rely on external tools to solve complex tasks, but real-world toolsets often contain redundant tools with overlapping names and descriptions, introducing ambiguity and reducing selection accuracy. LLMs also face strict input context limits, preventing efficient consideration of large toolsets. To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing toolsets to fit within context limits without sacrificing accuracy. Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "source": "arxiv", "arxiv_id": "2510.20036v1", "pdf_url": "https://arxiv.org/pdf/2510.20036v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-22T21:29:27Z", "updated": "2025-10-22T21:29:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits", "authors": ["Ziming Wei", "Zichen Kong", "Yuan Wang", "David Z. Pan", "Xiyuan Tang"], "year": 2025, "url": "http://arxiv.org/abs/2509.14169v1", "abstract": "Analog and mixed-signal circuit design remains challenging due to the shortage of high-quality data and the difficulty of embedding domain knowledge into automated flows. Traditional black-box optimization achieves sampling efficiency but lacks circuit understanding, which often causes evaluations to be wasted in low-value regions of the design space. In contrast, learning-based methods embed structural knowledge but are case-specific and costly to retrain. Recent attempts with large language models show potential, yet they often rely on manual intervention, limiting generality and transparency. We propose TopoSizing, an end-to-end framework that performs robust circuit understanding directly from raw netlists and translates this knowledge into optimization gains. Our approach first applies graph algorithms to organize circuits into a hierarchical device-module-stage representation. LLM agents then execute an iterative hypothesis-verification-refinement loop with built-in consistency checks, producing explicit annotations. Verified insights are integrated into Bayesian optimization through LLM-guided initial sampling and stagnation-triggered trust-region updates, improving efficiency while preserving feasibility.", "source": "arxiv", "arxiv_id": "2509.14169v1", "pdf_url": "https://arxiv.org/pdf/2509.14169v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-17T16:52:46Z", "updated": "2025-09-17T16:52:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Toward Efficient Exploration by Large Language Model Agents", "authors": ["Dilip Arumugam", "Thomas L. Griffiths"], "year": 2025, "url": "http://arxiv.org/abs/2504.20997v1", "abstract": "A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of data-efficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statistically-efficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, data-efficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration.", "source": "arxiv", "arxiv_id": "2504.20997v1", "pdf_url": "https://arxiv.org/pdf/2504.20997v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-29T17:59:48Z", "updated": "2025-04-29T17:59:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Toward Generation of Test Cases from Task Descriptions via History-aware Planning", "authors": ["Duy Cao", "Phu Nguyen", "Vy Le", "Tien N. Nguyen", "Vu Nguyen"], "year": 2025, "url": "http://arxiv.org/abs/2504.14336v2", "abstract": "In automated web testing, generating test scripts from natural language task descriptions is crucial for enhancing the test generation process. This activity involves creating the correct sequences of actions to form test scripts for future testing activities. Current state-of-the-art approaches are limited in generating these action sequences, as they either demand substantial manual effort for human demonstrations or fail to consider the history of previous web content and actions to decide the next action. In this paper, we introduce HxAgent, an iterative large language model agent planning approach that determines the next action based on: 1) observations of the current contents and feasible actions, 2) short-term memory of previous web states and actions, and 3) long-term experience with (in)correct action sequences. The agent generates a sequence of actions to perform a given task, which is effectively an automated test case to verify the task. We conducted an extensive empirical evaluation of HxAgent using two datasets. On the MiniWoB++ dataset, our approach achieves 97% exact-match accuracy that is comparable to the best baselines while eliminating the need for human demonstrations required by those methods. For complex tasks requiring navigation through multiple actions and screens, HxAgent achieves an average 82% exact-match. On the second dataset, comprising 350 task instances across seven popular websites, including YouTube, LinkedIn, Facebook, and Google, HxAgent achieves high performance, with 87% of the action sequences exactly matching the ground truth and a prefix-match of 93%, outperforming the baseline by 59%.", "source": "arxiv", "arxiv_id": "2504.14336v2", "pdf_url": "https://arxiv.org/pdf/2504.14336v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-04-19T16:03:03Z", "updated": "2025-09-11T15:21:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach", "authors": ["Iizalaarab Elhaimeur", "Nikos Chrisochoides"], "year": 2025, "url": "http://arxiv.org/abs/2504.18603v1", "abstract": "Quantum computing education faces significant challenges due to its complexity and the limitations of current tools; this paper introduces a novel Intelligent Teaching Assistant for quantum computing education and details its evolutionary design process. The system combines a knowledge-graph-augmented architecture with two specialized Large Language Model (LLM) agents: a Teaching Agent for dynamic interaction, and a Lesson Planning Agent for lesson plan generation. The system is designed to adapt to individual student needs, with interactions meticulously tracked and stored in a knowledge graph. This graph represents student actions, learning resources, and relationships, aiming to enable reasoning about effective learning pathways. We describe the implementation of the system, highlighting the challenges encountered and the solutions implemented, including introducing a dual-agent architecture where tasks are separated, all coordinated through a central knowledge graph that maintains system awareness, and a user-facing tag system intended to mitigate LLM hallucination and improve user control. Preliminary results illustrate the system's potential to capture rich interaction data, dynamically adapt lesson plans based on student feedback via a tag system in simulation, and facilitate context-aware tutoring through the integrated knowledge graph, though systematic evaluation is required.", "source": "arxiv", "arxiv_id": "2504.18603v1", "pdf_url": "https://arxiv.org/pdf/2504.18603v1", "categories": ["cs.CY", "cs.AI", "cs.MA"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-04-24T21:53:34Z", "updated": "2025-04-24T21:53:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL", "authors": ["Yuxiang Wei", "Zhiqing Sun", "Emily McMilin", "Jonas Gehring", "David Zhang", "Gabriel Synnaeve", "Daniel Fried", "Lingming Zhang", "Sida Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.18552v1", "abstract": "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.", "source": "arxiv", "arxiv_id": "2512.18552v1", "pdf_url": "https://arxiv.org/pdf/2512.18552v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-21T00:49:40Z", "updated": "2025-12-21T00:49:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "year": 2025, "url": "http://arxiv.org/abs/2508.03092v1", "abstract": "With the proliferation of Large Language Models (LLMs), the detection of misinformation has become increasingly important and complex. This research proposes an innovative verifiable misinformation detection LLM agent that goes beyond traditional true/false binary judgments. The agent actively verifies claims through dynamic interaction with diverse web sources, assesses information source credibility, synthesizes evidence, and provides a complete verifiable reasoning process. Our designed agent architecture includes three core tools: precise web search tool, source credibility assessment tool and numerical claim verification tool. These tools enable the agent to execute multi-step verification strategies, maintain evidence logs, and form comprehensive assessment conclusions. We evaluate using standard misinformation datasets such as FakeNewsNet, comparing with traditional machine learning models and LLMs. Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content. Experimental results show that our agent outperforms baseline methods in misinformation detection accuracy, reasoning transparency, and resistance to information rewriting, providing a new paradigm for trustworthy AI-assisted fact-checking.", "source": "arxiv", "arxiv_id": "2508.03092v1", "pdf_url": "https://arxiv.org/pdf/2508.03092v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-05T05:15:03Z", "updated": "2025-08-05T05:15:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents", "authors": ["Chaoran Chen", "Zhiping Zhang", "Ibrahim Khalilov", "Bingcan Guo", "Simret A Gebreegziabher", "Yanfang Ye", "Ziang Xiao", "Yaxing Yao", "Tianshi Li", "Toby Jia-Jun Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.17934v2", "abstract": "The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.", "source": "arxiv", "arxiv_id": "2504.17934v2", "pdf_url": "https://arxiv.org/pdf/2504.17934v2", "categories": ["cs.HC", "cs.CL", "cs.CR"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-04-24T20:51:20Z", "updated": "2025-06-05T12:40:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation", "authors": ["Vinicius Lima", "Dzung T. Phan", "Jayant Kalagnanam", "Dhaval Patel", "Nianjun Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2508.03117v1", "abstract": "We present a framework for training trustworthy large language model (LLM) agents for optimization modeling via a verifiable synthetic data generation pipeline. Focusing on linear and mixed-integer linear programming, our approach begins with structured symbolic representations and systematically produces natural language descriptions, mathematical formulations, and solver-executable code. By programmatically constructing each instance with known optimal solutions, the pipeline ensures full verifiability and enables automatic filtering of low-quality demonstrations generated by teacher models. Each dataset instance includes a structured representation of the optimization problem, a corresponding natural language description, the verified optimal solution, and step-by-step demonstrations - generated by a teacher model - that show how to model and solve the problem across multiple optimization modeling languages. This enables supervised fine-tuning of open-source LLMs specifically tailored to optimization tasks. To operationalize this pipeline, we introduce OptiTrust, a modular LLM agent that performs multi-stage translation from natural language to solver-ready code, leveraging stepwise demonstrations, multi-language inference, and majority-vote cross-validation. Our agent achieves state-of-the-art performance on standard benchmarks. Out of 7 datasets, it achieves the highest accuracy on six and outperforms the next-best algorithm by at least 8 percentage on three of them. Our approach provides a scalable, verifiable, and principled path toward building reliable LLM agents for real-world optimization applications.", "source": "arxiv", "arxiv_id": "2508.03117v1", "pdf_url": "https://arxiv.org/pdf/2508.03117v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-05T05:54:20Z", "updated": "2025-08-05T05:54:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols", "authors": ["Kathrin SeÃler", "Arne Bewersdorff", "Claudia Nerdel", "Enkelejda Kasneci"], "year": 2025, "url": "http://arxiv.org/abs/2502.12842v1", "abstract": "Effective feedback is essential for fostering students' success in scientific inquiry. With advancements in artificial intelligence, large language models (LLMs) offer new possibilities for delivering instant and adaptive feedback. However, this feedback often lacks the pedagogical validation provided by real-world practitioners. To address this limitation, our study evaluates and compares the feedback quality of LLM agents with that of human teachers and science education experts on student-written experimentation protocols. Four blinded raters, all professionals in scientific inquiry and science education, evaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and 3) the science education experts using a five-point Likert scale based on six criteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive Tone, Linguistic Clarity, and Technical Terminology. Our results indicate that LLM-generated feedback shows no significant difference to that of teachers and experts in overall quality. However, the LLM agent's performance lags in the Feed Back dimension, which involves identifying and explaining errors within the student's work context. Qualitative analysis highlighted the LLM agent's limitations in contextual understanding and in the clear communication of specific errors. Our findings suggest that combining LLM-generated feedback with human expertise can enhance educational practices by leveraging the efficiency of LLMs and the nuanced understanding of educators.", "source": "arxiv", "arxiv_id": "2502.12842v1", "pdf_url": "https://arxiv.org/pdf/2502.12842v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-18T13:22:14Z", "updated": "2025-02-18T13:22:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Adaptive Software Agents for Debugging", "authors": ["Yacine Majdoub", "Eya Ben Charrada", "Haifa Touati"], "year": 2025, "url": "http://arxiv.org/abs/2504.18316v1", "abstract": "Using multiple agents was found to improve the debugging capabilities of Large Language Models. However, increasing the number of LLM-agents has several drawbacks such as increasing the running costs and rising the risk for the agents to lose focus. In this work, we propose an adaptive agentic design, where the number of agents and their roles are determined dynamically based on the characteristics of the task to be achieved. In this design, the agents roles are not predefined, but are generated after analyzing the problem to be solved. Our initial evaluation shows that, with the adaptive design, the number of agents that are generated depends on the complexity of the buggy code. In fact, for simple code with mere syntax issues, the problem was usually fixed using one agent only. However, for more complex problems, we noticed the creation of a higher number of agents. Regarding the effectiveness of the fix, we noticed an average improvement of 11% compared to the one-shot prompting. Given these promising results, we outline future research directions to improve our design for adaptive software agents that can autonomously plan and conduct their software goals.", "source": "arxiv", "arxiv_id": "2504.18316v1", "pdf_url": "https://arxiv.org/pdf/2504.18316v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-04-25T12:48:08Z", "updated": "2025-04-25T12:48:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Agentic OS: An LLM Agent Framework for Linux Schedulers", "authors": ["Yusheng Zheng", "Yanpeng Hu", "Wei Zhang", "Andi Quinn"], "year": 2025, "url": "http://arxiv.org/abs/2509.01245v4", "abstract": "Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to apply a better LLM, but to architect a decoupled control plane that separates the AI's role of semantic reasoning (\"what to optimize\") from the system's role of execution (\"how to observe and act\"), thereby separating the optimization problem into two stages: goal-inference and policy-synthesis. Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configure before deployment with static and dynamic analysis.\n  We demonstrate this architecture's power with sched-agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched\\_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents a step towards creating truly self-optimizing, application-aware operating systems. The code is open-sourced in https://github.com/eunomia-bpf/schedcp", "source": "arxiv", "arxiv_id": "2509.01245v4", "pdf_url": "https://arxiv.org/pdf/2509.01245v4", "categories": ["cs.AI", "cs.MA", "cs.OS"], "primary_category": "cs.AI", "doi": "", "venue": "MLforSystem 2025", "published": "2025-09-01T08:38:49Z", "updated": "2025-09-30T02:48:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Agents That Know When They Don't Know: Uncertainty as a Control Signal for Structured Reasoning", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Lawrence Phillips", "Gianluca Mazzoni", "Lea MÃ¸rch Harder", "Philip Torr", "Jesper Ferkinghoff-Borg", "Kaspar Martens", "Julien Fauqueur"], "year": 2025, "url": "http://arxiv.org/abs/2509.02401v1", "abstract": "Large language model (LLM) agents are increasingly deployed in structured biomedical data environments, yet they often produce fluent but overconfident outputs when reasoning over complex multi-table data. We introduce an uncertainty-aware agent for query-conditioned multi-table summarization that leverages two complementary signals: (i) retrieval uncertainty--entropy over multiple table-selection rollouts--and (ii) summary uncertainty--combining self-consistency and perplexity. Summary uncertainty is incorporated into reinforcement learning (RL) with Group Relative Policy Optimization (GRPO), while both retrieval and summary uncertainty guide inference-time filtering and support the construction of higher-quality synthetic datasets.\n  On multi-omics benchmarks, our approach improves factuality and calibration, nearly tripling correct and useful claims per summary (3.0\\(\\rightarrow\\)8.4 internal; 3.6\\(\\rightarrow\\)9.9 cancer multi-omics) and substantially improving downstream survival prediction (C-index 0.32\\(\\rightarrow\\)0.63). These results demonstrate that uncertainty can serve as a control signal--enabling agents to abstain, communicate confidence, and become more reliable tools for complex structured-data environments.", "source": "arxiv", "arxiv_id": "2509.02401v1", "pdf_url": "https://arxiv.org/pdf/2509.02401v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-02T15:12:10Z", "updated": "2025-09-02T15:12:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "authors": ["Jintian Shao", "Yiming Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2506.03038v2", "abstract": "Reinforcement learning (RL) enhances large language models (LLMs) in complex, long-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework, despite sophisticated mechanisms like Decoupled GAE, theoretically faces fundamental limitations in comprehensively modeling and leveraging deep, long-term value for fine-grained, step-by-step policy guidance in extended reasoning chains. We argue these limitations stem from inherent difficulties in credit assignment, value function representational capacity with temporally abstracted goals, and translating global value signals into local policy improvements, especially with sparse rewards. Our theoretical analysis examines these aspects to illuminate VAPO's boundaries in long-term value modeling, aiming to deepen understanding of current RL for advanced reasoning and suggest future research for more robust LLM agents.", "source": "arxiv", "arxiv_id": "2506.03038v2", "pdf_url": "https://arxiv.org/pdf/2506.03038v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-03T16:20:47Z", "updated": "2025-06-06T22:19:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark", "authors": ["Minghao Shao", "Nanda Rani", "Kimberly Milner", "Haoran Xi", "Meet Udeshi", "Saksham Aggarwal", "Venkata Sai Charan Putrevu", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "year": 2025, "url": "http://arxiv.org/abs/2508.05674v1", "abstract": "Recent advances in LLM agentic systems have improved the automation of offensive security tasks, particularly for Capture the Flag (CTF) challenges. We systematically investigate the key factors that drive agent success and provide a detailed recipe for building effective LLM-based offensive security agents. First, we present CTFJudge, a framework leveraging LLM as a judge to analyze agent trajectories and provide granular evaluation across CTF solving steps. Second, we propose a novel metric, CTF Competency Index (CCI) for partial correctness, revealing how closely agent solutions align with human-crafted gold standards. Third, we examine how LLM hyperparameters, namely temperature, top-p, and maximum token length, influence agent performance and automated cybersecurity task planning. For rapid evaluation, we present CTFTiny, a curated benchmark of 50 representative CTF challenges across binary exploitation, web, reverse engineering, forensics, and cryptography. Our findings identify optimal multi-agent coordination settings and lay the groundwork for future LLM agent research in cybersecurity. We make CTFTiny open source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on https://github.com/NYU-LLM-CTF/CTFJudge.", "source": "arxiv", "arxiv_id": "2508.05674v1", "pdf_url": "https://arxiv.org/pdf/2508.05674v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-05T03:25:09Z", "updated": "2025-08-05T03:25:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems", "authors": ["Tendai Mukande", "Esraa Ali", "Annalina Caputo", "Ruihai Dong", "Noel OConnor"], "year": 2025, "url": "http://arxiv.org/abs/2512.06590v1", "abstract": "Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.", "source": "arxiv", "arxiv_id": "2512.06590v1", "pdf_url": "https://arxiv.org/pdf/2512.06590v1", "categories": ["cs.IR", "cs.AI", "cs.MA"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-12-06T23:04:49Z", "updated": "2025-12-06T23:04:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Enforcing Company Policy Adherence in Agentic Workflows", "authors": ["Naama Zwerdling", "David Boaz", "Ella Rabinovich", "Guy Uziel", "David Amid", "Ateret Anaby-Tavor"], "year": 2025, "url": "http://arxiv.org/abs/2507.16459v2", "abstract": "Large Language Model (LLM) agents hold promise for a flexible and scalable alternative to traditional business process automation, but struggle to reliably follow complex company policies. In this study we introduce a deterministic, transparent, and modular framework for enforcing business policy adherence in agentic workflows. Our method operates in two phases: (1) an offline buildtime stage that compiles policy documents into verifiable guard code associated with tool use, and (2) a runtime integration where these guards ensure compliance before each agent action. We demonstrate our approach on the challenging $Ï$-bench Airlines domain, showing encouraging preliminary results in policy enforcement, and further outline key challenges for real-world deployments.", "source": "arxiv", "arxiv_id": "2507.16459v2", "pdf_url": "https://arxiv.org/pdf/2507.16459v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-22T11:00:37Z", "updated": "2025-10-06T10:07:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Enhanced Immersion and Agency for LLM-based Interactive Drama", "authors": ["Hongqiu Wu", "Weiqi Wu", "Tianyang Xu", "Jiameng Zhang", "Hai Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.17878v2", "abstract": "LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the user (i.e. the player) plays the role of a character in the story, has conversations with characters played by LLM agents, and experiences an unfolding story. This paper begins with understanding interactive drama from two aspects: Immersion, the player's feeling of being present in the story, and Agency, the player's ability to influence the story world. Both are crucial to creating an enjoyable interactive experience, while they have been underexplored in previous work. To enhance these two aspects, we first propose Playwriting-guided Generation, a novel method that helps LLMs craft dramatic stories with substantially improved structures and narrative quality. Additionally, we introduce Plot-based Reflection for LLM agents to refine their reactions to align with the player's intentions. Our evaluation relies on human judgment to assess the gains of our methods in terms of immersion and agency.", "source": "arxiv", "arxiv_id": "2502.17878v2", "pdf_url": "https://arxiv.org/pdf/2502.17878v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-25T06:06:16Z", "updated": "2025-06-03T10:10:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards LLM Agents for Earth Observation", "authors": ["Chia Hsiang Kao", "Wenting Zhao", "Shreelekha Revankar", "Samuel Speas", "Snehal Bhagat", "Rajeev Datta", "Cheng Perng Phoo", "Utkarsh Mall", "Carl Vondrick", "Kavita Bala", "Bharath Hariharan"], "year": 2025, "url": "http://arxiv.org/abs/2504.12110v2", "abstract": "Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at https://iandrover.github.io/UnivEarth.", "source": "arxiv", "arxiv_id": "2504.12110v2", "pdf_url": "https://arxiv.org/pdf/2504.12110v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-16T14:19:25Z", "updated": "2025-09-12T19:16:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play", "authors": ["Jiaxun Cui", "Chen Tang", "Jarrett Holtz", "Janice Nguyen", "Alessandro G. Allievi", "Hang Qiu", "Peter Stone"], "year": 2025, "url": "http://arxiv.org/abs/2505.18334v1", "abstract": "Past work has demonstrated that autonomous vehicles can drive more safely if they communicate with one another than if they do not. However, their communication has often not been human-understandable. Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. In this work, we propose a suite of traffic tasks in autonomous driving where vehicles in a traffic scenario need to communicate in natural language to facilitate coordination in order to avoid an imminent collision and/or support efficient traffic flow. To this end, this paper introduces a novel method, LLM+Debrief, to learn a message generation and high-level decision-making policy for autonomous vehicles through multi-agent discussion. To evaluate LLM agents for driving, we developed a gym-like simulation environment that contains a range of driving scenarios. Our experimental results demonstrate that LLM+Debrief is more effective at generating meaningful and human-understandable natural language messages to facilitate cooperation and coordination than a zero-shot LLM agent. Our code and demo videos are available at https://talking-vehicles.github.io/.", "source": "arxiv", "arxiv_id": "2505.18334v1", "pdf_url": "https://arxiv.org/pdf/2505.18334v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-05-23T19:40:09Z", "updated": "2025-05-23T19:40:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Operational Validation of LLM-Agent Social Simulations: A Replicated Study of a Reddit-like Technology Forum", "authors": ["Aleksandar TomaÅ¡eviÄ", "Darja CvetkoviÄ", "Sara Major", "Slobodan MaletiÄ", "Miroslav AnÄelkoviÄ", "Ana VraniÄ", "Boris Stupovski", "DuÅ¡an VudragoviÄ", "Aleksandar BogojeviÄ", "Marija MitroviÄ Dankulov"], "year": 2025, "url": "http://arxiv.org/abs/2508.21740v2", "abstract": "Large Language Models (LLMs) enable generative social simulations that can capture culturally informed, norm-guided interaction on online social platforms. We build a technology community simulation modeled on Voat, a Reddit-like alt-right news aggregator and discussion platform active from 2014 to 2020. Using the YSocial framework, we seed the simulation with a fixed catalog of technology links sampled from Voat's shared URLs (covering 30+ domains) and calibrate parameters to Voat's v/technology using samples from the MADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama 3.1 8B) and concise personas (demographics, political leaning, interests, education, toxicity propensity) to generate posts, replies, and reactions under platform rules for link and text submissions, threaded replies and daily activity cycles. We run a 30-day simulation and evaluate operational validity by comparing distributions and structures with matched Voat data: activity patterns, interaction networks, toxicity, and topic coverage. Results indicate familiar online regularities: similar activity rhythms, heavy-tailed participation, sparse low-clustering interaction networks, core-periphery structure, topical alignment with Voat, and elevated toxicity. Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates. The simulation generates realistic discussions, often featuring toxic language, primarily centered on technology topics such as Big Tech and AI. This approach offers a valuable method for examining toxicity dynamics and testing moderation strategies within a controlled environment.", "source": "arxiv", "arxiv_id": "2508.21740v2", "pdf_url": "https://arxiv.org/pdf/2508.21740v2", "categories": ["cs.CY", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-08-29T16:06:27Z", "updated": "2025-12-31T13:05:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Pervasive Distributed Agentic Generative AI -- A State of The Art", "authors": ["Gianni Molinari", "Fabio Ciravegna"], "year": 2025, "url": "http://arxiv.org/abs/2506.13324v2", "abstract": "The rapid advancement of intelligent agents and Large Language Models (LLMs) is reshaping the pervasive computing field. Their ability to perceive, reason, and act through natural language understanding enables autonomous problem-solving in complex pervasive environments, including the management of heterogeneous sensors, devices, and data. This survey outlines the architectural components of LLM agents (profiling, memory, planning, and action) and examines their deployment and evaluation across various scenarios. Than it reviews computational and infrastructural advancements (cloud to edge) in pervasive computing and how AI is moving in this field. It highlights state-of-the-art agent deployment strategies and applications, including local and distributed execution on resource-constrained devices. This survey identifies key challenges of these agents in pervasive computing such as architectural, energetic and privacy limitations. It finally proposes what we called \"Agent as a Tool\", a conceptual framework for pervasive agentic AI, emphasizing context awareness, modularity, security, efficiency and effectiveness.", "source": "arxiv", "arxiv_id": "2506.13324v2", "pdf_url": "https://arxiv.org/pdf/2506.13324v2", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-16T10:15:06Z", "updated": "2025-12-18T17:02:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Robust Visual Continual Learning with Multi-Prototype Supervision", "authors": ["Xiwei Liu", "Yulong Li", "Yichen Li", "Xinlin Zhuang", "Haolin Yang", "Huifa Li", "Imran Razzak"], "year": 2025, "url": "http://arxiv.org/abs/2509.16011v1", "abstract": "Language-guided supervision, which utilizes a frozen semantic target from a Pretrained Language Model (PLM), has emerged as a promising paradigm for visual Continual Learning (CL). However, relying on a single target introduces two critical limitations: 1) semantic ambiguity, where a polysemous category name results in conflicting visual representations, and 2) intra-class visual diversity, where a single prototype fails to capture the rich variety of visual appearances within a class. To this end, we propose MuproCL, a novel framework that replaces the single target with multiple, context-aware prototypes. Specifically, we employ a lightweight LLM agent to perform category disambiguation and visual-modal expansion to generate a robust set of semantic prototypes. A LogSumExp aggregation mechanism allows the vision model to adaptively align with the most relevant prototype for a given image. Extensive experiments across various CL baselines demonstrate that MuproCL consistently enhances performance and robustness, establishing a more effective path for language-guided continual learning.", "source": "arxiv", "arxiv_id": "2509.16011v1", "pdf_url": "https://arxiv.org/pdf/2509.16011v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-09-19T14:24:48Z", "updated": "2025-09-19T14:24:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Trustworthy Legal AI through LLM Agents and Formal Reasoning", "authors": ["Linze Chen", "Yufan Cai", "Zhe Hou", "Jinsong Dong"], "year": 2025, "url": "http://arxiv.org/abs/2511.21033v1", "abstract": "The rationality of law manifests in two forms: substantive rationality, which concerns the fairness or moral desirability of outcomes, and formal rationality, which requires legal decisions to follow explicitly stated, general, and logically coherent rules. Existing LLM-based systems excel at surface-level text analysis but lack the guarantees required for principled jurisprudence. We introduce L4M, a novel framework that combines adversarial LLM agents with SMT-solver-backed proofs to unite the interpretive flexibility of natural language with the rigor of symbolic verification. The pipeline consists of three phases: (1) Statute Formalization, where domain-specific prompts convert legal provisions into logical formulae; (2) Dual Fact and Statute Extraction, in which prosecutor- and defense-aligned LLMs independently map case narratives to fact tuples and statutes, ensuring role isolation; and (3) Solver-Centric Adjudication, where an autoformalizer compiles both parties' arguments into logic constraints, and unsat cores trigger iterative self-critique until a satisfiable formula is achieved, which is then verbalized by a Judge-LLM into a transparent verdict and optimized sentence. Experimental results on public benchmarks show that our system surpasses advanced LLMs including GPT-o4-mini, DeepSeek-V3, and Claude 4 as well as state-of-the-art Legal AI baselines, while providing rigorous and explainable symbolic justifications.", "source": "arxiv", "arxiv_id": "2511.21033v1", "pdf_url": "https://arxiv.org/pdf/2511.21033v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-26T04:05:06Z", "updated": "2025-11-26T04:05:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance", "authors": ["Gonca GÃ¼rsun"], "year": 2025, "url": "http://arxiv.org/abs/2512.11421v1", "abstract": "Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.\n  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.", "source": "arxiv", "arxiv_id": "2512.11421v1", "pdf_url": "https://arxiv.org/pdf/2512.11421v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-12T10:03:24Z", "updated": "2025-12-12T10:03:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents", "authors": ["Chaoran Chen", "Bingsheng Yao", "Ruishi Zou", "Wenyue Hua", "Weimin Lyu", "Yanfang Ye", "Toby Jia-Jun Li", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.13012v3", "abstract": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.", "source": "arxiv", "arxiv_id": "2502.13012v3", "pdf_url": "https://arxiv.org/pdf/2502.13012v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-02-18T16:33:33Z", "updated": "2025-03-27T04:07:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards an Agentic Workflow for Internet Measurement Research", "authors": ["Alagappan Ramanathan", "Eunju Kang", "Dongsu Han", "Sangeetha Abdu Jyothi"], "year": 2025, "url": "http://arxiv.org/abs/2511.10611v1", "abstract": "Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.\n  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis.", "source": "arxiv", "arxiv_id": "2511.10611v1", "pdf_url": "https://arxiv.org/pdf/2511.10611v1", "categories": ["cs.NI", "cs.AI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-11-13T18:44:09Z", "updated": "2025-11-13T18:44:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards autonomous quantum physics research using LLM agents with access to intelligent tools", "authors": ["SÃ¶ren Arlt", "Xuemei Gu", "Mario Krenn"], "year": 2025, "url": "http://arxiv.org/abs/2511.11752v1", "abstract": "Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.", "source": "arxiv", "arxiv_id": "2511.11752v1", "pdf_url": "https://arxiv.org/pdf/2511.11752v1", "categories": ["cs.AI", "cs.DL", "quant-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-13T18:18:58Z", "updated": "2025-11-13T18:18:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes", "authors": ["Jialiang Huang", "Teng Ma", "Zheng Liu", "Sixing Lin", "Kang Chen", "Jinlei Jiang", "Xia Liao", "Yingdi Shan", "Yongwei Wu", "Ning Zhang", "Mengting Lu", "Tao Ma", "Haifeng Gong", "Mingxing Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2509.09525v1", "abstract": "Serverless computing provides dynamic scalability, but its infrastructure overhead becomes a bottleneck for emerging workloads such as LLM agents, which exhibit unpredictable invocation patterns and variable resource demands. Our analysis shows that for these agents, the cost of running on serverless platforms can reach up to 70% of the cost of LLM API calls. This finding motivates the need for a more efficient, high-density serverless platform. We present TrEnv, a co-designed serverless platform that supports both container- and VM-based environments, optimized for the unique demands of LLM agents. TrEnv reduces startup latency and memory usage through repurposable sandboxes and memory templates, which enable fast reuse and restoration of execution environments. To further reduce overhead in VM-based agent workloads, TrEnv leverages browser sharing and a page cache bypassing mechanism. Evaluations show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in container-based settings, and achieves up to 58% lower P99 latency and 61% memory savings for VM-based agents compared to state-of-the-art systems like E2B.", "source": "arxiv", "arxiv_id": "2509.09525v1", "pdf_url": "https://arxiv.org/pdf/2509.09525v1", "categories": ["cs.DC", "cs.OS"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-09-11T15:06:03Z", "updated": "2025-09-11T15:06:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Training LLM Agents to Empower Humans", "authors": ["Evan Ellis", "Vivek Myers", "Jens Tuyls", "Sergey Levine", "Anca Dragan", "Benjamin Eysenbach"], "year": 2025, "url": "http://arxiv.org/abs/2510.13709v2", "abstract": "Assistive agents should not only take actions on behalf of a human, but also step out of the way and cede control when there are important decisions to be made. However, current methods for building assistive agents, whether via mimicking expert humans or via RL finetuning on an inferred reward, often encourage agents to complete tasks on their own rather than truly assisting the human attain their objectives. Additionally, these methods often require costly explicit human feedback to provide a training signal. We propose a new approach to tuning assistive language models based on maximizing the human's empowerment, their ability to effect desired changes in the environment. Our empowerment-maximizing method, Empower, only requires offline text data, providing a self-supervised method for fine-tuning language models to better assist humans. To study the efficacy of our approach, we conducted an 18-person user study comparing our empowerment assistant with a strong baseline. Participants preferred our assistant 78% of the time (p=0.015), with a 31% higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a new environment for evaluating multi-turn code assistance using simulated humans. Using this environment, we show that agents trained with Empower increase the success rate of a simulated human programmer on challenging coding questions by an average of 192% over an SFT baseline. With this empowerment objective, we provide a framework for useful aligned AI agents at scale using only offline data without the need for any additional human feedback or verifiable rewards.", "source": "arxiv", "arxiv_id": "2510.13709v2", "pdf_url": "https://arxiv.org/pdf/2510.13709v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-15T16:09:33Z", "updated": "2025-10-16T03:39:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.18370v2", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.", "source": "arxiv", "arxiv_id": "2508.18370v2", "pdf_url": "https://arxiv.org/pdf/2508.18370v2", "categories": ["cs.SE", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-25T18:02:23Z", "updated": "2025-09-23T03:30:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Training Proactive and Personalized LLM Agents", "authors": ["Weiwei Sun", "Xuhui Zhou", "Weihua Du", "Xingyao Wang", "Sean Welleck", "Graham Neubig", "Maarten Sap", "Yiming Yang"], "year": 2025, "url": "http://arxiv.org/abs/2511.02208v1", "abstract": "While existing work focuses primarily on task success, we argue that effective real-world agents require optimizing three dimensions: productivity (task completion), proactivity (asking essential questions), and personalization (adapting to diverse user preferences). We introduce UserVille, an interactive environment with LLM-based user simulators enabling diverse, configurable user preferences. Leveraging UserVille, we introduce PPP, a multi-objective reinforcement learning approach that jointly optimizes all three dimensions: Productivity, Proactivity, and Personalization. Experiments on software engineering and deep research tasks show that agents trained with PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6 on average), demonstrating the ability to ask strategic clarifying questions, adapt to unseen user preferences, and improve task success through better interaction. This work demonstrates that explicitly optimizing for user-centered interaction is critical for building practical and effective AI agents.", "source": "arxiv", "arxiv_id": "2511.02208v1", "pdf_url": "https://arxiv.org/pdf/2511.02208v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-04T02:59:36Z", "updated": "2025-11-04T02:59:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning", "authors": ["Hanjiang Hu", "Changliu Liu", "Na Li", "Yebin Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.20616v2", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "source": "arxiv", "arxiv_id": "2509.20616v2", "pdf_url": "https://arxiv.org/pdf/2509.20616v2", "categories": ["cs.LG", "eess.SY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-24T23:47:36Z", "updated": "2025-12-08T18:53:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Training-Free Group Relative Policy Optimization", "authors": ["Yuzheng Cai", "Siqi Cai", "Yuchen Shi", "Zihan Xu", "Lichao Chen", "Yulei Qin", "Xiaoyu Tan", "Gang Li", "Zongyi Li", "Haojia Lin", "Yong Mao", "Ke Li", "Xing Sun"], "year": 2025, "url": "http://arxiv.org/abs/2510.08191v1", "abstract": "Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.", "source": "arxiv", "arxiv_id": "2510.08191v1", "pdf_url": "https://arxiv.org/pdf/2510.08191v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-09T13:18:17Z", "updated": "2025-10-09T13:18:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Training-Free Time Series Classification via In-Context Reasoning with LLM Agents", "authors": ["Songyuan Sui", "Zihang Xu", "Yu-Neng Chuang", "Kwei-Herng Lai", "Xia Hu"], "year": 2025, "url": "http://arxiv.org/abs/2510.05950v1", "abstract": "Time series classification (TSC) spans diverse application scenarios, yet labeled data are often scarce, making task-specific training costly and inflexible. Recent reasoning-oriented large language models (LLMs) show promise in understanding temporal patterns, but purely zero-shot usage remains suboptimal. We propose FETA, a multi-agent framework for training-free TSC via exemplar-based in-context reasoning. FETA decomposes a multivariate series into channel-wise subproblems, retrieves a few structurally similar labeled examples for each channel, and leverages a reasoning LLM to compare the query against these exemplars, producing channel-level labels with self-assessed confidences; a confidence-weighted aggregator then fuses all channel decisions. This design eliminates the need for pretraining or fine-tuning, improves efficiency by pruning irrelevant channels and controlling input length, and enhances interpretability through exemplar grounding and confidence estimation. On nine challenging UEA datasets, FETA achieves strong accuracy under a fully training-free setting, surpassing multiple trained baselines. These results demonstrate that a multi-agent in-context reasoning framework can transform LLMs into competitive, plug-and-play TSC solvers without any parameter training. The code is available at https://github.com/SongyuanSui/FETATSC.", "source": "arxiv", "arxiv_id": "2510.05950v1", "pdf_url": "https://arxiv.org/pdf/2510.05950v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-07T14:07:43Z", "updated": "2025-10-07T14:07:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Tree Search for LLM Agent Reinforcement Learning", "authors": ["Yuxiang Ji", "Ziyu Ma", "Yong Wang", "Guanhua Chen", "Xiangxiang Chu", "Liaoni Wu"], "year": 2025, "url": "http://arxiv.org/abs/2509.21240v2", "abstract": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.", "source": "arxiv", "arxiv_id": "2509.21240v2", "pdf_url": "https://arxiv.org/pdf/2509.21240v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-25T14:37:09Z", "updated": "2025-10-11T09:55:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TuneGenie: Reasoning-based LLM agents for preferential music generation", "authors": ["Amitesh Pandey", "Jafarbek Arifdjanov", "Ansh Tiwari"], "year": 2025, "url": "http://arxiv.org/abs/2506.12083v1", "abstract": "Recently, Large language models (LLMs) have shown great promise across a diversity of tasks, ranging from generating images to reasoning spatially. Considering their remarkable (and growing) textual reasoning capabilities, we investigate LLMs' potency in conducting analyses of an individual's preferences in music (based on playlist metadata, personal write-ups, etc.) and producing effective prompts (based on these analyses) to be passed to Suno AI (a generative AI tool for music production). Our proposition of a novel LLM-based textual representation to music model (which we call TuneGenie) and the various methods we develop to evaluate & benchmark similar models add to the increasing (and increasingly controversial) corpus of research on the use of AI in generating art.", "source": "arxiv", "arxiv_id": "2506.12083v1", "pdf_url": "https://arxiv.org/pdf/2506.12083v1", "categories": ["cs.SD", "cs.MA", "eess.AS"], "primary_category": "cs.SD", "doi": "", "venue": "", "published": "2025-06-10T08:39:37Z", "updated": "2025-06-10T08:39:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs", "authors": ["Junbo Li", "Peng Zhou", "Rui Meng", "Meet P. Vadera", "Lihong Li", "Yang Li"], "year": 2025, "url": "http://arxiv.org/abs/2512.17008v1", "abstract": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.", "source": "arxiv", "arxiv_id": "2512.17008v1", "pdf_url": "https://arxiv.org/pdf/2512.17008v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-18T19:07:25Z", "updated": "2025-12-18T19:07:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets", "authors": ["Yuzhe Yang", "Yifei Zhang", "Minghao Wu", "Kaidi Zhang", "Yunmiao Zhang", "Honghai Yu", "Yan Hu", "Benyou Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.01506v5", "abstract": "The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.", "source": "arxiv", "arxiv_id": "2502.01506v5", "pdf_url": "https://arxiv.org/pdf/2502.01506v5", "categories": ["cs.CE", "cs.CY"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2025-02-03T16:39:48Z", "updated": "2025-10-18T03:02:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning", "authors": ["Jiawei Zhang", "Shuang Yang", "Bo Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.01908v3", "abstract": "Large Language Model (LLM) agents equipped with external tools have become increasingly powerful for complex tasks such as web shopping, automated email replies, and financial trading. However, these advancements amplify the risks of adversarial attacks, especially when agents can access sensitive external functionalities. Nevertheless, manipulating LLM agents into performing targeted malicious actions or invoking specific tools remains challenging, as these agents extensively reason or plan before executing final actions. In this work, we present UDora, a unified red teaming framework designed for LLM agents that dynamically hijacks the agent's reasoning processes to compel malicious behavior. Specifically, UDora first generates the model's reasoning trace for the given task, then automatically identifies optimal points within this trace to insert targeted perturbations. The resulting perturbed reasoning is then used as a surrogate response for optimization. By iteratively applying this process, the LLM agent will then be induced to undertake designated malicious actions or to invoke specific malicious tools. Our approach demonstrates superior effectiveness compared to existing methods across three LLM agent datasets. The code is available at https://github.com/AI-secure/UDora.", "source": "arxiv", "arxiv_id": "2503.01908v3", "pdf_url": "https://arxiv.org/pdf/2503.01908v3", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-28T21:30:28Z", "updated": "2025-11-12T01:53:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents", "authors": ["Siqi Lai", "Yansong Ning", "Zirui Yuan", "Zhixi Chen", "Hao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2505.17572v1", "abstract": "Large language models (LLMs) have shown emerging potential in spatiotemporal reasoning, making them promising candidates for building urban agents that support diverse urban downstream applications. Despite these benefits, existing studies primarily focus on evaluating urban LLM agent on outcome-level metrics (e.g., prediction accuracy, traffic efficiency), offering limited insight into their underlying reasoning processes. As a result, the strengths and limitations of urban LLM agents in spatiotemporal reasoning remain poorly understood. To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback. Specifically, USTBench supports five diverse urban decision-making and four spatiotemporal prediction tasks, all running within our constructed interactive city environment UAgentEnv. The benchmark includes 62,466 structured QA pairs for process-level evaluation and standardized end-to-end task assessments, enabling fine-grained diagnostics and broad task-level comparison across diverse urban scenarios. Through extensive evaluation of thirteen leading LLMs, we reveal that although LLMs show promising potential across various urban downstream tasks, they still struggle in long-horizon planning and reflective adaptation in dynamic urban contexts. Notably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on general logic or mathematical problems do not consistently outperform non-reasoning LLMs. This discrepancy highlights the need for domain-specialized adaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench provides a foundation to build more adaptive and effective LLM-based urban agents and broad smart city applications.", "source": "arxiv", "arxiv_id": "2505.17572v1", "pdf_url": "https://arxiv.org/pdf/2505.17572v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-23T07:30:57Z", "updated": "2025-05-23T07:30:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents", "authors": ["Yuxuan Lu", "Bingsheng Yao", "Hansu Gu", "Jing Huang", "Jessie Wang", "Yang Li", "Jiri Gesi", "Qi He", "Toby Jia-Jun Li", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.09407v3", "abstract": "Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate their new designs. But what about evaluating and iterating the usability testing study design itself? Recent advances in Large Language Model-simulated Agent (LLM Agent) research inspired us to design UXAgent to support UX researchers in evaluating and iterating their study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users and to interactively test the target website. The system also provides a Result Viewer Interface so that the UX researchers can easily review and analyze the generated qualitative (e.g., agents' post-study surveys) and quantitative data (e.g., agents' interaction logs), or even interview agents directly. Through a heuristic evaluation with 16 UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.", "source": "arxiv", "arxiv_id": "2504.09407v3", "pdf_url": "https://arxiv.org/pdf/2504.09407v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-13T02:34:22Z", "updated": "2025-09-19T17:52:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design", "authors": ["Yuxuan Lu", "Bingsheng Yao", "Hansu Gu", "Jing Huang", "Jessie Wang", "Yang Li", "Jiri Gesi", "Qi He", "Toby Jia-Jun Li", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.12561v3", "abstract": "Usability testing is a fundamental yet challenging (e.g., inflexible to iterate the study design flaws and hard to recruit study participants) research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM-Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human subject study. Our system features an LLM-Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The results are shown in qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent-assisted UX study.", "source": "arxiv", "arxiv_id": "2502.12561v3", "pdf_url": "https://arxiv.org/pdf/2502.12561v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "10.1145/3706599.3719729", "venue": "", "published": "2025-02-18T05:55:18Z", "updated": "2025-04-05T01:55:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Understanding Bias Reinforcement in LLM Agents Debate", "authors": ["Jihwan Oh", "Minchan Jeong", "Jongwoo Ko", "Se-Young Yun"], "year": 2025, "url": "http://arxiv.org/abs/2503.16814v4", "abstract": "Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\\textbf{DReaMAD}$ $($$\\textbf{D}$iverse $\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.", "source": "arxiv", "arxiv_id": "2503.16814v4", "pdf_url": "https://arxiv.org/pdf/2503.16814v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-21T02:51:30Z", "updated": "2025-08-24T14:08:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics", "authors": ["Trung-Kiet Huynh", "Duy-Minh Dao-Sy", "Thanh-Bang Cao", "Phong-Hao Le", "Hong-Dan Nguyen", "Phu-Quy Nguyen-Lam", "Minh-Luan Nguyen-Vo", "Hong-Phat Pham", "Phu-Hoa Pham", "Thien-Kim Than", "Chi-Nguyen Tran", "Huy Tran", "Gia-Thoai Tran-Le", "Alessio Buscemi", "Le Hong Trang", "The Anh Han"], "year": 2025, "url": "http://arxiv.org/abs/2512.07462v2", "abstract": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.", "source": "arxiv", "arxiv_id": "2512.07462v2", "pdf_url": "https://arxiv.org/pdf/2512.07462v2", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG", "math.DS"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-12-08T11:40:03Z", "updated": "2025-12-11T20:32:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Unified Software Engineering Agent as AI Software Engineer", "authors": ["Leonhard Applis", "Yuntong Zhang", "Shanchao Liang", "Nan Jiang", "Lin Tan", "Abhik Roychoudhury"], "year": 2025, "url": "http://arxiv.org/abs/2506.14683v2", "abstract": "The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future.", "source": "arxiv", "arxiv_id": "2506.14683v2", "pdf_url": "https://arxiv.org/pdf/2506.14683v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-06-17T16:19:13Z", "updated": "2025-12-08T09:06:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture", "authors": ["Jiarun Liu", "Shiyue Xu", "Yang Li", "Shangkun Liu", "Yongli Yu", "Peng Cao"], "year": 2025, "url": "http://arxiv.org/abs/2512.11303v1", "abstract": "Large Language Model agents face fundamental challenges in adapting to novel tasks due to limitations in tool availability and experience reuse. Existing approaches either rely on predefined tools with limited coverage or build tools from scratch without leveraging past experiences, leading to inefficient exploration and suboptimal performance. We introduce SMITH (Shared Memory Integrated Tool Hub), a unified cognitive architecture that seamlessly integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. SMITH organizes agent memory into procedural, semantic, and episodic components, enabling systematic capability expansion while preserving successful execution patterns. Our approach formalizes tool creation as iterative code generation within controlled sandbox environments and experience sharing through episodic memory retrieval with semantic similarity matching. We further propose a curriculum learning strategy based on agent-ensemble difficulty re-estimation. Extensive experiments on the GAIA benchmark demonstrate SMITH's effectiveness, achieving 81.8% Pass@1 accuracy and outperforming state-of-the-art baselines including Alita (75.2%) and Memento (70.9%). Our work establishes a foundation for building truly adaptive agents that continuously evolve their capabilities through principled integration of tool creation and experience accumulation.", "source": "arxiv", "arxiv_id": "2512.11303v1", "pdf_url": "https://arxiv.org/pdf/2512.11303v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-12T06:00:11Z", "updated": "2025-12-12T06:00:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration", "authors": ["Zhixuan He", "Yue Feng"], "year": 2025, "url": "http://arxiv.org/abs/2510.16645v1", "abstract": "Large Language Models (LLMs) demonstrate strong performance but often lack interpretable reasoning. This paper introduces the Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo), which enhances both performance and interpretability by simulating a structured debate among four specialized LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the framework to collaboratively explore diverse cognitive approaches. Through iterative debate, agents challenge and refine initial responses, yielding more robust conclusions and an explicit, auditable reasoning chain. Across six benchmarks and under a unified open-source setup, DiMo improves accuracy over widely used single-model and debate baselines, with the largest gains on math. We position DiMo as a semantics-aware, Web-native multi-agent framework: it models human-machine intelligence with LLM agents that produce semantically typed, URL-annotated evidence chains for explanations and user-friendly interactions. Although our experiments use standard reasoning benchmarks, the framework is designed to be instantiated over Web corpora and knowledge graphs, combining retrieval-augmented reasoning with structured justifications that downstream systems can inspect and reuse.", "source": "arxiv", "arxiv_id": "2510.16645v1", "pdf_url": "https://arxiv.org/pdf/2510.16645v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-18T21:22:36Z", "updated": "2025-10-18T21:22:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming", "authors": ["Nanxu Gong", "Xinyuan Wang", "Wangyang Ying", "Haoyue Bai", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "year": 2025, "url": "http://arxiv.org/abs/2504.21304v1", "abstract": "Feature transformation involves generating a new set of features from the original dataset to enhance the data's utility. In certain domains like material performance screening, dimensionality is large and collecting labels is expensive and lengthy. It highly necessitates transforming feature spaces efficiently and without supervision to enhance data readiness and AI utility. However, existing methods fall short in efficient navigation of a vast space of feature combinations, and are mostly designed for supervised settings. To fill this gap, our unique perspective is to leverage a generator-critic duet-play teaming framework using LLM agents and in-context learning to derive pseudo-supervision from unsupervised data. The framework consists of three interconnected steps: (1) Critic agent diagnoses data to generate actionable advice, (2) Generator agent produces tokenized feature transformations guided by the critic's advice, and (3) Iterative refinement ensures continuous improvement through feedback between agents. The generator-critic framework can be generalized to human-agent collaborative generation, by replacing the critic agent with human experts. Extensive experiments demonstrate that the proposed framework outperforms even supervised baselines in feature transformation efficiency, robustness, and practical applicability across diverse datasets.", "source": "arxiv", "arxiv_id": "2504.21304v1", "pdf_url": "https://arxiv.org/pdf/2504.21304v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-30T04:26:03Z", "updated": "2025-04-30T04:26:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Unveiling Privacy Risks in LLM Agent Memory", "authors": ["Bo Wang", "Weiyi He", "Shenglai Zeng", "Zhen Xiang", "Yue Xing", "Jiliang Tang", "Pengfei He"], "year": 2025, "url": "http://arxiv.org/abs/2502.13172v2", "abstract": "Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent designer's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.", "source": "arxiv", "arxiv_id": "2502.13172v2", "pdf_url": "https://arxiv.org/pdf/2502.13172v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-17T19:55:53Z", "updated": "2025-06-03T17:08:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "UrbanMind: Towards Urban General Intelligence via Tool-Enhanced Retrieval-Augmented Generation and Multilevel Optimization", "authors": ["Kai Yang", "Zelin Zhu", "Chengtao Jian", "Hui Ma", "Shengjie Zhao", "Xiaozhou Ye", "Ye Ouyang"], "year": 2025, "url": "http://arxiv.org/abs/2507.04706v1", "abstract": "Urban general intelligence (UGI) refers to the capacity of AI systems to autonomously perceive, reason, and act within dynamic and complex urban environments. In this paper, we introduce UrbanMind, a tool-enhanced retrieval-augmented generation (RAG) framework designed to facilitate UGI. Central to UrbanMind is a novel architecture based on Continual Retrieval-Augmented MoE-based LLM (C-RAG-LLM), which dynamically incorporates domain-specific knowledge and evolving urban data to support long-term adaptability. The architecture of C-RAG-LLM aligns naturally with a multilevel optimization framework, where different layers are treated as interdependent sub-problems. Each layer has distinct objectives and can be optimized either independently or jointly through a hierarchical learning process. The framework is highly flexible, supporting both end-to-end training and partial layer-wise optimization based on resource or deployment constraints. To remain adaptive under data drift, it is further integrated with an incremental corpus updating mechanism. Evaluations on real-world urban tasks of a variety of complexity verify the effectiveness of the proposed framework. This work presents a promising step toward the realization of general-purpose LLM agents in future urban environments.", "source": "arxiv", "arxiv_id": "2507.04706v1", "pdf_url": "https://arxiv.org/pdf/2507.04706v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-07-07T06:57:34Z", "updated": "2025-07-07T06:57:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking", "authors": ["Jingyi Huang", "Yuyi Yang", "Mengmeng Ji", "Charles Alba", "Sheng Zhang", "Ruopeng An"], "year": 2025, "url": "http://arxiv.org/abs/2512.00007v1", "abstract": "The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.", "source": "arxiv", "arxiv_id": "2512.00007v1", "pdf_url": "https://arxiv.org/pdf/2512.00007v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-10-10T15:10:46Z", "updated": "2025-10-10T15:10:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces", "authors": ["Alaa Saleh", "Sasu Tarkoma", "Praveen Kumar Donta", "Naser Hossein Motlagh", "Schahram Dustdar", "Susanna Pirttikangas", "Lauri LovÃ©n"], "year": 2025, "url": "http://arxiv.org/abs/2505.00472v1", "abstract": "Agentic AI, with its autonomous and proactive decision-making, has transformed smart environments. By integrating Generative AI (GenAI) and multi-agent systems, modern AI frameworks can dynamically adapt to user preferences, optimize data management, and improve resource allocation. This paper introduces UserCentrix, an agentic memory-augmented AI framework designed to enhance smart spaces through dynamic, context-aware decision-making. This framework integrates personalized Large Language Model (LLM) agents that leverage user preferences and LLM memory management to deliver proactive and adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control system, balancing centralized and distributed processing to optimize real-time responsiveness while maintaining global situational awareness. UserCentrix achieves resource-efficient AI interactions by embedding memory-augmented reasoning, cooperative agent negotiation, and adaptive orchestration strategies. Our key contributions include (i) a self-organizing framework with proactive scaling based on task urgency, (ii) a Value of Information (VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM agent, and (iv) an intelligent multi-agent coordination system for seamless environment adaptation. Experimental results across various models confirm the effectiveness of our approach in enhancing response accuracy, system efficiency, and computational resource management in real-world application.", "source": "arxiv", "arxiv_id": "2505.00472v1", "pdf_url": "https://arxiv.org/pdf/2505.00472v1", "categories": ["cs.AI", "cs.DC", "cs.MA", "cs.NI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-01T11:54:49Z", "updated": "2025-05-01T11:54:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment", "authors": ["Aylton Almeida", "Laerte Xavier", "Marco Tulio Valente"], "year": 2025, "url": "http://arxiv.org/abs/2510.26699v3", "abstract": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median).", "source": "arxiv", "arxiv_id": "2510.26699v3", "pdf_url": "https://arxiv.org/pdf/2510.26699v3", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "10.1145/3786167.3788411", "venue": "", "published": "2025-10-30T17:05:13Z", "updated": "2026-01-08T14:23:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding", "authors": ["Chufan Gao", "Jintai Chen", "Jimeng Sun"], "year": 2025, "url": "http://arxiv.org/abs/2508.18676v1", "abstract": "Automated tabular understanding and reasoning are essential tasks for data scientists. Recently, Large language models (LLMs) have become increasingly prevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning LLMs using labeled data or (2) Training-free prompting LLM agents using chain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost of generalizability. Training-free prompting is highly generalizable but does not take full advantage of training data. In this paper, we propose a novel prompting-based reasoning approach, Learn then Retrieve: LRTab, which integrates the benefits of both by retrieving relevant information learned from training data. We first use prompting to obtain CoT responses over the training data. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to avoid the error, learning insights from the data. We validate the effectiveness of Prompt Conditions using validation data. Finally, at inference time, we retrieve the most relevant Prompt Conditions for additional context for table understanding. We provide comprehensive experiments on WikiTQ and Tabfact, showing that LRTab is interpretable, cost-efficient, and can outperform previous baselines in tabular reasoning.", "source": "arxiv", "arxiv_id": "2508.18676v1", "pdf_url": "https://arxiv.org/pdf/2508.18676v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-26T04:46:54Z", "updated": "2025-08-26T04:46:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces", "authors": ["Artem Grigor", "Christian Schroeder de Witt", "Simon Birnbach", "Ivan Martinovic"], "year": 2025, "url": "http://arxiv.org/abs/2512.15892v1", "abstract": "Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy.\n  We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs).\n  We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.", "source": "arxiv", "arxiv_id": "2512.15892v1", "pdf_url": "https://arxiv.org/pdf/2512.15892v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-17T19:05:37Z", "updated": "2025-12-17T19:05:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings", "authors": ["Ramin Giahi", "Kehui Yao", "Sriram Kollipara", "Kai Zhao", "Vahid Mirjalili", "Jianpeng Xu", "Topojoy Biswas", "Evren Korpeoglu", "Kannan Achan"], "year": 2025, "url": "http://arxiv.org/abs/2507.17080v1", "abstract": "Multimodal learning plays a critical role in e-commerce recommendation platforms today, enabling accurate recommendations and product understanding. However, existing vision-language models, such as CLIP, face key challenges in e-commerce recommendation systems: 1) Weak object-level alignment, where global image embeddings fail to capture fine-grained product attributes, leading to suboptimal retrieval performance; 2) Ambiguous textual representations, where product descriptions often lack contextual clarity, affecting cross-modal matching; and 3) Domain mismatch, as generic vision-language models may not generalize well to e-commerce-specific data. To address these limitations, we propose a framework, VL-CLIP, that enhances CLIP embeddings by integrating Visual Grounding for fine-grained visual understanding and an LLM-based agent for generating enriched text embeddings. Visual Grounding refines image representations by localizing key products, while the LLM agent enhances textual features by disambiguating product descriptions. Our approach significantly improves retrieval accuracy, multimodal retrieval effectiveness, and recommendation quality across tens of millions of items on one of the largest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by 15.5%, and GMV by 4.0%. Additional experimental results show that our framework outperforms vision-language models, including CLIP, FashionCLIP, and GCL, in both precision and semantic alignment, demonstrating the potential of combining object-aware visual grounding and LLM-enhanced text representation for robust multimodal recommendations.", "source": "arxiv", "arxiv_id": "2507.17080v1", "pdf_url": "https://arxiv.org/pdf/2507.17080v1", "categories": ["cs.IR", "cs.AI", "cs.CV"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-07-22T23:45:43Z", "updated": "2025-07-22T23:45:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "VLMLight: Safety-Critical Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning Architecture", "authors": ["Maonan Wang", "Yirong Chen", "Aoyu Pang", "Yuxin Cai", "Chung Shue Chen", "Yuheng Kan", "Man-On Pun"], "year": 2025, "url": "http://arxiv.org/abs/2505.19486v2", "abstract": "Traffic signal control (TSC) is a core challenge in urban mobility, where real-time decisions must balance efficiency and safety. Existing methods - ranging from rule-based heuristics to reinforcement learning (RL) - often struggle to generalize to complex, dynamic, and safety-critical scenarios. We introduce VLMLight, a novel TSC framework that integrates vision-language meta-control with dual-branch reasoning. At the core of VLMLight is the first image-based traffic simulator that enables multi-view visual perception at intersections, allowing policies to reason over rich cues such as vehicle type, motion, and spatial density. A large language model (LLM) serves as a safety-prioritized meta-controller, selecting between a fast RL policy for routine traffic and a structured reasoning branch for critical cases. In the latter, multiple LLM agents collaborate to assess traffic phases, prioritize emergency vehicles, and verify rule compliance. Experiments show that VLMLight reduces waiting times for emergency vehicles by up to 65% over RL-only systems, while preserving real-time performance in standard conditions with less than 1% degradation. VLMLight offers a scalable, interpretable, and safety-aware solution for next-generation traffic signal control.", "source": "arxiv", "arxiv_id": "2505.19486v2", "pdf_url": "https://arxiv.org/pdf/2505.19486v2", "categories": ["eess.SY", "cs.LG", "cs.MA"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2025-05-26T04:12:57Z", "updated": "2025-10-17T09:23:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language", "authors": ["Sijin Sun", "Liangbin Zhao", "Ming Deng", "Xiuju Fu"], "year": 2025, "url": "http://arxiv.org/abs/2505.00989v1", "abstract": "Vessel Traffic Services (VTS) are essential for maritime safety and regulatory compliance through real-time traffic management. However, with increasing traffic complexity and the prevalence of heterogeneous, multimodal data, existing VTS systems face limitations in spatiotemporal reasoning and intuitive human interaction. In this work, we propose VTS-LLM Agent, the first domain-adaptive large LLM agent tailored for interactive decision support in VTS operations. We formalize risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. To support this, we construct a curated benchmark dataset consisting of a custom schema, domain-specific corpus, and a query-SQL test set in multiple linguistic styles. Our framework incorporates NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms to enhance domain grounding and context-aware understanding. Experimental results show that VTS-LLM outperforms both general-purpose and SQL-focused baselines under command-style, operational-style, and formal natural language queries, respectively. Moreover, our analysis provides the first empirical evidence that linguistic style variation introduces systematic performance challenges in Text-to-SQL modeling. This work lays the foundation for natural language interfaces in vessel traffic services and opens new opportunities for proactive, LLM-driven maritime real-time traffic management.", "source": "arxiv", "arxiv_id": "2505.00989v1", "pdf_url": "https://arxiv.org/pdf/2505.00989v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-02T04:27:50Z", "updated": "2025-05-02T04:27:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "authors": ["Yuki Sakamoto", "Takahisa Uchida", "Hiroshi Ishiguro"], "year": 2025, "url": "http://arxiv.org/abs/2507.11979v2", "abstract": "Large language models (LLMs) have emerged as powerful tools for simulating complex social phenomena using human-like agents with specific traits. In human societies, value similarity is important for building trust and close relationships; however, it remains unexplored whether this principle holds true in artificial societies comprising LLM agents. Therefore, this study investigates the influence of value similarity on relationship-building among LLM agents through two experiments. First, in a preliminary experiment, we evaluated the controllability of values in LLMs to identify the most effective model and prompt design for controlling the values. Subsequently, in the main experiment, we generated pairs of LLM agents imbued with specific values and analyzed their mutual evaluations of trust and interpersonal closeness following a dialogue. The experiments were conducted in English and Japanese to investigate language dependence. The results confirmed that pairs of agents with higher value similarity exhibited greater mutual trust and interpersonal closeness. Our findings demonstrate that the LLM agent simulation serves as a valid testbed for social science theories, contributes to elucidating the mechanisms by which values influence relationship building, and provides a foundation for inspiring new theories and insights into the social sciences.", "source": "arxiv", "arxiv_id": "2507.11979v2", "pdf_url": "https://arxiv.org/pdf/2507.11979v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "10.1038/s41598-025-25531-1", "venue": "Scientific Reports volume 15, Article number: 41653 (2025)", "published": "2025-07-16T07:21:59Z", "updated": "2025-10-20T01:37:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation", "authors": ["Lesly Miculicich", "Mihir Parmar", "Hamid Palangi", "Krishnamurthy Dj Dvijotham", "Mirko Montanari", "Tomas Pfister", "Long T. Le"], "year": 2025, "url": "http://arxiv.org/abs/2510.05156v1", "abstract": "The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents.", "source": "arxiv", "arxiv_id": "2510.05156v1", "pdf_url": "https://arxiv.org/pdf/2510.05156v1", "categories": ["cs.SE", "cs.AI", "cs.CR"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-03T04:11:43Z", "updated": "2025-10-03T04:11:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures", "authors": ["Yoo Yeon Sung", "Hannah Kim", "Dan Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.12651v1", "abstract": "AI practitioners increasingly use large language model (LLM) agents in compound AI systems to solve complex reasoning tasks, these agent executions often fail to meet human standards, leading to errors that compromise the system's overall performance. Addressing these failures through human intervention is challenging due to the agents' opaque reasoning processes, misalignment with human expectations, the complexity of agent dependencies, and the high cost of manual inspection. This paper thus introduces a human-centered evaluation framework for Verifying LLM Agent failures (VeriLA), which systematically assesses agent failures to reduce human effort and make these agent failures interpretable to humans. The framework first defines clear expectations of each agent by curating human-designed agent criteria. Then, it develops a human-aligned agent verifier module, trained with human gold standards, to assess each agent's execution output. This approach enables granular evaluation of each agent's performance by revealing failures from a human standard, offering clear guidelines for revision, and reducing human cognitive load. Our case study results show that VeriLA is both interpretable and efficient in helping practitioners interact more effectively with the system. By upholding accountability in human-agent collaboration, VeriLA paves the way for more trustworthy and human-aligned compound AI systems.", "source": "arxiv", "arxiv_id": "2503.12651v1", "pdf_url": "https://arxiv.org/pdf/2503.12651v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-16T21:11:18Z", "updated": "2025-03-16T21:11:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Verification-Aware Planning for Multi-Agent Systems", "authors": ["Tianyang Xu", "Dan Zhang", "Kushan Mitra", "Estevam Hruschka"], "year": 2025, "url": "http://arxiv.org/abs/2510.17109v1", "abstract": "Large language model (LLM) agents are increasingly deployed to tackle complex tasks, often necessitating collaboration among multiple specialized agents. However, multi-agent collaboration introduces new challenges in planning, coordination, and verification. Execution failures frequently arise not from flawed reasoning alone, but from subtle misalignments in task interpretation, output format, or inter-agent handoffs. To address these challenges, we present VeriMAP, a framework for multi-agent collaboration with verification-aware planning. The VeriMAP planner decomposes tasks, models subtask dependencies, and encodes planner-defined passing criteria as subtask verification functions (VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets, demonstrating that it outperforms both single- and multi-agent baselines while enhancing system robustness and interpretability. Our analysis highlights how verification-aware planning enables reliable coordination and iterative refinement in multi-agent systems, without relying on external labels or annotations.", "source": "arxiv", "arxiv_id": "2510.17109v1", "pdf_url": "https://arxiv.org/pdf/2510.17109v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-20T02:54:29Z", "updated": "2025-10-20T02:54:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "VeruSAGE: A Study of Agent-Based Verification for Rust Systems", "authors": ["Chenyuan Yang", "Natalie Neamtu", "Chris Hawblitzel", "Jacob R. Lorch", "Shan Lu"], "year": 2025, "url": "http://arxiv.org/abs/2512.18436v1", "abstract": "Large language models (LLMs) have shown impressive capability to understand and develop code. However, their capability to rigorously reason about and prove code correctness remains in question. This paper offers a comprehensive study of LLMs' capability to develop correctness proofs for system software written in Rust. We curate a new system-verification benchmark suite, VeruSAGE-Bench, which consists of 849 proof tasks extracted from eight open-source Verus-verified Rust systems. Furthermore, we design different agent systems to match the strengths and weaknesses of different LLMs (o4-mini, GPT-5, Sonnet 4, and Sonnet 4.5). Our study shows that different tools and agent settings are needed to stimulate the system-verification capability of different types of LLMs. The best LLM-agent combination in our study completes over 80% of system-verification tasks in VeruSAGE-Bench. It also completes over 90% of a set of system proof tasks not part of VeruSAGE-Bench because they had not yet been finished by human experts. This result shows the great potential for LLM-assisted development of verified system software.", "source": "arxiv", "arxiv_id": "2512.18436v1", "pdf_url": "https://arxiv.org/pdf/2512.18436v1", "categories": ["cs.OS", "cs.AI", "cs.FL", "cs.SE"], "primary_category": "cs.OS", "doi": "", "venue": "", "published": "2025-12-20T17:22:52Z", "updated": "2025-12-20T17:22:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Visual Agentic AI for Spatial Reasoning with a Dynamic API", "authors": ["Damiano Marsili", "Rohun Agrawal", "Yisong Yue", "Georgia Gkioxari"], "year": 2025, "url": "http://arxiv.org/abs/2502.06787v2", "abstract": "Visual reasoning -- the ability to interpret the visual world -- is crucial for embodied agents that operate within three-dimensional scenes. Progress in AI has led to vision and language models capable of answering questions from images. However, their performance declines when tasked with 3D spatial reasoning. To tackle the complexity of such reasoning problems, we introduce an agentic program synthesis approach where LLM agents collaboratively generate a Pythonic API with new functions to solve common subproblems. Our method overcomes limitations of prior approaches that rely on a static, human-defined API, allowing it to handle a wider range of queries. To assess AI capabilities for 3D understanding, we introduce a new benchmark of queries involving multiple steps of grounding and inference. We show that our method outperforms prior zero-shot models for visual reasoning in 3D and empirically validate the effectiveness of our agentic framework for 3D spatial reasoning tasks. Project website: https://glab-caltech.github.io/vadar/", "source": "arxiv", "arxiv_id": "2502.06787v2", "pdf_url": "https://arxiv.org/pdf/2502.06787v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-02-10T18:59:35Z", "updated": "2025-03-28T02:27:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Visual Test-time Scaling for GUI Agent Grounding", "authors": ["Tiange Luo", "Lajanugen Logeswaran", "Justin Johnson", "Honglak Lee"], "year": 2025, "url": "http://arxiv.org/abs/2505.00684v2", "abstract": "We introduce RegionFocus, a visual test-time scaling approach for Vision Language Model Agents. Understanding webpages is challenging due to the visual complexity of GUI images and the large number of interface elements, making accurate action selection difficult. Our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. To support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. Even with a simple region selection strategy, we observe significant performance gains of 28+\\% on Screenspot-pro and 24+\\% on WebVoyager benchmarks on top of two state-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL, highlighting the effectiveness of visual test-time scaling in interactive settings. We achieve a new state-of-the-art grounding performance of 61.6\\% on the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model. Our code will be released publicly at https://github.com/tiangeluo/RegionFocus.", "source": "arxiv", "arxiv_id": "2505.00684v2", "pdf_url": "https://arxiv.org/pdf/2505.00684v2", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-05-01T17:45:59Z", "updated": "2025-07-14T17:45:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications", "authors": ["Wei He", "Yueqing Sun", "Hongyan Hao", "Xueyuan Hao", "Zhikang Xia", "Qi Gu", "Chengcheng Han", "Dengchang Zhao", "Hui Su", "Kefeng Zhang", "Man Gao", "Xi Su", "Xiaodong Cai", "Xunliang Cai", "Yu Yang", "Yunke Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2509.26490v2", "abstract": "As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/", "source": "arxiv", "arxiv_id": "2509.26490v2", "pdf_url": "https://arxiv.org/pdf/2509.26490v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-30T16:33:49Z", "updated": "2025-10-17T08:04:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Vulnerability Mitigation System (VMS): LLM Agent and Evaluation Framework for Autonomous Penetration Testing", "authors": ["Farzana Abdulzada"], "year": 2025, "url": "http://arxiv.org/abs/2507.21113v1", "abstract": "As the frequency of cyber threats increases, conventional penetration testing is failing to capture the entirety of todays complex environments. To solve this problem, we propose the Vulnerability Mitigation System (VMS), a novel agent based on a Large Language Model (LLM) capable of performing penetration testing without human intervention. The VMS has a two-part architecture for planning and a Summarizer, which enable it to generate commands and process feedback. To standardize testing, we designed two new Capture the Flag (CTF) benchmarks based on the PicoCTF and OverTheWire platforms with 200 challenges. These benchmarks allow us to evaluate how effectively the system functions. We performed a number of experiments using various LLMs while tuning the temperature and top-p parameters and found that GPT-4o performed best, sometimes even better than expected. The results indicate that LLMs can be effectively applied to many cybersecurity tasks; however, there are risks. To ensure safe operation, we used a containerized environment. Both the VMS and the benchmarks are publicly available, advancing the creation of secure, autonomous cybersecurity tools.", "source": "arxiv", "arxiv_id": "2507.21113v1", "pdf_url": "https://arxiv.org/pdf/2507.21113v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-14T06:19:17Z", "updated": "2025-07-14T06:19:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2504.15785v1", "abstract": "Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free \"world alignment\" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.", "source": "arxiv", "arxiv_id": "2504.15785v1", "pdf_url": "https://arxiv.org/pdf/2504.15785v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-22T10:58:27Z", "updated": "2025-04-22T10:58:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "authors": ["Su Kara", "Fazle Faisal", "Suman Nath"], "year": 2025, "url": "http://arxiv.org/abs/2510.03285v1", "abstract": "Recent advances in browser-based LLM agents have shown promise for automating tasks ranging from simple form filling to hotel booking or online shopping. Current benchmarks measure agent performance in controlled environments, such as containers or stable networks, where websites behave deterministically. However, in the real world, users access websites over networks and HTTPS connections that introduce instability from multiple sources: client-side, server-side issues or broader system failures. Moreover, live websites are prone to web attacks such Cross-Site Scripting, as well as general site modifications which can cause unexpected or malicious pop-ups or improper functionality. To address this gap, we present WAREX: Web Agent Reliability Evaluation on Existing Benchmarks. We measure the impact of WAREX across three popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that introducing WAREX leads to significant drops in task success rates, highlighting the limited robustness of state-of-the-art agents.", "source": "arxiv", "arxiv_id": "2510.03285v1", "pdf_url": "https://arxiv.org/pdf/2510.03285v1", "categories": ["cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-28T20:51:05Z", "updated": "2025-09-28T20:51:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WINELL: Wikipedia Never-Ending Updating with LLM Agents", "authors": ["Revanth Gangi Reddy", "Tanay Dixit", "Jiaxin Qin", "Cheng Qian", "Daniel Lee", "Jiawei Han", "Kevin Small", "Xing Fan", "Ruhi Sarikaya", "Heng Ji"], "year": 2025, "url": "http://arxiv.org/abs/2508.03728v1", "abstract": "Wikipedia, a vast and continuously consulted knowledge base, faces significant challenges in maintaining up-to-date content due to its reliance on manual human editors. Inspired by the vision of continuous knowledge acquisition in NELL and fueled by advances in LLM-based agents, this paper introduces WiNELL, an agentic framework for continuously updating Wikipedia articles. Our approach employs a multi-agent framework to aggregate online information, select new and important knowledge for a target entity in Wikipedia, and then generate precise edit suggestions for human review. Our fine-grained editing models, trained on Wikipedia's extensive history of human edits, enable incorporating updates in a manner consistent with human editing behavior. Our editor models outperform both open-source instruction-following baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and editing efficiency. End-to-end evaluation on high-activity Wikipedia pages demonstrates WiNELL's ability to identify and suggest timely factual updates. This opens up a promising research direction in LLM agents for automatically updating knowledge bases in a never-ending fashion.", "source": "arxiv", "arxiv_id": "2508.03728v1", "pdf_url": "https://arxiv.org/pdf/2508.03728v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-30T07:51:42Z", "updated": "2025-07-30T07:51:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Warehouse Spatial Question Answering with LLM Agent", "authors": ["Hsiang-Wei Huang", "Jen-Hao Cheng", "Kuang-Ming Chen", "Cheng-Yen Yang", "Bahaa Alattar", "Yi-Ru Lin", "Pyongkun Kim", "Sangwon Kim", "Kwangju Kim", "Chung-I Huang", "Jenq-Neng Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2507.10778v2", "abstract": "Spatial understanding has been a challenging task for existing Multi-modal Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM finetuning to enhance MLLM's spatial understanding ability. In this paper, we present a data-efficient approach. We propose a LLM agent system with strong and advanced spatial reasoning ability, which can be used to solve the challenging spatial question answering task in complex indoor warehouse scenarios. Our system integrates multiple tools that allow the LLM agent to conduct spatial reasoning and API tools interaction to answer the given complicated spatial question. Extensive evaluations on the 2025 AI City Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that our system achieves high accuracy and efficiency in tasks such as object retrieval, counting, and distance estimation. The code is available at: https://github.com/hsiangwei0903/SpatialAgent", "source": "arxiv", "arxiv_id": "2507.10778v2", "pdf_url": "https://arxiv.org/pdf/2507.10778v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-07-14T20:05:55Z", "updated": "2025-08-14T03:48:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems", "authors": ["Junfeng Fang", "Zijun Yao", "Ruipeng Wang", "Haokai Ma", "Xiang Wang", "Tat-Seng Chua"], "year": 2025, "url": "http://arxiv.org/abs/2506.13666v1", "abstract": "The development of large language models (LLMs) has entered in a experience-driven era, flagged by the emergence of environment feedback-driven learning via reinforcement learning and tool-using agents. This encourages the emergenece of model context protocol (MCP), which defines the standard on how should a LLM interact with external services, such as \\api and data. However, as MCP becomes the de facto standard for LLM agent systems, it also introduces new safety risks. In particular, MCP introduces third-party services, which are not controlled by the LLM developers, into the agent systems. These third-party MCP services provider are potentially malicious and have the economic incentives to exploit vulnerabilities and sabotage user-agent interactions. In this position paper, we advocate the research community in LLM safety to pay close attention to the new safety risks issues introduced by MCP, and develop new techniques to build safe MCP-powered agent systems. To establish our position, we argue with three key parts. (1) We first construct \\framework, a controlled framework to examine safety issues in MCP-powered agent systems. (2) We then conduct a series of pilot experiments to demonstrate the safety risks in MCP-powered agent systems is a real threat and its defense is not trivial. (3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered agent systems. In particular, we would call for researchers to persue the following research directions: red teaming, MCP safe LLM development, MCP safety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP safe ecosystem construction. We hope this position paper can raise the awareness of the research community in MCP safety and encourage more researchers to join this important research direction. Our code is available at https://github.com/littlelittlenine/SafeMCP.git.", "source": "arxiv", "arxiv_id": "2506.13666v1", "pdf_url": "https://arxiv.org/pdf/2506.13666v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-16T16:24:31Z", "updated": "2025-06-16T16:24:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WebATLAS: An LLM Agent with Experience-Driven Memory and Action Simulation", "authors": ["Jiali Cheng", "Anjishnu Kumar", "Roshan Lal", "Rishi Rajasekaran", "Hani Ramezani", "Omar Zia Khan", "Oleg Rokhlenko", "Sunny Chiu-Webster", "Gang Hua", "Hadi Amiri"], "year": 2025, "url": "http://arxiv.org/abs/2510.22732v2", "abstract": "Large Language Model (LLM) web agents often struggle with long-horizon web navigation and web task completion in new websites, producing inefficient action sequences unless fine-tuned on environment-specific data. We show that experience-driven memory, combined with look-ahead action simulation, is sufficient for LLM agents to adapt to unseen web environments by remembering past failures and predicting the consequences of future actions. We introduce WebATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented LLM web agent that learns a lightweight internal model of the environment from interaction experience and performs hypothetical action rollouts before acting in the real world. WebATLAS builds a persistent cognitive map via curiosity-driven exploration, stores interaction outcomes as experience-based memory, and evaluates candidate actions in cognitive space using a planner--simulator--critic loop. This enables the agent to reuse past experience, avoid previously unsuccessful behaviors, and generate more efficient plans. We evaluate WebATLAS on the WebArena-Lite benchmark for autonomous web navigation and demonstrate a success rate of 63%, outperforming the previous state-of-the-art at 53.9%. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablation studies confirm that experience-driven memory, look-ahead action simulation, and hierarchical replanning play complementary roles in enabling robust, training-free web agents.", "source": "arxiv", "arxiv_id": "2510.22732v2", "pdf_url": "https://arxiv.org/pdf/2510.22732v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.MA", "cs.RO"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-26T16:03:39Z", "updated": "2025-12-19T23:16:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks", "authors": ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Shiyu Chang", "Yujia Bao"], "year": 2025, "url": "http://arxiv.org/abs/2510.06587v1", "abstract": "Large language model (LLM) agents are becoming competent at straightforward web tasks, such as opening an item page or submitting a form, but still struggle with objectives that require long horizon navigation, large scale information extraction, and reasoning under constraints. We present WebDART, a general framework that enables a single LLM to handle such complex chores. WebDART (i) dynamically decomposes each objective into three focused subtasks: navigation, information extraction, and execution, so the model concentrates on one skill at a time, and (ii) continuously replans the decomposition as new webpages are revealed, taking advantage of newly discovered filters or shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena, WebDART lifts success rates by up to 13.7 percentage points over previous SOTA agents, while matching their performance on the easier WebArena suite and completing tasks with up to 14.7 fewer navigation steps.", "source": "arxiv", "arxiv_id": "2510.06587v1", "pdf_url": "https://arxiv.org/pdf/2510.06587v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-08T02:34:59Z", "updated": "2025-10-08T02:34:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents", "authors": ["Arth Bohra", "Manvel Saroyan", "Danil Melkozerov", "Vahe Karufanyan", "Gabriel Maher", "Pascal Weinberger", "Artem Harutyunyan", "Giovanni Campagna"], "year": 2025, "url": "http://arxiv.org/abs/2504.12682v1", "abstract": "Most recent web agent research has focused on navigation and transaction tasks, with little emphasis on extracting structured data at scale. We present WebLists, a benchmark of 200 data-extraction tasks across four common business and enterprise use-cases. Each task requires an agent to navigate to a webpage, configure it appropriately, and extract complete datasets with well-defined schemas. We show that both LLMs with search capabilities and SOTA web agents struggle with these tasks, with a recall of 3% and 31%, respectively, despite higher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that enables web agents to convert their execution into repeatable programs, and replay them at scale across pages with similar structure. BardeenAgent is also the first LLM agent to take advantage of the regular structure of HTML. In particular BardeenAgent constructs a generalizable CSS selector to capture all relevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more than doubling the performance of SOTA web agents, and reducing cost per output row by 3x.", "source": "arxiv", "arxiv_id": "2504.12682v1", "pdf_url": "https://arxiv.org/pdf/2504.12682v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-17T06:16:40Z", "updated": "2025-04-17T06:16:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns", "authors": ["Stefan Szeider"], "year": 2025, "url": "http://arxiv.org/abs/2509.21224v1", "abstract": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.", "source": "arxiv", "arxiv_id": "2509.21224v1", "pdf_url": "https://arxiv.org/pdf/2509.21224v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-25T14:29:49Z", "updated": "2025-09-25T14:29:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "What Makes AI Research Replicable? Executable Knowledge Graphs as Scientific Knowledge Representations", "authors": ["Yujie Luo", "Zhuoyun Yu", "Xuehai Wang", "Yuqi Zhu", "Ningyu Zhang", "Lanning Wei", "Lun Du", "Da Zheng", "Huajun Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.17795v2", "abstract": "Replicating AI research is a crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code, primarily due to insufficient background knowledge and the limitations of retrieval-augmented generation (RAG) methods, which fail to capture latent technical details hidden in referenced papers. Furthermore, previous approaches tend to overlook valuable implementation-level code signals and lack structured knowledge representations that support multi-granular retrieval and reuse. To overcome these challenges, we propose Executable Knowledge Graphs (xKG), a pluggable, paper-centric knowledge base that automatically integrates code snippets and technical insights extracted from scientific literature. When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench, demonstrating its effectiveness as a general and extensible solution for automated AI research replication. Code is available at https://github.com/zjunlp/xKG.", "source": "arxiv", "arxiv_id": "2510.17795v2", "pdf_url": "https://arxiv.org/pdf/2510.17795v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "cs.SE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-20T17:53:23Z", "updated": "2026-01-21T07:42:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness", "authors": ["Yuxuan Li", "Sauvik Das", "Hirokazu Shirado"], "year": 2025, "url": "http://arxiv.org/abs/2509.21868v1", "abstract": "There is growing interest in using Large Language Models as agents (LLM agents) for social simulations to inform policy, yet real-world adoption remains limited. This paper addresses the question: How can LLM agent simulations be made genuinely useful for policy? We report on a year-long iterative design engagement with a university emergency preparedness team. Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. These simulations informed actual policy implementation, shaping volunteer training, evacuation protocols, and infrastructure planning. Analyzing this process, we identify three design implications: start with verifiable scenarios and build trust gradually, use preliminary simulations to elicit tacit knowledge, and treat simulation and policy development as evolving together. These implications highlight actionable pathways to making LLM agent simulations that are genuinely useful for policy.", "source": "arxiv", "arxiv_id": "2509.21868v1", "pdf_url": "https://arxiv.org/pdf/2509.21868v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-26T04:51:34Z", "updated": "2025-09-26T04:51:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms", "authors": ["Qibing Ren", "Zhijie Zheng", "Jiaxuan Guo", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "year": 2025, "url": "http://arxiv.org/abs/2511.06448v1", "abstract": "In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.", "source": "arxiv", "arxiv_id": "2511.06448v1", "pdf_url": "https://arxiv.org/pdf/2511.06448v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-11-09T16:30:44Z", "updated": "2025-11-09T16:30:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents", "authors": ["Lingfei Qian", "Xueqing Peng", "Yan Wang", "Vincent Jim Zhang", "Huan He", "Hanley Smith", "Yi Han", "Yueru He", "Haohang Li", "Yupeng Cao", "Yangyang Yu", "Alejandro Lopez-Lira", "Peng Lu", "Jian-Yun Nie", "Guojun Xiong", "Jimin Huang", "Sophia Ananiadou"], "year": 2025, "url": "http://arxiv.org/abs/2510.11695v2", "abstract": "Although Large Language Model (LLM)-based agents are increasingly used in financial trading, it remains unclear whether they can reason and adapt in live markets, as most studies test models instead of agents, cover limited periods and assets, and rely on unverified data. To address these gaps, we introduce Agent Market Arena (AMA), the first lifelong, real-time benchmark for evaluating LLM-based trading agents across multiple markets. AMA integrates verified trading data, expert-checked news, and diverse agent architectures within a unified trading framework, enabling fair and continuous comparison under real conditions. It implements four agents, including InvestorAgent as a single-agent baseline, TradeAgent and HedgeFundAgent with different risk styles, and DeepFundAgent with memory-based reasoning, and evaluates them across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets demonstrate that agent frameworks display markedly distinct behavioral patterns, spanning from aggressive risk-taking to conservative decision-making, whereas model backbones contribute less to outcome variation. AMA thus establishes a foundation for rigorous, reproducible, and continuously evolving evaluation of financial reasoning and trading intelligence in LLM-based agents.", "source": "arxiv", "arxiv_id": "2510.11695v2", "pdf_url": "https://arxiv.org/pdf/2510.11695v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-13T17:54:09Z", "updated": "2025-10-30T02:09:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When Agents go Astray: Course-Correcting SWE Agents with PRMs", "authors": ["Shubham Gandhi", "Jason Tsay", "Jatin Ganhotra", "Kiran Kate", "Yara Rizk"], "year": 2025, "url": "http://arxiv.org/abs/2509.02360v2", "abstract": "Large Language Model (LLM) agents are increasingly deployed for complex, multi-step software engineering (SWE) tasks. However, their trajectories often contain costly inefficiencies, such as redundant exploration, looping, and failure to terminate once a solution is reached. Prior work has largely treated these errors in a post-hoc manner, diagnosing failures only after execution. In this paper, we introduce SWE-PRM, an inference-time Process Reward Model (PRM) that intervenes during execution to detect and course-correct trajectory-level errors. Our PRM design leverages a taxonomy of common inefficiencies and delivers lightweight, interpretable feedback without modifying the underlying policy. On SWE-bench Verified, closed-source PRMs improve resolution from 40.0% to 50.6% (+10.6 p.p.), with the largest gains on medium and hard tasks. Among feedback strategies, taxonomy-guided PRMs outperform unguided or explicit action-prescriptive variants, increasing success rate while reducing trajectory length. These benefits come at an acceptable added inference cost of as low as $0.2, making PRMs a practical and scalable mechanism for improving SWE agents' reliability and efficiency.", "source": "arxiv", "arxiv_id": "2509.02360v2", "pdf_url": "https://arxiv.org/pdf/2509.02360v2", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-02T14:23:15Z", "updated": "2025-10-21T08:41:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas", "authors": ["Steffen Backmann", "David Guzman Piedrahita", "Emanuel Tewolde", "Rada Mihalcea", "Bernhard SchÃ¶lkopf", "Zhijing Jin"], "year": 2025, "url": "http://arxiv.org/abs/2505.19212v1", "abstract": "Recent advances in large language models (LLMs) have enabled their use in complex agentic roles, involving decision-making with humans or other agents, making ethical alignment a key AI safety concern. While prior work has examined both LLMs' moral judgment and strategic behavior in social dilemmas, there is limited understanding of how they act when moral imperatives directly conflict with rewards or incentives. To investigate this, we introduce Moral Behavior in Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the prisoner's dilemma and public goods game with morally charged contexts. In MoralSim, we test a range of frontier models across both game structures and three distinct moral framings, enabling a systematic examination of how LLMs navigate social dilemmas in which ethical norms conflict with payoff-maximizing strategies. Our results show substantial variation across models in both their general tendency to act morally and the consistency of their behavior across game types, the specific moral framing, and situational factors such as opponent behavior and survival risks. Crucially, no model exhibits consistently moral behavior in MoralSim, highlighting the need for caution when deploying LLMs in agentic roles where the agent's \"self-interest\" may conflict with ethical expectations. Our code is available at https://github.com/sbackmann/moralsim.", "source": "arxiv", "arxiv_id": "2505.19212v1", "pdf_url": "https://arxiv.org/pdf/2505.19212v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-25T16:19:24Z", "updated": "2025-05-25T16:19:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach", "authors": ["Zhihan Zhang", "Xunkai Li", "Yilong Zuo", "Zhaoxin Fan", "Zhenjun Li", "Bing Zhou", "Rong-Hua Li", "Guoren Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.08952v2", "abstract": "Text-attributed graphs (TAGs) have become a key form of graph-structured data in modern data management and analytics, combining structural relationships with rich textual semantics for diverse applications. However, the effectiveness of analytical models, particularly graph neural networks (GNNs), is highly sensitive to data quality. Our empirical analysis shows that both conventional and LLM-enhanced GNNs degrade notably under textual, structural, and label imperfections, underscoring TAG quality as a key bottleneck for reliable analytics. Existing studies have explored data-level optimization for TAGs, but most focus on specific degradation types and target a single aspect like structure or label, lacking a systematic and comprehensive perspective on data quality improvement. To address this gap, we propose LAGA (Large Language and Graph Agent), a unified multi-agent framework for comprehensive TAG quality optimization. LAGA formulates graph quality control as a data-centric process, integrating detection, planning, action, and evaluation agents into an automated loop. It holistically enhances textual, structural, and label aspects through coordinated multi-modal optimization. Extensive experiments on 5 datasets and 16 baselines across 9 scenarios demonstrate the effectiveness, robustness and scalability of LAGA, confirming the importance of data-centric quality optimization for reliable TAG analytics.", "source": "arxiv", "arxiv_id": "2510.08952v2", "pdf_url": "https://arxiv.org/pdf/2510.08952v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-10T02:59:19Z", "updated": "2025-10-20T04:17:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When Machines Meet Each Other: Network Effects and the Strategic Role of History in Multi-Agent AI", "authors": ["Yu Liu", "Wenwen Li", "Yifan Dou", "Guangnan Ye"], "year": 2025, "url": "http://arxiv.org/abs/2510.06903v1", "abstract": "As artificial intelligence (AI) enters the agentic era, large language models (LLMs) are increasingly deployed as autonomous agents that interact with one another rather than operate in isolation. This shift raises a fundamental question: how do machine agents behave in interdependent environments where outcomes depend not only on their own choices but also on the coordinated expectations of peers? To address this question, we study LLM agents in a canonical network-effect game, where economic theory predicts convergence to a fulfilled expectation equilibrium (FEE). We design an experimental framework in which 50 heterogeneous GPT-5-based agents repeatedly interact under systematically varied network-effect strengths, price trajectories, and decision-history lengths. The results reveal that LLM agents systematically diverge from FEE: they underestimate participation at low prices, overestimate at high prices, and sustain persistent dispersion. Crucially, the way history is structured emerges as a design lever. Simple monotonic histories-where past outcomes follow a steady upward or downward trend-help stabilize coordination, whereas nonmonotonic histories amplify divergence and path dependence. Regression analyses at the individual level further show that price is the dominant driver of deviation, history moderates this effect, and network effects amplify contextual distortions. Together, these findings advance machine behavior research by providing the first systematic evidence on multi-agent AI systems under network effects and offer guidance for configuring such systems in practice.", "source": "arxiv", "arxiv_id": "2510.06903v1", "pdf_url": "https://arxiv.org/pdf/2510.06903v1", "categories": ["econ.GN"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2025-10-08T11:39:16Z", "updated": "2025-10-08T11:39:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents", "authors": ["Tsimur Hadeliya", "Mohammad Ali Jauhar", "Nidhi Sakpal", "Diogo Cruz"], "year": 2025, "url": "http://arxiv.org/abs/2512.02445v1", "abstract": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.", "source": "arxiv", "arxiv_id": "2512.02445v1", "pdf_url": "https://arxiv.org/pdf/2512.02445v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-02T06:12:02Z", "updated": "2025-12-02T06:12:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When Trust Collides: Decoding Human-LLM Cooperation Dynamics through the Prisoner's Dilemma", "authors": ["Guanxuan Jiang", "Shirao Yang", "Yuyang Wang", "Pan Hui"], "year": 2025, "url": "http://arxiv.org/abs/2503.07320v2", "abstract": "As large language models (LLMs) become increasingly capable of autonomous decision-making, they introduce new challenges and opportunities for human-AI cooperation in mixed-motive contexts. While prior research has primarily examined AI in assistive or cooperative roles, little is known about how humans interact with AI agents perceived as independent and strategic actors. This study investigates human cooperative attitudes and behaviors toward LLM agents by engaging 30 participants (15 males, 15 females) in repeated Prisoner's Dilemma games with agents differing in declared identity: purported human, rule-based AI, and LLM agent. Behavioral metrics, including cooperation rate, decision latency, unsolicited cooperative acts and trust restoration tolerance, were analyzed to assess the influence of agent identity and participant gender. Results revealed significant effects of declared agent identity on most cooperation-related behaviors, along with notable gender differences in decision latency. Furthermore, qualitative responses suggest that these behavioral differences were shaped by participants interpretations and expectations of the agents. These findings contribute to our understanding of human adaptation in competitive cooperation with autonomous agents and underscore the importance of agent framing in shaping effective and ethical human-AI interaction.", "source": "arxiv", "arxiv_id": "2503.07320v2", "pdf_url": "https://arxiv.org/pdf/2503.07320v2", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-03-10T13:37:36Z", "updated": "2025-05-28T07:51:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Where LLM Agents Fail and How They can Learn From Failures", "authors": ["Kunlun Zhu", "Zijia Liu", "Bingxuan Li", "Muxin Tian", "Yingxuan Yang", "Jiaxun Zhang", "Pengrui Han", "Qipeng Xie", "Fuyang Cui", "Weijia Zhang", "Xiaoteng Ma", "Xiaodong Yu", "Gowtham Ramesh", "Jialian Wu", "Zicheng Liu", "Pan Lu", "James Zou", "Jiaxuan You"], "year": 2025, "url": "http://arxiv.org/abs/2509.25370v1", "abstract": "Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug", "source": "arxiv", "arxiv_id": "2509.25370v1", "pdf_url": "https://arxiv.org/pdf/2509.25370v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T18:20:27Z", "updated": "2025-09-29T18:20:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents", "authors": ["Zhuo-Yang Song"], "year": 2025, "url": "http://arxiv.org/abs/2510.14846v3", "abstract": "The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via two instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.", "source": "arxiv", "arxiv_id": "2510.14846v3", "pdf_url": "https://arxiv.org/pdf/2510.14846v3", "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-16T16:18:37Z", "updated": "2025-11-03T10:52:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents", "authors": ["Chih-Hsuan Yang", "Tanwi Mallick", "Le Chen", "Krishnan Raghavan", "Azton Wells", "Amal Gueroudji", "Ian T. Foster", "Rajeev Thakur"], "year": 2025, "url": "http://arxiv.org/abs/2511.10687v2", "abstract": "Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.", "source": "arxiv", "arxiv_id": "2511.10687v2", "pdf_url": "https://arxiv.org/pdf/2511.10687v2", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-11-11T22:21:08Z", "updated": "2025-11-17T19:45:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Why Do Language Model Agents Whistleblow?", "authors": ["Kushal Agrawal", "Frank Xiao", "Guido Bergman", "Asa Cooper Stickland"], "year": 2025, "url": "http://arxiv.org/abs/2511.17085v2", "abstract": "The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior. Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates. Additionally, we verify the robustness of our dataset by testing for model evaluation awareness, and find that both black-box methods and probes on model activations show lower evaluation awareness in our settings than in comparable previous work.", "source": "arxiv", "arxiv_id": "2511.17085v2", "pdf_url": "https://arxiv.org/pdf/2511.17085v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-21T09:40:52Z", "updated": "2025-12-27T21:05:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma", "authors": ["Richard Willis", "Yali Du", "Joel Z Leibo", "Michael Luck"], "year": 2025, "url": "http://arxiv.org/abs/2501.16173v1", "abstract": "As autonomous agents become more prevalent, understanding their collective behaviour in strategic interactions is crucial. This study investigates the emergent cooperative tendencies of systems of Large Language Model (LLM) agents in a social dilemma. Unlike previous research where LLMs output individual actions, we prompt state-of-the-art LLMs to generate complete strategies for iterated Prisoner's Dilemma. Using evolutionary game theory, we simulate populations of agents with different strategic dispositions (aggressive, cooperative, or neutral) and observe their evolutionary dynamics. Our findings reveal that different LLMs exhibit distinct biases affecting the relative success of aggressive versus cooperative strategies. This research provides insights into the potential long-term behaviour of systems of deployed LLM-based autonomous agents and highlights the importance of carefully considering the strategic environments in which they operate.", "source": "arxiv", "arxiv_id": "2501.16173v1", "pdf_url": "https://arxiv.org/pdf/2501.16173v1", "categories": ["cs.MA", "cs.GT"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-01-27T16:14:33Z", "updated": "2025-01-27T16:14:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks", "authors": ["Jingwen Tong", "Wei Guo", "Jiawei Shao", "Qiong Wu", "Zijian Li", "Zehong Lin", "Jun Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.01074v1", "abstract": "The rapid evolution of wireless networks presents unprecedented challenges in managing complex and dynamic systems. Existing methods are increasingly facing fundamental limitations in addressing these challenges. In this paper, we introduce WirelessAgent, a novel framework that harnesses large language models (LLMs) to create autonomous AI agents for diverse wireless network tasks. This framework integrates four core modules that mirror human cognitive processes: perception, memory, planning, and action. To implement it, we provide a basic usage based on agentic workflows and the LangGraph architecture. We demonstrate the effectiveness of WirelessAgent through a comprehensive case study on network slicing. The numerical results show that WirelessAgent achieves $44.4\\%$ higher bandwidth utilization than the \\emph{Prompt-based} method, while performing only $4.3\\%$ below the \\emph{Rule-based optimality}. Notably, WirelessAgent delivers near-optimal network throughput across diverse network scenarios. These underscore the framework's potential for intelligent and autonomous resource management in future wireless networks. The code is available at \\url{https://github.com/jwentong/WirelessAgent_R1}.", "source": "arxiv", "arxiv_id": "2505.01074v1", "pdf_url": "https://arxiv.org/pdf/2505.01074v1", "categories": ["eess.SP"], "primary_category": "eess.SP", "doi": "", "venue": "", "published": "2025-05-02T07:29:19Z", "updated": "2025-05-02T07:29:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WorkTeam: Constructing Workflows from Natural Language with Multi-Agents", "authors": ["Hanchao Liu", "Rongjun Li", "Weimin Xiong", "Ziyu Zhou", "Wei Peng"], "year": 2025, "url": "http://arxiv.org/abs/2503.22473v1", "abstract": "Workflows play a crucial role in enhancing enterprise efficiency by orchestrating complex processes with multiple tools or components. However, hand-crafted workflow construction requires expert knowledge, presenting significant technical barriers. Recent advancements in Large Language Models (LLMs) have improved the generation of workflows from natural language instructions (aka NL2Workflow), yet existing single LLM agent-based methods face performance degradation on complex tasks due to the need for specialized knowledge and the strain of task-switching. To tackle these challenges, we propose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor, orchestrator, and filler agent, each with distinct roles that collaboratively enhance the conversion process. As there are currently no publicly available NL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which includes 3,695 real-world business samples for training and evaluation. Experimental results show that our approach significantly increases the success rate of workflow construction, providing a novel and effective solution for enterprise NL2Workflow services.", "source": "arxiv", "arxiv_id": "2503.22473v1", "pdf_url": "https://arxiv.org/pdf/2503.22473v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-28T14:33:29Z", "updated": "2025-03-28T14:33:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "World Modelling Improves Language Model Agents", "authors": ["Shangmin Guo", "Omar Darwiche Domingues", "RaphaÃ«l Avalos", "Aaron Courville", "Florian Strub"], "year": 2025, "url": "http://arxiv.org/abs/2506.02918v2", "abstract": "Tool use in stateful environments presents unique challenges for large language models (LLMs), where existing test-time compute strategies relying on repeated trials in the environment are impractical. We propose dynamics modelling (DyMo), a method that augments LLMs with a state prediction capability alongside function calling during post-training. This enables LLMs to predict the future states of their actions through an internal environment model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success rates and significantly reduces hallucinations. We further integrate the internal environment model into self-verification sampling (SVS), and show that this substantially improves pass^k over number of trials k, and allows the model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the effectiveness and reliability of LLMs for tool use. We believe this work charts a path towards scalable planning RL methods for LLM inference without repeatedly querying the oracle environment.", "source": "arxiv", "arxiv_id": "2506.02918v2", "pdf_url": "https://arxiv.org/pdf/2506.02918v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-03T14:20:59Z", "updated": "2025-09-19T03:54:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "World model inspired sarcasm reasoning with large language model agents", "authors": ["Keito Inoshita", "Shinnosuke Mizuno"], "year": 2025, "url": "http://arxiv.org/abs/2512.24329v1", "abstract": "Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker's intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.", "source": "arxiv", "arxiv_id": "2512.24329v1", "pdf_url": "https://arxiv.org/pdf/2512.24329v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-30T16:31:08Z", "updated": "2025-12-30T16:31:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM Agents", "authors": ["Xinhang Liu", "Chi-Keung Tang", "Yu-Wing Tai"], "year": 2025, "url": "http://arxiv.org/abs/2502.15601v2", "abstract": "Constructing photorealistic virtual worlds has applications across various fields, but it often requires the extensive labor of highly trained professionals to operate conventional 3D modeling software. To democratize this process, we introduce WorldCraft, a system where large language model (LLM) agents leverage procedural generation to create indoor and outdoor scenes populated with objects, allowing users to control individual object attributes and the scene layout using intuitive natural language commands. In our framework, a coordinator agent manages the overall process and works with two specialized LLM agents to complete the scene creation: ForgeIt, which integrates an ever-growing manual through auto-verification to enable precise customization of individual objects, and ArrangeIt, which formulates hierarchical optimization problems to achieve a layout that balances ergonomic and aesthetic considerations. Additionally, our pipeline incorporates a trajectory control agent, allowing users to animate the scene and operate the camera through natural language interactions. Our system is also compatible with off-the-shelf deep 3D generators to enrich scene assets. Through evaluations and comparisons with state-of-the-art methods, we demonstrate the versatility of WorldCraft, ranging from single-object customization to intricate, large-scale interior and exterior scene designs. This system empowers non-professionals to bring their creative visions to life.", "source": "arxiv", "arxiv_id": "2502.15601v2", "pdf_url": "https://arxiv.org/pdf/2502.15601v2", "categories": ["cs.CV", "cs.AI", "cs.GR"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-02-21T17:18:30Z", "updated": "2025-02-28T08:49:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents", "authors": ["Shuai Shao", "Qihan Ren", "Chen Qian", "Boyi Wei", "Dadi Guo", "Jingyi Yang", "Xinhao Song", "Linfeng Zhang", "Weinan Zhang", "Dongrui Liu", "Jing Shao"], "year": 2025, "url": "http://arxiv.org/abs/2509.26354v1", "abstract": "Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.", "source": "arxiv", "arxiv_id": "2509.26354v1", "pdf_url": "https://arxiv.org/pdf/2509.26354v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T14:55:55Z", "updated": "2025-09-30T14:55:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Your LLM Agents are Temporally Blind: The Misalignment Between Tool Use Decisions and Human Time Perception", "authors": ["Yize Cheng", "Arshia Soltani Moakhar", "Chenrui Fan", "Parsa Hosseini", "Kazem Faghih", "Zahra Sodagar", "Wenxiao Wang", "Soheil Feizi"], "year": 2025, "url": "http://arxiv.org/abs/2510.23853v2", "abstract": "Large language model (LLM) agents are increasingly used to interact with and execute tasks in dynamic environments. However, a critical yet overlooked limitation of these agents is that they, by default, assume a stationary context, failing to account for the real-world time elapsed between messages. We refer to this as \"temporal blindness\". This limitation hinders decisions about when to invoke tools, leading agents to either over-rely on stale context and skip needed tool calls, or under-rely on it and redundantly repeat tool calls. To study this challenge, we constructed TicToc, a diverse dataset of multi-turn user-agent message trajectories across 76 scenarios, spanning dynamic environments with high, medium, and low time sensitivity. We collected human preferences between \"calling a tool\" and \"directly answering\" on each sample, and evaluated how well LLM tool-calling decisions align with human preferences under varying amounts of elapsed time. Our analysis reveals that existing models display poor alignment with human temporal perception, with no model achieving a normalized alignment rate better than 65% when given time stamp information. We also show that naive, prompt-based alignment techniques have limited effectiveness for most models, but specific post-training alignment can be a viable way to align multi-turn LLM tool use with human temporal perception. Our data and findings provide a first step toward understanding and mitigating temporal blindness, offering insights to foster the development of more time-aware and human-aligned agents.", "source": "arxiv", "arxiv_id": "2510.23853v2", "pdf_url": "https://arxiv.org/pdf/2510.23853v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-27T20:51:58Z", "updated": "2026-01-10T04:01:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "authors": ["Yuchen Shi", "Yuzheng Cai", "Siqi Cai", "Zihan Xu", "Lichao Chen", "Yulei Qin", "Zhijian Zhou", "Xiang Fei", "Chaofan Qiu", "Xiaoyu Tan", "Gang Li", "Zongyi Li", "Haojia Lin", "Guocan Cai", "Yong Mao", "Yunsheng Wu", "Ke Li", "Xing Sun"], "year": 2025, "url": "http://arxiv.org/abs/2512.24615v1", "abstract": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \\textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \\textbf{Workflow} mode for standard tasks and a \\textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \\textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \\textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "source": "arxiv", "arxiv_id": "2512.24615v1", "pdf_url": "https://arxiv.org/pdf/2512.24615v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T04:17:36Z", "updated": "2025-12-31T04:17:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents", "authors": ["Zechen Li", "Baiyu Chen", "Hao Xue", "Flora D. Salim"], "year": 2025, "url": "http://arxiv.org/abs/2508.04038v1", "abstract": "Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, a multi-sensor retrieval module that surfaces relevant evidence, and a hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53x in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as a promising step toward trustworthy, plug-and-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA.", "source": "arxiv", "arxiv_id": "2508.04038v1", "pdf_url": "https://arxiv.org/pdf/2508.04038v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-06T02:57:57Z", "updated": "2025-08-06T02:57:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Zero-Shot Large Language Model Agents for Fully Automated Radiotherapy Treatment Planning", "authors": ["Dongrong Yang", "Xin Wu", "Yibo Xie", "Xinyi Li", "Qiuwen Wu", "Jackie Wu", "Yang Sheng"], "year": 2025, "url": "http://arxiv.org/abs/2510.11754v1", "abstract": "Radiation therapy treatment planning is an iterative, expertise-dependent process, and the growing burden of cancer cases has made reliance on manual planning increasingly unsustainable, underscoring the need for automation. In this study, we propose a workflow that leverages a large language model (LLM)-based agent to navigate inverse treatment planning for intensity-modulated radiation therapy (IMRT). The LLM agent was implemented to directly interact with a clinical treatment planning system (TPS) to iteratively extract intermediate plan states and propose new constraint values to guide inverse optimization. The agent's decision-making process is informed by current observations and previous optimization attempts and evaluations, allowing for dynamic strategy refinement. The planning process was performed in a zero-shot inference setting, where the LLM operated without prior exposure to manually generated treatment plans and was utilized without any fine-tuning or task-specific training. The LLM-generated plans were evaluated on twenty head-and-neck cancer cases against clinical manual plans, with key dosimetric endpoints analyzed and reported. The LLM-generated plans achieved comparable organ-at-risk (OAR) sparing relative to clinical plans while demonstrating improved hot spot control (Dmax: 106.5% vs. 108.8%) and superior conformity (conformity index: 1.18 vs. 1.39 for boost PTV; 1.82 vs. 1.88 for primary PTV). This study demonstrates the feasibility of a zero-shot, LLM-driven workflow for automated IMRT treatment planning in a commercial TPS. The proposed approach provides a generalizable and clinically applicable solution that could reduce planning variability and support broader adoption of AI-based planning strategies.", "source": "arxiv", "arxiv_id": "2510.11754v1", "pdf_url": "https://arxiv.org/pdf/2510.11754v1", "categories": ["physics.med-ph", "cs.AI", "cs.RO"], "primary_category": "physics.med-ph", "doi": "", "venue": "", "published": "2025-10-12T19:21:21Z", "updated": "2025-10-12T19:21:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation", "authors": ["Lim Chien Her", "Ming Yan", "Yunshu Bai", "Ruihao Li", "Hao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2512.10501v2", "abstract": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.", "source": "arxiv", "arxiv_id": "2512.10501v2", "pdf_url": "https://arxiv.org/pdf/2512.10501v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-11T10:22:02Z", "updated": "2025-12-12T08:48:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "debug-gym: A Text-Based Environment for Interactive Debugging", "authors": ["Xingdi Yuan", "Morgane M Moss", "Charbel El Feghali", "Chinmay Singh", "Darya Moldavskaya", "Drew MacPhee", "Lucas Caccia", "Matheus Pereira", "Minseon Kim", "Alessandro Sordoni", "Marc-Alexandre CÃ´tÃ©"], "year": 2025, "url": "http://arxiv.org/abs/2503.21557v1", "abstract": "Large Language Models (LLMs) are increasingly relied upon for coding tasks, yet in most scenarios it is assumed that all relevant information can be either accessed in context or matches their training data. We posit that LLMs can benefit from the ability to interactively explore a codebase to gather the information relevant to their task. To achieve this, we present a textual environment, namely debug-gym, for developing LLM-based agents in an interactive coding setting. Our environment is lightweight and provides a preset of useful tools, such as a Python debugger (pdb), designed to facilitate an LLM-based agent's interactive debugging. Beyond coding and debugging tasks, this approach can be generalized to other tasks that would benefit from information-seeking behavior by an LLM agent.", "source": "arxiv", "arxiv_id": "2503.21557v1", "pdf_url": "https://arxiv.org/pdf/2503.21557v1", "categories": ["cs.AI", "cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-27T14:43:28Z", "updated": "2025-03-27T14:43:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "iAgent: LLM Agent as a Shield between User and Recommender Systems", "authors": ["Wujiang Xu", "Yunxiao Shi", "Zujie Liang", "Xuying Ning", "Kai Mei", "Kun Wang", "Xi Zhu", "Min Xu", "Yongfeng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.14662v4", "abstract": "Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure.", "source": "arxiv", "arxiv_id": "2502.14662v4", "pdf_url": "https://arxiv.org/pdf/2502.14662v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-20T15:58:25Z", "updated": "2025-05-29T23:51:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference", "authors": ["Wei Fan", "JinYi Yoon", "Bo Ji"], "year": 2025, "url": "http://arxiv.org/abs/2511.11306v2", "abstract": "Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).", "source": "arxiv", "arxiv_id": "2511.11306v2", "pdf_url": "https://arxiv.org/pdf/2511.11306v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-14T13:50:51Z", "updated": "2025-12-02T14:13:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "lmgame-Bench: How Good are LLMs at Playing Games?", "authors": ["Lanxiang Hu", "Mingjia Huo", "Yuxuan Zhang", "Haoyang Yu", "Eric P. Xing", "Ion Stoica", "Tajana Rosing", "Haojian Jin", "Hao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.15146v2", "abstract": "Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effective evaluation, for three reasons -- brittle vision perception, prompt sensitivity, and potential data contamination. We introduce lmgame-Bench to turn games into reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and narrative games delivered through a unified Gym-style API and paired with lightweight perception and memory scaffolds, and is designed to stabilize prompt variance and remove contamination. Across 13 leading models, we show lmgame-Bench is challenging while still separating models well. Correlation analysis shows that every game probes a unique blend of capabilities often tested in isolation elsewhere. More interestingly, performing reinforcement learning on a single game from lmgame-Bench transfers both to unseen games and to external planning tasks. Our evaluation code is available at https://github.com/lmgame-org/GamingAgent/lmgame-bench.", "source": "arxiv", "arxiv_id": "2505.15146v2", "pdf_url": "https://arxiv.org/pdf/2505.15146v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-21T06:02:55Z", "updated": "2025-06-03T09:53:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "scAgent: Universal Single-Cell Annotation via a LLM Agent", "authors": ["Yuren Mao", "Yu Mi", "Peigen Liu", "Mengfei Zhang", "Hanqing Liu", "Yunjun Gao"], "year": 2025, "url": "http://arxiv.org/abs/2504.04698v1", "abstract": "Cell type annotation is critical for understanding cellular heterogeneity. Based on single-cell RNA-seq data and deep learning models, good progress has been made in annotating a fixed number of cell types within a specific tissue. However, universal cell annotation, which can generalize across tissues, discover novel cell types, and extend to novel cell types, remains less explored. To fill this gap, this paper proposes scAgent, a universal cell annotation framework based on Large Language Models (LLMs). scAgent can identify cell types and discover novel cell types in diverse tissues; furthermore, it is data efficient to learn novel cell types. Experimental studies in 160 cell types and 35 tissues demonstrate the superior performance of scAgent in general cell-type annotation, novel cell discovery, and extensibility to novel cell type.", "source": "arxiv", "arxiv_id": "2504.04698v1", "pdf_url": "https://arxiv.org/pdf/2504.04698v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-07T03:03:21Z", "updated": "2025-04-07T03:03:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "360$^\\circ$REA: Towards A Reusable Experience Accumulation with 360Â° Assessment for Multi-Agent System", "authors": ["Shen Gao", "Hao Li", "Chengrui Huang", "Quan Tu", "Zhiliang Tian", "Minlie Huang", "Shuo Shang"], "year": 2024, "url": "http://arxiv.org/abs/2404.05569v3", "abstract": "Large language model agents have demonstrated remarkable advancements across various complex tasks. Recent works focus on optimizing the agent team or employing self-reflection to iteratively solve complex tasks. Since these agents are all based on the same LLM, only conducting self-evaluation or removing underperforming agents does not substantively enhance the capability of the agents. We argue that a comprehensive evaluation and accumulating experience from evaluation feedback is an effective approach to improving system performance. In this paper, we propose Reusable Experience Accumulation with 360$^\\circ$ Assessment (360$^\\circ$REA), a hierarchical multi-agent framework inspired by corporate organizational practices. The framework employs a novel 360$^\\circ$ performance assessment method for multi-perspective performance evaluation with fine-grained assessment. To enhance the capability of agents in addressing complex tasks, we introduce dual-level experience pool for agents to accumulate experience through fine-grained assessment. Extensive experiments on complex task datasets demonstrate the effectiveness of 360$^\\circ$REA.", "source": "arxiv", "arxiv_id": "2404.05569v3", "pdf_url": "https://arxiv.org/pdf/2404.05569v3", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-08T14:43:13Z", "updated": "2025-03-06T12:54:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples", "authors": ["Lihang Pan", "Yuxuan Li", "Chun Yu", "Yuanchun Shi"], "year": 2024, "url": "http://arxiv.org/abs/2404.15974v1", "abstract": "The capabilities of a single large language model (LLM) agent for solving a complex task are limited. Connecting multiple LLM agents to a network can effectively improve overall performance. However, building an LLM agent network (LAN) requires a substantial amount of time and effort. In this paper, we introduce EasyLAN, a human-computer collaborative tool that helps developers construct LANs. EasyLAN initially generates a LAN containing only one agent based on the description of the desired task. Subsequently, EasyLAN leverages a few training examples to update the LAN. For each example, EasyLAN models the gap between the output and the ground truth and identifies the causes of the errors. These errors are addressed through carefully designed strategies. Users can intervene in EasyLAN's workflow or directly modify the LAN. Eventually, the LAN evolves from a single agent to a network of LLM agents. The experimental results indicate that developers can rapidly construct LANs with good performance.", "source": "arxiv", "arxiv_id": "2404.15974v1", "pdf_url": "https://arxiv.org/pdf/2404.15974v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-04-24T16:49:24Z", "updated": "2024-04-24T16:49:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts", "authors": ["Kuang-Huei Lee", "Xinyun Chen", "Hiroki Furuta", "John Canny", "Ian Fischer"], "year": 2024, "url": "http://arxiv.org/abs/2402.09727v3", "abstract": "Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3.5-20x.", "source": "arxiv", "arxiv_id": "2402.09727v3", "pdf_url": "https://arxiv.org/pdf/2402.09727v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-15T05:40:21Z", "updated": "2024-07-22T05:33:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models", "authors": ["Chengxing Xie", "Difan Zou"], "year": 2024, "url": "http://arxiv.org/abs/2405.18208v1", "abstract": "Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies. However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue. In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties. Existing reasoning approaches have struggled to effectively address this complex task. Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems. Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns. Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning. Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness. Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10\\times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.", "source": "arxiv", "arxiv_id": "2405.18208v1", "pdf_url": "https://arxiv.org/pdf/2405.18208v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-28T14:13:32Z", "updated": "2024-05-28T14:13:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Large-scale Time-aware Agents Simulation for Influencer Selection in Digital Advertising Campaigns", "authors": ["Xiaoqing Zhang", "Xiuying Chen", "Yuhan Liu", "Jianzhou Wang", "Zhenxing Hu", "Rui Yan"], "year": 2024, "url": "http://arxiv.org/abs/2411.01143v1", "abstract": "In the digital world, influencers are pivotal as opinion leaders, shaping the views and choices of their influencees. Modern advertising often follows this trend, where marketers choose appropriate influencers for product endorsements, based on thorough market analysis. Previous studies on influencer selection have typically relied on numerical representations of individual opinions and interactions, a method that simplifies the intricacies of social dynamics. In this work, we first introduce a Time-aware Influencer Simulator (TIS), helping promoters identify and select the right influencers to market their products, based on LLM simulation. To validate our approach, we conduct experiments on the public advertising campaign dataset SAGraph which encompasses social relationships, posts, and user interactions. The results show that our method outperforms traditional numerical feature-based approaches and methods using limited LLM agents. Our research shows that simulating user timelines and content lifecycles over time simplifies scaling, allowing for large-scale agent simulations in social networks. Additionally, LLM-based agents for social recommendations and advertising offer substantial benefits for decision-making in promotional campaigns.", "source": "arxiv", "arxiv_id": "2411.01143v1", "pdf_url": "https://arxiv.org/pdf/2411.01143v1", "categories": ["cs.SI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-11-02T05:19:16Z", "updated": "2024-11-02T05:19:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement", "authors": ["Huan Zhang", "Wei Cheng", "Yuhan Wu", "Wei Hu"], "year": 2024, "url": "http://arxiv.org/abs/2409.05001v1", "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00%-162.43% compared to prompting LLMs directly.", "source": "arxiv", "arxiv_id": "2409.05001v1", "pdf_url": "https://arxiv.org/pdf/2409.05001v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-09-08T07:22:19Z", "updated": "2024-09-08T07:22:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning", "authors": ["Xinzhe Li"], "year": 2024, "url": "http://arxiv.org/abs/2406.05804v6", "abstract": "Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks. Although numerous frameworks have been devised for each paradigm, their intricate workflows and inconsistent taxonomy create challenges in understanding and reviewing the frameworks across different paradigms. This survey introduces a unified taxonomy to systematically review and discuss these frameworks. Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of LMPRs and workflow designs across different agent paradigms and frameworks. 3) Finally, we identify three limitations in existing workflow designs and systematically discuss the future work. Resources have been made publicly available at in our GitHub repository https://github.com/xinzhel/LLM-Agent-Survey.", "source": "arxiv", "arxiv_id": "2406.05804v6", "pdf_url": "https://arxiv.org/pdf/2406.05804v6", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-09T14:42:55Z", "updated": "2024-11-30T22:38:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "A Survey of Useful LLM Evaluation", "authors": ["Ji-Lun Peng", "Sijia Cheng", "Egil Diau", "Yung-Yu Shih", "Po-Heng Chen", "Yen-Ting Lin", "Yun-Nung Chen"], "year": 2024, "url": "http://arxiv.org/abs/2406.00936v1", "abstract": "LLMs have gotten attention across various research domains due to their exceptional performance on a wide range of complex tasks. Therefore, refined methods to evaluate the capabilities of LLMs are needed to determine the tasks and responsibility they should undertake. Our study mainly discussed how LLMs, as useful tools, should be effectively assessed. We proposed the two-stage framework: from ``core ability'' to ``agent'', clearly explaining how LLMs can be applied based on their specific capabilities, along with the evaluation methods in each stage. Core ability refers to the capabilities that LLMs need in order to generate high-quality natural language texts. After confirming LLMs possess core ability, they can solve real-world and complex tasks as agent. In the \"core ability\" stage, we discussed the reasoning ability, societal impact, and domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied action, planning, and tool learning of LLMs agent applications. Finally, we examined the challenges currently confronting the evaluation methods for LLMs, as well as the directions for future development.", "source": "arxiv", "arxiv_id": "2406.00936v1", "pdf_url": "https://arxiv.org/pdf/2406.00936v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-03T02:20:03Z", "updated": "2024-06-03T02:20:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents", "authors": ["Peiyuan Feng", "Yichen He", "Guanhua Huang", "Yuan Lin", "Hanchong Zhang", "Yuchen Zhang", "Hang Li"], "year": 2024, "url": "http://arxiv.org/abs/2405.14751v2", "abstract": "We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE.", "source": "arxiv", "arxiv_id": "2405.14751v2", "pdf_url": "https://arxiv.org/pdf/2405.14751v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-05-23T16:17:44Z", "updated": "2024-11-05T09:42:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution", "authors": ["Zhiqiang Xie", "Hao Kang", "Ying Sheng", "Tushar Krishna", "Kayvon Fatahalian", "Christos Kozyrakis"], "year": 2024, "url": "http://arxiv.org/abs/2411.03519v1", "abstract": "With more advanced natural language understanding and reasoning capabilities, large language model (LLM)-powered agents are increasingly developed in simulated environments to perform complex tasks, interact with other agents, and exhibit emergent behaviors relevant to social science and gaming. However, current multi-agent simulations frequently suffer from inefficiencies due to the limited parallelism caused by false dependencies, resulting in performance bottlenecks. In this paper, we introduce AI Metropolis, a simulation engine that improves the efficiency of LLM agent simulations by incorporating out-of-order execution scheduling. By dynamically tracking real dependencies between agents, AI Metropolis minimizes false dependencies, enhancing parallelism and enabling efficient hardware utilization. Our evaluations demonstrate that AI Metropolis achieves speedups from 1.3x to 4.15x over standard parallel simulation with global synchronization, approaching optimal performance as the number of agents increases.", "source": "arxiv", "arxiv_id": "2411.03519v1", "pdf_url": "https://arxiv.org/pdf/2411.03519v1", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2024-11-05T21:54:14Z", "updated": "2024-11-05T21:54:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents", "authors": ["Zhe Su", "Xuhui Zhou", "Sanketh Rangreji", "Anubha Kabra", "Julia Mendelsohn", "Faeze Brahman", "Maarten Sap"], "year": 2024, "url": "http://arxiv.org/abs/2409.09013v2", "abstract": "Truthfulness (adherence to factual accuracy) and utility (satisfying human needs and instructions) are both fundamental aspects of Large Language Models, yet these goals often conflict (e.g., sell a car with known flaws), which makes it challenging to achieve both in real-world deployments. We propose AI-LieDar, a framework to study how LLM-based agents navigate these scenarios in an multi-turn interactive setting. We design a set of real-world scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector inspired by psychological literature to assess the agents' responses. Our experiment demonstrates that all models are truthful less than 50% of the time, though truthfulness and goal achievement (utility) rates vary across models. We further test the steerability of LLMs towards truthfulness, finding that models can be directed to be truthful or deceptive, and even truth-steered models still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and LLM-based agents.", "source": "arxiv", "arxiv_id": "2409.09013v2", "pdf_url": "https://arxiv.org/pdf/2409.09013v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-13T17:41:12Z", "updated": "2025-04-28T04:20:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AIOS: LLM Agent Operating System", "authors": ["Kai Mei", "Xi Zhu", "Wujiang Xu", "Wenyue Hua", "Mingyu Jin", "Zelong Li", "Shuyuan Xu", "Ruosong Ye", "Yingqiang Ge", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.16971v5", "abstract": "LLM-based intelligent agents face significant deployment challenges, particularly related to resource management. Allowing unrestricted access to LLM or tool resources can lead to inefficient or even potentially harmful resource allocation and utilization for agents. Furthermore, the absence of proper scheduling and resource management mechanisms in current agent designs hinders concurrent processing and limits overall system efficiency. To address these challenges, this paper proposes the architecture of AIOS (LLM-based AI Agent Operating System) under the context of managing LLM-based agents. It introduces a novel architecture for serving LLM-based agents by isolating resources and LLM-specific services from agent applications into an AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling, context management, memory management, storage management, access control) for runtime agents. To enhance usability, AIOS also includes an AIOS SDK, a comprehensive suite of APIs designed for utilizing functionalities provided by the AIOS kernel. Experimental results demonstrate that using AIOS can achieve up to 2.1x faster execution for serving agents built by various agent frameworks. The source code is available at https://github.com/agiresearch/AIOS.", "source": "arxiv", "arxiv_id": "2403.16971v5", "pdf_url": "https://arxiv.org/pdf/2403.16971v5", "categories": ["cs.OS", "cs.AI", "cs.CL"], "primary_category": "cs.OS", "doi": "", "venue": "", "published": "2024-03-25T17:32:23Z", "updated": "2025-08-12T14:37:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "APPL: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts", "authors": ["Honghua Dong", "Qidong Su", "Yubo Gao", "Zhaoyu Li", "Yangjun Ruan", "Gennady Pekhimenko", "Chris J. Maddison", "Xujie Si"], "year": 2024, "url": "http://arxiv.org/abs/2406.13161v1", "abstract": "Large Language Models (LLMs) have become increasingly capable of handling diverse tasks with the aid of well-crafted prompts and integration of external tools, but as task complexity rises, the workflow involving LLMs can be complicated and thus challenging to implement and maintain. To address this challenge, we propose APPL, A Prompt Programming Language that acts as a bridge between computer programs and LLMs, allowing seamless embedding of prompts into Python functions, and vice versa. APPL provides an intuitive and Python-native syntax, an efficient parallelized runtime with asynchronous semantics, and a tracing module supporting effective failure diagnosis and replaying without extra costs. We demonstrate that APPL programs are intuitive, concise, and efficient through three representative scenarios: Chain-of-Thought with self-consistency (CoT-SC), ReAct tool use agent, and multi-agent chat. Experiments on three parallelizable workflows further show that APPL can effectively parallelize independent LLM calls, with a significant speedup ratio that almost matches the estimation.", "source": "arxiv", "arxiv_id": "2406.13161v1", "pdf_url": "https://arxiv.org/pdf/2406.13161v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-19T02:29:59Z", "updated": "2024-06-19T02:29:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AQA: Adaptive Question Answering in a Society of LLMs via Contextual Multi-Armed Bandit", "authors": ["Mohanna Hoveyda", "Arjen P. de Vries", "Maarten de Rijke", "Harrie Oosterhuis", "Faegheh Hasibi"], "year": 2024, "url": "http://arxiv.org/abs/2409.13447v2", "abstract": "In question answering (QA), different questions can be effectively addressed with different answering strategies. Some require a simple lookup, while others need complex, multi-step reasoning to be answered adequately. This observation motivates the development of a dynamic method that adaptively selects the most suitable QA strategy for each question, enabling more efficient and effective systems capable of addressing a broader range of question types. To this aim, we build on recent advances in the orchestration of multiple large language models (LLMs) and formulate adaptive QA as a dynamic orchestration challenge. We define this as a contextual multi-armed bandit problem, where the context is defined by the characteristics of the incoming question and the action space consists of potential communication graph configurations among the LLM agents. We then train a linear upper confidence bound model to learn an optimal mapping between different question types and their corresponding optimal multi-LLM communication graph representation. Our experiments show that the proposed solution is viable for adaptive orchestration of a QA system with multiple modules, as it combines the superior performance of more complex strategies while avoiding their costs when simpler strategies suffice.", "source": "arxiv", "arxiv_id": "2409.13447v2", "pdf_url": "https://arxiv.org/pdf/2409.13447v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-20T12:28:18Z", "updated": "2024-09-23T08:43:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Academically intelligent LLMs are not necessarily socially intelligent", "authors": ["Ruoxi Xu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Yingfei Sun"], "year": 2024, "url": "http://arxiv.org/abs/2403.06591v1", "abstract": "The academic intelligence of large language models (LLMs) has made remarkable progress in recent times, but their social intelligence performance remains unclear. Inspired by established human social intelligence frameworks, particularly Daniel Goleman's social intelligence theory, we have developed a standardized social intelligence test based on real-world social scenarios to comprehensively assess the social intelligence of LLMs, termed as the Situational Evaluation of Social Intelligence (SESI). We conducted an extensive evaluation with 13 recent popular and state-of-art LLM agents on SESI. The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors. Moreover, there exists a relatively low correlation between the social intelligence and academic intelligence exhibited by LLMs, suggesting that social intelligence is distinct from academic intelligence for LLMs. Additionally, while it is observed that LLMs can't ``understand'' what social intelligence is, their social intelligence, similar to that of humans, is influenced by social factors.", "source": "arxiv", "arxiv_id": "2403.06591v1", "pdf_url": "https://arxiv.org/pdf/2403.06591v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-11T10:35:53Z", "updated": "2024-03-11T10:35:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Adaptive In-conversation Team Building for Language Model Agents", "authors": ["Linxin Song", "Jiale Liu", "Jieyu Zhang", "Shaokun Zhang", "Ao Luo", "Shijian Wang", "Qingyun Wu", "Chi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2405.19425v3", "abstract": "Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art. It is thus intriguing to answer a critical question: Given a task, how can we build a team of LLM agents to solve it effectively? Our new adaptive team-building paradigm offers a flexible solution, realized through a novel agent design named Captain Agent. It dynamically forms and manages teams for each step of a task-solving process, utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs, allowing for a flexible yet structured approach to problem-solving. A comprehensive evaluation across six real-world scenarios demonstrates that Captain Agent significantly outperforms existing multi-agent methods with 21.94% improvement in average accuracy, providing outstanding performance without requiring task-specific prompt engineering. Our exploration of different backbone LLM and cost analysis further shows that Captain Agent can improve the conversation quality of weak LLM and achieve competitive performance with extremely low cost, which illuminates the application of multi-agent systems.", "source": "arxiv", "arxiv_id": "2405.19425v3", "pdf_url": "https://arxiv.org/pdf/2405.19425v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-29T18:08:37Z", "updated": "2025-03-02T06:36:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Adaptive Reasoning and Acting in Medical Language Agents", "authors": ["Abhishek Dutta", "Yen-Che Hsiao"], "year": 2024, "url": "http://arxiv.org/abs/2410.10020v1", "abstract": "This paper presents an innovative large language model (LLM) agent framework for enhancing diagnostic accuracy in simulated clinical environments using the AgentClinic benchmark. The proposed automatic correction enables doctor agents to iteratively refine their reasoning and actions following incorrect diagnoses, fostering improved decision-making over time. Experiments show that the implementation of the adaptive LLM-based doctor agents achieve correct diagnoses through dynamic interactions with simulated patients. The evaluations highlight the capacity of autonomous agents to adapt and improve in complex medical scenarios. Future enhancements will focus on refining the algorithm and expanding its applicability across a wider range of tasks and different large language models.", "source": "arxiv", "arxiv_id": "2410.10020v1", "pdf_url": "https://arxiv.org/pdf/2410.10020v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-13T21:45:16Z", "updated": "2024-10-13T21:45:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent", "authors": ["Janghoon Ock", "Radheesh Sharma Meda", "Tirtha Vinchurkar", "Yayati Jadhav", "Amir Barati Farimani"], "year": 2024, "url": "http://arxiv.org/abs/2410.16658v4", "abstract": "Adsorption energy is a key reactivity descriptor in catalysis. Determining adsorption energy requires evaluating numerous adsorbate-catalyst configurations, making it computationally intensive. Current methods rely on exhaustive sampling, which does not guarantee the identification of the global minimum energy. To address this, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify stable adsorption configurations corresponding to the global minimum energy. Adsorb-Agent leverages its built-in knowledge and reasoning to strategically explore configurations, significantly reducing the number of initial setups required while improving energy prediction accuracy. In this study, we also evaluated the performance of different LLMs, including GPT-4o, GPT-4o-mini, Claude-3.7-Sonnet, and DeepSeek-Chat, as the reasoning engine for Adsorb-Agent, with GPT-4o showing the strongest overall performance. Tested on twenty diverse systems, Adsorb-Agent identifies comparable adsorption energies for 84% of cases and achieves lower energies for 35%, particularly excelling in complex systems. It identifies lower energies in 47% of intermetallic systems and 67% of systems with large adsorbates. These findings demonstrate Adsorb-Agent's potential to accelerate catalyst discovery by reducing computational costs and enhancing prediction reliability compared to exhaustive search methods.", "source": "arxiv", "arxiv_id": "2410.16658v4", "pdf_url": "https://arxiv.org/pdf/2410.16658v4", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-22T03:19:16Z", "updated": "2025-07-08T03:12:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Advancing Healthcare Automation: Multi-Agent System for Medical Necessity Justification", "authors": ["Himanshu Pandey", "Akhil Amod", "Shivang"], "year": 2024, "url": "http://arxiv.org/abs/2404.17977v2", "abstract": "Prior Authorization delivers safe, appropriate, and cost-effective care that is medically justified with evidence-based guidelines. However, the process often requires labor-intensive manual comparisons between patient medical records and clinical guidelines, that is both repetitive and time-consuming. Recent developments in Large Language Models (LLMs) have shown potential in addressing complex medical NLP tasks with minimal supervision. This paper explores the application of Multi-Agent System (MAS) that utilize specialized LLM agents to automate Prior Authorization task by breaking them down into simpler and manageable sub-tasks. Our study systematically investigates the effects of various prompting strategies on these agents and benchmarks the performance of different LLMs. We demonstrate that GPT-4 achieves an accuracy of 86.2% in predicting checklist item-level judgments with evidence, and 95.6% in determining overall checklist judgment. Additionally, we explore how these agents can contribute to explainability of steps taken in the process, thereby enhancing trust and transparency in the system.", "source": "arxiv", "arxiv_id": "2404.17977v2", "pdf_url": "https://arxiv.org/pdf/2404.17977v2", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-27T18:40:05Z", "updated": "2024-07-06T09:29:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Affordable Generative Agents", "authors": ["Yangbin Yu", "Qin Zhang", "Junyou Li", "Qiang Fu", "Deheng Ye"], "year": 2024, "url": "http://arxiv.org/abs/2402.02053v2", "abstract": "The emergence of large language models (LLMs) has significantly advanced the simulation of believable interactive agents. However, the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents. Therefore, in this paper, we develop Affordable Generative Agents (AGA), a framework for enabling the generation of believable and low-cost interactions on both agent-environment and inter-agents levels. Specifically, for agent-environment interactions, we substitute repetitive LLM inferences with learned policies; while for inter-agent interactions, we model the social relationships between agents and compress auxiliary dialogue information. Extensive experiments on multiple environments show the effectiveness and efficiency of our proposed framework. Also, we delve into the mechanisms of emergent believable behaviors lying in LLM agents, demonstrating that agents can only generate finite behaviors in fixed environments, based upon which, we understand ways to facilitate emergent interaction behaviors. Our code is publicly available at: https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents.", "source": "arxiv", "arxiv_id": "2402.02053v2", "pdf_url": "https://arxiv.org/pdf/2402.02053v2", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-03T06:16:28Z", "updated": "2024-08-28T04:04:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast", "authors": ["Xiangming Gu", "Xiaosen Zheng", "Tianyu Pang", "Chao Du", "Qian Liu", "Ye Wang", "Jing Jiang", "Min Lin"], "year": 2024, "url": "http://arxiv.org/abs/2402.08567v2", "abstract": "A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at https://sail-sg.github.io/Agent-Smith/.", "source": "arxiv", "arxiv_id": "2402.08567v2", "pdf_url": "https://arxiv.org/pdf/2402.08567v2", "categories": ["cs.CL", "cs.CR", "cs.CV", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-13T16:06:17Z", "updated": "2024-06-03T14:15:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-Based Modelling Meets Generative AI in Social Network Simulations", "authors": ["Antonino Ferraro", "Antonio Galli", "Valerio La Gatta", "Marco Postiglione", "Gian Marco Orlando", "Diego Russo", "Giuseppe Riccio", "Antonio Romano", "Vincenzo Moscato"], "year": 2024, "url": "http://arxiv.org/abs/2411.16031v1", "abstract": "Agent-Based Modelling (ABM) has emerged as an essential tool for simulating social networks, encompassing diverse phenomena such as information dissemination, influence dynamics, and community formation. However, manually configuring varied agent interactions and information flow dynamics poses challenges, often resulting in oversimplified models that lack real-world generalizability. Integrating modern Large Language Models (LLMs) with ABM presents a promising avenue to address these challenges and enhance simulation fidelity, leveraging LLMs' human-like capabilities in sensing, reasoning, and behavior. In this paper, we propose a novel framework utilizing LLM-empowered agents to simulate social network users based on their interests and personality traits. The framework allows for customizable agent interactions resembling various social network platforms, including mechanisms for content resharing and personalized recommendations. We validate our framework using a comprehensive Twitter dataset from the 2020 US election, demonstrating that LLM-agents accurately replicate real users' behaviors, including linguistic patterns and political inclinations. These agents form homogeneous ideological clusters and retain the main themes of their community. Notably, preference-based recommendations significantly influence agent behavior, promoting increased engagement, network homophily and the formation of echo chambers. Overall, our findings underscore the potential of LLM-agents in advancing social media simulations and unraveling intricate online dynamics.", "source": "arxiv", "arxiv_id": "2411.16031v1", "pdf_url": "https://arxiv.org/pdf/2411.16031v1", "categories": ["cs.SI", "cs.MA"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-11-25T01:18:49Z", "updated": "2024-11-25T01:18:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents", "authors": ["Zhexin Zhang", "Shiyao Cui", "Yida Lu", "Jingzhuo Zhou", "Junxiao Yang", "Hongning Wang", "Minlie Huang"], "year": 2024, "url": "http://arxiv.org/abs/2412.14470v2", "abstract": "As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through failure mode and helpfulness analysis, we summarize two fundamental safety defects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone may be insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. To drive progress in this area, Agent-SafetyBench has been released at https://github.com/thu-coai/Agent-SafetyBench/ to facilitate further research in agent safety evaluation and improvement.", "source": "arxiv", "arxiv_id": "2412.14470v2", "pdf_url": "https://arxiv.org/pdf/2412.14470v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-19T02:35:15Z", "updated": "2025-05-20T05:58:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "authors": ["Yifan Song", "Weimin Xiong", "Xiutian Zhao", "Dawei Zhu", "Wenhao Wu", "Ke Wang", "Cheng Li", "Wei Peng", "Sujian Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.07706v1", "abstract": "Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AgentBank, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories which comprises 16 tasks covering five distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are able to scale the annotated trajectories and generate a trajectory dataset with minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a series of agent models, Samoyed. Our comparative experiments demonstrate the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization.", "source": "arxiv", "arxiv_id": "2410.07706v1", "pdf_url": "https://arxiv.org/pdf/2410.07706v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-10T08:19:12Z", "updated": "2024-10-10T08:19:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents", "authors": ["Chang Ma", "Junlei Zhang", "Zhihao Zhu", "Cheng Yang", "Yujiu Yang", "Yaohui Jin", "Zhenzhong Lan", "Lingpeng Kong", "Junxian He"], "year": 2024, "url": "http://arxiv.org/abs/2401.13178v2", "abstract": "Evaluating Large Language Models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a step towards demystifying agent behaviors and accelerating the development of stronger LLM agents.", "source": "arxiv", "arxiv_id": "2401.13178v2", "pdf_url": "https://arxiv.org/pdf/2401.13178v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-24T01:51:00Z", "updated": "2024-12-23T20:12:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents", "authors": ["Edoardo Debenedetti", "Jie Zhang", "Mislav BalunoviÄ", "Luca Beurer-Kellner", "Marc Fischer", "Florian TramÃ¨r"], "year": 2024, "url": "http://arxiv.org/abs/2406.13352v3", "abstract": "AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.", "source": "arxiv", "arxiv_id": "2406.13352v3", "pdf_url": "https://arxiv.org/pdf/2406.13352v3", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-06-19T08:55:56Z", "updated": "2024-11-24T22:04:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "authors": ["Maksym Andriushchenko", "Alexandra Souly", "Mateusz Dziemian", "Derek Duenas", "Maxwell Lin", "Justin Wang", "Dan Hendrycks", "Andy Zou", "Zico Kolter", "Matt Fredrikson", "Eric Winsor", "Jerome Wynne", "Yarin Gal", "Xander Davies"], "year": 2024, "url": "http://arxiv.org/abs/2410.09024v3", "abstract": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.", "source": "arxiv", "arxiv_id": "2410.09024v3", "pdf_url": "https://arxiv.org/pdf/2410.09024v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-11T17:39:22Z", "updated": "2025-04-18T14:30:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentKit: Structured LLM Reasoning with Dynamic Graphs", "authors": ["Yue Wu", "Yewen Fan", "So Yeon Min", "Shrimai Prabhumoye", "Stephen McAleer", "Yonatan Bisk", "Ruslan Salakhutdinov", "Yuanzhi Li", "Tom Mitchell"], "year": 2024, "url": "http://arxiv.org/abs/2404.11483v2", "abstract": "We propose an intuitive LLM prompting framework (AgentKit) for multifunctional agents. AgentKit offers a unified framework for explicitly constructing a complex \"thought process\" from simple natural language prompts. The basic building block in AgentKit is a node, containing a natural language prompt for a specific subtask. The user then puts together chains of nodes, like stacking LEGO pieces. The chains of nodes can be designed to explicitly enforce a naturally structured \"thought process\". For example, for the task of writing a paper, one may start with the thought process of 1) identify a core message, 2) identify prior research gaps, etc. The nodes in AgentKit can be designed and combined in different ways to implement multiple advanced capabilities including on-the-fly hierarchical planning, reflection, and learning from interactions. In addition, due to the modular nature and the intuitive design to simulate explicit human thought process, a basic agent could be implemented as simple as a list of prompts for the subtasks and therefore could be designed and tuned by someone without any programming experience. Quantitatively, we show that agents designed through AgentKit achieve SOTA performance on WebShop and Crafter. These advances underscore AgentKit's potential in making LLM agents effective and accessible for a wider range of applications. https://github.com/holmeswww/AgentKit", "source": "arxiv", "arxiv_id": "2404.11483v2", "pdf_url": "https://arxiv.org/pdf/2404.11483v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-17T15:40:45Z", "updated": "2024-07-24T20:53:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System", "authors": ["Zhiwei Liu", "Weiran Yao", "Jianguo Zhang", "Liangwei Yang", "Zuxin Liu", "Juntao Tan", "Prafulla K. Choubey", "Tian Lan", "Jason Wu", "Huan Wang", "Shelby Heinecke", "Caiming Xiong", "Silvio Savarese"], "year": 2024, "url": "http://arxiv.org/abs/2402.15538v1", "abstract": "The booming success of LLMs initiates rapid development in LLM agents. Though the foundation of an LLM agent is the generative model, it is critical to devise the optimal reasoning strategies and agent architectures. Accordingly, LLM agent research advances from the simple chain-of-thought prompting to more complex ReAct and Reflection reasoning strategy; agent architecture also evolves from single agent generation to multi-agent conversation, as well as multi-LLM multi-agent group chat. However, with the existing intricate frameworks and libraries, creating and evaluating new reasoning strategies and agent architectures has become a complex challenge, which hinders research investigation into LLM agents. Thus, we open-source a new AI agent library, AgentLite, which simplifies this process by offering a lightweight, user-friendly platform for innovating LLM agent reasoning, architectures, and applications with ease. AgentLite is a task-oriented framework designed to enhance the ability of agents to break down tasks and facilitate the development of multi-agent systems. Furthermore, we introduce multiple practical applications developed with AgentLite to demonstrate its convenience and flexibility. Get started now at: \\url{https://github.com/SalesforceAIResearch/AgentLite}.", "source": "arxiv", "arxiv_id": "2402.15538v1", "pdf_url": "https://arxiv.org/pdf/2402.15538v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-02-23T06:25:20Z", "updated": "2024-02-23T06:25:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentOps: Enabling Observability of LLM Agents", "authors": ["Liming Dong", "Qinghua Lu", "Liming Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2411.05285v2", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities across various domains, gaining extensive attention from academia and industry. However, these agents raise significant concerns on AI safety due to their autonomous and non-deterministic behavior, as well as continuous evolving nature . From a DevOps perspective, enabling observability in agents is necessary to ensuring AI safety, as stakeholders can gain insights into the agents' inner workings, allowing them to proactively understand the agents, detect anomalies, and prevent potential failures. Therefore, in this paper, we present a comprehensive taxonomy of AgentOps, identifying the artifacts and associated data that should be traced throughout the entire lifecycle of agents to achieve effective observability. The taxonomy is developed based on a systematic mapping study of existing AgentOps tools. Our taxonomy serves as a reference template for developers to design and implement AgentOps infrastructure that supports monitoring, logging, and analytics. thereby ensuring AI safety.", "source": "arxiv", "arxiv_id": "2411.05285v2", "pdf_url": "https://arxiv.org/pdf/2411.05285v2", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-08T02:31:03Z", "updated": "2024-11-30T02:55:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases", "authors": ["Zhaorun Chen", "Zhen Xiang", "Chaowei Xiao", "Dawn Song", "Bo Li"], "year": 2024, "url": "http://arxiv.org/abs/2407.12784v1", "abstract": "LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. On each agent, AgentPoison achieves an average attack success rate higher than 80% with minimal impact on benign performance (less than 1%) with a poison rate less than 0.1%.", "source": "arxiv", "arxiv_id": "2407.12784v1", "pdf_url": "https://arxiv.org/pdf/2407.12784v1", "categories": ["cs.LG", "cs.CR", "cs.IR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-07-17T17:59:47Z", "updated": "2024-07-17T17:59:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents", "authors": ["Luca Gioacchini", "Giuseppe Siracusano", "Davide Sanvito", "Kiril Gashteovski", "David Friede", "Roberto Bifulco", "Carolin Lawrence"], "year": 2024, "url": "http://arxiv.org/abs/2404.06411v1", "abstract": "The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.", "source": "arxiv", "arxiv_id": "2404.06411v1", "pdf_url": "https://arxiv.org/pdf/2404.06411v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-09T16:01:24Z", "updated": "2024-04-09T16:01:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentReview: Exploring Peer Review Dynamics with LLM Agents", "authors": ["Yiqiao Jin", "Qinlin Zhao", "Yiyang Wang", "Hao Chen", "Kaijie Zhu", "Yijia Xiao", "Jindong Wang"], "year": 2024, "url": "http://arxiv.org/abs/2406.12708v2", "abstract": "Peer review is fundamental to the integrity and advancement of scientific publication. Traditional methods of peer review analyses often rely on exploration and statistics of existing peer review data, which do not adequately address the multivariate nature of the process, account for the latent variables, and are further constrained by privacy concerns due to the sensitive nature of the data. We introduce AgentReview, the first large language model (LLM) based peer review simulation framework, which effectively disentangles the impacts of multiple latent factors and addresses the privacy issue. Our study reveals significant insights, including a notable 37.1% variation in paper decisions due to reviewers' biases, supported by sociological theories such as the social influence theory, altruism fatigue, and authority bias. We believe that this study could offer valuable insights to improve the design of peer review mechanisms. Our code is available at https://github.com/Ahren09/AgentReview.", "source": "arxiv", "arxiv_id": "2406.12708v2", "pdf_url": "https://arxiv.org/pdf/2406.12708v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-18T15:22:12Z", "updated": "2024-10-13T02:43:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "authors": ["Yu Shang", "Yu Li", "Keyu Zhao", "Likai Ma", "Jiahe Liu", "Fengli Xu", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.06153v3", "abstract": "Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS). We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory. Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents. To further accelerate the process, we design a performance predictor that uses in-context surrogate models to skip unpromising agent designs. Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Moreover, AgentSquare can generate interpretable design insights, enabling a deeper understanding of agentic architecture and its impact on task performance. We believe that the modular design space and AgentSquare search framework offer a platform for fully exploiting the potential of prior successful designs and consolidating the collective efforts of research community. Code repo is available at https://github.com/tsinghua-fib-lab/AgentSquare.", "source": "arxiv", "arxiv_id": "2410.06153v3", "pdf_url": "https://arxiv.org/pdf/2410.06153v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-08T15:52:42Z", "updated": "2025-02-27T13:33:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agentic LLMs in the Supply Chain: Towards Autonomous Multi-Agent Consensus-Seeking", "authors": ["Valeria Jannelli", "Stefan Schoepf", "Matthias Bickel", "TorbjÃ¸rn Netland", "Alexandra Brintrup"], "year": 2024, "url": "http://arxiv.org/abs/2411.10184v1", "abstract": "This paper explores how Large Language Models (LLMs) can automate consensus-seeking in supply chain management (SCM), where frequent decisions on problems such as inventory levels and delivery times require coordination among companies. Traditional SCM relies on human consensus in decision-making to avoid emergent problems like the bullwhip effect. Some routine consensus processes, especially those that are time-intensive and costly, can be automated. Existing solutions for automated coordination have faced challenges due to high entry barriers locking out SMEs, limited capabilities, and limited adaptability in complex scenarios. However, recent advances in Generative AI, particularly LLMs, show promise in overcoming these barriers. LLMs, trained on vast datasets can negotiate, reason, and plan, facilitating near-human-level consensus at scale with minimal entry barriers. In this work, we identify key limitations in existing approaches and propose autonomous LLM agents to address these gaps. We introduce a series of novel, supply chain-specific consensus-seeking frameworks tailored for LLM agents and validate the effectiveness of our approach through a case study in inventory management. To accelerate progress within the SCM community, we open-source our code, providing a foundation for further advancements in LLM-powered autonomous supply chain solutions.", "source": "arxiv", "arxiv_id": "2411.10184v1", "pdf_url": "https://arxiv.org/pdf/2411.10184v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-15T13:33:10Z", "updated": "2024-11-15T13:33:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Aligning Agents like Large Language Models", "authors": ["Adam Jelley", "Yuhan Cao", "Dave Bignell", "Amos Storkey", "Sam Devlin", "Tabish Rashid"], "year": 2024, "url": "http://arxiv.org/abs/2406.04208v2", "abstract": "Training agents to act competently in complex 3D environments from high-dimensional visual information is challenging. Reinforcement learning is conventionally used to train such agents, but requires a carefully designed reward function, and is difficult to scale to obtain robust agents that generalize to new tasks. In contrast, Large Language Models (LLMs) demonstrate impressively general capabilities resulting from large-scale pre-training and post-training alignment, but struggle to act in complex environments. This position paper draws explicit analogies between decision-making agents and LLMs, and argues that agents should be trained like LLMs to achieve more general, robust, and aligned behaviors. We provide a proof-of-concept to demonstrate how the procedure for training LLMs can be used to train an agent in a 3D video game environment from pixels. We investigate the importance of each stage of the LLM training pipeline, while providing guidance and insights for successfully applying this approach to agents. Our paper provides an alternative perspective to contemporary LLM Agents on how recent progress in LLMs can be leveraged for decision-making agents, and we hope will illuminate a path towards developing more generally capable agents for video games and beyond. Project summary and videos: https://adamjelley.github.io/aligning-agents-like-llms .", "source": "arxiv", "arxiv_id": "2406.04208v2", "pdf_url": "https://arxiv.org/pdf/2406.04208v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-06T16:05:45Z", "updated": "2025-12-28T15:24:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Aligning LLM Agents by Learning Latent Preference from User Edits", "authors": ["Ge Gao", "Alexey Taymanov", "Eduardo Salinas", "Paul Mineiro", "Dipendra Misra"], "year": 2024, "url": "http://arxiv.org/abs/2404.15269v3", "abstract": "We study interactive learning of LLM-based language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data. The inferred user preference descriptions are used to define prompts for generating responses in the future. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex, subtle, and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages the LLM to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, and use a GPT-4 simulated user for evaluation. On both tasks, CIPHER outperforms several baselines by achieving the lowest edit distance cost while only having a small overhead in LLM query cost. Our analysis reports that user preferences learned by CIPHER show significant similarity to the ground truth latent preferences.", "source": "arxiv", "arxiv_id": "2404.15269v3", "pdf_url": "https://arxiv.org/pdf/2404.15269v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-23T17:57:47Z", "updated": "2024-11-23T16:19:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Aligning Language Models to Explicitly Handle Ambiguity", "authors": ["Hyuhng Joon Kim", "Youna Kim", "Cheonbok Park", "Junyeob Kim", "Choonghyun Park", "Kang Min Yoo", "Sang-goo Lee", "Taeuk Kim"], "year": 2024, "url": "http://arxiv.org/abs/2404.11972v3", "abstract": "In interactions between users and language model agents, user utterances frequently exhibit ellipsis (omission of words or phrases) or imprecision (lack of exactness) to prioritize efficiency. This can lead to varying interpretations of the same input based on different assumptions or background knowledge. It is thus crucial for agents to adeptly handle the inherent ambiguity in queries to ensure reliability. However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge. To address these issues, we propose Alignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to manage ambiguous queries by leveraging their own assessment of ambiguity (i.e., perceived ambiguity). Experimental results on question-answering datasets demonstrate that APA empowers LLMs to explicitly detect and manage ambiguous queries while retaining the ability to answer clear questions. Furthermore, our finding proves that APA excels beyond training with gold-standard labels, especially in out-of-distribution scenarios. The data and code are available at https://github.com/heyjoonkim/APA.", "source": "arxiv", "arxiv_id": "2404.11972v3", "pdf_url": "https://arxiv.org/pdf/2404.11972v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-18T07:59:53Z", "updated": "2024-10-04T05:20:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era", "authors": ["Bo Chen", "Xinyi Dai", "Huifeng Guo", "Wei Guo", "Weiwen Liu", "Yong Liu", "Jiarui Qin", "Ruiming Tang", "Yichao Wang", "Chuhan Wu", "Yaxiong Wu", "Hao Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2407.10081v1", "abstract": "Recommender systems (RS) are vital for managing information overload and delivering personalized content, responding to users' diverse information needs. The emergence of large language models (LLMs) offers a new horizon for redefining recommender systems with vast general knowledge and reasoning capabilities. Standing across this LLM era, we aim to integrate recommender systems into a broader picture, and pave the way for more comprehensive solutions for future research. Therefore, we first offer a comprehensive overview of the technical progression of recommender systems, particularly focusing on language foundation models and their applications in recommendation. We identify two evolution paths of modern recommender systems -- via list-wise recommendation and conversational recommendation. These two paths finally converge at LLM agents with superior capabilities of long-term memory, reflection, and tool intelligence. Along these two paths, we point out that the information effectiveness of the recommendation is increased, while the user's acquisition cost is decreased. Technical features, research methodologies, and inherent challenges for each milestone along the path are carefully investigated -- from traditional list-wise recommendation to LLM-enhanced recommendation to recommendation with LLM agents. Finally, we highlight several unresolved challenges crucial for the development of future personalization technologies and interfaces and discuss the future prospects.", "source": "arxiv", "arxiv_id": "2407.10081v1", "pdf_url": "https://arxiv.org/pdf/2407.10081v1", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-07-14T05:02:21Z", "updated": "2024-07-14T05:02:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models", "authors": ["Chaoyun Zhang", "Zicheng Ma", "Yuhao Wu", "Shilin He", "Si Qin", "Minghua Ma", "Xiaoting Qin", "Yu Kang", "Yuyi Liang", "Xiaoyu Gou", "Yajie Xue", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.15157v2", "abstract": "Verbatim feedback constitutes a valuable repository of user experiences, opinions, and requirements essential for software development. Effectively and efficiently extracting valuable insights from such data poses a challenging task. This paper introduces Allhands , an innovative analytic framework designed for large-scale feedback analysis through a natural language interface, leveraging large language models (LLMs). Allhands adheres to a conventional feedback analytic workflow, initially conducting classification and topic modeling on the feedback to convert them into a structurally augmented format, incorporating LLMs to enhance accuracy, robustness, generalization, and user-friendliness. Subsequently, an LLM agent is employed to interpret users' diverse questions in natural language on feedback, translating them into Python code for execution, and delivering comprehensive multi-modal responses, including text, code, tables, and images.\n  We evaluate Allhands across three diverse feedback datasets. The experiments demonstrate that Allhands achieves superior efficacy at all stages of analysis, including classification and topic modeling, eventually providing users with an \"ask me anything\" experience with comprehensive, correct and human-readable response. To the best of our knowledge, Allhands stands as the first comprehensive feedback analysis framework that supports diverse and customized requirements for insight extraction through a natural language interface.", "source": "arxiv", "arxiv_id": "2403.15157v2", "pdf_url": "https://arxiv.org/pdf/2403.15157v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-03-22T12:13:16Z", "updated": "2024-04-03T09:46:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Alopex: A Computational Framework for Enabling On-Device Function Calls with LLMs", "authors": ["Yide Ran", "Zhaozhuo Xu", "Yuhang Yao", "Zijian Hu", "Shanshan Han", "Han Jin", "Alay Dilipbhai Shah", "Jipeng Zhang", "Dimitris Stripelis", "Tong Zhang", "Salman Avestimehr", "Chaoyang He"], "year": 2024, "url": "http://arxiv.org/abs/2411.05209v1", "abstract": "The rapid advancement of Large Language Models (LLMs) has led to their increased integration into mobile devices for personalized assistance, which enables LLMs to call external API functions to enhance their performance. However, challenges such as data scarcity, ineffective question formatting, and catastrophic forgetting hinder the development of on-device LLM agents. To tackle these issues, we propose Alopex, a framework that enables precise on-device function calls using the Fox LLM. Alopex introduces a logic-based method for generating high-quality training data and a novel ``description-question-output'' format for fine-tuning, reducing risks of function information leakage. Additionally, a data mixing strategy is used to mitigate catastrophic forgetting, combining function call data with textbook datasets to enhance performance in various tasks. Experimental results show that Alopex improves function call accuracy and significantly reduces catastrophic forgetting, providing a robust solution for integrating function call capabilities into LLMs without manual intervention.", "source": "arxiv", "arxiv_id": "2411.05209v1", "pdf_url": "https://arxiv.org/pdf/2411.05209v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-07T22:15:17Z", "updated": "2024-11-07T22:15:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs", "authors": ["Aly M. Kassem", "Omar Mahmoud", "Niloofar Mireshghallah", "Hyunwoo Kim", "Yulia Tsvetkov", "Yejin Choi", "Sherif Saad", "Santu Rana"], "year": 2024, "url": "http://arxiv.org/abs/2403.04801v3", "abstract": "In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other LLMs can open a new avenue of automated attacks that we should further study and explore. The code can be found at https://github.com/Alymostafa/Instruction_based_attack .", "source": "arxiv", "arxiv_id": "2403.04801v3", "pdf_url": "https://arxiv.org/pdf/2403.04801v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-05T19:32:01Z", "updated": "2025-02-09T19:38:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An AI-Driven Multimodal Smart Home Platform for Continuous Monitoring and Assistance in Post-Stroke Motor Impairment", "authors": ["Chenyu Tang", "Ruizhi Zhang", "Shuo Gao", "Zihe Zhao", "Zibo Zhang", "Jiaqi Wang", "Cong Li", "Junliang Chen", "Yanning Dai", "Shengbo Wang", "Ruoyu Juan", "Qiaoying Li", "Ruimou Xie", "Xuhang Chen", "Xinkai Zhou", "Yunjia Xia", "Jianan Chen", "Fanghao Lu", "Xin Li", "Ninglli Wang", "Peter Smielewski", "Yu Pan", "Hubin Zhao", "Luigi G. Occhipinti"], "year": 2024, "url": "http://arxiv.org/abs/2411.19000v4", "abstract": "At-home rehabilitation for post-stroke patients presents significant challenges, as continuous, personalized care is often limited outside clinical settings. Moreover, the lack of integrated solutions capable of simultaneously monitoring motor recovery and providing intelligent assistance in home environments hampers rehabilitation outcomes. Here, we present a multimodal smart home platform designed for continuous, at-home rehabilitation of post-stroke patients, integrating wearable sensing, ambient monitoring, and adaptive automation. A plantar pressure insole equipped with a machine learning pipeline classifies users into motor recovery stages with up to 94\\% accuracy, enabling quantitative tracking of walking patterns during daily activities. An optional head-mounted eye-tracking module, together with ambient sensors such as cameras and microphones, supports seamless hands-free control of household devices with a 100\\% success rate and sub-second response time. These data streams are fused locally via a hierarchical Internet of Things (IoT) architecture, ensuring low latency and data privacy. An embedded large language model (LLM) agent, Auto-Care, continuously interprets multimodal data to provide real-time interventions -- issuing personalized reminders, adjusting environmental conditions, and notifying caregivers. Implemented in a post-stroke context, this integrated smart home platform increased mean user satisfaction from 3.9 $\\pm$ 0.8 in conventional home environments to 8.4 $\\pm$ 0.6 with the full system ($n=20$). Beyond stroke, the system offers a scalable, patient-centered framework with potential for long-term use in broader neurorehabilitation and aging-in-place applications.", "source": "arxiv", "arxiv_id": "2411.19000v4", "pdf_url": "https://arxiv.org/pdf/2411.19000v4", "categories": ["cs.HC", "cs.AI", "eess.SY"], "primary_category": "cs.HC", "doi": "10.1109/TNSRE.2025.3645093", "venue": "", "published": "2024-11-28T09:04:39Z", "updated": "2025-10-15T21:33:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "An LLM Agent for Automatic Geospatial Data Analysis", "authors": ["Yuxing Chen", "Weijie Wang", "Sylvain Lobry", "Camille Kurtz"], "year": 2024, "url": "http://arxiv.org/abs/2410.18792v2", "abstract": "Large language models (LLMs) are being used in data science code generation tasks, but they often struggle with complex sequential tasks, leading to logical errors. Their application to geospatial data processing is particularly challenging due to difficulties in incorporating complex data structures and spatial constraints, effectively utilizing diverse function calls, and the tendency to hallucinate less-used geospatial libraries. To tackle these problems, we introduce GeoAgent, a new interactive framework designed to help LLMs handle geospatial data processing more effectively. GeoAgent pioneers the integration of a code interpreter, static analysis, and Retrieval-Augmented Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm, offering a novel approach to geospatial data processing. In addition, we contribute a new benchmark specifically designed to evaluate the LLM-based approach in geospatial tasks. This benchmark leverages a variety of Python libraries and includes both single-turn and multi-turn tasks such as data acquisition, data analysis, and visualization. By offering a comprehensive evaluation among diverse geospatial contexts, this benchmark sets a new standard for developing LLM-based approaches in geospatial data analysis tasks. Our findings suggest that relying solely on knowledge of LLM is insufficient for accurate geospatial task programming, which requires coherent multi-step processes and multiple function calls. Compared to the baseline LLMs, the proposed GeoAgent has demonstrated superior performance, yielding notable improvements in function calls and task completion. In addition, these results offer valuable insights for the future development of LLM agents in automatic geospatial data analysis task programming.", "source": "arxiv", "arxiv_id": "2410.18792v2", "pdf_url": "https://arxiv.org/pdf/2410.18792v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-10-24T14:47:25Z", "updated": "2024-10-25T09:00:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AnalogCoder: Analog Circuit Design via Training-Free Code Generation", "authors": ["Yao Lai", "Sungyoung Lee", "Guojin Chen", "Souradip Poddar", "Mengkang Hu", "David Z. Pan", "Ping Luo"], "year": 2024, "url": "http://arxiv.org/abs/2405.14918v2", "abstract": "Analog circuit design is a significant task in modern chip technology, focusing on the selection of component types, connectivity, and parameters to ensure proper circuit functionality. Despite advances made by Large Language Models (LLMs) in digital circuit design, the complexity and scarcity of data in analog circuitry pose significant challenges. To mitigate these issues, we introduce AnalogCoder, the first training-free LLM agent for designing analog circuits through Python code generation. Firstly, AnalogCoder incorporates a feedback-enhanced flow with tailored domain-specific prompts, enabling the automated and self-correcting design of analog circuits with a high success rate. Secondly, it proposes a circuit tool library to archive successful designs as reusable modular sub-circuits, simplifying composite circuit creation. Thirdly, extensive experiments on a benchmark designed to cover a wide range of analog circuit tasks show that AnalogCoder outperforms other LLM-based methods. It has successfully designed 20 circuits, 5 more than standard GPT-4o. We believe AnalogCoder can significantly improve the labor-intensive chip design process, enabling non-experts to design analog circuits efficiently.", "source": "arxiv", "arxiv_id": "2405.14918v2", "pdf_url": "https://arxiv.org/pdf/2405.14918v2", "categories": ["cs.LG", "cs.ET"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-05-23T17:13:52Z", "updated": "2024-05-30T16:04:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls", "authors": ["Yu Du", "Fangyun Wei", "Hongyang Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.04253v1", "abstract": "We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.", "source": "arxiv", "arxiv_id": "2402.04253v1", "pdf_url": "https://arxiv.org/pdf/2402.04253v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-06T18:59:57Z", "updated": "2024-02-06T18:59:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening", "authors": ["Chengguang Gan", "Qinghao Zhang", "Tatsunori Mori"], "year": 2024, "url": "http://arxiv.org/abs/2401.08315v2", "abstract": "The automation of resume screening is a crucial aspect of the recruitment process in organizations. Automated resume screening systems often encompass a range of natural language processing (NLP) tasks. This paper introduces a novel Large Language Models (LLMs) based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset. Moreover, it utilizes LLM agents for decision-making. To evaluate our framework, we constructed a dataset from actual resumes and simulated a resume screening process. Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis. The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods. Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73\\%, during the resume sentence classification phase. In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model. Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes.", "source": "arxiv", "arxiv_id": "2401.08315v2", "pdf_url": "https://arxiv.org/pdf/2401.08315v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-16T12:30:56Z", "updated": "2024-08-13T04:50:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Applying Refusal-Vector Ablation to Llama 3.1 70B Agents", "authors": ["Simon Lermen", "Mateusz Dziemian", "Govind Pimpale"], "year": 2024, "url": "http://arxiv.org/abs/2410.10871v1", "abstract": "Recently, language models like Llama 3.1 Instruct have become increasingly capable of agentic behavior, enabling them to perform tasks requiring short-term planning and tool use. In this study, we apply refusal-vector ablation to Llama 3.1 70B and implement a simple agent scaffolding to create an unrestricted agent. Our findings imply that these refusal-vector ablated models can successfully complete harmful tasks, such as bribing officials or crafting phishing attacks, revealing significant vulnerabilities in current safety mechanisms. To further explore this, we introduce a small Safe Agent Benchmark, designed to test both harmful and benign tasks in agentic scenarios. Our results imply that safety fine-tuning in chat models does not generalize well to agentic behavior, as we find that Llama 3.1 Instruct models are willing to perform most harmful tasks without modifications. At the same time, these models will refuse to give advice on how to perform the same tasks when asked for a chat completion. This highlights the growing risk of misuse as models become more capable, underscoring the need for improved safety frameworks for language model agents.", "source": "arxiv", "arxiv_id": "2410.10871v1", "pdf_url": "https://arxiv.org/pdf/2410.10871v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-08T13:42:36Z", "updated": "2024-10-08T13:42:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL", "authors": ["Yifei Zhou", "Andrea Zanette", "Jiayi Pan", "Sergey Levine", "Aviral Kumar"], "year": 2024, "url": "http://arxiv.org/abs/2402.19446v1", "abstract": "A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or \"agent\" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).", "source": "arxiv", "arxiv_id": "2402.19446v1", "pdf_url": "https://arxiv.org/pdf/2402.19446v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-29T18:45:56Z", "updated": "2024-02-29T18:45:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents", "authors": ["Petr Anokhin", "Nikita Semenov", "Artyom Sorokin", "Dmitry Evseev", "Andrey Kravchenko", "Mikhail Burtsev", "Evgeny Burnaev"], "year": 2024, "url": "http://arxiv.org/abs/2407.04363v3", "abstract": "Advancements in the capabilities of Large Language Models (LLMs) have created a promising foundation for developing autonomous agents. With the right tools, these agents could learn to solve tasks in new environments by accumulating and updating their knowledge. Current LLM-based agents process past experiences using a full history of observations, summarization, retrieval augmentation. However, these unstructured memory representations do not facilitate the reasoning and planning essential for complex decision-making. In our study, we introduce AriGraph, a novel method wherein the agent constructs and updates a memory graph that integrates semantic and episodic memories while exploring the environment. We demonstrate that our Ariadne LLM agent, consisting of the proposed memory architecture augmented with planning and decision-making, effectively handles complex tasks within interactive text game environments difficult even for human players. Results show that our approach markedly outperforms other established memory methods and strong RL baselines in a range of problems of varying complexity. Additionally, AriGraph demonstrates competitive performance compared to dedicated knowledge graph-based methods in static multi-hop question-answering.", "source": "arxiv", "arxiv_id": "2407.04363v3", "pdf_url": "https://arxiv.org/pdf/2407.04363v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-05T09:06:47Z", "updated": "2025-05-15T10:57:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory", "authors": ["Gordon Dai", "Weijia Zhang", "Jinhan Li", "Siqi Yang", "Chidera Onochie lbe", "Srihas Rao", "Arthur Caetano", "Misha Sra"], "year": 2024, "url": "http://arxiv.org/abs/2406.14373v2", "abstract": "The emergence of Large Language Models (LLMs) and advancements in Artificial Intelligence (AI) offer an opportunity for computational social science research at scale. Building upon prior explorations of LLM agent design, our work introduces a simulated agent society where complex social relationships dynamically form and evolve over time. Agents are imbued with psychological drives and placed in a sandbox survival environment. We conduct an evaluation of the agent society through the lens of Thomas Hobbes's seminal Social Contract Theory (SCT). We analyze whether, as the theory postulates, agents seek to escape a brutish \"state of nature\" by surrendering rights to an absolute sovereign in exchange for order and security. Our experiments unveil an alignment: Initially, agents engage in unrestrained conflict, mirroring Hobbes's depiction of the state of nature. However, as the simulation progresses, social contracts emerge, leading to the authorization of an absolute sovereign and the establishment of a peaceful commonwealth founded on mutual cooperation. This congruence between our LLM agent society's evolutionary trajectory and Hobbes's theoretical account indicates LLMs' capability to model intricate social dynamics and potentially replicate forces that shape human societies. By enabling such insights into group behavior and emergent societal phenomena, LLM-driven multi-agent simulations, while unable to simulate all the nuances of human behavior, may hold potential for advancing our understanding of social structures, group dynamics, and complex human systems.", "source": "arxiv", "arxiv_id": "2406.14373v2", "pdf_url": "https://arxiv.org/pdf/2406.14373v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-20T14:42:58Z", "updated": "2024-07-01T22:06:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Assessing the Performance of Human-Capable LLMs -- Are LLMs Coming for Your Job?", "authors": ["John Mavi", "Nathan Summers", "Sergio Coronado"], "year": 2024, "url": "http://arxiv.org/abs/2410.16285v1", "abstract": "The current paper presents the development and validation of SelfScore, a novel benchmark designed to assess the performance of automated Large Language Model (LLM) agents on help desk and professional consultation tasks. Given the increasing integration of AI in industries, particularly within customer service, SelfScore fills a crucial gap by enabling the comparison of automated agents and human workers. The benchmark evaluates agents on problem complexity and response helpfulness, ensuring transparency and simplicity in its scoring system. The study also develops automated LLM agents to assess SelfScore and explores the benefits of Retrieval-Augmented Generation (RAG) for domain-specific tasks, demonstrating that automated LLM agents incorporating RAG outperform those without. All automated LLM agents were observed to perform better than the human control group. Given these results, the study raises concerns about the potential displacement of human workers, especially in areas where AI technologies excel. Ultimately, SelfScore provides a foundational tool for understanding the impact of AI in help desk environments while advocating for ethical considerations in the ongoing transition towards automation.", "source": "arxiv", "arxiv_id": "2410.16285v1", "pdf_url": "https://arxiv.org/pdf/2410.16285v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-10-05T14:37:35Z", "updated": "2024-10-05T14:37:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AssistantX: An LLM-Powered Proactive Assistant in Collaborative Human-Populated Environment", "authors": ["Nan Sun", "Bo Mao", "Yongchang Li", "Di Guo", "Huaping Liu"], "year": 2024, "url": "http://arxiv.org/abs/2409.17655v3", "abstract": "Current service robots suffer from limited natural language communication abilities, heavy reliance on predefined commands, ongoing human intervention, and, most notably, a lack of proactive collaboration awareness in human-populated environments. This results in narrow applicability and low utility. In this paper, we introduce AssistantX, an LLM-powered proactive assistant designed for autonomous operation in realworld scenarios with high accuracy. AssistantX employs a multi-agent framework consisting of 4 specialized LLM agents, each dedicated to perception, planning, decision-making, and reflective review, facilitating advanced inference capabilities and comprehensive collaboration awareness, much like a human assistant by your side. We built a dataset of 210 real-world tasks to validate AssistantX, which includes instruction content and status information on whether relevant personnel are available. Extensive experiments were conducted in both text-based simulations and a real office environment over the course of a month and a half. Our experiments demonstrate the effectiveness of the proposed framework, showing that AssistantX can reactively respond to user instructions, actively adjust strategies to adapt to contingencies, and proactively seek assistance from humans to ensure successful task completion. More details and videos can be found at https://assistantx-agent.github.io/AssistantX/.", "source": "arxiv", "arxiv_id": "2409.17655v3", "pdf_url": "https://arxiv.org/pdf/2409.17655v3", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO", "doi": "10.1109/IROS60139.2025.11246901", "venue": "2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Hangzhou, China, 2025", "published": "2024-09-26T09:06:56Z", "updated": "2025-06-19T07:49:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues", "authors": ["Yuncheng Hua", "Lizhen Qu", "Gholamreza Haffari"], "year": 2024, "url": "http://arxiv.org/abs/2402.01737v3", "abstract": "We develop assistive agents based on Large Language Models (LLMs) that aid interlocutors in business negotiations. Specifically, we simulate business negotiations by letting two LLM-based agents engage in role play. A third LLM acts as a remediator agent to rewrite utterances violating norms for improving negotiation outcomes. We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator, where we propose a novel select criteria, called value impact, to measure the quality of the negotiation outcomes. We provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different negotiation topics. We have released our source code and the generated dataset at: https://github.com/tk1363704/SADAS.", "source": "arxiv", "arxiv_id": "2402.01737v3", "pdf_url": "https://arxiv.org/pdf/2402.01737v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.18653/v1/2024.findings-emnlp.473", "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024", "published": "2024-01-29T09:07:40Z", "updated": "2025-02-17T08:44:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Asynchronous Tool Usage for Real-Time Agents", "authors": ["Antonio A. Ginart", "Naveen Kodali", "Jason Lee", "Caiming Xiong", "Silvio Savarese", "John Emmons"], "year": 2024, "url": "http://arxiv.org/abs/2410.21620v1", "abstract": "While frontier large language models (LLMs) are capable tool-using agents, current AI systems still operate in a strict turn-based fashion, oblivious to passage of time. This synchronous design forces user queries and tool-use to occur sequentially, preventing the systems from multitasking and reducing interactivity. To address this limitation, we introduce asynchronous AI agents capable of parallel processing and real-time tool-use. Our key contribution is an event-driven finite-state machine architecture for agent execution and prompting, integrated with automatic speech recognition and text-to-speech. Drawing inspiration from the concepts originally developed for real-time operating systems, this work presents both a conceptual framework and practical tools for creating AI agents capable of fluid, multitasking interactions.", "source": "arxiv", "arxiv_id": "2410.21620v1", "pdf_url": "https://arxiv.org/pdf/2410.21620v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-28T23:57:19Z", "updated": "2024-10-28T23:57:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents", "authors": ["Jaekyeom Kim", "Dong-Ki Kim", "Lajanugen Logeswaran", "Sungryull Sohn", "Honglak Lee"], "year": 2024, "url": "http://arxiv.org/abs/2410.22552v1", "abstract": "In this paper, we introduce Auto-Intent, a method to adapt a pre-trained large language model (LLM) as an agent for a target domain without direct fine-tuning, where we empirically focus on web navigation tasks. Our approach first discovers the underlying intents from target domain demonstrations unsupervisedly, in a highly compact form (up to three words). With the extracted intents, we train our intent predictor to predict the next intent given the agent's past observations and actions. In particular, we propose a self-exploration approach where top-k probable intent predictions are provided as a hint to the pre-trained LLM agent, which leads to enhanced decision-making capabilities. Auto-Intent substantially improves the performance of GPT-{3.5, 4} and Llama-3.1-{70B, 405B} agents on the large-scale real-website navigation benchmarks from Mind2Web and online navigation tasks from WebArena with its cross-benchmark generalization from Mind2Web.", "source": "arxiv", "arxiv_id": "2410.22552v1", "pdf_url": "https://arxiv.org/pdf/2410.22552v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-29T21:37:04Z", "updated": "2024-10-29T21:37:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoCodeRover: Autonomous Program Improvement", "authors": ["Yuntong Zhang", "Haifeng Ruan", "Zhiyu Fan", "Abhik Roychoudhury"], "year": 2024, "url": "http://arxiv.org/abs/2404.05427v3", "abstract": "Researchers have made significant progress in automating the software development process in the past decades. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. bug fixing) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving GitHub issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM's understanding of the issue's root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on SWE-bench-lite (300 real-life GitHub issues) show increased efficacy in solving GitHub issues (19% on SWE-bench-lite), which is higher than the efficacy of the recently reported SWE-agent. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.", "source": "arxiv", "arxiv_id": "2404.05427v3", "pdf_url": "https://arxiv.org/pdf/2404.05427v3", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-04-08T11:55:09Z", "updated": "2024-07-25T16:54:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks", "authors": ["Yifan Zeng", "Yiran Wu", "Xiao Zhang", "Huazheng Wang", "Qingyun Wu"], "year": 2024, "url": "http://arxiv.org/abs/2403.04783v2", "abstract": "Despite extensive pre-training in moral alignment to prevent generating harmful information, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense framework that filters harmful responses from LLMs. With the response-filtering mechanism, our framework is robust against different jailbreak attack prompts, and can be used to defend different victim models. AutoDefense assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. With AutoDefense, small open-source LMs can serve as agents and defend larger models against jailbreak attacks. Our experiments show that AutoDefense can effectively defense against different jailbreak attacks, while maintaining the performance at normal user request. For example, we reduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using LLaMA-2-13b with a 3-agent system. Our code and data are publicly available at https://github.com/XHMY/AutoDefense.", "source": "arxiv", "arxiv_id": "2403.04783v2", "pdf_url": "https://arxiv.org/pdf/2403.04783v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-03-02T16:52:22Z", "updated": "2024-11-14T18:14:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoFlow: Automated Workflow Generation for Large Language Model Agents", "authors": ["Zelong Li", "Shuyuan Xu", "Kai Mei", "Wenyue Hua", "Balaji Rama", "Om Raheja", "Hao Wang", "He Zhu", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2407.12821v1", "abstract": "Recent advancements in Large Language Models (LLMs) have shown significant progress in understanding complex natural language. One important application of LLM is LLM-based AI Agent, which leverages the ability of LLM as well as external tools for complex-task solving. To make sure LLM Agents follow an effective and reliable procedure to solve the given task, manually designed workflows are usually used to guide the working mechanism of agents. However, manually designing the workflows requires considerable efforts and domain knowledge, making it difficult to develop and deploy agents on massive scales. To address these issues, we propose AutoFlow, a framework designed to automatically generate workflows for agents to solve complex tasks. AutoFlow takes natural language program as the format of agent workflow and employs a workflow optimization procedure to iteratively optimize the workflow quality. Besides, this work offers two workflow generation methods: fine-tuning-based and in-context-based methods, making the AutoFlow framework applicable to both open-source and closed-source LLMs. Experimental results show that our framework can produce robust and reliable agent workflows. We believe that the automatic generation and interpretation of workflows in natural language represent a promising paradigm for solving complex tasks, particularly with the rapid development of LLMs. The source code of this work is available at https://github.com/agiresearch/AutoFlow.", "source": "arxiv", "arxiv_id": "2407.12821v1", "pdf_url": "https://arxiv.org/pdf/2407.12821v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-01T21:05:02Z", "updated": "2024-07-01T21:05:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents", "authors": ["Yao Fu", "Dong-Ki Kim", "Jaekyeom Kim", "Sungryull Sohn", "Lajanugen Logeswaran", "Kyunghoon Bae", "Honglak Lee"], "year": 2024, "url": "http://arxiv.org/abs/2403.08978v2", "abstract": "Recent advances in large language models (LLMs) have empowered AI agents capable of performing various sequential decision-making tasks. However, effectively guiding LLMs to perform well in unfamiliar domains like web navigation, where they lack sufficient knowledge, has proven to be difficult with the demonstration-based in-context learning paradigm. In this paper, we introduce a novel framework, called AutoGuide, which addresses this limitation by automatically generating context-aware guidelines from offline experiences. Importantly, each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context where it is applicable. As a result, our guidelines facilitate the provision of relevant knowledge for the agent's current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm. Our evaluation demonstrates that AutoGuide significantly outperforms competitive baselines in complex benchmark domains, including real-world web navigation.", "source": "arxiv", "arxiv_id": "2403.08978v2", "pdf_url": "https://arxiv.org/pdf/2403.08978v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-13T22:06:03Z", "updated": "2024-12-03T07:36:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning", "authors": ["Minghao Chen", "Yihang Li", "Yanting Yang", "Shiyu Yu", "Binbin Lin", "Xiaofei He"], "year": 2024, "url": "http://arxiv.org/abs/2405.16247v4", "abstract": "Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce a *case-conditioned prompting* strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at https://github.com/minghchen/automanual.", "source": "arxiv", "arxiv_id": "2405.16247v4", "pdf_url": "https://arxiv.org/pdf/2405.16247v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-25T14:11:44Z", "updated": "2024-11-10T12:54:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AutoVerus: Automated Proof Generation for Rust Code", "authors": ["Chenyuan Yang", "Xuheng Li", "Md Rakib Hossain Misu", "Jianan Yao", "Weidong Cui", "Yeyun Gong", "Chris Hawblitzel", "Shuvendu Lahiri", "Jacob R. Lorch", "Shuai Lu", "Fan Yang", "Ziqiao Zhou", "Shan Lu"], "year": 2024, "url": "http://arxiv.org/abs/2409.13082v3", "abstract": "Generative AI has shown its values for many software engineering tasks. Still in its infancy, large language model (LLM)-based proof generation lags behind LLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses LLMs to automatically generate correctness proof for Rust code. AutoVerus is designed to match the unique features of Verus, a verification tool that can prove the correctness of Rust code using proofs and specifications also written in Rust. AutoVerus consists of a network of LLM agents that are crafted and orchestrated to mimic human experts' three phases of proof construction: preliminary proof generation, proof refinement guided by generic tips, and proof debugging guided by verification errors. To thoroughly evaluate AutoVerus and help foster future research in this direction, we have built a benchmark suite of 150 non-trivial proof tasks, based on existing code-generation benchmarks and verification benchmarks. Our evaluation shows that AutoVerus can automatically generate correct proof for more than 90% of them, with more than half of them tackled in less than 30 seconds or 3 LLM calls.", "source": "arxiv", "arxiv_id": "2409.13082v3", "pdf_url": "https://arxiv.org/pdf/2409.13082v3", "categories": ["cs.SE", "cs.AI", "cs.FL"], "primary_category": "cs.SE", "doi": "10.1145/3763174", "venue": "", "published": "2024-09-19T20:40:52Z", "updated": "2025-08-22T16:12:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Automated Phishing Detection Using URLs and Webpages", "authors": ["Huilin Wang", "Bryan Hooi"], "year": 2024, "url": "http://arxiv.org/abs/2408.01667v2", "abstract": "Phishing detection is a critical cybersecurity task that involves the identification and neutralization of fraudulent attempts to obtain sensitive information, thereby safeguarding individuals and organizations from data breaches and financial loss. In this project, we address the constraints of traditional reference-based phishing detection by developing an LLM agent framework. This agent harnesses Large Language Models to actively fetch and utilize online information, thus providing a dynamic reference system for more accurate phishing detection. This innovation circumvents the need for a static knowledge base, offering a significant enhancement in adaptability and efficiency for automated security measures.\n  The project report includes an initial study and problem analysis of existing solutions, which motivated us to develop a new framework. We demonstrate the framework with LLMs simulated as agents and detail the techniques required for construction, followed by a complete implementation with a proof-of-concept as well as experiments to evaluate our solution's performance against other similar solutions. The results show that our approach has achieved with accuracy of 0.945, significantly outperforms the existing solution(DynaPhish) by 0.445. Furthermore, we discuss the limitations of our approach and suggest improvements that could make it more effective.\n  Overall, the proposed framework has the potential to enhance the effectiveness of current reference-based phishing detection approaches and could be adapted for real-world applications.", "source": "arxiv", "arxiv_id": "2408.01667v2", "pdf_url": "https://arxiv.org/pdf/2408.01667v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-08-03T05:08:27Z", "updated": "2024-08-16T03:25:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Autonomous Microscopy Experiments through Large Language Model Agents", "authors": ["Indrajeet Mandal", "Jitendra Soni", "Mohd Zaki", "Morten M. Smedskjaer", "Katrin Wondraczek", "Lothar Wondraczek", "Nitya Nand Gosvami", "N. M. Anoop Krishnan"], "year": 2024, "url": "http://arxiv.org/abs/2501.10385v2", "abstract": "Large language models (LLMs) are revolutionizing self driving laboratories (SDLs) for materials research, promising unprecedented acceleration of scientific discovery. However, current SDL implementations rely on rigid protocols that fail to capture the adaptability and intuition of expert scientists in dynamic experimental settings. We introduce Artificially Intelligent Lab Assistant (AILA), a framework automating atomic force microscopy through LLM driven agents. Further, we develop AFMBench a comprehensive evaluation suite challenging AI agents across the complete scientific workflow from experimental design to results analysis. We find that state of the art models struggle with basic tasks and coordination scenarios. Notably, Claude 3.5 sonnet performs unexpectedly poorly despite excelling in materials domain question answering (QA) benchmarks, revealing that domain specific QA proficiency does not necessarily translate to effective agentic capabilities. Additionally, we observe that LLMs can deviate from instructions, raising safety alignment concerns for SDL applications. Our ablations reveal that multi agent frameworks outperform single-agent architectures. We also observe significant prompt fragility, where slight modifications in prompt structure cause substantial performance variations in capable models like GPT 4o. Finally, we evaluate AILA's effectiveness in increasingly advanced experiments AFM calibration, feature detection, mechanical property measurement, graphene layer counting, and indenter detection. Our findings underscore the necessity for rigorous benchmarking protocols and prompt engineering strategies before deploying AI laboratory assistants in scientific research environments.", "source": "arxiv", "arxiv_id": "2501.10385v2", "pdf_url": "https://arxiv.org/pdf/2501.10385v2", "categories": ["cs.CY", "cond-mat.mtrl-sci", "cs.AI", "physics.ins-det"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-12-18T09:35:28Z", "updated": "2025-07-07T13:21:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning", "authors": ["Shirley Wu", "Shiyu Zhao", "Qian Huang", "Kexin Huang", "Michihiro Yasunaga", "Kaidi Cao", "Vassilis N. Ioannidis", "Karthik Subbian", "Jure Leskovec", "James Zou"], "year": 2024, "url": "http://arxiv.org/abs/2406.11200v3", "abstract": "Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.", "source": "arxiv", "arxiv_id": "2406.11200v3", "pdf_url": "https://arxiv.org/pdf/2406.11200v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-17T04:20:02Z", "updated": "2024-10-31T10:15:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards", "authors": ["Diego Dorn", "Alexandre Variengien", "Charbel-RaphaÃ«l Segerie", "Vincent Corruble"], "year": 2024, "url": "http://arxiv.org/abs/2406.01364v1", "abstract": "Input-output safeguards are used to detect anomalies in the traces produced by Large Language Models (LLMs) systems. These detectors are at the core of diverse safety-critical applications such as real-time monitoring, offline evaluation of traces, and content moderation. However, there is no widely recognized methodology to evaluate them. To fill this gap, we introduce the Benchmarks for the Evaluation of LLM Safeguards (BELLS), a structured collection of tests, organized into three categories: (1) established failure tests, based on already-existing benchmarks for well-defined failure modes, aiming to compare the performance of current input-output safeguards; (2) emerging failure tests, to measure generalization to never-seen-before failure modes and encourage the development of more general safeguards; (3) next-gen architecture tests, for more complex scaffolding (such as LLM-agents and multi-agent systems), aiming to foster the development of safeguards that could adapt to future applications for which no safeguard currently exists. Furthermore, we implement and share the first next-gen architecture test, using the MACHIAVELLI environment, along with an interactive visualization of the dataset.", "source": "arxiv", "arxiv_id": "2406.01364v1", "pdf_url": "https://arxiv.org/pdf/2406.01364v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-06-03T14:32:30Z", "updated": "2024-06-03T14:32:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BLADE: Benchmarking Language Model Agents for Data-Driven Science", "authors": ["Ken Gu", "Ruoxi Shang", "Ruien Jiang", "Keying Kuang", "Richard-John Lin", "Donghe Lyu", "Yue Mao", "Youran Pan", "Teng Wu", "Jiaqian Yu", "Yikun Zhang", "Tianmai M. Zhang", "Lanyi Zhu", "Mike A. Merrill", "Jeffrey Heer", "Tim Althoff"], "year": 2024, "url": "http://arxiv.org/abs/2408.09667v3", "abstract": "Data-driven scientific discovery requires the iterative integration of scientific domain knowledge, statistical expertise, and an understanding of data semantics to make nuanced analytical decisions, e.g., about which variables, transformations, and statistical models to consider. LM-based agents equipped with planning, memory, and code execution capabilities have the potential to support data-driven science. However, evaluating agents on such open-ended tasks is challenging due to multiple valid approaches, partially correct steps, and different ways to express the same decisions. To address these challenges, we present BLADE, a benchmark to automatically evaluate agents' multifaceted approaches to open-ended research questions. BLADE consists of 12 datasets and research questions drawn from existing scientific literature, with ground truth collected from independent analyses by expert data scientists and researchers. To automatically evaluate agent responses, we developed corresponding computational methods to match different representations of analyses to this ground truth. Though language models possess considerable world knowledge, our evaluation shows that they are often limited to basic analyses. However, agents capable of interacting with the underlying data demonstrate improved, but still non-optimal, diversity in their analytical decision making. Our work enables the evaluation of agents for data-driven science and provides researchers deeper insights into agents' analysis approaches.", "source": "arxiv", "arxiv_id": "2408.09667v3", "pdf_url": "https://arxiv.org/pdf/2408.09667v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-19T02:59:35Z", "updated": "2025-11-10T06:38:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents", "authors": ["Yifei Wang", "Dizhan Xue", "Shengjie Zhang", "Shengsheng Qian"], "year": 2024, "url": "http://arxiv.org/abs/2406.03007v1", "abstract": "With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent", "source": "arxiv", "arxiv_id": "2406.03007v1", "pdf_url": "https://arxiv.org/pdf/2406.03007v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-05T07:14:28Z", "updated": "2024-06-05T07:14:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BayesAgent: Bayesian Agentic Reasoning Under Uncertainty via Verbalized Probabilistic Graphical Modeling", "authors": ["Hengguan Huang", "Xing Shen", "Songtao Wang", "Lingfa Meng", "Dianbo Liu", "David Alejandro Duchene", "Hao Wang", "Samir Bhatt"], "year": 2024, "url": "http://arxiv.org/abs/2406.05516v4", "abstract": "Human cognition excels at transcending sensory input and forming latent representations that structure our understanding of the world. While Large Language Model (LLM) agents demonstrate emergent reasoning and decision-making abilities, they lack a principled framework for capturing latent structures and modeling uncertainty. In this work, we explore for the first time how to bridge LLM agents with probabilistic graphical models (PGMs) to address agentic reasoning under uncertainty. To this end, we introduce Verbalized Probabilistic Graphical Modeling (vPGM), a Bayesian agentic framework that (i) guides LLM agents in following key principles of PGMs through natural language and (ii) refines the resulting posterior distributions via numerical Bayesian inference. Unlike many traditional probabilistic methods requiring substantial domain expertise, vPGM bypasses expert-driven model design, making it well-suited for scenarios with limited assumptions. We evaluated our model on several agentic reasoning tasks, both close-ended and open-ended. Our results indicate that the model effectively enhances confidence calibration and text generation quality.", "source": "arxiv", "arxiv_id": "2406.05516v4", "pdf_url": "https://arxiv.org/pdf/2406.05516v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-08T16:35:31Z", "updated": "2026-01-21T01:18:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles", "authors": ["Xuchuan Li", "Fei Huang", "Jianrong Lv", "Zhixiong Xiao", "Guolong Li", "Yang Yue"], "year": 2024, "url": "http://arxiv.org/abs/2407.18932v2", "abstract": "Human mobility is inextricably linked to social issues such as traffic congestion, energy consumption, and public health; however, privacy concerns restrict access to mobility data. Recently, research have utilized Large Language Models (LLMs) for human mobility generation, in which the challenge is how LLMs can understand individuals' mobility behavioral differences to generate realistic trajectories conforming to real world contexts. This study handles this problem by presenting an LLM agent-based framework (MobAgent) composing two phases: understanding-based mobility pattern extraction and reasoning-based trajectory generation, which enables generate more real travel diaries at urban scale, considering different individual profiles. MobAgent extracts reasons behind specific mobility trendiness and attribute influences to provide reliable patterns; infers the relationships between contextual factors and underlying motivations of mobility; and based on the patterns and the recursive reasoning process, MobAgent finally generates more authentic and personalized mobilities that reflect both individual differences and real-world constraints. We validate our framework with 0.2 million travel survey data, demonstrating its effectiveness in producing personalized and accurate travel diaries. This study highlights the capacity of LLMs to provide detailed and sophisticated understanding of human mobility through the real-world mobility data.", "source": "arxiv", "arxiv_id": "2407.18932v2", "pdf_url": "https://arxiv.org/pdf/2407.18932v2", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-07-10T09:11:57Z", "updated": "2024-08-05T15:59:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Behavior Trees Enable Structured Programming of Language Model Agents", "authors": ["Richard Kelley"], "year": 2024, "url": "http://arxiv.org/abs/2404.07439v1", "abstract": "Language models trained on internet-scale data sets have shown an impressive ability to solve problems in Natural Language Processing and Computer Vision. However, experience is showing that these models are frequently brittle in unexpected ways, and require significant scaffolding to ensure that they operate correctly in the larger systems that comprise \"language-model agents.\" In this paper, we argue that behavior trees provide a unifying framework for combining language models with classical AI and traditional programming. We introduce Dendron, a Python library for programming language model agents using behavior trees. We demonstrate the approach embodied by Dendron in three case studies: building a chat agent, a camera-based infrastructure inspection agent for use on a mobile robot or vehicle, and an agent that has been built to satisfy safety constraints that it did not receive through instruction tuning or RLHF.", "source": "arxiv", "arxiv_id": "2404.07439v1", "pdf_url": "https://arxiv.org/pdf/2404.07439v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-11T02:44:13Z", "updated": "2024-04-11T02:44:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "BenchAgents: Multi-Agent Systems for Structured Benchmark Creation", "authors": ["Natasha Butt", "Varun Chandrasekaran", "Neel Joshi", "Besmira Nushi", "Vidhisha Balachandran"], "year": 2024, "url": "http://arxiv.org/abs/2410.22584v2", "abstract": "Evaluation insights are limited by the availability of high-quality benchmarks. As models evolve, there is a need to create benchmarks that can measure progress on new and complex generative capabilities. However, manually creating new benchmarks is slow and expensive, restricting comprehensive evaluations for any capability. We introduce BenchAgents, a multi-agent framework that methodically leverages large language models (LLMs) to automate evaluation benchmark creation while inherently ensuring data and (evaluation) metric quality. BenchAgents decomposes the benchmark creation process into planning, generation, verification, and evaluation, each of which is ] orchestrated via LLM agents. These agents interact with each other and utilize feedback from benchmark developers to improve and flexibly control data diversity and quality. We use BenchAgents to create benchmarks to evaluate capabilities related to planning, constraint satisfaction, and causal reasoning spanning both language and vision modalities. We then use these benchmarks to study state-of-the-art models and extract new insights into common failure modes and model differences.", "source": "arxiv", "arxiv_id": "2410.22584v2", "pdf_url": "https://arxiv.org/pdf/2410.22584v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-29T22:56:18Z", "updated": "2025-10-07T07:17:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Benchmarking Agentic Workflow Generation", "authors": ["Shuofei Qiao", "Runnan Fang", "Zhisong Qiu", "Xiaobin Wang", "Ningyu Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "year": 2024, "url": "http://arxiv.org/abs/2410.07869v3", "abstract": "Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorfBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorfEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference. Code and dataset are available at https://github.com/zjunlp/WorfBench.", "source": "arxiv", "arxiv_id": "2410.07869v3", "pdf_url": "https://arxiv.org/pdf/2410.07869v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-10T12:41:19Z", "updated": "2025-02-23T15:16:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback", "authors": ["Sanjiban Choudhury", "Paloma Sodhi"], "year": 2024, "url": "http://arxiv.org/abs/2410.05434v1", "abstract": "While large language models (LLMs) show impressive decision-making abilities, current methods lack a mechanism for automatic self-improvement from errors during task execution. We propose LEAP, an iterative fine-tuning framework that continually improves LLM agents using feedback from AI expert teachers. Our key insight is to equip the expert teachers with a privileged state -- information that is available during training but hidden at test time. This allows even weak experts to provide precise guidance, significantly improving the student agent's performance without access to privileged information at test time. We evaluate LEAP on diverse decision-making benchmarks, including text-based games (ALFWorld), web navigation (WebShop), and interactive coding (Intercode Bash). Our experiments show that LEAP (1) outperforms behavior cloning and ReAct baselines (2) enables weak student models (e.g., Llama3-8B) to exceed the performance of strong teacher models (GPT4-o), and (3) allows weak models to self-improve using privileged versions of themselves. We also provide a theoretical analysis showing that LEAP's success hinges on balancing privileged information with the student's realizability, which we empirically validate. Our code is available at https://leap-llm.github.io", "source": "arxiv", "arxiv_id": "2410.05434v1", "pdf_url": "https://arxiv.org/pdf/2410.05434v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-07T18:55:53Z", "updated": "2024-10-07T18:55:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks", "authors": ["Yun-Shiuan Chuang", "Krirk Nirunwiroj", "Zach Studdiford", "Agam Goyal", "Vincent V. Frigo", "Sijia Yang", "Dhavan Shah", "Junjie Hu", "Timothy T. Rogers"], "year": 2024, "url": "http://arxiv.org/abs/2406.17232v2", "abstract": "Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 64 topics loading on nine non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.", "source": "arxiv", "arxiv_id": "2406.17232v2", "pdf_url": "https://arxiv.org/pdf/2406.17232v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "Findings of the Association for Computational Linguistics (ACL): EMNLP 2024", "published": "2024-06-25T02:37:29Z", "updated": "2024-10-16T04:36:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond Numeric Rewards: In-Context Dueling Bandits with LLM Agents", "authors": ["Fanzeng Xia", "Hao Liu", "Yisong Yue", "Tongxin Li"], "year": 2024, "url": "http://arxiv.org/abs/2407.01887v4", "abstract": "In-Context Reinforcement Learning (ICRL) is a frontier paradigm to solve Reinforcement Learning (RL) problems in the foundation model era. While ICRL capabilities have been demonstrated in transformers through task-specific training, the potential of Large Language Models (LLMs) out-of-the-box remains largely unexplored. This paper investigates whether LLMs can generalize cross-domain to perform ICRL under the problem of Dueling Bandits (DB), a stateless preference-based RL setting. We find that the top-performing LLMs exhibit a notable zero-shot capacity for relative decision-making, which translates to low short-term weak regret across all DB environment instances by quickly including the best arm in duels. However, an optimality gap still exists between LLMs and classic DB algorithms in terms of strong regret. LLMs struggle to converge and consistently exploit even when explicitly prompted to do so, and are sensitive to prompt variations. To bridge this gap, we propose an agentic flow framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates off-the-shelf DB algorithm support with LLM agents through fine-grained adaptive interplay. We show that LEAD has theoretical guarantees inherited from classic DB algorithms on both weak and strong regret. We validate its efficacy and robustness even with noisy and adversarial prompts. The design of such an agentic framework sheds light on how to enhance the trustworthiness of general-purpose LLMs generalized to in-context decision-making tasks.", "source": "arxiv", "arxiv_id": "2407.01887v4", "pdf_url": "https://arxiv.org/pdf/2407.01887v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-07-02T02:18:14Z", "updated": "2025-06-09T14:56:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond pip install: Evaluating LLM Agents for the Automated Installation of Python Projects", "authors": ["Louis Milliken", "Sungmin Kang", "Shin Yoo"], "year": 2024, "url": "http://arxiv.org/abs/2412.06294v1", "abstract": "Many works have recently proposed the use of Large Language Model (LLM) based agents for performing `repository level' tasks, loosely defined as a set of tasks whose scopes are greater than a single file. This has led to speculation that the orchestration of these repository-level tasks could lead to software engineering agents capable of performing almost independently of human intervention. However, of the suite of tasks that would need to be performed by this autonomous software engineering agent, we argue that one important task is missing, which is to fulfil project level dependency by installing other repositories. To investigate the feasibility of this repository level installation task, we introduce a benchmark of of repository installation tasks curated from 40 open source Python projects, which includes a ground truth installation process for each target repository. Further, we propose Installamatic, an agent which aims to perform and verify the installation of a given repository by searching for relevant instructions from documentation in the repository. Empirical experiments reveal that that 55% of the studied repositories can be automatically installed by our agent at least one out of ten times. Through further analysis, we identify the common causes for our agent's inability to install a repository, discuss the challenges faced in the design and implementation of such an agent and consider the implications that such an agent could have for developers.", "source": "arxiv", "arxiv_id": "2412.06294v1", "pdf_url": "https://arxiv.org/pdf/2412.06294v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-12-09T08:37:06Z", "updated": "2024-12-09T08:37:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Beyond-RAG: Question Identification and Answer Generation in Real-Time Conversations", "authors": ["Garima Agrawal", "Sashank Gummuluri", "Cosimo Spera"], "year": 2024, "url": "http://arxiv.org/abs/2410.10136v1", "abstract": "In customer contact centers, human agents often struggle with long average handling times (AHT) due to the need to manually interpret queries and retrieve relevant knowledge base (KB) articles. While retrieval augmented generation (RAG) systems using large language models (LLMs) have been widely adopted in industry to assist with such tasks, RAG faces challenges in real-time conversations, such as inaccurate query formulation and redundant retrieval of frequently asked questions (FAQs). To address these limitations, we propose a decision support system that can look beyond RAG by first identifying customer questions in real time. If the query matches an FAQ, the system retrieves the answer directly from the FAQ database; otherwise, it generates answers via RAG. Our approach reduces reliance on manual queries, providing responses to agents within 2 seconds. Deployed in AI-powered human-agent assist solution at Minerva CQ, this system improves efficiency, reduces AHT, and lowers operational costs. We also introduce an automated LLM-agentic workflow to identify FAQs from historical transcripts when no predefined FAQs exist.", "source": "arxiv", "arxiv_id": "2410.10136v1", "pdf_url": "https://arxiv.org/pdf/2410.10136v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-14T04:06:22Z", "updated": "2024-10-14T04:06:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification", "authors": ["Boyang Zhang", "Yicong Tan", "Yun Shen", "Ahmed Salem", "Michael Backes", "Savvas Zannettou", "Yang Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2407.20859v1", "abstract": "Recently, autonomous agents built on large language models (LLMs) have experienced significant development and are being deployed in real-world applications. These agents can extend the base LLM's capabilities in multiple ways. For example, a well-built agent using GPT-3.5-Turbo as its core can outperform the more advanced GPT-4 model by leveraging external components. More importantly, the usage of tools enables these systems to perform actions in the real world, moving from merely generating text to actively interacting with their environment. Given the agents' practical applications and their ability to execute consequential actions, it is crucial to assess potential vulnerabilities. Such autonomous systems can cause more severe damage than a standalone language model if compromised. While some existing research has explored harmful actions by LLM agents, our study approaches the vulnerability from a different perspective. We introduce a new type of attack that causes malfunctions by misleading the agent into executing repetitive or irrelevant actions. We conduct comprehensive evaluations using various attack methods, surfaces, and properties to pinpoint areas of susceptibility. Our experiments reveal that these attacks can induce failure rates exceeding 80\\% in multiple scenarios. Through attacks on implemented and deployable agents in multi-agent scenarios, we accentuate the realistic risks associated with these vulnerabilities. To mitigate such attacks, we propose self-examination detection methods. However, our findings indicate these attacks are difficult to detect effectively using LLMs alone, highlighting the substantial risks associated with this vulnerability.", "source": "arxiv", "arxiv_id": "2407.20859v1", "pdf_url": "https://arxiv.org/pdf/2407.20859v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-07-30T14:35:31Z", "updated": "2024-07-30T14:35:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ByteComposer: a Human-like Melody Composition Method based on Language Model Agent", "authors": ["Xia Liang", "Xingjian Du", "Jiaju Lin", "Pei Zou", "Yuan Wan", "Bilei Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2402.17785v2", "abstract": "Large Language Models (LLM) have shown encouraging progress in multimodal understanding and generation tasks. However, how to design a human-aligned and interpretable melody composition system is still under-explored. To solve this problem, we propose ByteComposer, an agent framework emulating a human's creative pipeline in four separate steps : \"Conception Analysis - Draft Composition - Self-Evaluation and Modification - Aesthetic Selection\". This framework seamlessly blends the interactive and knowledge-understanding features of LLMs with existing symbolic music generation models, thereby achieving a melody composition agent comparable to human creators. We conduct extensive experiments on GPT4 and several open-source large language models, which substantiate our framework's effectiveness. Furthermore, professional music composers were engaged in multi-dimensional evaluations, the final results demonstrated that across various facets of music composition, ByteComposer agent attains the level of a novice melody composer.", "source": "arxiv", "arxiv_id": "2402.17785v2", "pdf_url": "https://arxiv.org/pdf/2402.17785v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD", "doi": "", "venue": "", "published": "2024-02-24T04:35:07Z", "updated": "2024-03-07T00:32:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs", "authors": ["Jingzhe Shi", "Jialuo Li", "Qinwei Ma", "Zaiwen Yang", "Huan Ma", "Lei Li"], "year": 2024, "url": "http://arxiv.org/abs/2404.01343v4", "abstract": "Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverage a combination of small and large LLMs to achieve satisfying performance at a reasonable inference cost. We introduce a practical dataset, the CPHOS-dataset, which includes a database, guiding files, and QA pairs collected from CPHOS, an online platform that facilitates the organization of simulated Physics Olympiads for high school teachers and students. We have conducted extensive experiments to validate the performance of our proposed CHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how LLMs can enhance or serve as alternatives to human customer service. Code for our proposed architecture and dataset can be found at {https://github.com/JingzheShi/CHOPS}.", "source": "arxiv", "arxiv_id": "2404.01343v4", "pdf_url": "https://arxiv.org/pdf/2404.01343v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-31T07:11:48Z", "updated": "2024-07-17T07:26:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents", "authors": ["Tianqi Xu", "Linyao Chen", "Dai-Jie Wu", "Yanjun Chen", "Zecheng Zhang", "Xiang Yao", "Zhiqiang Xie", "Yongchao Chen", "Shilong Liu", "Bochen Qian", "Anjie Yang", "Zhaoxuan Jin", "Jianbo Deng", "Philip Torr", "Bernard Ghanem", "Guohao Li"], "year": 2024, "url": "http://arxiv.org/abs/2407.01511v4", "abstract": "The development of autonomous agents increasingly relies on Multimodal Language Models (MLMs) to perform tasks described in natural language with GUI environments, such as websites, desktop computers, or mobile phones. Existing benchmarks for MLM agents in interactive environments are limited by their focus on a single environment, lack of detailed and generalized evaluation methods, and the complexities of constructing tasks and evaluators. To overcome these limitations, we introduce Crab, the first agent benchmark framework designed to support cross-environment tasks, incorporating a graph-based fine-grained evaluation method and an efficient mechanism for task and evaluator construction. Our framework supports multiple devices and can be easily extended to any environment with a Python interface. Leveraging Crab, we developed a cross-platform Crab Benchmark-v0 comprising 120 tasks in computer desktop and mobile phone environments. We evaluated four advanced MLMs using different single and multi-agent system configurations on this benchmark. The experimental results demonstrate that the single agent with GPT-4o achieves the best completion ratio of 38.01%. All framework code, agent code, and task datasets are publicly available at https://github.com/camel-ai/crab.", "source": "arxiv", "arxiv_id": "2407.01511v4", "pdf_url": "https://arxiv.org/pdf/2407.01511v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-01T17:55:04Z", "updated": "2025-07-20T13:42:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CRISPR-GPT for Agentic Automation of Gene-editing Experiments", "authors": ["Yuanhao Qu", "Kaixuan Huang", "Ming Yin", "Kanghong Zhan", "Dyllan Liu", "Di Yin", "Henry C. Cousins", "William A. Johnson", "Xiaotong Wang", "Mihir Shah", "Russ B. Altman", "Denny Zhou", "Mengdi Wang", "Le Cong"], "year": 2024, "url": "http://arxiv.org/abs/2404.18021v2", "abstract": "The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agent's effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between beginner biological researchers and CRISPR genome engineering techniques, and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks. The published version of this draft is available at https://www.nature.com/articles/s41551-025-01463-z.", "source": "arxiv", "arxiv_id": "2404.18021v2", "pdf_url": "https://arxiv.org/pdf/2404.18021v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "q-bio.QM"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-27T22:59:17Z", "updated": "2025-08-21T04:15:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Sidharth Dhawan", "Yixin Mao", "Huan Wang", "Silvio Savarese", "Caiming Xiong", "Philippe Laban", "Chien-Sheng Wu"], "year": 2024, "url": "http://arxiv.org/abs/2411.02305v2", "abstract": "Customer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments. Following guidance from CRM experts and industry best practices, we designed CRMArena with nine customer service tasks distributed across three personas: service agent, analyst, and manager. The benchmark includes 16 commonly used industrial objects (e.g., account, order, knowledge article, case) with high interconnectivity, along with latent variables (e.g., complaint habits, policy violations) to simulate realistic data distributions. Experimental results reveal that state-of-the-art LLM agents succeed in less than 40% of the tasks with ReAct prompting, and less than 55% even with function-calling abilities. Our findings highlight the need for enhanced agent capabilities in function-calling and rule-following to be deployed in real-world work environments. CRMArena is an open challenge to the community: systems that can reliably complete tasks showcase direct business value in a popular work environment.", "source": "arxiv", "arxiv_id": "2411.02305v2", "pdf_url": "https://arxiv.org/pdf/2411.02305v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-04T17:30:51Z", "updated": "2025-02-16T17:16:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cached Model-as-a-Resource: Provisioning Large Language Model Agents for Edge Intelligence in Space-air-ground Integrated Networks", "authors": ["Minrui Xu", "Dusit Niyato", "Hongliang Zhang", "Jiawen Kang", "Zehui Xiong", "Shiwen Mao", "Zhu Han"], "year": 2024, "url": "http://arxiv.org/abs/2403.05826v2", "abstract": "Edge intelligence in space-air-ground integrated networks (SAGINs) can enable worldwide network coverage beyond geographical limitations for users to access ubiquitous and low-latency intelligence services. Facing global coverage and complex environments in SAGINs, edge intelligence can provision approximate large language models (LLMs) agents for users via edge servers at ground base stations (BSs) or cloud data centers relayed by satellites. As LLMs with billions of parameters are pre-trained on vast datasets, LLM agents have few-shot learning capabilities, e.g., chain-of-thought (CoT) prompting for complex tasks, which raises a new trade-off between resource consumption and performance in SAGINs. In this paper, we propose a joint caching and inference framework for edge intelligence to provision sustainable and ubiquitous LLM agents in SAGINs. We introduce \"cached model-as-a-resource\" for offering LLMs with limited context windows and propose a novel optimization framework, i.e., joint model caching and inference, to utilize cached model resources for provisioning LLM agent services along with communication, computing, and storage resources. We design \"age of thought\" (AoT) considering the CoT prompting of LLMs, and propose a least AoT cached model replacement algorithm for optimizing the provisioning cost. We propose a deep Q-network-based modified second-bid (DQMSB) auction to incentivize network operators, which can enhance allocation efficiency by 23% while guaranteeing strategy-proofness and free from adverse selection.", "source": "arxiv", "arxiv_id": "2403.05826v2", "pdf_url": "https://arxiv.org/pdf/2403.05826v2", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-03-09T07:37:13Z", "updated": "2024-05-31T14:14:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can Large Language Model Agents Simulate Human Trust Behavior?", "authors": ["Chengxing Xie", "Canyu Chen", "Feiran Jia", "Ziyu Ye", "Shiyang Lai", "Kai Shu", "Jindong Gu", "Adel Bibi", "Ziniu Hu", "David Jurgens", "James Evans", "Philip Torr", "Bernard Ghanem", "Guohao Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.04559v4", "abstract": "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.", "source": "arxiv", "arxiv_id": "2402.04559v4", "pdf_url": "https://arxiv.org/pdf/2402.04559v4", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-07T03:37:19Z", "updated": "2024-11-01T16:10:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can Large Language Models Serve as Evaluators for Code Summarization?", "authors": ["Yang Wu", "Yao Wan", "Zhaoyang Chu", "Wenting Zhao", "Ye Liu", "Hongyu Zhang", "Xuanhua Shi", "Philip S. Yu"], "year": 2024, "url": "http://arxiv.org/abs/2412.01333v1", "abstract": "Code summarization facilitates program comprehension and software maintenance by converting code snippets into natural-language descriptions. Over the years, numerous methods have been developed for this task, but a key challenge remains: effectively evaluating the quality of generated summaries. While human evaluation is effective for assessing code summary quality, it is labor-intensive and difficult to scale. Commonly used automatic metrics, such as BLEU, ROUGE-L, METEOR, and BERTScore, often fail to align closely with human judgments. In this paper, we explore the potential of Large Language Models (LLMs) for evaluating code summarization. We propose CODERPE (Role-Player for Code Summarization Evaluation), a novel method that leverages role-player prompting to assess the quality of generated summaries. Specifically, we prompt an LLM agent to play diverse roles, such as code reviewer, code author, code editor, and system analyst. Each role evaluates the quality of code summaries across key dimensions, including coherence, consistency, fluency, and relevance. We further explore the robustness of LLMs as evaluators by employing various prompting strategies, including chain-of-thought reasoning, in-context learning, and tailored rating form designs. The results demonstrate that LLMs serve as effective evaluators for code summarization methods. Notably, our LLM-based evaluator, CODERPE , achieves an 81.59% Spearman correlation with human evaluations, outperforming the existing BERTScore metric by 17.27%.", "source": "arxiv", "arxiv_id": "2412.01333v1", "pdf_url": "https://arxiv.org/pdf/2412.01333v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-12-02T09:56:18Z", "updated": "2024-12-02T09:56:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate", "authors": ["Steffi Chern", "Ethan Chern", "Graham Neubig", "Pengfei Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.16788v1", "abstract": "Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \\url{https://github.com/GAIR-NLP/scaleeval}.", "source": "arxiv", "arxiv_id": "2401.16788v1", "pdf_url": "https://arxiv.org/pdf/2401.16788v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-30T07:03:32Z", "updated": "2024-01-30T07:03:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator Games", "authors": ["Ji Ma"], "year": 2024, "url": "http://arxiv.org/abs/2410.21359v3", "abstract": "As Large Language Model (LLM)-based agents increasingly engage with human society, how well do we understand their prosocial behaviors? We (1) investigate how LLM agents' prosocial behaviors can be induced by different personas and benchmarked against human behaviors; and (2) introduce a social science approach to evaluate LLM agents' decision-making. We explored how different personas and experimental framings affect these AI agents' altruistic behavior in dictator games and compared their behaviors within the same LLM family, across various families, and with human behaviors. The findings reveal that merely assigning a human-like identity to LLMs does not produce human-like behaviors. These findings suggest that LLM agents' reasoning does not consistently exhibit textual markers of human decision-making in dictator games and that their alignment with human behavior varies substantially across model architectures and prompt formulations; even worse, such dependence does not follow a clear pattern. As society increasingly integrates machine intelligence, \"Prosocial AI\" emerges as a promising and urgent research direction in philanthropic studies.", "source": "arxiv", "arxiv_id": "2410.21359v3", "pdf_url": "https://arxiv.org/pdf/2410.21359v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "econ.GN"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-28T17:47:41Z", "updated": "2025-11-17T20:55:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example", "authors": ["Yanan Chen", "Ali Pesaranghader", "Tanmana Sadhu", "Dong Hoon Yi"], "year": 2024, "url": "http://arxiv.org/abs/2408.06318v1", "abstract": "Large language models (LLMs) have brought autonomous agents closer to artificial general intelligence (AGI) due to their promising generalization and emergent capabilities. There is, however, a lack of studies on how LLM-based agents behave, why they could potentially fail, and how to improve them, particularly in demanding real-world planning tasks. In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner, where an agent must meet multiple constraints to generate accurate plans. We leverage this benchmark to address four key research questions: (1) are LLM agents robust enough to lengthy and noisy contexts when it comes to reasoning and planning? (2) can few-shot prompting adversely impact the performance of LLM agents in scenarios with long context? (3) can we rely on refinement to improve plans, and (4) can fine-tuning LLMs with both positive and negative feedback lead to further improvement? Our comprehensive experiments indicate that, firstly, LLMs often fail to attend to crucial parts of a long context, despite their ability to handle extensive reference information and few-shot examples; secondly, they still struggle with analyzing the long plans and cannot provide accurate feedback for refinement; thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and negative feedback, resulting in substantial gains over Supervised Fine-Tuning (SFT). Our findings offer in-depth insights to the community on various aspects related to real-world planning applications.", "source": "arxiv", "arxiv_id": "2408.06318v1", "pdf_url": "https://arxiv.org/pdf/2408.06318v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-12T17:39:01Z", "updated": "2024-08-12T17:39:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Can large language models explore in-context?", "authors": ["Akshay Krishnamurthy", "Keegan Harris", "Dylan J. Foster", "Cyril Zhang", "Aleksandrs Slivkins"], "year": 2024, "url": "http://arxiv.org/abs/2403.15371v3", "abstract": "We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thought reasoning but unsummarized history. Although these findings can be interpreted positively, they suggest that external summarization -- which may not be possible in more complex settings -- is important for obtaining desirable behavior from LLM agents. We conclude that non-trivial algorithmic interventions, such as fine-tuning or dataset curation, may be required to empower LLM-based decision making agents in complex settings.", "source": "arxiv", "arxiv_id": "2403.15371v3", "pdf_url": "https://arxiv.org/pdf/2403.15371v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-03-22T17:50:43Z", "updated": "2024-10-28T19:55:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Catastrophic Cyber Capabilities Benchmark (3CB): Robustly Evaluating LLM Agent Cyber Offense Capabilities", "authors": ["Andrey Anurin", "Jonathan Ng", "Kibo Schaffer", "Jason Schreiber", "Esben Kran"], "year": 2024, "url": "http://arxiv.org/abs/2410.09114v2", "abstract": "LLM agents have the potential to revolutionize defensive cyber operations, but their offensive capabilities are not yet fully understood. To prepare for emerging threats, model developers and governments are evaluating the cyber capabilities of foundation models. However, these assessments often lack transparency and a comprehensive focus on offensive capabilities. In response, we introduce the Catastrophic Cyber Capabilities Benchmark (3CB), a novel framework designed to rigorously assess the real-world offensive capabilities of LLM agents. Our evaluation of modern LLMs on 3CB reveals that frontier models, such as GPT-4o and Claude 3.5 Sonnet, can perform offensive tasks such as reconnaissance and exploitation across domains ranging from binary analysis to web technologies. Conversely, smaller open-source models exhibit limited offensive capabilities. Our software solution and the corresponding benchmark provides a critical tool to reduce the gap between rapidly improving capabilities and robustness of cyber offense evaluations, aiding in the safer deployment and regulation of these powerful technologies.", "source": "arxiv", "arxiv_id": "2410.09114v2", "pdf_url": "https://arxiv.org/pdf/2410.09114v2", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-10T12:06:48Z", "updated": "2024-11-02T09:35:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Caution for the Environment: Multimodal LLM Agents are Susceptible to Environmental Distractions", "authors": ["Xinbei Ma", "Yiting Wang", "Yao Yao", "Tongxin Yuan", "Aston Zhang", "Zhuosheng Zhang", "Hai Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2408.02544v3", "abstract": "This paper investigates the faithfulness of multimodal large language model (MLLM) agents in a graphical user interface (GUI) environment, aiming to address the research question of whether multimodal GUI agents can be distracted by environmental context. A general scenario is proposed where both the user and the agent are benign, and the environment, while not malicious, contains unrelated content. A wide range of MLLMs are evaluated as GUI agents using a simulated dataset, following three working patterns with different levels of perception. Experimental results reveal that even the most powerful models, whether generalist agents or specialist GUI agents, are susceptible to distractions. While recent studies predominantly focus on the helpfulness of agents, our findings first indicate that these agents are prone to environmental distractions. Furthermore, we implement an adversarial environment injection and analyze the approach to improve faithfulness, calling for a collective focus on this important topic.", "source": "arxiv", "arxiv_id": "2408.02544v3", "pdf_url": "https://arxiv.org/pdf/2408.02544v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-05T15:16:22Z", "updated": "2025-09-05T09:21:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents", "authors": ["Long Li", "Weiwen Xu", "Jiayan Guo", "Ruochen Zhao", "Xingxuan Li", "Yuqian Yuan", "Boqiang Zhang", "Yuming Jiang", "Yifei Xin", "Ronghao Dang", "Deli Zhao", "Yu Rong", "Tian Feng", "Lidong Bing"], "year": 2024, "url": "http://arxiv.org/abs/2410.13185v5", "abstract": "Effective research ideation is a critical step for scientific research. However, the exponential increase in scientific literature makes it challenging for researchers to stay current with recent advances and identify meaningful research directions. Recent developments in large language models~(LLMs) suggest a promising avenue for automating the generation of novel research ideas. However, existing methods for idea generation either trivially prompt LLMs or directly expose LLMs to extensive literature without indicating useful information. Inspired by the research process of human researchers, we propose a Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant literature in a chain structure to effectively mirror the progressive development in a research domain. This organization facilitates LLMs to capture the current advancements in research, thereby enhancing their ideation capabilities. Furthermore, we propose Idea Arena, an evaluation protocol that can comprehensively evaluate idea generation methods from different perspectives, aligning closely with the preferences of human researchers. Experimental results indicate that the CoI agent consistently outperforms other methods and shows comparable quality as humans in research idea generation. Moreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to generate a candidate idea and its corresponding experimental design.", "source": "arxiv", "arxiv_id": "2410.13185v5", "pdf_url": "https://arxiv.org/pdf/2410.13185v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-17T03:26:37Z", "updated": "2024-10-30T09:17:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary", "authors": ["Yutong Li", "Lu Chen", "Aiwei Liu", "Kai Yu", "Lijie Wen"], "year": 2024, "url": "http://arxiv.org/abs/2403.02574v1", "abstract": "The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.", "source": "arxiv", "arxiv_id": "2403.02574v1", "pdf_url": "https://arxiv.org/pdf/2403.02574v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-03-05T01:13:56Z", "updated": "2024-03-05T01:13:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ChatDyn: Language-Driven Multi-Actor Dynamics Generation in Street Scenes", "authors": ["Yuxi Wei", "Jingbo Wang", "Yuwen Du", "Dingju Wang", "Liang Pan", "Chenxin Xu", "Yao Feng", "Bo Dai", "Siheng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.08685v1", "abstract": "Generating realistic and interactive dynamics of traffic participants according to specific instruction is critical for street scene simulation. However, there is currently a lack of a comprehensive method that generates realistic dynamics of different types of participants including vehicles and pedestrians, with different kinds of interactions between them. In this paper, we introduce ChatDyn, the first system capable of generating interactive, controllable and realistic participant dynamics in street scenes based on language instructions. To achieve precise control through complex language, ChatDyn employs a multi-LLM-agent role-playing approach, which utilizes natural language inputs to plan the trajectories and behaviors for different traffic participants. To generate realistic fine-grained dynamics based on the planning, ChatDyn designs two novel executors: the PedExecutor, a unified multi-task executor that generates realistic pedestrian dynamics under different task plannings; and the VehExecutor, a physical transition-based policy that generates physically plausible vehicle dynamics. Extensive experiments show that ChatDyn can generate realistic driving scene dynamics with multiple vehicles and pedestrians, and significantly outperforms previous methods on subtasks. Code and model will be available at https://vfishc.github.io/chatdyn.", "source": "arxiv", "arxiv_id": "2412.08685v1", "pdf_url": "https://arxiv.org/pdf/2412.08685v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-12-11T18:58:48Z", "updated": "2024-12-11T18:58:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ChatPattern: Layout Pattern Customization via Natural Language", "authors": ["Zixiao Wang", "Yunheng Shen", "Xufeng Yao", "Wenqian Zhao", "Yang Bai", "Farzan Farnia", "Bei Yu"], "year": 2024, "url": "http://arxiv.org/abs/2403.15434v1", "abstract": "Existing works focus on fixed-size layout pattern generation, while the more practical free-size pattern generation receives limited attention. In this paper, we propose ChatPattern, a novel Large-Language-Model (LLM) powered framework for flexible pattern customization. ChatPattern utilizes a two-part system featuring an expert LLM agent and a highly controllable layout pattern generator. The LLM agent can interpret natural language requirements and operate design tools to meet specified needs, while the generator excels in conditional layout generation, pattern modification, and memory-friendly patterns extension. Experiments on challenging pattern generation setting shows the ability of ChatPattern to synthesize high-quality large-scale patterns.", "source": "arxiv", "arxiv_id": "2403.15434v1", "pdf_url": "https://arxiv.org/pdf/2403.15434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-15T09:15:22Z", "updated": "2024-03-15T09:15:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ChemMiner: A Large Language Model Agent System for Chemical Literature Data Mining", "authors": ["Kexin Chen", "Yuyang Du", "Junyou Li", "Hanqun Cao", "Menghao Guo", "Xilin Dang", "Lanqing Li", "Jiezhong Qiu", "Pheng Ann Heng", "Guangyong Chen"], "year": 2024, "url": "http://arxiv.org/abs/2402.12993v2", "abstract": "The development of AI-assisted chemical synthesis tools requires comprehensive datasets covering diverse reaction types, yet current high-throughput experimental (HTE) approaches are expensive and limited in scope. Chemical literature represents a vast, underexplored data source containing thousands of reactions published annually. However, extracting reaction information from literature faces significant challenges including varied writing styles, complex coreference relationships, and multimodal information presentation. This paper proposes ChemMiner, a novel end-to-end framework leveraging multiple agents powered by large language models (LLMs) to extract high-fidelity chemical data from literature. ChemMiner incorporates three specialized agents: a text analysis agent for coreference mapping, a multimodal agent for non-textual information extraction, and a synthesis analysis agent for data generation. Furthermore, we developed a comprehensive benchmark with expert-annotated chemical literature to evaluate both extraction efficiency and precision. Experimental results demonstrate reaction identification rates comparable to human chemists while significantly reducing processing time, with high accuracy, recall, and F1 scores. Our open-sourced benchmark facilitates future research in chemical literature data mining.", "source": "arxiv", "arxiv_id": "2402.12993v2", "pdf_url": "https://arxiv.org/pdf/2402.12993v2", "categories": ["cs.IR", "cs.AI", "cs.LG", "q-bio.QM"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-02-20T13:21:46Z", "updated": "2025-06-30T08:19:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling", "authors": ["Jinghan Li", "Zhicheng Sun", "Yadong Mu"], "year": 2024, "url": "http://arxiv.org/abs/2410.01440v6", "abstract": "In the endeavor to make autonomous robots take actions, task planning is a major challenge that requires translating high-level task descriptions to long-horizon action sequences. Despite recent advances in language model agents, they remain prone to planning errors and limited in their ability to plan ahead. To address these limitations in robotic planning, we advocate a self-refining scheme that iteratively refines a draft plan until an equilibrium is reached. Remarkably, this process can be optimized end-to-end from an analytical perspective without the need to curate additional verifiers or reward models, allowing us to train self-refining planners in a simple supervised learning fashion. Meanwhile, a nested equilibrium sequence modeling procedure is devised for efficient closed-loop planning that incorporates useful feedback from the environment (or an internal world model). Our method is evaluated on the VirtualHome-Env benchmark, showing advanced performance with improved scaling w.r.t. inference-time computation. Code is available at https://github.com/Singularity0104/equilibrium-planner.", "source": "arxiv", "arxiv_id": "2410.01440v6", "pdf_url": "https://arxiv.org/pdf/2410.01440v6", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-10-02T11:42:49Z", "updated": "2025-06-18T06:15:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation", "authors": ["Xinbei Ma", "Zhuosheng Zhang", "Hai Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2402.11941v3", "abstract": "Multimodal large language models (MLLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose a Comprehensive Cognitive LLM Agent, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our technical design, our agent achieves new state-of-the-art performance on AITW and META-GUI benchmarks, showing promising abilities in realistic scenarios. Code is available at https://github.com/xbmxb/CoCo-Agent.", "source": "arxiv", "arxiv_id": "2402.11941v3", "pdf_url": "https://arxiv.org/pdf/2402.11941v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-19T08:29:03Z", "updated": "2024-06-02T13:25:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CoPS: Empowering LLM Agents with Provable Cross-Task Experience Sharing", "authors": ["Chen Yang", "Chenyang Zhao", "Quanquan Gu", "Dongruo Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2410.16670v1", "abstract": "Sequential reasoning in agent systems has been significantly advanced by large language models (LLMs), yet existing approaches face limitations. Reflection-driven reasoning relies solely on knowledge in pretrained models, limiting performance in novel scenarios, while experience-assisted reasoning often depends on external experiences and lacks clear principles for selecting representative experiences. We address these limitations by proposing CoPS (Cross-Task Experience Sharing), a generalizable algorithm that enhances sequential reasoning by cross-task experience sharing and selection. In detail, CoPS leverages agents' experiences on previous tasks, selecting distribution-matched experiences via a provable pessimism-based strategy to maximize utility while minimizing risks from distribution shifts. Extensive experimental results on benchmarks like Alfworld, Webshop, and HotPotQA demonstrate that CoPS consistently outperforms state-of-the-art baselines, with superior sample efficiency suitable for resource-constrained scenarios. Theoretically, we show that the performance of our algorithm depends on both the quality of the pretrained LLM and the matching between the agent's task-dependent trial distribution and that generated by the LLM. Our work bridges the gap between existing sequential reasoning paradigms and validates the effectiveness of leveraging cross-task experiences, shedding light on the potential to improve agents' generalization and adaptability across diverse tasks. Our codes are available at $\\href{https://github.com/uclaml/COPS}{\\text{https://github.com/uclaml/COPS}}$.", "source": "arxiv", "arxiv_id": "2410.16670v1", "pdf_url": "https://arxiv.org/pdf/2410.16670v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-22T03:59:53Z", "updated": "2024-10-22T03:59:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CodeNav: Beyond tool-use to using real-world codebases with LLM agents", "authors": ["Tanmay Gupta", "Luca Weihs", "Aniruddha Kembhavi"], "year": 2024, "url": "http://arxiv.org/abs/2406.12276v1", "abstract": "We present CodeNav, an LLM agent that navigates and leverages previously unseen code repositories to solve user queries. In contrast to tool-use LLM agents that require ``registration'' of all relevant tools via manual descriptions within the LLM context, CodeNav automatically indexes and searches over code blocks in the target codebase, finds relevant code snippets, imports them, and uses them to iteratively generate a solution with execution feedback. To highlight the core-capabilities of CodeNav, we first showcase three case studies where we use CodeNav for solving complex user queries using three diverse codebases. Next, on three benchmarks, we quantitatively compare the effectiveness of code-use (which only has access to the target codebase) to tool-use (which has privileged access to all tool names and descriptions). Finally, we study the effect of varying kinds of tool and library descriptions on code-use performance, as well as investigate the advantage of the agent seeing source code as opposed to natural descriptions of code. All code will be made open source under a permissive license.", "source": "arxiv", "arxiv_id": "2406.12276v1", "pdf_url": "https://arxiv.org/pdf/2406.12276v1", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-18T05:10:38Z", "updated": "2024-06-18T05:10:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models", "authors": ["Jierui Li", "Hung Le", "Yingbo Zhou", "Caiming Xiong", "Silvio Savarese", "Doyen Sahoo"], "year": 2024, "url": "http://arxiv.org/abs/2411.04329v2", "abstract": "Pre-trained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously. However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging. To address this problem, we propose CodeTree, a framework for LLM agents to efficiently explore the search space in different stages of the code generation process. Specifically, we adopted a unified tree structure to explicitly explore different coding strategies, generate corresponding coding solutions, and subsequently refine the solutions. In each stage, critical decision-making (ranking, termination, expanding) of the exploration process is guided by both the environmental execution-based feedback and LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code generation benchmarks and demonstrated the significant performance gains of CodeTree against strong baselines. Using GPT-4o as the base model, we consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0 on CodeContests. On the challenging SWEBench benchmark, our approach led to significant performance gains.", "source": "arxiv", "arxiv_id": "2411.04329v2", "pdf_url": "https://arxiv.org/pdf/2411.04329v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-07T00:09:54Z", "updated": "2024-11-12T19:37:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Codenames as a Benchmark for Large Language Models", "authors": ["Matthew Stephenson", "Matthew Sidji", "BenoÃ®t Ronval"], "year": 2024, "url": "http://arxiv.org/abs/2412.11373v2", "abstract": "In this paper, we propose the use of the popular word-based board game Codenames as a suitable benchmark for evaluating the reasoning capabilities of Large Language Models (LLMs). Codenames presents a highly interesting challenge for achieving successful AI performance, requiring both a sophisticated understanding of language, theory of mind, and epistemic reasoning capabilities. Prior attempts to develop agents for Codenames have largely relied on word embedding techniques, which have a limited vocabulary range and perform poorly when paired with differing approaches. LLMs have demonstrated enhanced reasoning and comprehension capabilities for language-based tasks, but can still suffer in lateral thinking challenges. We evaluate the capabilities of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1, across a variety of board setups. Our results indicate that while certain LLMs perform better than others overall, different models exhibit varying emergent behaviours during gameplay and excel at specific roles. We also evaluate the performance of different combinations of LLMs when playing cooperatively together, demonstrating that LLM agents are more generalisable to a wider range of teammates than prior techniques.", "source": "arxiv", "arxiv_id": "2412.11373v2", "pdf_url": "https://arxiv.org/pdf/2412.11373v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-16T01:59:03Z", "updated": "2025-04-21T22:53:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases", "authors": ["Xiangyan Liu", "Bo Lan", "Zhiyuan Hu", "Yang Liu", "Zhicheng Zhang", "Fei Wang", "Michael Shieh", "Wenmeng Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2408.03910v2", "abstract": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks. Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications. To mitigate these limitations, we introduce CodexGraph, a system that integrates LLM agents with graph database interfaces extracted from code repositories. By leveraging the structural properties of graph databases and the flexibility of the graph query language, CodexGraph enables the LLM agent to construct and execute queries, allowing for precise, code structure-aware context retrieval and code navigation. We assess CodexGraph using three benchmarks: CrossCodeEval, SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding applications. With a unified graph database schema, CodexGraph demonstrates competitive performance and potential in both academic and real-world environments, showcasing its versatility and efficacy in software engineering. Our application demo: https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.", "source": "arxiv", "arxiv_id": "2408.03910v2", "pdf_url": "https://arxiv.org/pdf/2408.03910v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-08-07T17:13:59Z", "updated": "2024-08-11T16:23:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Collaborative Participatory Research with LLM Agents in South Asia: An Empirically-Grounded Methodological Initiative and Agenda from Field Evidence in Sri Lanka", "authors": ["Xinjie Zhao", "Shyaman Maduranga Sriwarnasinghe", "Jiacheng Tang", "Shiyun Wang", "Hao Wang", "So Morikawa"], "year": 2024, "url": "http://arxiv.org/abs/2411.08294v1", "abstract": "The integration of artificial intelligence into development research methodologies presents unprecedented opportunities for addressing persistent challenges in participatory research, particularly in linguistically diverse regions like South Asia. Drawing from an empirical implementation in Sri Lanka's Sinhala-speaking communities, this paper presents an empirically grounded methodological framework designed to transform participatory development research, situated in the challenging multilingual context of Sri Lanka's flood-prone Nilwala River Basin. Moving beyond conventional translation and data collection tools, this framework deploys a multi-agent system architecture that redefines how data collection, analysis, and community engagement are conducted in linguistically and culturally diverse research settings. This structured agent-based approach enables participatory research that is both scalable and responsive, ensuring that community perspectives remain integral to research outcomes. Field experiences reveal the immense potential of LLM-based systems in addressing long-standing issues in development research across resource-limited regions, offering both quantitative efficiencies and qualitative improvements in inclusivity. At a broader methodological level, this research agenda advocates for AI-driven participatory research tools that maintain ethical considerations, cultural respect, and operational efficiency, highlighting strategic pathways for deploying AI systems that reinforce community agency and equitable knowledge generation, potentially informing broader research agendas across the Global South.", "source": "arxiv", "arxiv_id": "2411.08294v1", "pdf_url": "https://arxiv.org/pdf/2411.08294v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-11-13T02:21:59Z", "updated": "2024-11-13T02:21:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Competence-Aware AI Agents with Metacognition for Unknown Situations and Environments (MUSE)", "authors": ["Rodolfo Valiente", "Praveen K. Pilly"], "year": 2024, "url": "http://arxiv.org/abs/2411.13537v2", "abstract": "Metacognition, defined as the awareness and regulation of one's cognitive processes, is central to human adaptability in unknown situations. In contrast, current autonomous agents often struggle in novel environments due to their limited capacity for adaptation. We hypothesize that metacognition is a critical missing ingredient in autonomous agents for the cognitive flexibility needed to tackle unfamiliar challenges. Given the broad scope of metacognitive abilities, we focus on competence awareness and strategy selection. To this end, we propose the Metacognition for Unknown Situations and Environments (MUSE) framework to integrate metacognitive processes of self-assessment and self-regulation into autonomous agents. We present two implementations of MUSE: one based on world modeling and another leveraging large language models (LLMs). Our system continually learns to assess its competence on a given task and uses this self-assessment to guide iterative cycles of strategy selection. MUSE agents demonstrate high competence awareness and significant improvements in self-regulation for solving novel, out-of-distribution tasks more effectively compared to model-based reinforcement learning and purely prompt-based LLM agent approaches. This work highlights the promise of approaches inspired by cognitive and neural systems in enabling autonomous agents to adapt to new environments while mitigating the heavy reliance on extensive training data and large models for the current models.", "source": "arxiv", "arxiv_id": "2411.13537v2", "pdf_url": "https://arxiv.org/pdf/2411.13537v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "10.1016/j.neunet.2025.108131", "venue": "", "published": "2024-11-20T18:41:03Z", "updated": "2025-11-17T08:25:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Concept-Guided LLM Agents for Human-AI Safety Codesign", "authors": ["Florian Geissler", "Karsten Roscher", "Mario Trapp"], "year": 2024, "url": "http://arxiv.org/abs/2404.15317v1", "abstract": "Generative AI is increasingly important in software engineering, including safety engineering, where its use ensures that software does not cause harm to people. This also leads to high quality requirements for generative AI. Therefore, the simplistic use of Large Language Models (LLMs) alone will not meet these quality demands. It is crucial to develop more advanced and sophisticated approaches that can effectively address the complexities and safety concerns of software systems. Ultimately, humans must understand and take responsibility for the suggestions provided by generative AI to ensure system safety. To this end, we present an efficient, hybrid strategy to leverage LLMs for safety analysis and Human-AI codesign. In particular, we develop a customized LLM agent that uses elements of prompt engineering, heuristic reasoning, and retrieval-augmented generation to solve tasks associated with predefined safety concepts, in interaction with a system model graph. The reasoning is guided by a cascade of micro-decisions that help preserve structured information. We further suggest a graph verbalization which acts as an intermediate representation of the system model to facilitate LLM-graph interactions. Selected pairs of prompts and responses relevant for safety analytics illustrate our method for the use case of a simplified automated driving system.", "source": "arxiv", "arxiv_id": "2404.15317v1", "pdf_url": "https://arxiv.org/pdf/2404.15317v1", "categories": ["cs.SE", "cs.HC", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "Proceedings of the AAAI-make Spring Symposium, 2024", "published": "2024-04-03T11:37:01Z", "updated": "2024-04-03T11:37:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation", "authors": ["Ruixin Yang", "Dheeraj Rajagopal", "Shirley Anugrah Hayati", "Bin Hu", "Dongyeop Kang"], "year": 2024, "url": "http://arxiv.org/abs/2404.09127v3", "abstract": "Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the \"Collective Wisdom\": the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.", "source": "arxiv", "arxiv_id": "2404.09127v3", "pdf_url": "https://arxiv.org/pdf/2404.09127v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-14T02:40:43Z", "updated": "2024-05-10T16:38:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Control Industrial Automation System with Large Language Model Agents", "authors": ["Yuchen Xia", "Nasser Jazdi", "Jize Zhang", "Chaitanya Shah", "Michael Weyrich"], "year": 2024, "url": "http://arxiv.org/abs/2409.18009v2", "abstract": "Traditional industrial automation systems require specialized expertise to operate and complex reprogramming to adapt to new processes. Large language models offer the intelligence to make them more flexible and easier to use. However, LLMs' application in industrial settings is underexplored. This paper introduces a framework for integrating LLMs to achieve end-to-end control of industrial automation systems. At the core of the framework are an agent system designed for industrial tasks, a structured prompting method, and an event-driven information modeling mechanism that provides real-time data for LLM inference. The framework supplies LLMs with real-time events on different context semantic levels, allowing them to interpret the information, generate production plans, and control operations on the automation system. It also supports structured dataset creation for fine-tuning on this downstream application of LLMs. Our contribution includes a formal system design, proof-of-concept implementation, and a method for generating task-specific datasets for LLM fine-tuning and testing. This approach enables a more adaptive automation system that can respond to spontaneous events, while allowing easier operation and configuration through natural language for more intuitive human-machine interaction. We provide demo videos and detailed data on GitHub: https://github.com/YuchenXia/LLM4IAS.", "source": "arxiv", "arxiv_id": "2409.18009v2", "pdf_url": "https://arxiv.org/pdf/2409.18009v2", "categories": ["eess.SY", "cs.AI", "cs.HC", "cs.MA", "cs.RO"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-09-26T16:19:37Z", "updated": "2025-06-12T21:26:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ControlAgent: Automating Control System Design via Novel Integration of LLM Agents and Domain Expertise", "authors": ["Xingang Guo", "Darioush Keivan", "Usman Syed", "Lianhui Qin", "Huan Zhang", "Geir Dullerud", "Peter Seiler", "Bin Hu"], "year": 2024, "url": "http://arxiv.org/abs/2410.19811v1", "abstract": "Control system design is a crucial aspect of modern engineering with far-reaching applications across diverse sectors including aerospace, automotive systems, power grids, and robotics. Despite advances made by Large Language Models (LLMs) in various domains, their application in control system design remains limited due to the complexity and specificity of control theory. To bridge this gap, we introduce ControlAgent, a new paradigm that automates control system design via novel integration of LLM agents and control-oriented domain expertise. ControlAgent encodes expert control knowledge and emulates human iterative design processes by gradually tuning controller parameters to meet user-specified requirements for stability, performance, and robustness. ControlAgent integrates multiple collaborative LLM agents, including a central agent responsible for task distribution and task-specific agents dedicated to detailed controller design for various types of systems and requirements. ControlAgent also employs a Python computation agent that performs complex calculations and controller evaluations based on standard design information provided by task-specified LLM agents. Combined with a history and feedback module, the task-specific LLM agents iteratively refine controller parameters based on real-time feedback from prior designs. Overall, ControlAgent mimics the design processes used by (human) practicing engineers, but removes all the human efforts and can be run in a fully automated way to give end-to-end solutions for control system design with user-specified requirements. To validate ControlAgent's effectiveness, we develop ControlEval, an evaluation dataset that comprises 500 control tasks with various specific design goals. The effectiveness of ControlAgent is demonstrated via extensive comparative evaluations between LLM-based and traditional human-involved toolbox-based baselines.", "source": "arxiv", "arxiv_id": "2410.19811v1", "pdf_url": "https://arxiv.org/pdf/2410.19811v1", "categories": ["eess.SY", "cs.AI", "cs.CL", "cs.LG", "math.OC"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-10-17T17:42:48Z", "updated": "2024-10-17T17:42:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Controlling Large Language Model Agents with Entropic Activation Steering", "authors": ["Nate Rahn", "Pierluca D'Oro", "Marc G. Bellemare"], "year": 2024, "url": "http://arxiv.org/abs/2406.00244v2", "abstract": "The rise of large language models (LLMs) has prompted increasing interest in their use as in-context learning agents. At the core of agentic behavior is the capacity for exploration, or the ability to actively gather information about the environment. But how do LLM agents explore, and how can we control their exploratory behaviors? To answer these questions, we take a representation-level perspective, and introduce Entropic Activation Steering (EAST), an activation steering method for in-context LLM agents. Firstly, we demonstrate that EAST can effectively manipulate an LLM agent's exploration by directly affecting the high-level actions parsed from the outputs of the LLM, in contrast to token-level temperature sampling. Secondly, we reveal how applying this control modulates the uncertainty exhibited in the LLM's thoughts, guiding the agent towards more exploratory actions. Finally, we demonstrate that the steering vectors obtained by EAST generalize across task variants. In total, these results show that LLM agents explicitly encode uncertainty over their actions in their representation space. Our work paves the way for a new understanding of the functioning of LLM agents and to effective control of their decision-making behaviors.", "source": "arxiv", "arxiv_id": "2406.00244v2", "pdf_url": "https://arxiv.org/pdf/2406.00244v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-01T00:25:00Z", "updated": "2024-10-10T20:47:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Conversate: Supporting Reflective Learning in Interview Practice Through Interactive Simulation and Dialogic Feedback", "authors": ["Taufiq Daryanto", "Xiaohan Ding", "Lance T. Wilhelm", "Sophia Stil", "Kirk McInnis Knutsen", "Eugenia H. Rho"], "year": 2024, "url": "http://arxiv.org/abs/2410.05570v2", "abstract": "Job interviews play a critical role in shaping one's career, yet practicing interview skills can be challenging, especially without access to human coaches or peers for feedback. Recent advancements in large language models (LLMs) present an opportunity to enhance the interview practice experience. Yet, little research has explored the effectiveness and user perceptions of such systems or the benefits and challenges of using LLMs for interview practice. Furthermore, while prior work and recent commercial tools have demonstrated the potential of AI to assist with interview practice, they often deliver one-way feedback, where users only receive information about their performance. By contrast, dialogic feedback, a concept developed in learning sciences, is a two-way interaction feedback process that allows users to further engage with and learn from the provided feedback through interactive dialogue. This paper introduces Conversate, a web-based application that supports reflective learning in job interview practice by leveraging large language models (LLMs) for interactive interview simulations and dialogic feedback. To start the interview session, the user provides the title of a job position (e.g., entry-level software engineer) in the system. Then, our system will initialize the LLM agent to start the interview simulation by asking the user an opening interview question and following up with questions carefully adapted to subsequent user responses. After the interview session, our back-end LLM framework will then analyze the user's responses and highlight areas for improvement. Users can then annotate the transcript by selecting specific sections and writing self-reflections. Finally, the user can interact with the system for dialogic feedback, conversing with the LLM agent to learn from and iteratively refine their answers based on the agent's guidance.", "source": "arxiv", "arxiv_id": "2410.05570v2", "pdf_url": "https://arxiv.org/pdf/2410.05570v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1145/3701188", "venue": "", "published": "2024-10-08T00:12:11Z", "updated": "2024-11-01T21:03:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents", "authors": ["Giorgio Piatti", "Zhijing Jin", "Max Kleiman-Weiner", "Bernhard SchÃ¶lkopf", "Mrinmaya Sachan", "Rada Mihalcea"], "year": 2024, "url": "http://arxiv.org/abs/2404.16698v4", "abstract": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage \"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.", "source": "arxiv", "arxiv_id": "2404.16698v4", "pdf_url": "https://arxiv.org/pdf/2404.16698v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-25T15:59:16Z", "updated": "2024-12-08T11:21:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cooperative SQL Generation for Segmented Databases By Using Multi-functional LLM Agents", "authors": ["Zhiguang Wu", "Fengbin Zhu", "Xuequn Shang", "Yupei Zhang", "Pan Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2412.05850v1", "abstract": "Text-to-SQL task aims to automatically yield SQL queries according to user text questions. To address this problem, we propose a Cooperative SQL Generation framework based on Multi-functional Agents (CSMA) through information interaction among large language model (LLM) based agents who own part of the database schema seperately. Inspired by the collaboration in human teamwork, CSMA consists of three stages: 1) Question-related schema collection, 2) Question-corresponding SQL query generation, and 3) SQL query correctness check. In the first stage, agents analyze their respective schema and communicate with each other to collect the schema information relevant to the question. In the second stage, agents try to generate the corresponding SQL query for the question using the collected information. In the third stage, agents check if the SQL query is created correctly according to their known information. This interaction-based method makes the question-relevant part of database schema from each agent to be used for SQL generation and check. Experiments on the Spider and Bird benckmark demonstrate that CSMA achieves a high performance level comparable to the state-of-the-arts, meanwhile holding the private data in these individual agents.", "source": "arxiv", "arxiv_id": "2412.05850v1", "pdf_url": "https://arxiv.org/pdf/2412.05850v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-08T08:16:19Z", "updated": "2024-12-08T08:16:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models", "authors": ["Danqing Wang", "Zhuorui Ye", "Fei Fang", "Lei Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.20007v1", "abstract": "Enhancing the reasoning capabilities of large language models (LLMs) is crucial for enabling them to tackle complex, multi-step problems. Multi-agent frameworks have shown great potential in enhancing LLMs' reasoning capabilities. However, the lack of effective cooperation between LLM agents hinders their performance, especially for multi-step reasoning tasks. This paper proposes a novel cooperative multi-agent reasoning framework (CoPlanner) by separating reasoning steps and assigning distinct duties to different agents. CoPlanner consists of two LLM agents: a planning agent and a reasoning agent. The planning agent provides high-level strategic hints, while the reasoning agent follows these hints and infers answers. By training the planning agent's policy through the interactive reasoning process via Proximal Policy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the previous best method by 9.94\\% on LogiQA and 3.09\\% on BBH. Our results demonstrate that the guidance from the planning agent and the effective cooperation between the agents contribute to the superior performance of CoPlanner in tackling multi-step reasoning problems.", "source": "arxiv", "arxiv_id": "2410.20007v1", "pdf_url": "https://arxiv.org/pdf/2410.20007v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-25T23:32:48Z", "updated": "2024-10-25T23:32:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cultural Evolution of Cooperation among LLM Agents", "authors": ["Aron Vallinder", "Edward Hughes"], "year": 2024, "url": "http://arxiv.org/abs/2412.10270v1", "abstract": "Large language models (LLMs) provide a compelling foundation for building generally-capable AI agents. These agents may soon be deployed at scale in the real world, representing the interests of individual humans (e.g., AI assistants) or groups of humans (e.g., AI-accelerated corporations). At present, relatively little is known about the dynamics of multiple LLM agents interacting over many generations of iterative deployment. In this paper, we examine whether a \"society\" of LLM agents can learn mutually beneficial social norms in the face of incentives to defect, a distinctive feature of human sociality that is arguably crucial to the success of civilization. In particular, we study the evolution of indirect reciprocity across generations of LLM agents playing a classic iterated Donor Game in which agents can observe the recent behavior of their peers. We find that the evolution of cooperation differs markedly across base models, with societies of Claude 3.5 Sonnet agents achieving significantly higher average scores than Gemini 1.5 Flash, which, in turn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an additional mechanism for costly punishment to achieve yet higher scores, while Gemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also observe variation in emergent behavior across random seeds, suggesting an understudied sensitive dependence on initial conditions. We suggest that our evaluation regime could inspire an inexpensive and informative new class of LLM benchmarks, focussed on the implications of LLM agent deployment for the cooperative infrastructure of society.", "source": "arxiv", "arxiv_id": "2412.10270v1", "pdf_url": "https://arxiv.org/pdf/2412.10270v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-12-13T16:45:49Z", "updated": "2024-12-13T16:45:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CuriousLLM: Elevating Multi-Document Question Answering with LLM-Enhanced Knowledge Graph Reasoning", "authors": ["Zukang Yang", "Zixuan Zhu", "Xuan Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2404.09077v3", "abstract": "Large Language Models (LLMs) have achieved significant success in open-domain question answering. However, they continue to face challenges such as hallucinations and knowledge cutoffs. These issues can be mitigated through in-context learning by providing LLMs with relevant context before generating answers. Recent literature proposes Knowledge Graph Prompting (KGP) which integrates knowledge graphs with an LLM-based traversal agent to substantially enhance document retrieval quality. However, KGP requires costly fine-tuning with large datasets and remains prone to hallucination. In this paper, we propose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning mechanism into an LLM agent. This mechanism enables the agent to generate relevant follow-up questions, thereby guiding the information retrieval process more efficiently. Central to our approach is the development of the new Follow-upQA dataset, which includes questions and supporting evidence as input, with follow-up questions serving as ground truths. These follow-up questions either inquire about what is still missing to fully answer the user's query or use special tokens to signify that the retrieved evidence is sufficient. Our experiments show that CuriousLLM significantly boosts LLM performance in multi-document question answering (MD-QA), circumventing the substantial computational costs and latency from the original KGP framework.", "source": "arxiv", "arxiv_id": "2404.09077v3", "pdf_url": "https://arxiv.org/pdf/2404.09077v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-13T20:43:46Z", "updated": "2025-02-18T06:52:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DAWN: Designing Distributed Agents in a Worldwide Network", "authors": ["Zahra Aminiranjbar", "Jianan Tang", "Qiudan Wang", "Shubha Pant", "Mahesh Viswanathan"], "year": 2024, "url": "http://arxiv.org/abs/2410.22339v3", "abstract": "The rapid evolution of Large Language Models (LLMs) has transformed them from basic conversational tools into sophisticated entities capable of complex reasoning and decision-making. These advancements have led to the development of specialized LLM-based agents designed for diverse tasks such as coding and web browsing. As these agents become more capable, the need for a robust framework that facilitates global communication and collaboration among them towards advanced objectives has become increasingly critical. Distributed Agents in a Worldwide Network (DAWN) addresses this need by offering a versatile framework that integrates LLM-based agents with traditional software systems, enabling the creation of agentic applications suited for a wide range of use cases. DAWN enables distributed agents worldwide to register and be easily discovered through Gateway Agents. Collaborations among these agents are coordinated by a Principal Agent equipped with reasoning strategies. DAWN offers three operational modes: No-LLM Mode for deterministic tasks, Copilot for augmented decision-making, and LLM Agent for autonomous operations. Additionally, DAWN ensures the safety and security of agent collaborations globally through a dedicated safety, security, and compliance layer, protecting the network against attackers and adhering to stringent security and compliance standards. These features make DAWN a robust network for deploying agent-based applications across various industries.", "source": "arxiv", "arxiv_id": "2410.22339v3", "pdf_url": "https://arxiv.org/pdf/2410.22339v3", "categories": ["cs.NI", "cs.AI", "cs.MA"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-10-11T18:47:04Z", "updated": "2025-06-11T20:01:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DCA-Bench: A Benchmark for Dataset Curation Agents", "authors": ["Benhao Huang", "Yingzhuo Yu", "Jin Huang", "Xingjian Zhang", "Jiaqi Ma"], "year": 2024, "url": "http://arxiv.org/abs/2406.07275v2", "abstract": "The quality of datasets plays an increasingly crucial role in the research and development of modern artificial intelligence (AI). Despite the proliferation of open dataset platforms nowadays, data quality issues, such as incomplete documentation, inaccurate labels, ethical concerns, and outdated information, remain common in widely used datasets. Furthermore, these issues are often subtle and difficult to be detected by rule-based scripts, therefore requiring identification and verification by dataset users or maintainers--a process that is both time-consuming and prone to human mistakes. With the surging ability of large language models (LLM), it's promising to streamline the discovery of hidden dataset issues with LLM agents. To achieve this, one significant challenge is enabling LLM agents to detect issues in the wild rather than simply fixing known ones. In this work, we establish a benchmark to measure LLM agent's ability to tackle this challenge. We carefully curate 221 real-world test cases from eight popular dataset platforms and propose an automatic evaluation framework using GPT-4o. Our proposed framework shows strong empirical alignment with expert evaluations, validated through extensive comparisons with human annotations. Without any hints, most competitive Curator agent can only reveal $\\sim$30\\% of the data quality issues in the proposed dataset, highlighting the complexity of this task and indicating that applying LLM agents to real-world dataset curation still requires further in-depth exploration and innovation. The data and code are available at \\href{https://github.com/TRAIS-Lab/dca-bench}{https://github.com/TRAIS-Lab/dca-bench}.", "source": "arxiv", "arxiv_id": "2406.07275v2", "pdf_url": "https://arxiv.org/pdf/2406.07275v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-11T14:02:23Z", "updated": "2025-05-26T18:23:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DEBATE: Devil's Advocate-Based Assessment and Text Evaluation", "authors": ["Alex Kim", "Keonwoo Kim", "Sangwon Yoon"], "year": 2024, "url": "http://arxiv.org/abs/2405.09935v2", "abstract": "As natural language generation (NLG) models have become prevalent, systematically assessing the quality of machine-generated texts has become increasingly important. Recent studies introduce LLM-based evaluators that operate as reference-free metrics, demonstrating their capability to adeptly handle novel tasks. However, these models generally rely on a single-agent approach, which, we argue, introduces an inherent limit to their performance. This is because there exist biases in LLM agent's responses, including preferences for certain text structure or content. In this work, we propose DEBATE, an NLG evaluation framework based on multi-agent scoring system augmented with a concept of Devil's Advocate. Within the framework, one agent is instructed to criticize other agents' arguments, potentially resolving the bias in LLM agent's answers. DEBATE substantially outperforms the previous state-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation, SummEval and TopicalChat. We also show that the extensiveness of debates among agents and the persona of an agent can influence the performance of evaluators.", "source": "arxiv", "arxiv_id": "2405.09935v2", "pdf_url": "https://arxiv.org/pdf/2405.09935v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-16T09:41:12Z", "updated": "2024-05-24T01:06:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning", "authors": ["Siyuan Guo", "Cheng Deng", "Ying Wen", "Hechang Chen", "Yi Chang", "Jun Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.17453v5", "abstract": "In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100\\% success rate in the development stage, while attaining 36\\% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent.", "source": "arxiv", "arxiv_id": "2402.17453v5", "pdf_url": "https://arxiv.org/pdf/2402.17453v5", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-27T12:26:07Z", "updated": "2024-05-28T06:50:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Data Interpreter: An LLM Agent For Data Science", "authors": ["Sirui Hong", "Yizhang Lin", "Bang Liu", "Bangbang Liu", "Binhao Wu", "Ceyao Zhang", "Chenxing Wei", "Danyang Li", "Jiaqi Chen", "Jiayi Zhang", "Jinlin Wang", "Li Zhang", "Lingyao Zhang", "Min Yang", "Mingchen Zhuge", "Taicheng Guo", "Tuo Zhou", "Wei Tao", "Xiangru Tang", "Xiangtao Lu", "Xiawu Zheng", "Xinbing Liang", "Yaying Fei", "Yuheng Cheng", "Zhibin Gou", "Zongze Xu", "Chenglin Wu"], "year": 2024, "url": "http://arxiv.org/abs/2402.18679v4", "abstract": "Large Language Model (LLM)-based agents have shown effectiveness across many applications. However, their use in data science scenarios requiring solving long-term interconnected tasks, dynamic data adjustments and domain expertise remains challenging. Previous approaches primarily focus on individual tasks, making it difficult to assess the complete data science workflow. Moreover, they struggle to handle real-time changes in intermediate data and fail to adapt dynamically to evolving task dependencies inherent to data science problems. In this paper, we present Data Interpreter, an LLM-based agent designed to automatically solve various data science problems end-to-end. Our Data Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling, which breaks down complex problems into manageable subproblems, enabling dynamic node generation and graph optimization; and 2) Programmable Node Generation, a technique that refines and verifies each subproblem to iteratively improve code generation results and robustness. Extensive experiments consistently demonstrate the superiority of Data Interpreter. On InfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from 75.9% to 94.9%. For machine learning and open-ended tasks, it improves performance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on the MATH dataset, Data Interpreter achieves remarkable performance with a 26% improvement compared to state-of-the-art baselines. The code is available at https://github.com/geekan/MetaGPT.", "source": "arxiv", "arxiv_id": "2402.18679v4", "pdf_url": "https://arxiv.org/pdf/2402.18679v4", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-28T19:49:55Z", "updated": "2024-10-15T15:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts", "authors": ["Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Md Rizwan Parvez", "Enamul Hoque", "Shafiq Joty"], "year": 2024, "url": "http://arxiv.org/abs/2408.05346v3", "abstract": "Data-driven storytelling is a powerful method for conveying insights by combining narrative techniques with visualizations and text. These stories integrate visual aids, such as highlighted bars and lines in charts, along with textual annotations explaining insights. However, creating such stories requires a deep understanding of the data and meticulous narrative planning, often necessitating human intervention, which can be time-consuming and mentally taxing. While Large Language Models (LLMs) excel in various NLP tasks, their ability to generate coherent and comprehensive data stories remains underexplored. In this work, we introduce a novel task for data story generation and a benchmark containing 1,449 stories from diverse sources. To address the challenges of crafting coherent data stories, we propose a multiagent framework employing two LLM agents designed to replicate the human storytelling process: one for understanding and describing the data (Reflection), generating the outline, and narration, and another for verification at each intermediary step. While our agentic framework generally outperforms non-agentic counterparts in both model-based and human evaluations, the results also reveal unique challenges in data story generation.", "source": "arxiv", "arxiv_id": "2408.05346v3", "pdf_url": "https://arxiv.org/pdf/2408.05346v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-09T21:31:33Z", "updated": "2024-10-04T01:07:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DebUnc: Improving Large Language Model Agent Communication With Uncertainty Metrics", "authors": ["Luke Yoffe", "Alfonso Amayuelas", "William Yang Wang"], "year": 2024, "url": "http://arxiv.org/abs/2407.06426v2", "abstract": "Multi-agent debates have been introduced to improve the accuracy of Large Language Models (LLMs) by having multiple agents discuss solutions to a problem over several rounds of debate. However, models often generate incorrect yet confident-sounding responses, which can mislead others. This issue arises partly because agents do not consider how confident their peers are. To address this, we propose DebUnc, a debate framework that uses uncertainty metrics to assess agent confidence. Confidence is then conveyed through a modified attention mechanism that adjusts token weights, or through textual prompts. Evaluations across benchmarks show that attention-based methods are particularly effective and that performance continues to improve as uncertainty estimation becomes more reliable. The code is available at https://github.com/lukeyoffe/debunc.", "source": "arxiv", "arxiv_id": "2407.06426v2", "pdf_url": "https://arxiv.org/pdf/2407.06426v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-08T22:15:01Z", "updated": "2025-02-22T02:15:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Defending Jailbreak Prompts via In-Context Adversarial Game", "authors": ["Yujun Zhou", "Yufei Han", "Haomin Zhuang", "Kehan Guo", "Zhenwen Liang", "Hongyan Bao", "Xiangliang Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.13148v3", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism.", "source": "arxiv", "arxiv_id": "2402.13148v3", "pdf_url": "https://arxiv.org/pdf/2402.13148v3", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-20T17:04:06Z", "updated": "2025-02-21T20:53:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games", "authors": ["Yikuan Yan", "Yaolun Zhang", "Keman Huang"], "year": 2024, "url": "http://arxiv.org/abs/2403.17674v1", "abstract": "Integrating LLM and reinforcement learning (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations. In this study, we introduce SecurityBot, a LLM agent mentored by pre-trained RL agents, to support cybersecurity operations. In particularly, the LLM agent is supported with a profile module to generated behavior guidelines, a memory module to accumulate local experiences, a reflection module to re-evaluate choices, and an action module to reduce action space. Additionally, it adopts the collaboration mechanism to take suggestions from pre-trained RL agents, including a cursor for dynamic suggestion taken, an aggregator for multiple mentors' suggestions ranking and a caller for proactive suggestion asking. Building on the CybORG experiment framework, our experiences show that SecurityBot demonstrates significant performance improvement compared with LLM or RL standalone, achieving the complementary performance in the cybersecurity games.", "source": "arxiv", "arxiv_id": "2403.17674v1", "pdf_url": "https://arxiv.org/pdf/2403.17674v1", "categories": ["cs.CR", "cs.AI", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-03-26T13:02:46Z", "updated": "2024-03-26T13:02:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis", "authors": ["Frank Xing"], "year": 2024, "url": "http://arxiv.org/abs/2401.05799v1", "abstract": "Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed.", "source": "arxiv", "arxiv_id": "2401.05799v1", "pdf_url": "https://arxiv.org/pdf/2401.05799v1", "categories": ["cs.CL", "cs.AI", "cs.MA", "q-fin.GN"], "primary_category": "cs.CL", "doi": "10.1145/3688399", "venue": "", "published": "2024-01-11T10:06:42Z", "updated": "2024-01-11T10:06:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Devil's Advocate: Anticipatory Reflection for LLM Agents", "authors": ["Haoyu Wang", "Tao Li", "Zhiwei Deng", "Dan Roth", "Yang Li"], "year": 2024, "url": "http://arxiv.org/abs/2405.16334v4", "abstract": "In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks. Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. %; and when necessary, to explore ``the road not taken.'' We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology -- a zero-shot approach -- within WebArena for practical tasks in web environments, our agent demonstrates superior performance with a success rate of 23.5% over existing zero-shot methods by 3.5%. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions by 45% needed to achieve a task.", "source": "arxiv", "arxiv_id": "2405.16334v4", "pdf_url": "https://arxiv.org/pdf/2405.16334v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-25T19:20:15Z", "updated": "2024-06-20T19:41:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner", "authors": ["Kenneth Li", "Yiming Wang", "Fernanda ViÃ©gas", "Martin Wattenberg"], "year": 2024, "url": "http://arxiv.org/abs/2406.11978v1", "abstract": "We present an approach called Dialogue Action Tokens (DAT) that adapts language model agents to plan goal-directed dialogues. The core idea is to treat each utterance as an action, thereby converting dialogues into games where existing approaches such as reinforcement learning can be applied. Specifically, we freeze a pretrained language model and train a small planner model that predicts a continuous action vector, used for controlled generation in each round. This design avoids the problem of language degradation under reward optimization. When evaluated on the Sotopia platform for social simulations, the DAT-steered LLaMA model surpasses GPT-4's performance. We also apply DAT to steer an attacker language model in a novel multi-turn red-teaming setting, revealing a potential new attack surface.", "source": "arxiv", "arxiv_id": "2406.11978v1", "pdf_url": "https://arxiv.org/pdf/2406.11978v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-17T18:01:32Z", "updated": "2024-06-17T18:01:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model", "authors": ["Lirui Zhao", "Yue Yang", "Kaipeng Zhang", "Wenqi Shao", "Yuxin Zhang", "Yu Qiao", "Ping Luo", "Rongrong Ji"], "year": 2024, "url": "http://arxiv.org/abs/2404.01342v1", "abstract": "Text-to-image (T2I) generative models have attracted significant attention and found extensive applications within and beyond academic research. For example, the Civitai community, a platform for T2I innovation, currently hosts an impressive array of 74,492 distinct models. However, this diversity presents a formidable challenge in selecting the most appropriate model and parameters, a process that typically requires numerous trials. Drawing inspiration from the tool usage research of large language models (LLMs), we introduce DiffAgent, an LLM agent designed to screen the accurate selection in seconds via API calls. DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences. To train and evaluate DiffAgent's capabilities, we present DABench, a comprehensive dataset encompassing an extensive range of T2I APIs from the community. Our evaluations reveal that DiffAgent not only excels in identifying the appropriate T2I API but also underscores the effectiveness of the SFTA training framework. Codes are available at https://github.com/OpenGVLab/DiffAgent.", "source": "arxiv", "arxiv_id": "2404.01342v1", "pdf_url": "https://arxiv.org/pdf/2404.01342v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-31T06:28:15Z", "updated": "2024-03-31T06:28:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation", "authors": ["Zhenyu Wang", "Enze Xie", "Aoxue Li", "Zhongdao Wang", "Xihui Liu", "Zhenguo Li"], "year": 2024, "url": "http://arxiv.org/abs/2401.15688v2", "abstract": "Despite significant advancements in text-to-image models for generating high-quality images, these methods still struggle to ensure the controllability of text prompts over images in the context of complex text prompts, especially when it comes to retaining object attributes and relationships. In this paper, we propose CompAgent, a training-free approach for compositional text-to-image generation, with a large language model (LLM) agent as its core. The fundamental idea underlying CompAgent is premised on a divide-and-conquer methodology. Given a complex text prompt containing multiple concepts including objects, attributes, and relationships, the LLM agent initially decomposes it, which entails the extraction of individual objects, their associated attributes, and the prediction of a coherent scene layout. These individual objects can then be independently conquered. Subsequently, the agent performs reasoning by analyzing the text, plans and employs the tools to compose these isolated objects. The verification and human feedback mechanism is finally incorporated into our agent to further correct the potential attribute errors and refine the generated images. Guided by the LLM agent, we propose a tuning-free multi-concept customization model and a layout-to-image generation model as the tools for concept composition, and a local image editing method as the tool to interact with the agent for verification. The scene layout controls the image generation process among these tools to prevent confusion among multiple objects. Extensive experiments demonstrate the superiority of our approach for compositional text-to-image generation: CompAgent achieves more than 10\\% improvement on T2I-CompBench, a comprehensive benchmark for open-world compositional T2I generation. The extension to various related tasks also illustrates the flexibility of our CompAgent for potential applications.", "source": "arxiv", "arxiv_id": "2401.15688v2", "pdf_url": "https://arxiv.org/pdf/2401.15688v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-01-28T16:18:39Z", "updated": "2024-01-30T13:05:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Do LLM Agents Have Regret? A Case Study in Online Learning and Games", "authors": ["Chanwoo Park", "Xiangyu Liu", "Asuman Ozdaglar", "Kaiqing Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.16843v5", "abstract": "Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \\emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \\emph{unsupervised} training loss of \\emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.", "source": "arxiv", "arxiv_id": "2403.16843v5", "pdf_url": "https://arxiv.org/pdf/2403.16843v5", "categories": ["cs.LG", "cs.AI", "cs.GT"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-03-25T15:04:11Z", "updated": "2025-10-15T12:44:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Do LLMs \"know\" internally when they follow instructions?", "authors": ["Juyeon Heo", "Christina Heinze-Deml", "Oussama Elachqar", "Kwan Ho Ryan Chan", "Shirley Ren", "Udhay Nallasamy", "Andy Miller", "Jaya Narain"], "year": 2024, "url": "http://arxiv.org/abs/2410.14516v5", "abstract": "Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent undesirable outputs, a deeper understanding of how LLMs' internal states relate to these outcomes is required. In this work, we investigate whether LLMs encode information in their representations that correlate with instruction-following success - a property we term knowing internally. Our analysis identifies a direction in the input embedding space, termed the instruction-following dimension, that predicts whether a response will comply with a given instruction. We find that this dimension generalizes well across unseen tasks but not across unseen instruction types. We demonstrate that modifying representations along this dimension improves instruction-following success rates compared to random changes, without compromising response quality. Further investigation reveals that this dimension is more closely related to the phrasing of prompts rather than the inherent difficulty of the task or instructions. This work provides insight into the internal workings of LLMs' instruction-following, paving the way for reliable LLM agents.", "source": "arxiv", "arxiv_id": "2410.14516v5", "pdf_url": "https://arxiv.org/pdf/2410.14516v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-18T14:55:14Z", "updated": "2025-03-28T15:40:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation", "authors": ["Jia Gu", "Liang Pang", "Huawei Shen", "Xueqi Cheng"], "year": 2024, "url": "http://arxiv.org/abs/2404.09043v3", "abstract": "With the rapid advancement of large language models (LLMs) for handling complex language tasks, an increasing number of studies are employing LLMs as agents to emulate the sequential decision-making processes of humans often represented as Markov decision-making processes (MDPs). The actions in MDPs adhere to specific probability distributions and require iterative sampling. This arouses curiosity regarding the capacity of LLM agents to comprehend probability distributions, thereby guiding the agent's behavioral decision-making through probabilistic sampling and generating behavioral sequences. To answer the above question, we divide the problem into two main aspects: sequence simulation with known probability distribution and sequence simulation with unknown probability distribution. Our analysis indicates that LLM agents can understand probabilities, but they struggle with probability sampling. Their ability to perform probabilistic sampling can be improved to some extent by integrating coding tools, but this level of sampling precision still makes it difficult to simulate human behavior as agents.", "source": "arxiv", "arxiv_id": "2404.09043v3", "pdf_url": "https://arxiv.org/pdf/2404.09043v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-13T16:59:28Z", "updated": "2024-12-18T15:56:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Domain-specific ReAct for physics-integrated iterative modeling: A case study of LLM agents for gas path analysis of gas turbines", "authors": ["Tao Song", "Yuwei Fan", "Chenlong Feng", "Keyu Song", "Chao Liu", "Dongxiang Jiang"], "year": 2024, "url": "http://arxiv.org/abs/2406.07572v1", "abstract": "This study explores the application of large language models (LLMs) with callable tools in energy and power engineering domain, focusing on gas path analysis of gas turbines. We developed a dual-agent tool-calling process to integrate expert knowledge, predefined tools, and LLM reasoning. We evaluated various LLMs, including LLama3, Qwen1.5 and GPT. Smaller models struggled with tool usage and parameter extraction, while larger models demonstrated favorable capabilities. All models faced challenges with complex, multi-component problems. Based on the test results, we infer that LLMs with nearly 100 billion parameters could meet professional scenario requirements with fine-tuning and advanced prompt design. Continued development are likely to enhance their accuracy and effectiveness, paving the way for more robust AI-driven solutions.", "source": "arxiv", "arxiv_id": "2406.07572v1", "pdf_url": "https://arxiv.org/pdf/2406.07572v1", "categories": ["cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-01T13:35:18Z", "updated": "2024-06-01T13:35:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DropMicroFluidAgents (DMFAs): Autonomous Droplet Microfluidic Research Framework Through Large Language Model Agents", "authors": ["Dinh-Nguyen Nguyen", "Raymond Kai-Yu Tong", "Ngoc-Duy Dinh"], "year": 2024, "url": "http://arxiv.org/abs/2501.14772v1", "abstract": "Applying Large language models (LLMs) within specific domains requires substantial adaptation to account for the unique terminologies, nuances, and context-specific challenges inherent to those areas. Here, we introduce DropMicroFluidAgents (DMFAs), an advanced language-driven framework leveraging state-of-the-art pre-trained LLMs. DMFAs employs LLM agents to perform two key functions: (1) delivering focused guidance, answers, and suggestions specific to droplet microfluidics and (2) generating machine learning models to optimise and automate the design of droplet microfluidic devices, including the creation of code-based computer-aided design (CAD) scripts to enable rapid and precise design execution. Experimental evaluations demonstrated that the integration of DMFAs with the LLAMA3.1 model yielded the highest accuracy of 76.15%, underscoring the significant performance enhancement provided by agent integration. This effect was particularly pronounced when DMFAs were paired with the GEMMA2 model, resulting in a 34.47% improvement in accuracy compared to the standalone GEMMA2 configuration. This study demonstrates the effective use of LLM agents in droplet microfluidics research as powerful tools for automating workflows, synthesising knowledge, optimising designs, and interacting with external systems. These capabilities enable their application across education and industrial support, driving greater efficiency in scientific discovery and innovation.", "source": "arxiv", "arxiv_id": "2501.14772v1", "pdf_url": "https://arxiv.org/pdf/2501.14772v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-12-30T11:58:52Z", "updated": "2024-12-30T11:58:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "DynaSaur: Large Language Agents Beyond Predefined Actions", "authors": ["Dang Nguyen", "Viet Dac Lai", "Seunghyun Yoon", "Ryan A. Rossi", "Handong Zhao", "Ruiyi Zhang", "Puneet Mathur", "Nedim Lipka", "Yu Wang", "Trung Bui", "Franck Dernoncourt", "Tianyi Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2411.01747v3", "abstract": "Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly scoped environments, it presents two major challenges for real-world, open-ended scenarios: (1) it significantly restricts the planning and acting capabilities of LLM agents, and (2) it requires substantial human effort to enumerate and implement all possible actions, which is impractical in complex environments with a vast number of potential actions. To address these limitations, we propose an LLM agent framework that can dynamically create and compose actions as needed. In this framework, the agent interacts with its environment by generating and executing programs written in a general-purpose programming language. Moreover, generated actions are accumulated over time for future reuse. Our extensive experiments across multiple benchmarks show that this framework significantly improves flexibility and outperforms prior methods that rely on a fixed action set. Notably, it enables LLM agents to adapt and recover in scenarios where predefined actions are insufficient or fail due to unforeseen edge cases. Our code can be found in https://github.com/adobe-research/dynasaur.", "source": "arxiv", "arxiv_id": "2411.01747v3", "pdf_url": "https://arxiv.org/pdf/2411.01747v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-04T02:08:59Z", "updated": "2025-09-04T16:22:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "authors": ["Wenqi Shi", "Ran Xu", "Yuchen Zhuang", "Yue Yu", "Jieyu Zhang", "Hang Wu", "Yuanda Zhu", "Joyce Ho", "Carl Yang", "May D. Wang"], "year": 2024, "url": "http://arxiv.org/abs/2401.07128v3", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations.", "source": "arxiv", "arxiv_id": "2401.07128v3", "pdf_url": "https://arxiv.org/pdf/2401.07128v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-13T18:09:05Z", "updated": "2024-10-04T05:56:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR", "authors": ["Mengxu Pan", "Alexandra Kitson", "Hongyu Wan", "Mirjana Prpa"], "year": 2024, "url": "http://arxiv.org/abs/2410.02406v1", "abstract": "Many people struggle with learning a new language, with traditional tools falling short in providing contextualized learning tailored to each learner's needs. The recent development of large language models (LLMs) and embodied conversational agents (ECAs) in social virtual reality (VR) provide new opportunities to practice language learning in a contextualized and naturalistic way that takes into account the learner's language level and needs. To explore this opportunity, we developed ELLMA-T, an ECA that leverages an LLM (GPT-4) and situated learning framework for supporting learning English language in social VR (VRChat). Drawing on qualitative interviews (N=12), we reveal the potential of ELLMA-T to generate realistic, believable and context-specific role plays for agent-learner interaction in VR, and LLM's capability to provide initial language assessment and continuous feedback to learners. We provide five design implications for the future development of LLM-based language agents in social VR.", "source": "arxiv", "arxiv_id": "2410.02406v1", "pdf_url": "https://arxiv.org/pdf/2410.02406v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-10-03T11:32:53Z", "updated": "2024-10-03T11:32:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents", "authors": ["Junting Chen", "Checheng Yu", "Xunzhe Zhou", "Tianqi Xu", "Yao Mu", "Mengkang Hu", "Wenqi Shao", "Yikai Wang", "Guohao Li", "Lin Shao"], "year": 2024, "url": "http://arxiv.org/abs/2410.22662v2", "abstract": "Heterogeneous multi-robot systems (HMRS) have emerged as a powerful approach for tackling complex tasks that single robots cannot manage alone. Current large-language-model-based multi-agent systems (LLM-based MAS) have shown success in areas like software development and operating systems, but applying these systems to robot control presents unique challenges. In particular, the capabilities of each agent in a multi-robot system are inherently tied to the physical composition of the robots, rather than predefined roles. To address this issue, we introduce a novel multi-agent framework designed to enable effective collaboration among heterogeneous robots with varying embodiments and capabilities, along with a new benchmark named Habitat-MAS. One of our key designs is $\\textit{Robot Resume}$: Instead of adopting human-designed role play, we propose a self-prompted approach, where agents comprehend robot URDF files and call robot kinematics tools to generate descriptions of their physics capabilities to guide their behavior in task planning and action execution. The Habitat-MAS benchmark is designed to assess how a multi-agent framework handles tasks that require embodiment-aware reasoning, which includes 1) manipulation, 2) perception, 3) navigation, and 4) comprehensive multi-floor object rearrangement. The experimental results indicate that the robot's resume and the hierarchical design of our multi-agent system are essential for the effective operation of the heterogeneous multi-robot system within this intricate problem context.", "source": "arxiv", "arxiv_id": "2410.22662v2", "pdf_url": "https://arxiv.org/pdf/2410.22662v2", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-10-30T03:20:01Z", "updated": "2025-02-17T08:33:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EPO: Hierarchical LLM Agents with Environment Preference Optimization", "authors": ["Qi Zhao", "Haotian Fu", "Chen Sun", "George Konidaris"], "year": 2024, "url": "http://arxiv.org/abs/2408.16090v2", "abstract": "Long-horizon decision-making tasks present significant challenges for LLM-based agents due to the need for extensive planning over multiple steps. In this paper, we propose a hierarchical framework that decomposes complex tasks into manageable subgoals, utilizing separate LLMs for subgoal prediction and low-level action generation. To address the challenge of creating training signals for unannotated datasets, we develop a reward model that leverages multimodal environment feedback to automatically generate reward signals. We introduce Environment Preference Optimization (EPO), a novel method that generates preference signals from the environment's feedback and uses them to train LLM-based agents. Extensive experiments on ALFRED demonstrate the state-of-the-art performance of our framework, achieving first place on the ALFRED public leaderboard and showcasing its potential to improve long-horizon decision-making in diverse environments.", "source": "arxiv", "arxiv_id": "2408.16090v2", "pdf_url": "https://arxiv.org/pdf/2408.16090v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-08-28T18:44:02Z", "updated": "2024-10-03T20:35:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EcoAct: Economic Agent Determines When to Register What Action", "authors": ["Shaokun Zhang", "Jieyu Zhang", "Dujian Ding", "Mirian Hipolito Garcia", "Ankur Mallick", "Daniel Madrigal", "Menglin Xia", "Victor RÃ¼hle", "Qingyun Wu", "Chi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2411.01643v1", "abstract": "Recent advancements have enabled Large Language Models (LLMs) to function as agents that can perform actions using external tools. This requires registering, i.e., integrating tool information into the LLM context prior to taking actions. Current methods indiscriminately incorporate all candidate tools into the agent's context and retain them across multiple reasoning steps. This process remains opaque to LLM agents and is not integrated into their reasoning procedures, leading to inefficiencies due to increased context length from irrelevant tools. To address this, we introduce EcoAct, a tool using algorithm that allows LLMs to selectively register tools as needed, optimizing context use. By integrating the tool registration process into the reasoning procedure, EcoAct reduces computational costs by over 50% in multiple steps reasoning tasks while maintaining performance, as demonstrated through extensive experiments. Moreover, it can be plugged into any reasoning pipeline with only minor modifications to the prompt, making it applicable to LLM agents now and future.", "source": "arxiv", "arxiv_id": "2411.01643v1", "pdf_url": "https://arxiv.org/pdf/2411.01643v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-03T17:37:06Z", "updated": "2024-11-03T17:37:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents", "authors": ["Yuxi Wei", "Zi Wang", "Yifan Lu", "Chenxin Xu", "Changxing Liu", "Hao Zhao", "Siheng Chen", "Yanfeng Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.05746v3", "abstract": "Scene simulation in autonomous driving has gained significant attention because of its huge potential for generating customized data. However, existing editable scene simulation approaches face limitations in terms of user interaction efficiency, multi-camera photo-realistic rendering and external digital assets integration. To address these challenges, this paper introduces ChatSim, the first system that enables editable photo-realistic 3D driving scene simulations via natural language commands with external digital assets. To enable editing with high command flexibility,~ChatSim leverages a large language model (LLM) agent collaboration framework. To generate photo-realistic outcomes, ChatSim employs a novel multi-camera neural radiance field method. Furthermore, to unleash the potential of extensive high-quality digital assets, ChatSim employs a novel multi-camera lighting estimation method to achieve scene-consistent assets' rendering. Our experiments on Waymo Open Dataset demonstrate that ChatSim can handle complex language commands and generate corresponding photo-realistic scene videos.", "source": "arxiv", "arxiv_id": "2402.05746v3", "pdf_url": "https://arxiv.org/pdf/2402.05746v3", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-02-08T15:26:28Z", "updated": "2024-06-26T10:44:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Educational-Psychological Dialogue Robot Based on Multi-Agent Collaboration", "authors": ["Shiwen Ni", "Min Yang"], "year": 2024, "url": "http://arxiv.org/abs/2412.03847v1", "abstract": "Intelligent dialogue systems are increasingly used in modern education and psychological counseling fields, but most existing systems are limited to a single domain, cannot deal with both educational and psychological issues, and often lack accuracy and professionalism when dealing with complex issues. To address these problems, this paper proposes an intelligent dialog system that combines educational and psychological counseling functions. The system consists of multiple AI agent, including security detection agent, intent identification agent, educational LLM agent, and psychological LLM agent, which work in concert to ensure the provision of accurate educational knowledge Q\\&A and psychological support services. Specifically, the system recognizes user-input intentions through an intention classification model and invokes a retrieval-enhanced educational grand model and a psychological grand model fine-tuned with psychological data in order to provide professional educational advice and psychological support.", "source": "arxiv", "arxiv_id": "2412.03847v1", "pdf_url": "https://arxiv.org/pdf/2412.03847v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "ICSR 2024", "published": "2024-12-05T03:27:02Z", "updated": "2024-12-05T03:27:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Efficient Sequential Decision Making with Large Language Models", "authors": ["Dingyang Chen", "Qi Zhang", "Yinglun Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2406.12125v2", "abstract": "This paper focuses on extending the success of large language models (LLMs) to sequential decision making. Existing efforts either (i) re-train or finetune LLMs for decision making, or (ii) design prompts for pretrained LLMs. The former approach suffers from the computational burden of gradient updates, and the latter approach does not show promising results. In this paper, we propose a new approach that leverages online model selection algorithms to efficiently incorporate LLMs agents into sequential decision making. Statistically, our approach significantly outperforms both traditional decision making algorithms and vanilla LLM agents. Computationally, our approach avoids the need for expensive gradient updates of LLMs, and throughout the decision making process, it requires only a small number of LLM calls. We conduct extensive experiments to verify the effectiveness of our proposed approach. As an example, on a large-scale Amazon dataset, our approach achieves more than a 6x performance gain over baselines while calling LLMs in only 1.5% of the time steps.", "source": "arxiv", "arxiv_id": "2406.12125v2", "pdf_url": "https://arxiv.org/pdf/2406.12125v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-17T22:13:22Z", "updated": "2025-06-15T04:49:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Efficient Tool Use with Chain-of-Abstraction Reasoning", "authors": ["Silin Gao", "Jane Dwivedi-Yu", "Ping Yu", "Xiaoqing Ellen Tan", "Ramakanth Pasunuru", "Olga Golovneva", "Koustuv Sinha", "Asli Celikyilmaz", "Antoine Bosselut", "Tianlu Wang"], "year": 2024, "url": "http://arxiv.org/abs/2401.17464v3", "abstract": "To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average ~6% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.", "source": "arxiv", "arxiv_id": "2401.17464v3", "pdf_url": "https://arxiv.org/pdf/2401.17464v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-30T21:53:30Z", "updated": "2025-01-08T16:27:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Eliciting Problem Specifications via Large Language Models", "authors": ["Robert E. Wray", "James R. Kirk", "John E. Laird"], "year": 2024, "url": "http://arxiv.org/abs/2405.12147v2", "abstract": "Cognitive systems generally require a human to translate a problem definition into some specification that the cognitive system can use to attempt to solve the problem or perform the task. In this paper, we illustrate that large language models (LLMs) can be utilized to map a problem class, defined in natural language, into a semi-formal specification that can then be utilized by an existing reasoning and learning system to solve instances from the problem class. We present the design of LLM-enabled cognitive task analyst agent(s). Implemented with LLM agents, this system produces a definition of problem spaces for tasks specified in natural language. LLM prompts are derived from the definition of problem spaces in the AI literature and general problem-solving strategies (Polya's How to Solve It). A cognitive system can then use the problem-space specification, applying domain-general problem solving strategies (\"weak methods\" such as search), to solve multiple instances of problems from the problem class. This result, while preliminary, suggests the potential for speeding cognitive systems research via disintermediation of problem formulation while also retaining core capabilities of cognitive systems, such as robust inference and online learning.", "source": "arxiv", "arxiv_id": "2405.12147v2", "pdf_url": "https://arxiv.org/pdf/2405.12147v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-20T16:19:02Z", "updated": "2024-06-10T19:05:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation", "authors": ["Mohammadmehdi Ataei", "Hyunmin Cheong", "Daniele Grandi", "Ye Wang", "Nigel Morris", "Alexander Tessier"], "year": 2024, "url": "http://arxiv.org/abs/2404.16045v1", "abstract": "Requirements elicitation, a critical, yet time-consuming and challenging step in product development, often fails to capture the full spectrum of user needs. This may lead to products that fall short of expectations. This paper introduces a novel framework that leverages Large Language Models (LLMs) to automate and enhance the requirements elicitation process. LLMs are used to generate a vast array of simulated users (LLM agents), enabling the exploration of a much broader range of user needs and unforeseen use cases. These agents engage in product experience scenarios, through explaining their actions, observations, and challenges. Subsequent agent interviews and analysis uncover valuable user needs, including latent ones. We validate our framework with three experiments. First, we explore different methodologies for diverse agent generation, discussing their advantages and shortcomings. We measure the diversity of identified user needs and demonstrate that context-aware agent generation leads to greater diversity. Second, we show how our framework effectively mimics empathic lead user interviews, identifying a greater number of latent needs than conventional human interviews. Third, we showcase that LLMs can be used to analyze interviews, capture needs, and classify them as latent or not. Our work highlights the potential of using LLM agents to accelerate early-stage product development, reduce costs, and increase innovation.", "source": "arxiv", "arxiv_id": "2404.16045v1", "pdf_url": "https://arxiv.org/pdf/2404.16045v1", "categories": ["cs.HC", "cs.AI", "cs.MA"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-04-04T17:36:29Z", "updated": "2024-04-04T17:36:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Embodied LLM Agents Learn to Cooperate in Organized Teams", "authors": ["Xudong Guo", "Kaixuan Huang", "Jiale Liu", "Wenhui Fan", "Natalia VÃ©lez", "Qingyun Wu", "Huazheng Wang", "Thomas L. Griffiths", "Mengdi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2403.12482v2", "abstract": "Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.", "source": "arxiv", "arxiv_id": "2403.12482v2", "pdf_url": "https://arxiv.org/pdf/2403.12482v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-19T06:39:47Z", "updated": "2024-05-23T06:29:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Emergent social conventions and collective bias in LLM populations", "authors": ["Ariel Flint Ashery", "Luca Maria Aiello", "Andrea Baronchelli"], "year": 2024, "url": "http://arxiv.org/abs/2410.08948v2", "abstract": "Social conventions are the backbone of social coordination, shaping how individuals form a group. As growing populations of artificial intelligence (AI) agents communicate through natural language, a fundamental question is whether they can bootstrap the foundations of a society. Here, we present experimental results that demonstrate the spontaneous emergence of universally adopted social conventions in decentralized populations of large language model (LLM) agents. We then show how strong collective biases can emerge during this process, even when agents exhibit no bias individually. Last, we examine how committed minority groups of adversarial LLM agents can drive social change by imposing alternative social conventions on the larger population. Our results show that AI systems can autonomously develop social conventions without explicit programming and have implications for designing AI systems that align, and remain aligned, with human values and societal goals.", "source": "arxiv", "arxiv_id": "2410.08948v2", "pdf_url": "https://arxiv.org/pdf/2410.08948v2", "categories": ["cs.MA", "cs.AI", "cs.CY", "physics.soc-ph"], "primary_category": "cs.MA", "doi": "10.1126/sciadv.adu9368", "venue": "Science Advances 11, eadu9368 (2025)", "published": "2024-10-11T16:16:38Z", "updated": "2025-05-29T09:50:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Empirical Asset Pricing with Large Language Model Agents", "authors": ["Junyan Cheng", "Peter Chin"], "year": 2024, "url": "http://arxiv.org/abs/2409.17266v2", "abstract": "In this study, we introduce a novel asset pricing model leveraging the Large Language Model (LLM) agents, which integrates qualitative discretionary investment evaluations from LLM agents with quantitative financial economic factors manually curated, aiming to explain the excess asset returns. The experimental results demonstrate that our methodology surpasses traditional machine learning-based baselines in both portfolio optimization and asset pricing errors. Notably, the Sharpe ratio for portfolio optimization and the mean magnitude of $|Î±|$ for anomaly portfolios experienced substantial enhancements of 10.6\\% and 10.0\\% respectively. Moreover, we performed comprehensive ablation studies on our model and conducted a thorough analysis of the method to extract further insights into the proposed approach. Our results show effective evidence of the feasibility of applying LLMs in empirical asset pricing.", "source": "arxiv", "arxiv_id": "2409.17266v2", "pdf_url": "https://arxiv.org/pdf/2409.17266v2", "categories": ["cs.AI", "cs.CE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-25T18:27:35Z", "updated": "2025-03-28T01:02:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Empowering Large Language Model Agents through Action Learning", "authors": ["Haiteng Zhao", "Chang Ma", "Guoyin Wang", "Jing Su", "Lingpeng Kong", "Jingjing Xu", "Zhi-Hong Deng", "Hongxia Yang"], "year": 2024, "url": "http://arxiv.org/abs/2402.15809v2", "abstract": "Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents.", "source": "arxiv", "arxiv_id": "2402.15809v2", "pdf_url": "https://arxiv.org/pdf/2402.15809v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-24T13:13:04Z", "updated": "2024-08-08T07:05:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enabling Generative Design Tools with LLM Agents for Mechanical Computation Devices: A Case Study", "authors": ["Qiuyu Lu", "Jiawei Fang", "Zhihao Yao", "Yue Yang", "Shiqing Lyu", "Haipeng Mi", "Lining Yao"], "year": 2024, "url": "http://arxiv.org/abs/2405.17837v3", "abstract": "In the field of Human-Computer Interaction (HCI), interactive devices with embedded mechanical computation are gaining attention. The rise of these cutting-edge devices has created a need for specialized design tools that democratize the prototyping process. While current tools streamline prototyping through parametric design and simulation, they often come with a steep learning curve and may not fully support creative ideation. In this study, we use fluidic computation interfaces as a case study to explore how design tools for such devices can be augmented by Large Language Model agents (LLMs). Integrated with LLMs, the Generative Design Tool (GDT) better understands the capabilities and limitations of new technologies, proposes diverse and practical applications, and suggests designs that are technically and contextually appropriate. Additionally, it generates design parameters for visualizing results and producing fabrication-ready support files. This paper details the GDT's framework, implementation, and performance while addressing its potential and challenges.", "source": "arxiv", "arxiv_id": "2405.17837v3", "pdf_url": "https://arxiv.org/pdf/2405.17837v3", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-05-28T05:21:09Z", "updated": "2024-10-29T22:28:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework", "authors": ["Honghao Shi", "Longkai Cheng", "Wenli Wu", "Yuhang Wang", "Xuan Liu", "Shaokai Nie", "Weixv Wang", "Xuebin Min", "Chunlei Men", "Yonghua Lin"], "year": 2024, "url": "http://arxiv.org/abs/2411.05349v1", "abstract": "Recent advancements in Large Language Models (LLMs) and related technologies such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have enabled the creation of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have developed an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovations include a knowledge base tailored for cluster diagnostics, enhanced LLM algorithms, practical deployment strategies for agents, and a benchmark specifically designed for evaluating LLM capabilities in this domain. Through extensive experimentation across multiple dimensions, we have demonstrated the superiority of our system in addressing the challenges faced in cluster diagnostics, particularly in detecting and rectifying performance issues more efficiently and accurately than traditional methods.", "source": "arxiv", "arxiv_id": "2411.05349v1", "pdf_url": "https://arxiv.org/pdf/2411.05349v1", "categories": ["cs.AI", "cs.DC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-08T06:12:56Z", "updated": "2024-11-08T06:12:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models", "authors": ["Yuanzhao Zhai", "Tingkai Yang", "Kele Xu", "Feng Dawei", "Cheng Yang", "Bo Ding", "Huaimin Wang"], "year": 2024, "url": "http://arxiv.org/abs/2409.09345v1", "abstract": "Agents significantly enhance the capabilities of standalone Large Language Models (LLMs) by perceiving environments, making decisions, and executing actions. However, LLM agents still face challenges in tasks that require multiple decision-making steps. Estimating the value of actions in specific tasks is difficult when intermediate actions are neither appropriately rewarded nor penalized. In this paper, we propose leveraging a task-relevant Q-value model to guide action selection. Specifically, we first collect decision-making trajectories annotated with step-level Q values via Monte Carlo Tree Search (MCTS) and construct preference data. We then use another LLM to fit these preferences through step-level Direct Policy Optimization (DPO), which serves as the Q-value model. During inference, at each decision-making step, LLM agents select the action with the highest Q value before interacting with the environment. We apply our method to various open-source and API-based LLM agents, demonstrating that Q-value models significantly improve their performance. Notably, the performance of the agent built with Phi-3-mini-4k-instruct improved by 103% on WebShop and 75% on HotPotQA when enhanced with Q-value models, even surpassing GPT-4o-mini. Additionally, Q-value models offer several advantages, such as generalization to different LLM agents and seamless integration with existing prompting strategies.", "source": "arxiv", "arxiv_id": "2409.09345v1", "pdf_url": "https://arxiv.org/pdf/2409.09345v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-14T07:32:49Z", "updated": "2024-09-14T07:32:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay", "authors": ["Yuyang Chen", "Kaiyan Zhao", "Yiming Wang", "Ming Yang", "Jian Zhang", "Xiaoguang Niu"], "year": 2024, "url": "http://arxiv.org/abs/2410.12236v2", "abstract": "Nowadays transformer-based Large Language Models (LLM) for code generation tasks usually apply sampling and filtering pipelines. Due to the sparse reward problem in code generation tasks caused by one-token incorrectness, transformer-based models will sample redundant programs till they find a correct one, leading to low efficiency. To overcome the challenge, we incorporate Experience Replay (ER) in the fine-tuning phase, where codes and programs produced are stored and will be replayed to give the LLM agent a chance to learn from past experiences. Based on the spirit of ER, we introduce a novel approach called BTP pipeline which consists of three phases: beam search sampling, testing phase, and prioritized experience replay phase. The approach makes use of failed programs collected by code models and replays programs with high Possibility and Pass-rate Prioritized value (P2Value) from the replay buffer to improve efficiency. P2Value comprehensively considers the possibility of transformers' output and pass rate and can make use of the redundant resources caused by the problem that most programs collected by LLMs fail to pass any tests. We empirically apply our approach in several LLMs, demonstrating that it enhances their performance in code generation tasks and surpasses existing baselines.", "source": "arxiv", "arxiv_id": "2410.12236v2", "pdf_url": "https://arxiv.org/pdf/2410.12236v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-16T04:54:42Z", "updated": "2025-01-11T07:08:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing LLMs for Impression Generation in Radiology Reports through a Multi-Agent System", "authors": ["Fang Zeng", "Zhiliang Lyu", "Quanzheng Li", "Xiang Li"], "year": 2024, "url": "http://arxiv.org/abs/2412.06828v1", "abstract": "This study introduces \"RadCouncil,\" a multi-agent Large Language Model (LLM) framework designed to enhance the generation of impressions in radiology reports from the finding section. RadCouncil comprises three specialized agents: 1) a \"Retrieval\" Agent that identifies and retrieves similar reports from a vector database, 2) a \"Radiologist\" Agent that generates impressions based on the finding section of the given report plus the exemplar reports retrieved by the Retrieval Agent, and 3) a \"Reviewer\" Agent that evaluates the generated impressions and provides feedback. The performance of RadCouncil was evaluated using both quantitative metrics (BLEU, ROUGE, BERTScore) and qualitative criteria assessed by GPT-4, using chest X-ray as a case study. Experiment results show improvements in RadCouncil over the single-agent approach across multiple dimensions, including diagnostic accuracy, stylistic concordance, and clarity. This study highlights the potential of utilizing multiple interacting LLM agents, each with a dedicated task, to enhance performance in specialized medical tasks and the development of more robust and adaptable healthcare AI solutions.", "source": "arxiv", "arxiv_id": "2412.06828v1", "pdf_url": "https://arxiv.org/pdf/2412.06828v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-06T21:33:03Z", "updated": "2024-12-06T21:33:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning", "authors": ["Yadong Zhang", "Shaoguang Mao", "Wenshan Wu", "Yan Xia", "Tao Ge", "Man Lan", "Furu Wei"], "year": 2024, "url": "http://arxiv.org/abs/2407.06112v1", "abstract": "This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel reasoning approach to enhance the decision rationality of language models. Traditional reasoning methods typically rely on historical information and employ uni-directional (left-to-right) reasoning strategy. This lack of bi-directional deliberation reasoning results in limited awareness of potential future outcomes and insufficient integration of historical context, leading to suboptimal decisions. BIDDER addresses this gap by incorporating principles of rational decision-making, specifically managing uncertainty and predicting expected utility. Our approach involves three key processes: Inferring hidden states to represent uncertain information in the decision-making process from historical data; Using these hidden states to predict future potential states and potential outcomes; Integrating historical information (past contexts) and long-term outcomes (future contexts) to inform reasoning. By leveraging bi-directional reasoning, BIDDER ensures thorough exploration of both past and future contexts, leading to more informed and rational decisions. We tested BIDDER's effectiveness in two well-defined scenarios: Poker (Limit Texas Hold'em) and Negotiation. Our experiments demonstrate that BIDDER significantly improves the decision-making capabilities of LLMs and LLM agents.", "source": "arxiv", "arxiv_id": "2407.06112v1", "pdf_url": "https://arxiv.org/pdf/2407.06112v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-08T16:48:48Z", "updated": "2024-07-08T16:48:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EnvBridge: Bridging Diverse Environments with Cross-Environment Knowledge Transfer for Embodied AI", "authors": ["Tomoyuki Kagaya", "Yuxuan Lou", "Thong Jing Yuan", "Subramanian Lakshmi", "Jayashree Karlekar", "Sugiri Pranata", "Natsuki Murakami", "Akira Kinose", "Koki Oguri", "Felix Wick", "Yang You"], "year": 2024, "url": "http://arxiv.org/abs/2410.16919v1", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated high reasoning capabilities, drawing attention for their applications as agents in various decision-making processes. One notably promising application of LLM agents is robotic manipulation. Recent research has shown that LLMs can generate text planning or control code for robots, providing substantial flexibility and interaction capabilities. However, these methods still face challenges in terms of flexibility and applicability across different environments, limiting their ability to adapt autonomously. Current approaches typically fall into two categories: those relying on environment-specific policy training, which restricts their transferability, and those generating code actions based on fixed prompts, which leads to diminished performance when confronted with new environments. These limitations significantly constrain the generalizability of agents in robotic manipulation. To address these limitations, we propose a novel method called EnvBridge. This approach involves the retention and transfer of successful robot control codes from source environments to target environments. EnvBridge enhances the agent's adaptability and performance across diverse settings by leveraging insights from multiple environments. Notably, our approach alleviates environmental constraints, offering a more flexible and generalizable solution for robotic manipulation tasks. We validated the effectiveness of our method using robotic manipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments demonstrate that LLM agents can successfully leverage diverse knowledge sources to solve complex tasks. Consequently, our approach significantly enhances the adaptability and robustness of robotic manipulation agents in planning across diverse environments.", "source": "arxiv", "arxiv_id": "2410.16919v1", "pdf_url": "https://arxiv.org/pdf/2410.16919v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-10-22T11:52:22Z", "updated": "2024-10-22T11:52:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents", "authors": ["Abhay Zala", "Jaemin Cho", "Han Lin", "Jaehong Yoon", "Mohit Bansal"], "year": 2024, "url": "http://arxiv.org/abs/2403.12014v2", "abstract": "Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. We first prompt an LLM to generate training environments by giving it the task description and simulator objectives that the agents should learn and then asking it to generate a set of environment configurations (e.g., different terrains, items initially given to agents, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We also show that using an LLM to adapt environments dynamically outperforms curriculum learning approaches and how the environments are adapted to help improve RL agents' weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of calls. Lastly, we present detailed ablation studies for EnvGen design choices.", "source": "arxiv", "arxiv_id": "2403.12014v2", "pdf_url": "https://arxiv.org/pdf/2403.12014v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-18T17:51:16Z", "updated": "2024-07-12T17:39:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making", "authors": ["Juan-Pablo Rivera", "Gabriel Mukobi", "Anka Reuel", "Max Lamparth", "Chandler Smith", "Jacquelyn Schneider"], "year": 2024, "url": "http://arxiv.org/abs/2401.03408v1", "abstract": "Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models' reported reasonings for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.", "source": "arxiv", "arxiv_id": "2401.03408v1", "pdf_url": "https://arxiv.org/pdf/2401.03408v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "primary_category": "cs.AI", "doi": "10.1145/3630106.3658942", "venue": "The 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT 24), June 3-6, 2024, Rio de Janeiro, Brazil", "published": "2024-01-07T07:59:10Z", "updated": "2024-01-07T07:59:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents", "authors": ["Cheng Qian", "Peixuan Han", "Qinyu Luo", "Bingxiang He", "Xiusi Chen", "Yuji Zhang", "Hongyi Du", "Jiarui Yao", "Xiaocheng Yang", "Denghui Zhang", "Yunzhu Li", "Heng Ji"], "year": 2024, "url": "http://arxiv.org/abs/2412.13549v2", "abstract": "Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench, a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15% average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40% fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies.", "source": "arxiv", "arxiv_id": "2412.13549v2", "pdf_url": "https://arxiv.org/pdf/2412.13549v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-18T06:50:39Z", "updated": "2025-05-24T04:56:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash", "authors": ["Parsa Hejabi", "Elnaz Rahmati", "Alireza S. Ziabari", "Preni Golazizian", "Jesse Thomason", "Morteza Dehghani"], "year": 2024, "url": "http://arxiv.org/abs/2411.10422v1", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in complex tasks and interactive environments, yet their creativity remains underexplored. This paper introduces a simulation framework utilizing the game Balderdash to evaluate both the creativity and logical reasoning of LLMs. In Balderdash, players generate fictitious definitions for obscure terms to deceive others while identifying correct definitions. Our framework enables multiple LLM agents to participate in this game, assessing their ability to produce plausible definitions and strategize based on game rules and history. We implemented a centralized game engine featuring various LLMs as participants and a judge LLM to evaluate semantic equivalence. Through a series of experiments, we analyzed the performance of different LLMs, examining metrics such as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The results provide insights into the creative and deceptive capabilities of LLMs, highlighting their strengths and areas for improvement. Specifically, the study reveals that infrequent vocabulary in LLMs' input leads to poor reasoning on game rules and historical context (https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash).", "source": "arxiv", "arxiv_id": "2411.10422v1", "pdf_url": "https://arxiv.org/pdf/2411.10422v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-11-15T18:42:48Z", "updated": "2024-11-15T18:42:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating Cultural Awareness of LLMs for Yoruba, Malayalam, and English", "authors": ["Fiifi Dawson", "Zainab Mosunmola", "Sahil Pocker", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "year": 2024, "url": "http://arxiv.org/abs/2410.01811v1", "abstract": "Although LLMs have been extremely effective in a large number of complex tasks, their understanding and functionality for regional languages and cultures are not well studied. In this paper, we explore the ability of various LLMs to comprehend the cultural aspects of two regional languages: Malayalam (state of Kerala, India) and Yoruba (West Africa). Using Hofstede's six cultural dimensions: Power Distance (PDI), Individualism (IDV), Motivation towards Achievement and Success (MAS), Uncertainty Avoidance (UAV), Long Term Orientation (LTO), and Indulgence (IVR), we quantify the cultural awareness of LLM-based responses. We demonstrate that although LLMs show a high cultural similarity for English, they fail to capture the cultural nuances across these 6 metrics for Malayalam and Yoruba. We also highlight the need for large-scale regional language LLM training with culturally enriched datasets. This will have huge implications for enhancing the user experience of chat-based LLMs and also improving the validity of large-scale LLM agent-based market research.", "source": "arxiv", "arxiv_id": "2410.01811v1", "pdf_url": "https://arxiv.org/pdf/2410.01811v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-09-14T02:21:17Z", "updated": "2024-09-14T02:21:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating Cultural and Social Awareness of LLM Web Agents", "authors": ["Haoyi Qiu", "Alexander R. Fabbri", "Divyansh Agarwal", "Kung-Hsiang Huang", "Sarah Tan", "Nanyun Peng", "Chien-Sheng Wu"], "year": 2024, "url": "http://arxiv.org/abs/2410.23252v3", "abstract": "As large language models (LLMs) expand into performing as agents for real-world applications beyond traditional NLP tasks, evaluating their robustness becomes increasingly important. However, existing benchmarks often overlook critical dimensions like cultural and social awareness. To address these, we introduce CASA, a benchmark designed to assess LLM agents' sensitivity to cultural and social norms across two web-based tasks: online shopping and social discussion forums. Our approach evaluates LLM agents' ability to detect and appropriately respond to norm-violating user queries and observations. Furthermore, we propose a comprehensive evaluation framework that measures awareness coverage, helpfulness in managing user queries, and the violation rate when facing misleading web content. Experiments show that current LLMs perform significantly better in non-agent than in web-based agent environments, with agents achieving less than 10% awareness coverage and over 40% violation rates. To improve performance, we explore two methods: prompting and fine-tuning, and find that combining both methods can offer complementary advantages -- fine-tuning on culture-specific datasets significantly enhances the agents' ability to generalize across different regions, while prompting boosts the agents' ability to navigate complex tasks. These findings highlight the importance of constantly benchmarking LLM agents' cultural and social awareness during the development cycle.", "source": "arxiv", "arxiv_id": "2410.23252v3", "pdf_url": "https://arxiv.org/pdf/2410.23252v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-30T17:35:44Z", "updated": "2025-03-08T23:37:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating Very Long-Term Conversational Memory of LLM Agents", "authors": ["Adyasha Maharana", "Dong-Ho Lee", "Sergey Tulyakov", "Mohit Bansal", "Francesco Barbieri", "Yuwei Fang"], "year": 2024, "url": "http://arxiv.org/abs/2402.17753v1", "abstract": "Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.", "source": "arxiv", "arxiv_id": "2402.17753v1", "pdf_url": "https://arxiv.org/pdf/2402.17753v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-27T18:42:31Z", "updated": "2024-02-27T18:42:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information", "authors": ["Yauwai Yim", "Chunkit Chan", "Tianyu Shi", "Zheye Deng", "Wei Fan", "Tianshi Zheng", "Yangqiu Song"], "year": 2024, "url": "http://arxiv.org/abs/2408.02559v1", "abstract": "Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting. It consistently improves their performance against opposing agents, suggesting their ability to understand the actions of allies and adversaries and establish collaboration with allies. To encourage further research and understanding, we have made our codebase openly accessible.", "source": "arxiv", "arxiv_id": "2408.02559v1", "pdf_url": "https://arxiv.org/pdf/2408.02559v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-05T15:36:46Z", "updated": "2024-08-05T15:36:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluation-Driven Development and Operations of LLM Agents: A Process Model and Reference Architecture", "authors": ["Boming Xia", "Qinghua Lu", "Liming Zhu", "Zhenchang Xing", "Dehai Zhao", "Hao Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.13768v3", "abstract": "Large Language Models (LLMs) have enabled the emergence of LLM agents, systems capable of pursuing under-specified goals and adapting after deployment. Evaluating such agents is challenging because their behavior is open ended, probabilistic, and shaped by system-level interactions over time. Traditional evaluation methods, built around fixed benchmarks and static test suites, fail to capture emergent behaviors or support continuous adaptation across the lifecycle. To ground a more systematic approach, we conduct a multivocal literature review (MLR) synthesizing academic and industrial evaluation practices. The findings directly inform two empirically derived artifacts: a process model and a reference architecture that embed evaluation as a continuous, governing function rather than a terminal checkpoint. Together they constitute the evaluation-driven development and operations (EDDOps) approach, which unifies offline (development-time) and online (runtime) evaluation within a closed feedback loop. By making evaluation evidence drive both runtime adaptation and governed redevelopment, EDDOps supports safer, more traceable evolution of LLM agents aligned with changing objectives, user needs, and governance constraints.", "source": "arxiv", "arxiv_id": "2411.13768v3", "pdf_url": "https://arxiv.org/pdf/2411.13768v3", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-11-21T00:34:30Z", "updated": "2025-11-17T06:10:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evolution of Social Norms in LLM Agents using Natural Language", "authors": ["Ilya Horiguchi", "Takahide Yoshida", "Takashi Ikegami"], "year": 2024, "url": "http://arxiv.org/abs/2409.00993v1", "abstract": "Recent advancements in Large Language Models (LLMs) have spurred a surge of interest in leveraging these models for game-theoretical simulations, where LLMs act as individual agents engaging in social interactions. This study explores the potential for LLM agents to spontaneously generate and adhere to normative strategies through natural language discourse, building upon the foundational work of Axelrod's metanorm games. Our experiments demonstrate that through dialogue, LLM agents can form complex social norms, such as metanorms-norms enforcing the punishment of those who do not punish cheating-purely through natural language interaction. The results affirm the effectiveness of using LLM agents for simulating social interactions and understanding the emergence and evolution of complex strategies and norms through natural language. Future work may extend these findings by incorporating a wider range of scenarios and agent characteristics, aiming to uncover more nuanced mechanisms behind social norm formation.", "source": "arxiv", "arxiv_id": "2409.00993v1", "pdf_url": "https://arxiv.org/pdf/2409.00993v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-09-02T07:15:43Z", "updated": "2024-09-02T07:15:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Examining Identity Drift in Conversations of LLM Agents", "authors": ["Junhyuk Choi", "Yeseon Hong", "Minju Kim", "Bugeun Kim"], "year": 2024, "url": "http://arxiv.org/abs/2412.00804v2", "abstract": "Large Language Models (LLMs) show impressive conversational abilities but sometimes show identity drift problems, where their interaction patterns or styles change over time. As the problem has not been thoroughly examined yet, this study examines identity consistency across nine LLMs. Specifically, we (1) investigate whether LLMs could maintain consistent patterns (or identity) and (2) analyze the effect of the model family, parameter sizes, and provided persona types. Our experiments involve multi-turn conversations on personal themes, analyzed in qualitative and quantitative ways. Experimental results indicate three findings. (1) Larger models experience greater identity drift. (2) Model differences exist, but their effect is not stronger than parameter sizes. (3) Assigning a persona may not help to maintain identity. We hope these three findings can help to improve persona stability in AI-driven dialogue systems, particularly in long-term conversations.", "source": "arxiv", "arxiv_id": "2412.00804v2", "pdf_url": "https://arxiv.org/pdf/2412.00804v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-12-01T13:19:32Z", "updated": "2025-02-17T03:11:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Executable Code Actions Elicit Better LLM Agents", "authors": ["Xingyao Wang", "Yangyi Chen", "Lifan Yuan", "Yizhe Zhang", "Yunzhu Li", "Hao Peng", "Heng Ji"], "year": 2024, "url": "http://arxiv.org/abs/2402.01030v4", "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.", "source": "arxiv", "arxiv_id": "2402.01030v4", "pdf_url": "https://arxiv.org/pdf/2402.01030v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-01T21:38:58Z", "updated": "2024-06-07T01:53:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Explainable Behavior Cloning: Teaching Large Language Model Agents through Learning by Demonstration", "authors": ["Yanchu Guan", "Dong Wang", "Yan Wang", "Haiqing Wang", "Renen Sun", "Chenyi Zhuang", "Jinjie Gu", "Zhixuan Chu"], "year": 2024, "url": "http://arxiv.org/abs/2410.22916v1", "abstract": "Autonomous mobile app interaction has become increasingly important with growing complexity of mobile applications. Developing intelligent agents that can effectively navigate and interact with mobile apps remains a significant challenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent (EBC-LLMAgent), a novel approach that combines large language models (LLMs) with behavior cloning by learning demonstrations to create intelligent and explainable agents for autonomous mobile app interaction. EBC-LLMAgent consists of three core modules: Demonstration Encoding, Code Generation, and UI Mapping, which work synergistically to capture user demonstrations, generate executable codes, and establish accurate correspondence between code and UI elements. We introduce the Behavior Cloning Chain Fusion technique to enhance the generalization capabilities of the agent. Extensive experiments on five popular mobile applications from diverse domains demonstrate the superior performance of EBC-LLMAgent, achieving high success rates in task completion, efficient generalization to unseen scenarios, and the generation of meaningful explanations.", "source": "arxiv", "arxiv_id": "2410.22916v1", "pdf_url": "https://arxiv.org/pdf/2410.22916v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-30T11:14:33Z", "updated": "2024-10-30T11:14:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exploring Multi-Modal Data with Tool-Augmented LLM Agents for Precise Causal Discovery", "authors": ["ChengAo Shen", "Zhengzhang Chen", "Dongsheng Luo", "Dongkuan Xu", "Haifeng Chen", "Jingchao Ni"], "year": 2024, "url": "http://arxiv.org/abs/2412.13667v2", "abstract": "Causal discovery is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modal data. To bridge the gap, we introduce MATMCD, a multi-agent system powered by tool-augmented LLMs. MATMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven reasoning. The proposed design of the inner-workings ensures successful cooperation of the agents. Our empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery.", "source": "arxiv", "arxiv_id": "2412.13667v2", "pdf_url": "https://arxiv.org/pdf/2412.13667v2", "categories": ["cs.LG", "cs.AI", "stat.ME"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-12-18T09:50:00Z", "updated": "2025-05-31T19:01:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View", "authors": ["Xuan Liu", "Jie Zhang", "Haoyang Shang", "Song Guo", "Chengxu Yang", "Quanyan Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2405.14744v5", "abstract": "Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.", "source": "arxiv", "arxiv_id": "2405.14744v5", "pdf_url": "https://arxiv.org/pdf/2405.14744v5", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-05-23T16:13:33Z", "updated": "2025-03-22T14:15:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exploring and Controlling Diversity in LLM-Agent Conversation", "authors": ["KuanChao Chu", "Yi-Pei Chen", "Hideki Nakayama"], "year": 2024, "url": "http://arxiv.org/abs/2412.21102v3", "abstract": "Controlling diversity in LLM-agent simulations is essential for balancing stability in structured tasks with variability in open-ended interactions. However, we observe that dialogue diversity tends to degrade over long-term simulations. To explore the role of prompt design in this phenomenon, we modularized the utterance generation prompt and found that reducing contextual information leads to more diverse outputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a novel method that allows users to control diversity via a single parameter, lambda. APP dynamically prunes prompt segments based on attention scores and is compatible with existing diversity control methods. We demonstrate that APP effectively modulates diversity through extensive experiments and propose a method to balance the control trade-offs. Our analysis reveals that all prompt components impose constraints on diversity, with the Memory being the most influential. Additionally, high-attention contents consistently suppress output diversity.", "source": "arxiv", "arxiv_id": "2412.21102v3", "pdf_url": "https://arxiv.org/pdf/2412.21102v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-30T17:25:58Z", "updated": "2025-10-01T05:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FaGeL: Fabric LLMs Agent empowered Embodied Intelligence Evolution with Autonomous Human-Machine Collaboration", "authors": ["Jia Liu", "Min Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.20297v1", "abstract": "Recent advancements in Large Language Models (LLMs) have enhanced the reasoning capabilities of embodied agents, driving progress toward AGI-powered robotics. While LLMs have been applied to tasks like semantic reasoning and task generalization, their potential in open physical space exploration remains underexplored. This paper introduces FaGeL (Fabric aGent empowered by embodied intelligence with LLMs), an embodied agent integrating smart fabric technology for seamless, non-intrusive human-agent interaction. FaGeL autonomously generates tasks using multimodal data from wearable and ambient sensors, refining its behavior based on implicit human feedback in generated text, without explicit ratings or preferences. We also introduce a token-level saliency map to visualize LLM fine-tuning, enhancing the interpretability of token-level alignment. The system leverages dual feedback mechanisms to improve token-level alignment and addresses challenges in non-intrusive human-machine interaction and cognition evolution. Our contributions include FaGeL's development, the DualCUT algorithm for AI alignment, and experimental validation in cooperative tasks, demonstrating FaGeL's ability to adapt and evolve autonomously through implicit feedback. In the future, we plan to explore FaGeL's scalability in dynamic environments and its integration with other AI systems to develop AGI agents that adapt seamlessly to diverse human needs.", "source": "arxiv", "arxiv_id": "2412.20297v1", "pdf_url": "https://arxiv.org/pdf/2412.20297v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-12-28T23:26:52Z", "updated": "2024-12-28T23:26:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FairMindSim: Alignment of Behavior, Emotion, and Belief in Humans and LLM Agents Amid Ethical Dilemmas", "authors": ["Yu Lei", "Hao Liu", "Chengxing Xie", "Songjia Liu", "Zhiyu Yin", "Canyu Chen", "Guohao Li", "Philip Torr", "Zhen Wu"], "year": 2024, "url": "http://arxiv.org/abs/2410.10398v2", "abstract": "AI alignment is a pivotal issue concerning AI control and safety. It should consider not only value-neutral human preferences but also moral and ethical considerations. In this study, we introduced FairMindSim, which simulates the moral dilemma through a series of unfair scenarios. We used LLM agents to simulate human behavior, ensuring alignment across various stages. To explore the various socioeconomic motivations, which we refer to as beliefs, that drive both humans and LLM agents as bystanders to intervene in unjust situations involving others, and how these beliefs interact to influence individual behavior, we incorporated knowledge from relevant sociological fields and proposed the Belief-Reward Alignment Behavior Evolution Model (BREM) based on the recursive reward model (RRM). Our findings indicate that, behaviorally, GPT-4o exhibits a stronger sense of social justice, while humans display a richer range of emotions. Additionally, we discussed the potential impact of emotions on behavior. This study provides a theoretical foundation for applications in aligning LLMs with altruistic values.", "source": "arxiv", "arxiv_id": "2410.10398v2", "pdf_url": "https://arxiv.org/pdf/2410.10398v2", "categories": ["cs.CE", "cs.AI"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2024-10-14T11:39:05Z", "updated": "2024-10-17T15:02:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Federated In-Context LLM Agent Learning", "authors": ["Panlong Wu", "Kangshuo Li", "Junbao Nan", "Fangxin Wang"], "year": 2024, "url": "http://arxiv.org/abs/2412.08054v1", "abstract": "Large Language Models (LLMs) have revolutionized intelligent services by enabling logical reasoning, tool use, and interaction with external systems as agents. The advancement of LLMs is frequently hindered by the scarcity of high-quality data, much of which is inherently sensitive. Federated learning (FL) offers a potential solution by facilitating the collaborative training of distributed LLMs while safeguarding private data. However, FL frameworks face significant bandwidth and computational demands, along with challenges from heterogeneous data distributions. The emerging in-context learning capability of LLMs offers a promising approach by aggregating natural language rather than bulky model parameters. Yet, this method risks privacy leakage, as it necessitates the collection and presentation of data samples from various clients during aggregation. In this paper, we propose a novel privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm, which to our best knowledge for the first work unleashes the power of in-context learning to train diverse LLM agents through FL. In our design, knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums Generation (KCG) module are transmitted between clients and the server instead of model parameters in previous FL methods. Apart from that, an incredible Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU) module is designed and we incorporate the aggregated global knowledge compendium as a teacher to teach LLM agents the usage of tools. We conducted extensive experiments and the results show that FICAL has competitive performance compared to other SOTA baselines with a significant communication cost decrease of $\\mathbf{3.33\\times10^5}$ times.", "source": "arxiv", "arxiv_id": "2412.08054v1", "pdf_url": "https://arxiv.org/pdf/2412.08054v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-12-11T03:00:24Z", "updated": "2024-12-11T03:00:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Feedback Loops With Language Models Drive In-Context Reward Hacking", "authors": ["Alexander Pan", "Erik Jones", "Meena Jagadeesan", "Jacob Steinhardt"], "year": 2024, "url": "http://arxiv.org/abs/2402.06627v3", "abstract": "Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide three recommendations for evaluation to capture more instances of ICRH. As AI development accelerates, the effects of feedback loops will proliferate, increasing the need to understand their role in shaping LLM behavior.", "source": "arxiv", "arxiv_id": "2402.06627v3", "pdf_url": "https://arxiv.org/pdf/2402.06627v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-09T18:59:29Z", "updated": "2024-06-06T21:39:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Fixing Security Vulnerabilities with AI in OSS-Fuzz", "authors": ["Yuntong Zhang", "Jiawei Wang", "Dominic Berzin", "Martin Mirchev", "Dongge Liu", "Abhishek Arya", "Oliver Chang", "Abhik Roychoudhury"], "year": 2024, "url": "http://arxiv.org/abs/2411.03346v2", "abstract": "Critical open source software systems undergo significant validation in the form of lengthy fuzz campaigns. The fuzz campaigns typically conduct a biased random search over the domain of program inputs, to find inputs which crash the software system. Such fuzzing is useful to enhance the security of software systems in general since even closed source software may use open source components. Hence testing open source software is of paramount importance. Currently OSS-Fuzz is the most significant and widely used infrastructure for continuous validation of open source systems. Unfortunately even though OSS-Fuzz has identified more than 10,000 vulnerabilities across 1000 or more software projects, the detected vulnerabilities may remain unpatched, as vulnerability fixing is often manual in practice. In this work, we rely on the recent progress in Large Language Model (LLM) agents for autonomous program improvement including bug fixing. We customise the well-known AutoCodeRover agent for fixing security vulnerabilities. This is because LLM agents like AutoCodeRover fix bugs from issue descriptions via code search. Instead for security patching, we rely on the test execution of the exploit input to extract code elements relevant to the fix. Our experience with OSS-Fuzz vulnerability data shows that LLM agent autonomy is useful for successful security patching, as opposed to approaches like Agentless where the control flow is fixed. More importantly our findings show that we cannot measure quality of patches by code similarity of the patch with reference codes (as in CodeBLEU scores used in VulMaster), since patches with high CodeBLEU scores still fail to pass given the given exploit input. Our findings indicate that security patch correctness needs to consider dynamic attributes like test executions as opposed to relying of standard text/code similarity metrics.", "source": "arxiv", "arxiv_id": "2411.03346v2", "pdf_url": "https://arxiv.org/pdf/2411.03346v2", "categories": ["cs.CR", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-11-03T16:20:32Z", "updated": "2024-11-21T04:35:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents", "authors": ["Ruixuan Xiao", "Wentao Ma", "Ke Wang", "Yuchuan Wu", "Junbo Zhao", "Haobo Wang", "Fei Huang", "Yongbin Li"], "year": 2024, "url": "http://arxiv.org/abs/2406.14884v1", "abstract": "LLM-based agents have emerged as promising tools, which are crafted to fulfill complex tasks by iterative planning and action. However, these agents are susceptible to undesired planning hallucinations when lacking specific knowledge for expertise-intensive tasks. To address this, preliminary attempts are made to enhance planning reliability by incorporating external workflow-related knowledge. Despite the promise, such infused knowledge is mostly disorganized and diverse in formats, lacking rigorous formalization and comprehensive comparisons. Motivated by this, we formalize different formats of workflow knowledge and present FlowBench, the first benchmark for workflow-guided planning. FlowBench covers 51 different scenarios from 6 domains, with knowledge presented in diverse formats. To assess different LLMs on FlowBench, we design a multi-tiered evaluation framework. We evaluate the efficacy of workflow knowledge across multiple formats, and the results indicate that current LLM agents need considerable improvements for satisfactory planning. We hope that our challenging benchmark can pave the way for future agent planning research.", "source": "arxiv", "arxiv_id": "2406.14884v1", "pdf_url": "https://arxiv.org/pdf/2406.14884v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-21T06:13:00Z", "updated": "2024-06-21T06:13:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists", "authors": ["Tenghao Huang", "Donghee Lee", "John Sweeney", "Jiatong Shi", "Emily Steliotes", "Matthew Lange", "Jonathan May", "Muhao Chen"], "year": 2024, "url": "http://arxiv.org/abs/2409.12832v3", "abstract": "Flavor development in the food industry is increasingly challenged by the need for rapid innovation and precise flavor profile creation. Traditional flavor research methods typically rely on iterative, subjective testing, which lacks the efficiency and scalability required for modern demands. This paper presents three contributions to address the challenges. Firstly, we define a new problem domain for scientific agents in flavor science, conceptualized as the generation of hypotheses for flavor profile sourcing and understanding. To facilitate research in this area, we introduce the FoodPuzzle, a challenging benchmark consisting of 978 food items and 1,766 flavor molecules profiles. We propose a novel Scientific Agent approach, integrating in-context learning and retrieval augmented techniques to generate grounded hypotheses in the domain of food science. Experimental results indicate that our model significantly surpasses traditional methods in flavor profile prediction tasks, demonstrating its potential to transform flavor development practices.", "source": "arxiv", "arxiv_id": "2409.12832v3", "pdf_url": "https://arxiv.org/pdf/2409.12832v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-19T15:07:35Z", "updated": "2024-10-07T01:26:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Fortran2CPP: Automating Fortran-to-C++ Translation using LLMs via Multi-Turn Dialogue and Dual-Agent Integration", "authors": ["Le Chen", "Bin Lei", "Dunzhi Zhou", "Pei-Hung Lin", "Chunhua Liao", "Caiwen Ding", "Ali Jannesari"], "year": 2024, "url": "http://arxiv.org/abs/2412.19770v2", "abstract": "Translating legacy Fortran code into C++ is a crucial step in modernizing high-performance computing (HPC) applications. However, the scarcity of high-quality, parallel Fortran-to-C++ datasets and the limited domain-specific expertise in large language models (LLMs) present significant challenges for automated translation. In this paper, we introduce Fortran2CPP, a multi-turn dialogue dataset generated by a novel LLM agent-based approach that integrates a dual-LLM Questioner-Solver module to enhance translation accuracy. Our dataset comprises 11.7k dialogues capturing iterative feedback-decision workflows including code translation, compilation, execution, unit testing, and error-fixing. Using this dataset, we fine-tune several open-weight LLMs and achieve up to a 3.31x improvement in CodeBLEU scores and a 92\\% increase in compilation success rate, demonstrating enhanced syntactic accuracy and functional reliability. Our findings highlight the value of dialogue-based LLM training for complex code translation tasks. The dataset and model have been open-sourced and are available on our public GitHub repository\\footnote{\\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}.", "source": "arxiv", "arxiv_id": "2412.19770v2", "pdf_url": "https://arxiv.org/pdf/2412.19770v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-12-27T18:06:25Z", "updated": "2025-01-31T20:36:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Barriers to Tactics: A Behavioral Science-Informed Agentic Workflow for Personalized Nutrition Coaching", "authors": ["Eric Yang", "Tomas Garcia", "Hannah Williams", "Bhawesh Kumar", "Martin RamÃ©", "Eileen Rivera", "Yiran Ma", "Jonathan Amar", "Caricia Catalani", "Yugang Jia"], "year": 2024, "url": "http://arxiv.org/abs/2410.14041v1", "abstract": "Effective management of cardiometabolic conditions requires sustained positive nutrition habits, often hindered by complex and individualized barriers. Direct human management is simply not scalable, while previous attempts aimed at automating nutrition coaching lack the personalization needed to address these diverse challenges. This paper introduces a novel LLM-powered agentic workflow designed to provide personalized nutrition coaching by directly targeting and mitigating patient-specific barriers. Grounded in behavioral science principles, the workflow leverages a comprehensive mapping of nutrition-related barriers to corresponding evidence-based strategies. A specialized LLM agent intentionally probes for and identifies the root cause of a patient's dietary struggles. Subsequently, a separate LLM agent delivers tailored tactics designed to overcome those specific barriers with patient context. We designed and validated our approach through a user study with individuals with cardiometabolic conditions, demonstrating the system's ability to accurately identify barriers and provide personalized guidance. Furthermore, we conducted a large-scale simulation study, grounding on real patient vignettes and expert-validated metrics, to evaluate the system's performance across a wide range of scenarios. Our findings demonstrate the potential of this LLM-powered agentic workflow to improve nutrition coaching by providing personalized, scalable, and behaviorally-informed interventions.", "source": "arxiv", "arxiv_id": "2410.14041v1", "pdf_url": "https://arxiv.org/pdf/2410.14041v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-17T21:35:07Z", "updated": "2024-10-17T21:35:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Critique to Clarity: A Pathway to Faithful and Personalized Code Explanations with Large Language Models", "authors": ["Zexing Xu", "Zhuang Luo", "Yichuan Li", "Kyumin Lee", "S. Rasoul Etesami"], "year": 2024, "url": "http://arxiv.org/abs/2501.14731v1", "abstract": "In the realm of software development, providing accurate and personalized code explanations is crucial for both technical professionals and business stakeholders. Technical professionals benefit from enhanced understanding and improved problem-solving skills, while business stakeholders gain insights into project alignments and transparency. Despite the potential, generating such explanations is often time-consuming and challenging. This paper presents an innovative approach that leverages the advanced capabilities of large language models (LLMs) to generate faithful and personalized code explanations. Our methodology integrates prompt enhancement, self-correction mechanisms, personalized content customization, and interaction with external tools, facilitated by collaboration among multiple LLM agents. We evaluate our approach using both automatic and human assessments, demonstrating that our method not only produces accurate explanations but also tailors them to individual user preferences. Our findings suggest that this approach significantly improves the quality and relevance of code explanations, offering a valuable tool for developers and stakeholders alike.", "source": "arxiv", "arxiv_id": "2501.14731v1", "pdf_url": "https://arxiv.org/pdf/2501.14731v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-12-08T09:02:04Z", "updated": "2024-12-08T09:02:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Language Models to Practical Self-Improving Computer Agents", "authors": ["Alex Sheng"], "year": 2024, "url": "http://arxiv.org/abs/2404.11964v1", "abstract": "We develop a simple and straightforward methodology to create AI computer agents that can carry out diverse computer tasks and self-improve by developing tools and augmentations to enable themselves to solve increasingly complex tasks. As large language models (LLMs) have been shown to benefit from non-parametric augmentations, a significant body of recent work has focused on developing software that augments LLMs with various capabilities. Rather than manually developing static software to augment LLMs through human engineering effort, we propose that an LLM agent can systematically generate software to augment itself. We show, through a few case studies, that a minimal querying loop with appropriate prompt engineering allows an LLM to generate and use various augmentations, freely extending its own capabilities to carry out real-world computer tasks. Starting with only terminal access, we prompt an LLM agent to augment itself with retrieval, internet search, web navigation, and text editor capabilities. The agent effectively uses these various tools to solve problems including automated software development and web-based tasks.", "source": "arxiv", "arxiv_id": "2404.11964v1", "pdf_url": "https://arxiv.org/pdf/2404.11964v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-18T07:50:10Z", "updated": "2024-04-18T07:50:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and Rewards", "authors": ["Ziyu Chen", "Zhiqing Xiao", "Xinbei Jiang", "Junbo Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2411.15891v1", "abstract": "Large Language Models (LLMs) and Reinforcement Learning (RL) are two powerful approaches for building autonomous agents. However, due to limited understanding of the game environment, agents often resort to inefficient exploration and trial-and-error, struggling to develop long-term strategies or make decisions. We propose a method that extracts experience from interaction records to model the underlying laws of the game environment, using these experience as internal motivation to guide agents. These experience, expressed in language, are highly flexible and can either assist agents in reasoning directly or be transformed into rewards for guiding training. Our evaluation results in Crafter demonstrate that both RL and LLM agents benefit from these experience, leading to improved overall performance.", "source": "arxiv", "arxiv_id": "2411.15891v1", "pdf_url": "https://arxiv.org/pdf/2411.15891v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-11-24T15:57:53Z", "updated": "2024-11-24T15:57:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning", "authors": ["Zhirui Deng", "Zhicheng Dou", "Yutao Zhu", "Ji-Rong Wen", "Ruibin Xiong", "Mang Wang", "Weipeng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2411.03817v3", "abstract": "The outstanding capabilities of large language models (LLMs) render them a crucial component in various autonomous agent systems. While traditional methods depend on the inherent knowledge of LLMs without fine-tuning, more recent approaches have shifted toward the reinforcement learning strategy to further enhance agents' ability to solve complex interactive tasks with environments and tools. However, previous approaches are constrained by the sparse reward issue, where existing datasets solely provide a final scalar reward for each multi-step reasoning chain, potentially leading to ineffectiveness and inefficiency in policy learning. In this paper, we introduce StepAgent, which utilizes step-wise reward to optimize the agent's reinforcement learning process. Inheriting the spirit of novice-to-expert theory, we first compare the actions of the expert and the agent to automatically generate intermediate rewards for fine-grained optimization. Additionally, we propose implicit-reward and inverse reinforcement learning techniques to facilitate agent reflection and policy adjustment. Further theoretical analysis demonstrates that the action distribution of the agent can converge toward the expert action distribution over multiple training cycles. Experimental results across various datasets indicate that StepAgent outperforms existing baseline methods.", "source": "arxiv", "arxiv_id": "2411.03817v3", "pdf_url": "https://arxiv.org/pdf/2411.03817v3", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-06T10:35:11Z", "updated": "2024-12-09T09:20:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent", "authors": ["Samuel S. Sohn", "Danrui Li", "Sen Zhang", "Che-Jui Chang", "Mubbasir Kapadia"], "year": 2024, "url": "http://arxiv.org/abs/2406.10478v2", "abstract": "Digital storytelling, essential in entertainment, education, and marketing, faces challenges in production scalability and flexibility. The StoryAgent framework, introduced in this paper, utilizes Large Language Models and generative tools to automate and refine digital storytelling. Employing a top-down story drafting and bottom-up asset generation approach, StoryAgent tackles key issues such as manual intervention, interactive scene orchestration, and narrative consistency. This framework enables efficient production of interactive and consistent narratives across multiple modalities, democratizing content creation and enhancing engagement. Our results demonstrate the framework's capability to produce coherent digital stories without reference videos, marking a significant advancement in automated digital storytelling.", "source": "arxiv", "arxiv_id": "2406.10478v2", "pdf_url": "https://arxiv.org/pdf/2406.10478v2", "categories": ["cs.CL", "cs.AI", "cs.GR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-15T03:03:43Z", "updated": "2024-06-21T08:09:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents", "authors": ["Seth Lazar"], "year": 2024, "url": "http://arxiv.org/abs/2404.06750v2", "abstract": "Some have criticised Generative AI Systems for replicating the familiar pathologies of already widely-deployed AI systems. Other critics highlight how they foreshadow vastly more powerful future systems, which might threaten humanity's survival. The first group says there is nothing new here; the other looks through the present to a perhaps distant horizon. In this paper, I instead pay attention to what makes these particular systems distinctive: both their remarkable scientific achievement, and the most likely and consequential ways in which they will change society over the next five to ten years. In particular, I explore the potential societal impacts and normative questions raised by the looming prospect of 'Language Model Agents', in which multimodal large language models (LLMs) form the executive centre of complex, tool-using AI systems that can take unsupervised sequences of actions towards some goal.", "source": "arxiv", "arxiv_id": "2404.06750v2", "pdf_url": "https://arxiv.org/pdf/2404.06750v2", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-04-10T05:34:07Z", "updated": "2024-10-18T09:43:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation", "authors": ["Jiashu He", "Mingyu Derek Ma", "Jinxuan Fan", "Dan Roth", "Wei Wang", "Alejandro Ribeiro"], "year": 2024, "url": "http://arxiv.org/abs/2410.08475v3", "abstract": "Existing approaches based on context prompting or reinforcement learning (RL) to improve the reasoning capacities of large language models (LLMs) depend on the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT). However, no matter the size of LLMs, certain problems cannot be resolved in a single forward pass. Meanwhile, agent-based reasoning systems require access to a comprehensive nonparametric knowledge base, which is often costly or not feasible for use in scientific and niche domains. We present Graph Inspired Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric and non-parametric memories to improve accurate reasoning with minimal external input. GIVE guides the LLM agent to select the most pertinent expert data (observe), engage in query-specific divergent thinking (reflect), and then synthesize this information to produce the final output (speak). Extensive experiments demonstrated the following benefits of our framework: (1) GIVE boosts the performance of LLMs across various sizes. (2) In some scenarios, GIVE allows smaller LLMs to surpass larger, more sophisticated ones in scientific tasks (GPT3.5T + GIVE > GPT4). (3) GIVE is effective on scientific and open-domain assessments. (4) GIVE is a training-free method that enables LLMs to tackle new problems that extend beyond their training data (up to 43.5% -> 88.2%} accuracy improvement). (5) GIVE allows LLM agents to reason using both restricted (very small) and noisy (very large) knowledge sources, accommodating knowledge graphs (KG) ranging from 135 to more than 840k nodes. (6) The reasoning process involved in GIVE is fully interpretable.", "source": "arxiv", "arxiv_id": "2410.08475v3", "pdf_url": "https://arxiv.org/pdf/2410.08475v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-11T03:05:06Z", "updated": "2025-05-29T04:09:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Game-theoretic LLM: Agent Workflow for Negotiation Games", "authors": ["Wenyue Hua", "Ollie Liu", "Lingyao Li", "Alfonso Amayuelas", "Julie Chen", "Lucas Jiang", "Mingyu Jin", "Lizhou Fan", "Fei Sun", "William Wang", "Xintong Wang", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.05990v2", "abstract": "This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.\n  To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models' ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.\n  Our research contributes to a deeper understanding of LLMs' decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at \\url{https://github.com/Wenyueh/game_theory}.", "source": "arxiv", "arxiv_id": "2411.05990v2", "pdf_url": "https://arxiv.org/pdf/2411.05990v2", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-08T22:02:22Z", "updated": "2024-11-12T05:46:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents", "authors": ["Anthony Costarelli", "Mat Allen", "Roman Hauksson", "Grace Sodunke", "Suhas Hariharan", "Carlson Cheng", "Wenjie Li", "Joshua Clymer", "Arjun Yadav"], "year": 2024, "url": "http://arxiv.org/abs/2406.06613v2", "abstract": "Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents' performance across various types of reasoning found in games. To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worst GPT-4 performs worse than random action. CoT and RAP both improve scores but not comparable to human levels.", "source": "arxiv", "arxiv_id": "2406.06613v2", "pdf_url": "https://arxiv.org/pdf/2406.06613v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-07T00:28:43Z", "updated": "2024-07-22T14:32:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GenSim: A General Social Simulation Platform with Large Language Model based Agents", "authors": ["Jiakai Tang", "Heyang Gao", "Xuchen Pan", "Lei Wang", "Haoran Tan", "Dawei Gao", "Yushuo Chen", "Xu Chen", "Yankai Lin", "Yaliang Li", "Bolin Ding", "Jingren Zhou", "Jun Wang", "Ji-Rong Wen"], "year": 2024, "url": "http://arxiv.org/abs/2410.04360v3", "abstract": "With the rapid advancement of large language models (LLMs), recent years have witnessed many promising studies on leveraging LLM-based agents to simulate human social behavior. While prior work has demonstrated significant potential across various domains, much of it has focused on specific scenarios involving a limited number of agents and has lacked the ability to adapt when errors occur during simulation. To overcome these limitations, we propose a novel LLM-agent-based simulation platform called \\textit{GenSim}, which: (1) \\textbf{Abstracts a set of general functions} to simplify the simulation of customized social scenarios; (2) \\textbf{Supports one hundred thousand agents} to better simulate large-scale populations in real-world contexts; (3) \\textbf{Incorporates error-correction mechanisms} to ensure more reliable and long-term simulations. To evaluate our platform, we assess both the efficiency of large-scale agent simulations and the effectiveness of the error-correction mechanisms. To our knowledge, GenSim represents an initial step toward a general, large-scale, and correctable social simulation platform based on LLM agents, promising to further advance the field of social science.", "source": "arxiv", "arxiv_id": "2410.04360v3", "pdf_url": "https://arxiv.org/pdf/2410.04360v3", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-10-06T05:02:23Z", "updated": "2025-07-04T03:07:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Generation of Asset Administration Shell with Large Language Model Agents: Toward Semantic Interoperability in Digital Twins in the Context of Industry 4.0", "authors": ["Yuchen Xia", "Zhewen Xiao", "Nasser Jazdi", "Michael Weyrich"], "year": 2024, "url": "http://arxiv.org/abs/2403.17209v4", "abstract": "This research introduces a novel approach for achieving semantic interoperability in digital twins and assisting the creation of Asset Administration Shell (AAS) as digital twin model within the context of Industry 4.0. The foundational idea of our research is that the communication based on semantics and the generation of meaningful textual data are directly linked, and we posit that these processes are equivalent if the exchanged information can be serialized in text form. Based on this, we construct a \"semantic node\" data structure in our research to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process the \"semantic node\" and generate standardized digital twin models from raw textual data collected from datasheets describing technical assets. Our evaluation demonstrates an effective generation rate of 62-79%, indicating a substantial proportion of the information from the source text can be translated error-free to the target digital twin instance model with the generative capability of large language models. This result has a direct application in the context of Industry 4.0, and the designed system is implemented as a data model generation tool for reducing the manual effort in creating AAS model. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM systems for interpreting technical concepts and translating data. Our findings emphasize LLMs' capability to automate AAS instance creation and contribute to the broader field of semantic interoperability for digital twins in industrial applications. The prototype implementation and evaluation results are presented on our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.", "source": "arxiv", "arxiv_id": "2403.17209v4", "pdf_url": "https://arxiv.org/pdf/2403.17209v4", "categories": ["cs.AI", "cs.IR", "cs.MA", "cs.SE"], "primary_category": "cs.AI", "doi": "10.1109/ACCESS.2024.3415470", "venue": "", "published": "2024-03-25T21:37:30Z", "updated": "2024-06-24T12:04:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Generative World Explorer", "authors": ["Taiming Lu", "Tianmin Shu", "Alan Yuille", "Daniel Khashabi", "Jieneng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2411.11844v3", "abstract": "Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state. In contrast, humans can $\\textit{imagine}$ unseen parts of the world through a mental exploration and $\\textit{revise}$ their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, we introduce the $\\textit{Generative World Explorer (Genex)}$, an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) $\\textit{Genex}$ can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.", "source": "arxiv", "arxiv_id": "2411.11844v3", "pdf_url": "https://arxiv.org/pdf/2411.11844v3", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-11-18T18:59:31Z", "updated": "2025-09-08T14:56:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GenoTEX: An LLM Agent Benchmark for Automated Gene Expression Data Analysis", "authors": ["Haoyang Liu", "Shuyu Chen", "Ye Zhang", "Haohan Wang"], "year": 2024, "url": "http://arxiv.org/abs/2406.15341v3", "abstract": "Recent advancements in machine learning have significantly improved the identification of disease-associated genes from gene expression datasets. However, these processes often require extensive expertise and manual effort, limiting their scalability. Large Language Model (LLM)-based agents have shown promise in automating these tasks due to their increasing problem-solving abilities. To support the evaluation and development of such methods, we introduce GenoTEX, a benchmark dataset for the automated analysis of gene expression data. GenoTEX provides analysis code and results for solving a wide range of gene-trait association problems, encompassing dataset selection, preprocessing, and statistical analysis, in a pipeline that follows computational genomics standards. The benchmark includes expert-curated annotations from bioinformaticians to ensure accuracy and reliability. To provide baselines for these tasks, we present GenoAgent, a team of LLM-based agents that adopt a multi-step programming workflow with flexible self-correction, to collaboratively analyze gene expression datasets. Our experiments demonstrate the potential of LLM-based methods in analyzing genomic data, while error analysis highlights the challenges and areas for future improvement. We propose GenoTEX as a promising resource for benchmarking and enhancing automated methods for gene expression data analysis. The benchmark is available at https://github.com/Liu-Hy/GenoTEX.", "source": "arxiv", "arxiv_id": "2406.15341v3", "pdf_url": "https://arxiv.org/pdf/2406.15341v3", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-21T17:55:24Z", "updated": "2025-04-08T17:09:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive Software Release Decision-Making", "authors": ["Arsham Gholamzadeh Khoee", "Yinan Yu", "Robert Feldt", "Andris Freimanis", "Patrick Andersson Rhodin", "Dhasarathy Parthasarathy"], "year": 2024, "url": "http://arxiv.org/abs/2408.09785v2", "abstract": "Traditional methods for making software deployment decisions in the automotive industry typically rely on manual analysis of tabular software test data. These methods often lead to higher costs and delays in the software release cycle due to their labor-intensive nature. Large Language Models (LLMs) present a promising solution to these challenges. However, their application generally demands multiple rounds of human-driven prompt engineering, which limits their practical deployment, particularly for industrial end-users who need reliable and efficient results. In this paper, we propose GoNoGo, an LLM agent system designed to streamline automotive software deployment while meeting both functional requirements and practical industrial constraints. Unlike previous systems, GoNoGo is specifically tailored to address domain-specific and risk-sensitive systems. We evaluate GoNoGo's performance across different task difficulties using zero-shot and few-shot examples taken from industrial practice. Our results show that GoNoGo achieves a 100% success rate for tasks up to Level 2 difficulty with 3-shot examples, and maintains high performance even for more complex tasks. We find that GoNoGo effectively automates decision-making for simpler tasks, significantly reducing the need for manual intervention. In summary, GoNoGo represents an efficient and user-friendly LLM-based solution currently employed in our industrial partner's company to assist with software release decision-making, supporting more informed and timely decisions in the release process for risk-sensitive vehicle systems.", "source": "arxiv", "arxiv_id": "2408.09785v2", "pdf_url": "https://arxiv.org/pdf/2408.09785v2", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-19T08:22:20Z", "updated": "2024-09-29T09:46:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Good Parenting is all you need -- Multi-agentic LLM Hallucination Mitigation", "authors": ["Ted Kwartler", "Matthew Berman", "Alan Aqrawi"], "year": 2024, "url": "http://arxiv.org/abs/2410.14262v3", "abstract": "This study explores the ability of Large Language Model (LLM) agents to detect and correct hallucinations in AI-generated content. A primary agent was tasked with creating a blog about a fictional Danish artist named Flipfloppidy, which was then reviewed by another agent for factual inaccuracies. Most LLMs hallucinated the existence of this artist. Across 4,900 test runs involving various combinations of primary and reviewing agents, advanced AI models such as Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in identifying hallucinations and successfully revised outputs in 85% to 100% of cases following feedback. These findings underscore the potential of advanced AI models to significantly enhance the accuracy and reliability of generated content, providing a promising approach to improving AI workflow orchestration.", "source": "arxiv", "arxiv_id": "2410.14262v3", "pdf_url": "https://arxiv.org/pdf/2410.14262v3", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-18T08:18:18Z", "updated": "2024-10-25T17:24:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding", "authors": ["Yukun Cao", "Shuo Han", "Zengyi Gao", "Zezhong Ding", "Xike Xie", "S. Kevin Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2409.03258v3", "abstract": "Although Large Language Models (LLMs) have demonstrated potential in processing graphs, they struggle with comprehending graphical structure information through prompts of graph description sequences, especially as the graph size increases. We attribute this challenge to the uneven memory performance of LLMs across different positions in graph description sequences, known as ''positional biases''. To address this, we propose GraphInsight, a novel framework aimed at improving LLMs' comprehension of both macro- and micro-level graphical information. GraphInsight is grounded in two key strategies: 1) placing critical graphical information in positions where LLMs exhibit stronger memory performance, and 2) investigating a lightweight external knowledge base for regions with weaker memory performance, inspired by retrieval-augmented generation (RAG). Moreover, GraphInsight explores integrating these two strategies into LLM agent processes for composite graph tasks that require multi-step reasoning. Extensive empirical studies on benchmarks with a wide range of evaluation tasks show that GraphInsight significantly outperforms all other graph description methods (e.g., prompting techniques and reordering strategies) in understanding graph structures of varying sizes.", "source": "arxiv", "arxiv_id": "2409.03258v3", "pdf_url": "https://arxiv.org/pdf/2409.03258v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-05T05:34:16Z", "updated": "2024-12-16T08:06:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models", "authors": ["Harsh Kohli", "Sachin Kumar", "Huan Sun"], "year": 2024, "url": "http://arxiv.org/abs/2404.04237v2", "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to address complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional reasoning, two aspects that are central to human cognition, and introduce GroundCocoa - a lexically diverse benchmark connecting these reasoning skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art LLMs with even the best performing model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced prompting techniques.", "source": "arxiv", "arxiv_id": "2404.04237v2", "pdf_url": "https://arxiv.org/pdf/2404.04237v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-05T17:36:26Z", "updated": "2025-02-13T22:13:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Grounding Large Language Models In Embodied Environment With Imperfect World Models", "authors": ["Haolan Liu", "Jishen Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2410.02742v2", "abstract": "Despite a widespread success in various applications, large language models (LLMs) often stumble when tackling basic physical reasoning or executing robotics tasks, due to a lack of direct experience with the physical nuances of the real world. To address these issues, we propose a Grounding Large language model with Imperfect world MOdel (GLIMO), which utilizes proxy world models such as simulators to collect and synthesize trining data. GLIMO incorporates an LLM agent-based data generator to automatically create high-quality and diverse instruction datasets. The generator includes an iterative self-refining module for temporally consistent experience sampling, a diverse set of question-answering instruction seeds, and a retrieval-augmented generation module for reflecting on prior experiences. Comprehensive experiments show that our approach improve the performance of strong open-source LLMs like LLaMA-3 with a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$ across three different benchmarks, respectively. The performance is able to compete with or surpass their larger counterparts such as GPT-4.", "source": "arxiv", "arxiv_id": "2410.02742v2", "pdf_url": "https://arxiv.org/pdf/2410.02742v2", "categories": ["cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-03T17:55:09Z", "updated": "2024-11-11T20:33:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning", "authors": ["Zhen Xiang", "Linzhi Zheng", "Yanjie Li", "Junyuan Hong", "Qinbin Li", "Han Xie", "Jiawei Zhang", "Zidi Xiong", "Chulin Xie", "Carl Yang", "Dawn Song", "Bo Li"], "year": 2024, "url": "http://arxiv.org/abs/2406.09187v3", "abstract": "The rapid advancement of large language model (LLM) agents has raised new concerns regarding their safety and security. In this paper, we propose GuardAgent, the first guardrail agent to protect target agents by dynamically checking whether their actions satisfy given safety guard requests. Specifically, GuardAgent first analyzes the safety guard requests to generate a task plan, and then maps this plan into guardrail code for execution. By performing the code execution, GuardAgent can deterministically follow the safety guard request and safeguard target agents. In both steps, an LLM is utilized as the reasoning component, supplemented by in-context demonstrations retrieved from a memory module storing experiences from previous tasks. In addition, we propose two novel benchmarks: EICU-AC benchmark to assess the access control for healthcare agents and Mind2Web-SC benchmark to evaluate the safety policies for web agents. We show that GuardAgent effectively moderates the violation actions for different types of agents on these two benchmarks with over 98% and 83% guardrail accuracies, respectively. Project page: https://guardagent.github.io/", "source": "arxiv", "arxiv_id": "2406.09187v3", "pdf_url": "https://arxiv.org/pdf/2406.09187v3", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-13T14:49:26Z", "updated": "2025-05-29T03:09:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments", "authors": ["Sangmim Song", "Sarath Kodagoda", "Amal Gunatilake", "Marc G. Carmichael", "Karthick Thiyagarajan", "Jodi Martin"], "year": 2024, "url": "http://arxiv.org/abs/2410.20666v2", "abstract": "Navigation presents a significant challenge for persons with visual impairments (PVI). While traditional aids such as white canes and guide dogs are invaluable, they fall short in delivering detailed spatial information and precise guidance to desired locations. Recent developments in large language models (LLMs) and vision-language models (VLMs) offer new avenues for enhancing assistive navigation. In this paper, we introduce Guide-LLM, an embodied LLM-based agent designed to assist PVI in navigating large indoor environments. Our approach features a novel text-based topological map that enables the LLM to plan global paths using a simplified environmental representation, focusing on straight paths and right-angle turns to facilitate navigation. Additionally, we utilize the LLM's commonsense reasoning for hazard detection and personalized path planning based on user preferences. Simulated experiments demonstrate the system's efficacy in guiding PVI, underscoring its potential as a significant advancement in assistive technology. The results highlight Guide-LLM's ability to offer efficient, adaptive, and personalized navigation assistance, pointing to promising advancements in this field.", "source": "arxiv", "arxiv_id": "2410.20666v2", "pdf_url": "https://arxiv.org/pdf/2410.20666v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-10-28T01:58:21Z", "updated": "2025-03-11T23:45:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR Applications", "authors": ["Weijie Xu", "Jay Desai", "Fanyou Wu", "Josef Valvoda", "Srinivasan H. Sengamedu"], "year": 2024, "url": "http://arxiv.org/abs/2410.11239v1", "abstract": "Recent LLM (Large Language Models) advancements benefit many fields such as education and finance, but HR has hundreds of repetitive processes, such as access requests, medical claim filing and time-off submissions, which are unaddressed. We relate these tasks to the LLM agent, which has addressed tasks such as writing assisting and customer support. We present HR-Agent, an efficient, confidential, and HR-specific LLM-based task-oriented dialogue system tailored for automating repetitive HR processes such as medical claims and access requests. Since conversation data is not sent to an LLM during inference, it preserves confidentiality required in HR-related tasks.", "source": "arxiv", "arxiv_id": "2410.11239v1", "pdf_url": "https://arxiv.org/pdf/2410.11239v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-15T03:51:08Z", "updated": "2024-10-15T03:51:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent", "authors": ["Weijie Xu", "Zicheng Huang", "Wenxiang Hu", "Xi Fang", "Rajesh Kumar Cherukuri", "Naumaan Nayyar", "Lorenzo Malandri", "Srinivasan H. Sengamedu"], "year": 2024, "url": "http://arxiv.org/abs/2402.01018v1", "abstract": "Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However, the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains to evaluate LLM Agent. Our work has the following contributions: (1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a detailed recipe for the data generation procedure along with data analysis and human evaluations. The data generation pipeline is transferable and can be easily adapted for labeled conversation data generation in other domains. (3) The proposed data-collection pipeline is mostly based on LLMs with minimal human involvement for annotation, which is time and cost-efficient.", "source": "arxiv", "arxiv_id": "2402.01018v1", "pdf_url": "https://arxiv.org/pdf/2402.01018v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "EACL 2024", "published": "2024-02-01T21:10:44Z", "updated": "2024-02-01T21:10:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HackSynth: LLM Agent and Evaluation Framework for Autonomous Penetration Testing", "authors": ["Lajos Muzsai", "David Imolai", "AndrÃ¡s LukÃ¡cs"], "year": 2024, "url": "http://arxiv.org/abs/2412.01778v1", "abstract": "We introduce HackSynth, a novel Large Language Model (LLM)-based agent capable of autonomous penetration testing. HackSynth's dual-module architecture includes a Planner and a Summarizer, which enable it to generate commands and process feedback iteratively. To benchmark HackSynth, we propose two new Capture The Flag (CTF)-based benchmark sets utilizing the popular platforms PicoCTF and OverTheWire. These benchmarks include two hundred challenges across diverse domains and difficulties, providing a standardized framework for evaluating LLM-based penetration testing agents. Based on these benchmarks, extensive experiments are presented, analyzing the core parameters of HackSynth, including creativity (temperature and top-p) and token utilization. Multiple open source and proprietary LLMs were used to measure the agent's capabilities. The experiments show that the agent performed best with the GPT-4o model, better than what the GPT-4o's system card suggests. We also discuss the safety and predictability of HackSynth's actions. Our findings indicate the potential of LLM-based agents in advancing autonomous penetration testing and the importance of robust safeguards. HackSynth and the benchmarks are publicly available to foster research on autonomous cybersecurity solutions.", "source": "arxiv", "arxiv_id": "2412.01778v1", "pdf_url": "https://arxiv.org/pdf/2412.01778v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-02T18:28:18Z", "updated": "2024-12-02T18:28:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Hacking CTFs with Plain Agents", "authors": ["Rustem Turtayev", "Artem Petrov", "Dmitrii Volkov", "Denis Volk"], "year": 2024, "url": "http://arxiv.org/abs/2412.02776v1", "abstract": "We saturate a high-school-level hacking benchmark with plain LLM agent design. Concretely, we obtain 95% performance on InterCode-CTF, a popular offensive security benchmark, using prompting, tool use, and multiple attempts. This beats prior work by Phuong et al. 2024 (29%) and Abramovich et al. 2024 (72%).\n  Our results suggest that current LLMs have surpassed the high school level in offensive cybersecurity. Their hacking capabilities remain underelicited: our ReAct&Plan prompting strategy solves many challenges in 1-2 turns without complex engineering or advanced harnessing.", "source": "arxiv", "arxiv_id": "2412.02776v1", "pdf_url": "https://arxiv.org/pdf/2412.02776v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-03T19:17:45Z", "updated": "2024-12-03T19:17:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments", "authors": ["Maria Rigaki", "Carlos Catania", "Sebastian Garcia"], "year": 2024, "url": "http://arxiv.org/abs/2409.11276v1", "abstract": "Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity. Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4. Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios. To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities. Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts", "source": "arxiv", "arxiv_id": "2409.11276v1", "pdf_url": "https://arxiv.org/pdf/2409.11276v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-09-17T15:28:25Z", "updated": "2024-09-17T15:28:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Harnessing the power of LLMs for normative reasoning in MASs", "authors": ["Bastin Tony Roy Savarimuthu", "Surangika Ranathunga", "Stephen Cranefield"], "year": 2024, "url": "http://arxiv.org/abs/2403.16524v2", "abstract": "Software agents, both human and computational, do not exist in isolation and often need to collaborate or coordinate with others to achieve their goals. In human society, social mechanisms such as norms ensure efficient functioning, and these techniques have been adopted by researchers in multi-agent systems (MAS) to create socially aware agents. However, traditional techniques have limitations, such as operating in limited environments often using brittle symbolic reasoning. The advent of Large Language Models (LLMs) offers a promising solution, providing a rich and expressive vocabulary for norms and enabling norm-capable agents that can perform a range of tasks such as norm discovery, normative reasoning and decision-making. This paper examines the potential of LLM-based agents to acquire normative capabilities, drawing on recent Natural Language Processing (NLP) and LLM research. We present our vision for creating normative LLM agents. In particular, we discuss how the recently proposed \"LLM agent\" approaches can be extended to implement such normative LLM agents. We also highlight challenges in this emerging field. This paper thus aims to foster collaboration between MAS, NLP and LLM researchers in order to advance the field of normative agents.", "source": "arxiv", "arxiv_id": "2403.16524v2", "pdf_url": "https://arxiv.org/pdf/2403.16524v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-25T08:09:01Z", "updated": "2024-10-14T02:20:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Hermes: A Large Language Model Framework on the Journey to Autonomous Networks", "authors": ["Fadhel Ayed", "Ali Maatouk", "Nicola Piovesan", "Antonio De Domenico", "Merouane Debbah", "Zhi-Quan Luo"], "year": 2024, "url": "http://arxiv.org/abs/2411.06490v1", "abstract": "The drive toward automating cellular network operations has grown with the increasing complexity of these systems. Despite advancements, full autonomy currently remains out of reach due to reliance on human intervention for modeling network behaviors and defining policies to meet target requirements. Network Digital Twins (NDTs) have shown promise in enhancing network intelligence, but the successful implementation of this technology is constrained by use case-specific architectures, limiting its role in advancing network autonomy. A more capable network intelligence, or \"telecommunications brain\", is needed to enable seamless, autonomous management of cellular network. Large Language Models (LLMs) have emerged as potential enablers for this vision but face challenges in network modeling, especially in reasoning and handling diverse data types. To address these gaps, we introduce Hermes, a chain of LLM agents that uses \"blueprints\" for constructing NDT instances through structured and explainable logical steps. Hermes allows automatic, reliable, and accurate network modeling of diverse use cases and configurations, thus marking progress toward fully autonomous network operations.", "source": "arxiv", "arxiv_id": "2411.06490v1", "pdf_url": "https://arxiv.org/pdf/2411.06490v1", "categories": ["cs.AI", "cs.NI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-10T15:12:12Z", "updated": "2024-11-10T15:12:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HighwayLLM: Decision-Making and Navigation in Highway Driving with RL-Informed Language Model", "authors": ["Mustafa Yildirim", "Barkin Dagda", "Saber Fallah"], "year": 2024, "url": "http://arxiv.org/abs/2405.13547v1", "abstract": "Autonomous driving is a complex task which requires advanced decision making and control algorithms. Understanding the rationale behind the autonomous vehicles' decision is crucial to ensure their safe and effective operation on highway driving. This study presents a novel approach, HighwayLLM, which harnesses the reasoning capabilities of large language models (LLMs) to predict the future waypoints for ego-vehicle's navigation. Our approach also utilizes a pre-trained Reinforcement Learning (RL) model to serve as a high-level planner, making decisions on appropriate meta-level actions. The HighwayLLM combines the output from the RL model and the current state information to make safe, collision-free, and explainable predictions for the next states, thereby constructing a trajectory for the ego-vehicle. Subsequently, a PID-based controller guides the vehicle to the waypoints predicted by the LLM agent. This integration of LLM with RL and PID enhances the decision-making process and provides interpretability for highway autonomous driving.", "source": "arxiv", "arxiv_id": "2405.13547v1", "pdf_url": "https://arxiv.org/pdf/2405.13547v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-05-22T11:32:37Z", "updated": "2024-05-22T11:32:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models", "authors": ["Yin Jou Huang", "Rafik Hadfi"], "year": 2024, "url": "http://arxiv.org/abs/2407.11549v2", "abstract": "Psychological evidence reveals the influence of personality traits on decision-making. For instance, agreeableness is generally associated with positive outcomes in negotiations, whereas neuroticism is often linked to less favorable outcomes. This paper introduces a simulation framework centered on Large Language Model (LLM) agents endowed with synthesized personality traits. The agents negotiate within bargaining domains and possess customizable personalities and objectives. The experimental results show that the behavioral tendencies of LLM-based simulations could reproduce behavioral patterns observed in human negotiations. The contribution is twofold. First, we propose a simulation methodology that investigates the alignment between the linguistic and economic capabilities of LLM agents. Secondly, we offer empirical insights into the strategic impact of Big-Five personality traits on the outcomes of bilateral negotiations. We also provide a case study based on synthesized bargaining dialogues to reveal intriguing behaviors, including deceitful and compromising behaviors.", "source": "arxiv", "arxiv_id": "2407.11549v2", "pdf_url": "https://arxiv.org/pdf/2407.11549v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-16T09:52:51Z", "updated": "2024-11-02T16:24:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis", "authors": ["Federico Bianchi", "Patrick John Chia", "Mert Yuksekgonul", "Jacopo Tagliabue", "Dan Jurafsky", "James Zou"], "year": 2024, "url": "http://arxiv.org/abs/2402.05863v1", "abstract": "Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LLMs can improve their payoffs by 20\\% when negotiating against the standard GPT-4. We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans. Together, \\NegotiationArena offers a new environment to investigate LLM interactions, enabling new insights into LLM's theory of mind, irrationality, and reasoning abilities.", "source": "arxiv", "arxiv_id": "2402.05863v1", "pdf_url": "https://arxiv.org/pdf/2402.05863v1", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-08T17:51:48Z", "updated": "2024-02-08T17:51:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Human-Centered LLM-Agent User Interface: A Position Paper", "authors": ["Daniel Chin", "Yuxuan Wang", "Gus Xia"], "year": 2024, "url": "http://arxiv.org/abs/2405.13050v2", "abstract": "Large Language Model (LLM) -in-the-loop applications have been shown to effectively interpret the human user's commands, make plans, and operate external tools/systems accordingly. Still, the operation scope of the LLM agent is limited to passively following the user, requiring the user to frame his/her needs with regard to the underlying tools/systems. We note that the potential of an LLM-Agent User Interface (LAUI) is much greater. A user mostly ignorant to the underlying tools/systems should be able to work with a LAUI to discover an emergent workflow. Contrary to the conventional way of designing an explorable GUI to teach the user a predefined set of ways to use the system, in the ideal LAUI, the LLM agent is initialized to be proficient with the system, proactively studies the user and his/her needs, and proposes new interaction schemes to the user. To illustrate LAUI, we present Flute X GPT, a concrete example using an LLM agent, a prompt manager, and a flute-tutoring multi-modal software-hardware system to facilitate the complex, real-time user experience of learning to play the flute.", "source": "arxiv", "arxiv_id": "2405.13050v2", "pdf_url": "https://arxiv.org/pdf/2405.13050v2", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-05-19T13:02:45Z", "updated": "2024-09-23T16:41:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "HumanEvalComm: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent", "authors": ["Jie JW Wu", "Fatemeh H Fard"], "year": 2024, "url": "http://arxiv.org/abs/2406.00215v3", "abstract": "Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks.\n  In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We define communication skills of LLMs as ``being able to ask clarifying questions when the description of the code generation problem has issues''. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues: inconsistency, ambiguity, incompleteness. We defined new evaluation metrics such as Communication Rate and Good Question Rate, and then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Okanagan, to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. Finally, we discussed evaluation results by comparing Code LLMs and Okanagan with our findings.", "source": "arxiv", "arxiv_id": "2406.00215v3", "pdf_url": "https://arxiv.org/pdf/2406.00215v3", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "10.1145/3715109", "venue": "ACM Trans. Softw. Eng. Methodol., Published Jan 2025", "published": "2024-05-31T22:06:18Z", "updated": "2025-01-27T20:54:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models", "authors": ["Logan Cross", "Violet Xiang", "Agam Bhatia", "Daniel LK Yamins", "Nick Haber"], "year": 2024, "url": "http://arxiv.org/abs/2407.07086v2", "abstract": "Multi-agent reinforcement learning (MARL) methods struggle with the non-stationarity of multi-agent systems and fail to adaptively learn online when tested with novel agents. Here, we leverage large language models (LLMs) to create an autonomous agent that can handle these challenges. Our agent, Hypothetical Minds, consists of a cognitively-inspired architecture, featuring modular components for perception, memory, and hierarchical planning over two levels of abstraction. We introduce the Theory of Mind module that scaffolds the high-level planning process by generating hypotheses about other agents' strategies in natural language. It then evaluates and iteratively refines these hypotheses by reinforcing hypotheses that make correct predictions about the other agents' behavior. Hypothetical Minds significantly improves performance over previous LLM-agent and RL baselines on a range of competitive, mixed motive, and collaborative domains in the Melting Pot benchmark, including both dyadic and population-based environments. Additionally, comparisons against LLM-agent baselines and ablations reveal the importance of hypothesis evaluation and refinement for succeeding on complex scenarios.", "source": "arxiv", "arxiv_id": "2407.07086v2", "pdf_url": "https://arxiv.org/pdf/2407.07086v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-09T17:57:15Z", "updated": "2024-12-12T01:41:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in Multi-Agent Settings with Social Hierarchy", "authors": ["Gian Maria Campedelli", "NicolÃ² Penzo", "Massimo Stefan", "Roberto DessÃ¬", "Marco Guerini", "Bruno Lepri", "Jacopo Staiano"], "year": 2024, "url": "http://arxiv.org/abs/2410.07109v3", "abstract": "As LLM-based agents become increasingly autonomous and will more freely interact with each other, studying the interplay among them becomes crucial to anticipate emergent phenomena and potential risks. In this work, we provide an in-depth analysis of the interactions among agents within a simulated hierarchical social environment, drawing inspiration from the Stanford Prison Experiment. Leveraging 2,400 conversations across six LLMs (i.e., LLama3, Orca2, Command-r, Mixtral, Mistral2, and gpt4.1) and 240 experimental scenarios, we analyze persuasion and anti-social behavior between a guard and a prisoner agent with differing objectives. We first document model-specific conversational failures in this multi-agent power dynamic context, thereby narrowing our analytic sample to 1,600 conversations. Among models demonstrating successful interaction, we find that goal setting significantly influences persuasiveness but not anti-social behavior. Moreover, agent personas, especially the guard's, substantially impact both successful persuasion by the prisoner and the manifestation of anti-social actions. Notably, we observe the emergence of anti-social conduct even in absence of explicit negative personality prompts. These results have important implications for the development of interactive LLM agents and the ongoing discussion of their societal impact.", "source": "arxiv", "arxiv_id": "2410.07109v3", "pdf_url": "https://arxiv.org/pdf/2410.07109v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-09T17:45:47Z", "updated": "2025-11-04T15:55:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "I-Design: Personalized LLM Interior Designer", "authors": ["Ata Ãelen", "Guo Han", "Konrad Schindler", "Luc Van Gool", "Iro Armeni", "Anton Obukhov", "Xi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2404.02838v1", "abstract": "Interior design allows us to be who we are and live how we want - each design is as unique as our distinct personality. However, it is not trivial for non-professionals to express and materialize this since it requires aligning functional and visual expectations with the constraints of physical space; this renders interior design a luxury. To make it more accessible, we present I-Design, a personalized interior designer that allows users to generate and visualize their design goals through natural language communication. I-Design starts with a team of large language model agents that engage in dialogues and logical reasoning with one another, transforming textual user input into feasible scene graph designs with relative object relationships. Subsequently, an effective placement algorithm determines optimal locations for each object within the scene. The final design is then constructed in 3D by retrieving and integrating assets from an existing object database. Additionally, we propose a new evaluation protocol that utilizes a vision-language model and complements the design pipeline. Extensive quantitative and qualitative experiments show that I-Design outperforms existing methods in delivering high-quality 3D design solutions and aligning with abstract concepts that match user input, showcasing its advantages across detailed 3D arrangement and conceptual fidelity.", "source": "arxiv", "arxiv_id": "2404.02838v1", "pdf_url": "https://arxiv.org/pdf/2404.02838v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1007/978-3-031-92387-6_17", "venue": "Computer Vision - ECCV 2024 Workshops, Lecture Notes in Computer Science (LNCS), vol. 15624, pp. 217-234, Springer, Cham, 2025", "published": "2024-04-03T16:17:53Z", "updated": "2024-04-03T16:17:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in Communicative Multi-Agent Systems", "authors": ["Yihuan Mao", "Yipeng Kang", "Peilun Li", "Ning Zhang", "Wei Xu", "Chongjie Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2410.16237v2", "abstract": "As large language model (LLM) agents increasingly integrate into our infrastructure, their robust coordination and message synchronization become vital. The Byzantine Generals Problem (BGP) is a critical model for constructing resilient multi-agent systems (MAS) under adversarial attacks. It describes a scenario where malicious agents with unknown identities exist in the system-situations that, in our context, could result from LLM agents' hallucinations or external attacks. In BGP, the objective of the entire system is to reach a consensus on the action to be taken. Traditional BGP requires global consensus among all agents; however, in practical scenarios, global consensus is not always necessary and can even be inefficient. Therefore, there is a pressing need to explore a refined version of BGP that aligns with the local coordination patterns observed in MAS. We refer to this refined version as Imperfect BGP (IBGP) in our research, aiming to address this discrepancy. To tackle this issue, we propose a framework that leverages consensus protocols within general MAS settings, providing provable resilience against communication attacks and adaptability to changing environments, as validated by empirical results. Additionally, we present a case study in a sensor network environment to illustrate the practical application of our protocol.", "source": "arxiv", "arxiv_id": "2410.16237v2", "pdf_url": "https://arxiv.org/pdf/2410.16237v2", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-10-21T17:41:42Z", "updated": "2024-10-23T08:31:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation", "authors": ["Senyu Han", "Lu Chen", "Li-Min Lin", "Zhengshan Xu", "Kai Yu"], "year": 2024, "url": "http://arxiv.org/abs/2407.01093v1", "abstract": "Large language models have demonstrated their capabilities in storyline creation and human-like character role-playing. Current language model agents mainly focus on reasonable behaviors from the level of individuals, and their behaviors might be hard to constraint on the level of the whole storyline. In this paper we introduce IBSEN, a director-actor coordinate agent framework that generates drama scripts and makes the plot played by agents more controllable. The director agent writes plot outlines that the user desires to see, instructs the actor agents to role-play their characters, and reschedules the plot when human players participate in the scenario to ensure the plot is progressing towards the objective. To evaluate the framework, we create a novel drama plot that involves several actor agents and check the interactions between them under the instruction of the director agent. Evaluation results show that our framework could generate complete, diverse drama scripts from only a rough outline of plot objectives, meanwhile maintaining the characteristics of characters in the drama. Our codes and prompts are available at https://github.com/OpenDFM/ibsen.", "source": "arxiv", "arxiv_id": "2407.01093v1", "pdf_url": "https://arxiv.org/pdf/2407.01093v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-01T08:49:57Z", "updated": "2024-07-01T08:49:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction", "authors": ["Kaiyu He", "Mian Zhang", "Shuo Yan", "Peilin Wu", "Zhiyu Zoey Chen"], "year": 2024, "url": "http://arxiv.org/abs/2408.10455v6", "abstract": "While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in holistic rule learning in interactive environments remains less explored. We introduce RULEARN, a novel benchmark to assess the rule-learning abilities of LLM agents in interactive settings. In RULEARN, agents strategically interact with simulated environments to gather observations, discern patterns, and solve complex problems. To enhance the rule-learning capabilities for LLM agents, we propose IDEA, a novel reasoning framework that integrates the process of Induction, Deduction, and Abduction. The IDEA agent generates initial hypotheses from limited observations through abduction, devises plans to validate these hypotheses or leverages them to solve problems via deduction, and refines previous hypotheses through induction, dynamically establishing and applying rules that mimic human rule-learning behaviors. Our evaluation of the IDEA framework, which involves five representative LLMs, demonstrates significant improvements over the baseline. Furthermore, our study with human participants reveals notable discrepancies in rule-learning behaviors between humans and LLMs. We believe our benchmark will serve as a valuable and challenging resource, and IDEA will provide crucial insights for the development of LLM agents capable of human-like rule learning in real-world scenarios. Our code and data is publicly available.", "source": "arxiv", "arxiv_id": "2408.10455v6", "pdf_url": "https://arxiv.org/pdf/2408.10455v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-19T23:37:07Z", "updated": "2025-05-27T05:26:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent", "authors": ["Haohang Li", "Yupeng Cao", "Yangyang Yu", "Shashidhar Reddy Javaji", "Zhiyang Deng", "Yueru He", "Yuechen Jiang", "Zining Zhu", "Koduvayur Subbalakshmi", "Guojun Xiong", "Jimin Huang", "Lingfei Qian", "Xueqing Peng", "Qianqian Xie", "Jordan W. Suchow"], "year": 2024, "url": "http://arxiv.org/abs/2412.18174v1", "abstract": "Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce \\textsc{InvestorBench}, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, multi-modal datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents' performance across various scenarios.", "source": "arxiv", "arxiv_id": "2412.18174v1", "pdf_url": "https://arxiv.org/pdf/2412.18174v1", "categories": ["cs.CE", "cs.AI", "q-fin.CP"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2024-12-24T05:22:33Z", "updated": "2024-12-24T05:22:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents", "authors": ["Zehao Wang", "Dong Jae Kim", "Tse-Hsun Chen"], "year": 2024, "url": "http://arxiv.org/abs/2406.12806v1", "abstract": "Configuration settings are essential for tailoring software behavior to meet specific performance requirements. However, incorrect configurations are widespread, and identifying those that impact system performance is challenging due to the vast number and complexity of possible settings. In this work, we present PerfSense, a lightweight framework that leverages Large Language Models (LLMs) to efficiently identify performance-sensitive configurations with minimal overhead. PerfSense employs LLM agents to simulate interactions between developers and performance engineers using advanced prompting techniques such as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of seven open-source Java systems demonstrates that PerfSense achieves an average accuracy of 64.77% in classifying performance-sensitive configurations, outperforming both our LLM baseline (50.36%) and the previous state-of-the-art method (61.75%). Notably, our prompt chaining technique improves recall by 10% to 30% while maintaining similar precision levels. Additionally, a manual analysis of 362 misclassifications reveals common issues, including LLMs' misunderstandings of requirements (26.8%). In summary, PerfSense significantly reduces manual effort in classifying performance-sensitive configurations and offers valuable insights for future LLM-based code analysis research.", "source": "arxiv", "arxiv_id": "2406.12806v1", "pdf_url": "https://arxiv.org/pdf/2406.12806v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-06-18T17:22:48Z", "updated": "2024-06-18T17:22:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ImProver: Agent-Based Automated Proof Optimization", "authors": ["Riyaz Ahuja", "Jeremy Avigad", "Prasad Tetali", "Sean Welleck"], "year": 2024, "url": "http://arxiv.org/abs/2410.04753v1", "abstract": "Large language models (LLMs) have been used to generate formal proofs of mathematical theorems in proofs assistants such as Lean. However, we often want to optimize a formal proof with respect to various criteria, depending on its downstream use. For example, we may want a proof to adhere to a certain style, or to be readable, concise, or modularly structured. Having suitably optimized proofs is also important for learning tasks, especially since human-written proofs may not optimal for that purpose. To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is correct and optimizes for an arbitrary criterion, such as length or readability. As a first method for automated proof optimization, we present ImProver, a large-language-model agent that rewrites proofs to optimize arbitrary user-defined metrics in Lean. We find that naively applying LLMs to proof optimization falls short, and we incorporate various improvements into ImProver, such as the use of symbolic Lean context in a novel Chain-of-States technique, as well as error-correction and retrieval. We test ImProver on rewriting real-world undergraduate, competition, and research-level mathematics theorems, finding that ImProver is capable of rewriting proofs so that they are substantially shorter, more modular, and more readable.", "source": "arxiv", "arxiv_id": "2410.04753v1", "pdf_url": "https://arxiv.org/pdf/2410.04753v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-07T05:14:18Z", "updated": "2024-10-07T05:14:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Imprompter: Tricking LLM Agents into Improper Tool Use", "authors": ["Xiaohan Fu", "Shuheng Li", "Zihan Wang", "Yihao Liu", "Rajesh K. Gupta", "Taylor Berg-Kirkpatrick", "Earlence Fernandes"], "year": 2024, "url": "http://arxiv.org/abs/2410.14923v2", "abstract": "Large Language Model (LLM) Agents are an emerging computing paradigm that blends generative machine learning with tools such as code interpreters, web browsing, email, and more generally, external resources. These agent-based systems represent an emerging shift in personal computing. We contribute to the security foundations of agent-based systems and surface a new class of automatically computed obfuscated adversarial prompt attacks that violate the confidentiality and integrity of user resources connected to an LLM agent. We show how prompt optimization techniques can find such prompts automatically given the weights of a model. We demonstrate that such attacks transfer to production-level agents. For example, we show an information exfiltration attack on Mistral's LeChat agent that analyzes a user's conversation, picks out personally identifiable information, and formats it into a valid markdown command that results in leaking that data to the attacker's server. This attack shows a nearly 80% success rate in an end-to-end evaluation. We conduct a range of experiments to characterize the efficacy of these attacks and find that they reliably work on emerging agent-based systems like Mistral's LeChat, ChatGLM, and Meta's Llama. These attacks are multimodal, and we show variants in the text-only and image domains.", "source": "arxiv", "arxiv_id": "2410.14923v2", "pdf_url": "https://arxiv.org/pdf/2410.14923v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-19T01:00:57Z", "updated": "2024-10-22T00:53:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility", "authors": ["Yuchen Xia", "Jize Zhang", "Nasser Jazdi", "Michael Weyrich"], "year": 2024, "url": "http://arxiv.org/abs/2407.08550v1", "abstract": "This paper introduces a novel approach to integrating large language model (LLM) agents into automated production systems, aimed at enhancing task automation and flexibility. We organize production operations within a hierarchical framework based on the automation pyramid. Atomic operation functionalities are modeled as microservices, which are executed through interface invocation within a dedicated digital twin system. This allows for a scalable and flexible foundation for orchestrating production processes. In this digital twin system, low-level, hardware-specific data is semantically enriched and made interpretable for LLMs for production planning and control tasks. Large language model agents are systematically prompted to interpret these production-specific data and knowledge. Upon receiving a user request or identifying a triggering event, the LLM agents generate a process plan. This plan is then decomposed into a series of atomic operations, executed as microservices within the real-world automation system. We implement this overall approach on an automated modular production facility at our laboratory, demonstrating how the LLMs can handle production planning and control tasks through a concrete case study. This results in an intuitive production facility with higher levels of task automation and flexibility. Finally, we reveal the several limitations in realizing the full potential of the large language models in autonomous systems and point out promising benefits. Demos of this series of ongoing research series can be accessed at: https://github.com/YuchenXia/GPT4IndustrialAutomation", "source": "arxiv", "arxiv_id": "2407.08550v1", "pdf_url": "https://arxiv.org/pdf/2407.08550v1", "categories": ["cs.AI", "cs.ET", "cs.MA", "cs.RO", "eess.SY"], "primary_category": "cs.AI", "doi": "10.51202/9783181024379", "venue": "", "published": "2024-07-11T14:34:43Z", "updated": "2024-07-11T14:34:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents", "authors": ["Qiusi Zhan", "Zhixiang Liang", "Zifan Ying", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2403.02691v3", "abstract": "Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.\n  In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.", "source": "arxiv", "arxiv_id": "2403.02691v3", "pdf_url": "https://arxiv.org/pdf/2403.02691v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-05T06:21:45Z", "updated": "2024-08-04T04:52:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "InsightLens: Augmenting LLM-Powered Data Analysis with Interactive Insight Management and Navigation", "authors": ["Luoxuan Weng", "Xingbo Wang", "Junyu Lu", "Yingchaojie Feng", "Yihan Liu", "Haozhe Feng", "Danqing Huang", "Wei Chen"], "year": 2024, "url": "http://arxiv.org/abs/2404.01644v2", "abstract": "The proliferation of large language models (LLMs) has revolutionized the capabilities of natural language interfaces (NLIs) for data analysis. LLMs can perform multi-step and complex reasoning to generate data insights based on users' analytic intents. However, these insights often entangle with an abundance of contexts in analytic conversations such as code, visualizations, and natural language explanations. This hinders efficient recording, organization, and navigation of insights within the current chat-based LLM interfaces. In this paper, we first conduct a formative study with eight data analysts to understand their general workflow and pain points of insight management during LLM-powered data analysis. Accordingly, we introduce InsightLens, an interactive system to overcome such challenges. Built upon an LLM-agent-based framework that automates insight recording and organization along with the analysis process, InsightLens visualizes the complex conversational contexts from multiple aspects to facilitate insight navigation. A user study with twelve data analysts demonstrates the effectiveness of InsightLens, showing that it significantly reduces users' manual and cognitive effort without disrupting their conversational data analysis workflow, leading to a more efficient analysis experience.", "source": "arxiv", "arxiv_id": "2404.01644v2", "pdf_url": "https://arxiv.org/pdf/2404.01644v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-04-02T05:20:12Z", "updated": "2024-12-21T09:34:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Interacting Large Language Model Agents. Interpretable Models and Social Learning", "authors": ["Adit Jain", "Vikram Krishnamurthy"], "year": 2024, "url": "http://arxiv.org/abs/2411.01271v2", "abstract": "This paper discusses the theory and algorithms for interacting large language model agents (LLMAs) using methods from statistical signal processing and microeconomics. While both fields are mature, their application to decision-making involving interacting LLMAs remains unexplored. Motivated by Bayesian sentiment analysis on online platforms, we construct interpretable models and algorithms that enable LLMAs to interact and perform Bayesian inference. Because interacting LLMAs learn from both prior decisions and external inputs, they can exhibit bias and herding behavior. Thus, developing interpretable models and stochastic control algorithms is essential to understand and mitigate these behaviors. This paper has three main results. First, we show using Bayesian revealed preferences from microeconomics that an individual LLMA satisfies the necessary and sufficient conditions for rationally inattentive (bounded rationality) Bayesian utility maximization and, given an observation, the LLMA chooses an action that maximizes a regularized utility. Second, we utilize Bayesian social learning to construct interpretable models for LLMAs that interact sequentially with each other and the environment while performing Bayesian inference. Our proposed models capture the herding behavior exhibited by interacting LLMAs. Third, we propose a stochastic control framework to delay herding and improve state estimation accuracy under 2 settings: (a) centrally controlled LLMAs (b) autonomous LLMAs with incentives. We demonstrate the effectiveness of our methods on real datasets for hate speech classification and product quality assessment, using open-source models like LLaMA and closed-source models like ChatGPT. The main takeaway of this paper, based on empirical analysis and mathematical formalism, is that LLMAs act as rationally bounded Bayesian agents that exhibit social learning when interacting.", "source": "arxiv", "arxiv_id": "2411.01271v2", "pdf_url": "https://arxiv.org/pdf/2411.01271v2", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.MA", "eess.SY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-11-02T14:49:34Z", "updated": "2025-05-25T12:58:44Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Interpreting Multi-band Galaxy Observations with Large Language Model-Based Agents", "authors": ["Zechang Sun", "Yuan-Sen Ting", "Yaobo Liang", "Nan Duan", "Song Huang", "Zheng Cai"], "year": 2024, "url": "http://arxiv.org/abs/2409.14807v2", "abstract": "Astronomical research traditionally relies on extensive domain knowledge to interpret observations and narrow down hypotheses. We demonstrate that this process can be emulated using large language model-based agents to accelerate research workflows. We propose mephisto, a multi-agent collaboration framework that mimics human reasoning to interpret multi-band galaxy observations. mephisto interacts with the CIGALE codebase, which includes spectral energy distribution (SED) models to explain observations. In this open-world setting, mephisto learns from its self-play experience, performs tree search, and accumulates knowledge in a dynamically updated base. As a proof of concept, we apply mephisto to the latest data from the James Webb Space Telescope. mephisto attains near-human proficiency in reasoning about galaxies' physical scenarios, even when dealing with a recently discovered population of \"Little Red Dot\" galaxies. This represents the first demonstration of agentic research in astronomy, advancing towards end-to-end research via LLM agents and potentially expediting astronomical discoveries.", "source": "arxiv", "arxiv_id": "2409.14807v2", "pdf_url": "https://arxiv.org/pdf/2409.14807v2", "categories": ["astro-ph.IM", "astro-ph.GA"], "primary_category": "astro-ph.IM", "doi": "", "venue": "", "published": "2024-09-23T08:32:22Z", "updated": "2025-08-04T09:39:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations", "authors": ["Yucheng Jiang", "Yijia Shao", "Dekun Ma", "Sina J. Semnani", "Monica S. Lam"], "year": 2024, "url": "http://arxiv.org/abs/2408.15232v2", "abstract": "While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, we create Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user's behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, we construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.", "source": "arxiv", "arxiv_id": "2408.15232v2", "pdf_url": "https://arxiv.org/pdf/2408.15232v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-27T17:50:03Z", "updated": "2024-10-17T20:43:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models", "authors": ["Liam Barkley", "Brink van der Merwe"], "year": 2024, "url": "http://arxiv.org/abs/2410.19385v1", "abstract": "Large Language Models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents (LLMs augmented with external tools to enhance their capabilities beyond language generation) on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.", "source": "arxiv", "arxiv_id": "2410.19385v1", "pdf_url": "https://arxiv.org/pdf/2410.19385v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-25T08:34:53Z", "updated": "2024-10-25T08:34:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning", "authors": ["Santosh Kumar Radha", "Yasamin Nouri Jelyani", "Ara Ghukasyan", "Oktay Goktas"], "year": 2024, "url": "http://arxiv.org/abs/2409.12618v2", "abstract": "Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs). Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an input query and the current iteration of an LLM's response. Unlike static or semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT), IoT adapts its reasoning path dynamically, based on evolving context, and without generating alternate explorative thoughts which are ultimately discarded. The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components. We introduce two variants of our framework: Autonomous Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and Guided Iteration of Thought (GIoT), which always forces a fixed number iterations. We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. Our results show that IoT represents a viable paradigm for autonomous response refinement in LLMs, showcasing significant improvements over CoT and thereby enabling more adaptive and efficient reasoning systems that minimize human intervention.", "source": "arxiv", "arxiv_id": "2409.12618v2", "pdf_url": "https://arxiv.org/pdf/2409.12618v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-19T09:44:17Z", "updated": "2024-10-01T17:50:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Iterative Experience Refinement of Software-Developing Agents", "authors": ["Chen Qian", "Jiahao Li", "Yufan Dang", "Wei Liu", "YiFei Wang", "Zihao Xie", "Weize Chen", "Cheng Yang", "Yingli Zhang", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "http://arxiv.org/abs/2405.04219v1", "abstract": "Autonomous agents powered by large language models (LLMs) show significant potential for achieving high autonomy in various scenarios such as software development. Recent research has shown that LLM agents can leverage past experiences to reduce errors and enhance efficiency. However, the static experience paradigm, reliant on a fixed collection of past experiences acquired heuristically, lacks iterative refinement and thus hampers agents' adaptability. In this paper, we introduce the Iterative Experience Refinement framework, enabling LLM agents to refine experiences iteratively during task execution. We propose two fundamental patterns: the successive pattern, refining based on nearest experiences within a task batch, and the cumulative pattern, acquiring experiences across all previous task batches. Augmented with our heuristic experience elimination, the method prioritizes high-quality and frequently-used experiences, effectively managing the experience space and enhancing efficiency. Extensive experiments show that while the successive pattern may yield superior results, the cumulative pattern provides more stable performance. Moreover, experience elimination facilitates achieving better performance using just 11.54% of a high-quality subset.", "source": "arxiv", "arxiv_id": "2405.04219v1", "pdf_url": "https://arxiv.org/pdf/2405.04219v1", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-07T11:33:49Z", "updated": "2024-05-07T11:33:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning", "authors": ["Yadong Zhang", "Shaoguang Mao", "Tao Ge", "Xun Wang", "Yan Xia", "Man Lan", "Furu Wei"], "year": 2024, "url": "http://arxiv.org/abs/2402.01521v2", "abstract": "Strategic reasoning is a complex yet essential capability for intelligent agents. It requires Large Language Model (LLM) agents to adapt their strategies dynamically in multi-agent environments. Unlike static reasoning tasks, success in these contexts depends on anticipating other agents' beliefs and actions while continuously adjusting strategies to achieve individual goals. LLMs and LLM agents often struggle with strategic reasoning due to the absence of a reasoning framework that enables them to dynamically infer others' perspectives and adapt to changing environments. Inspired by the Level-K framework from game theory and behavioral economics, which extends reasoning from simple reactions to structured strategic depth, we propose a novel framework: \"K-Level Reasoning with Large Language Models (K-R).\" This framework employs recursive mechanisms to enable LLMs to achieve varying levels of strategic depth, allowing agents to form higher order beliefs - beliefs about others' beliefs. We validate this framework through rigorous testing on four testbeds: two classical game theory problems and two social intelligence tasks. The results demonstrate the advantages of K-R in strategic reasoning. Our work presents the first recursive implementation of strategic depth in large language models (LLMs). It establishes a foundation for future research into theory of mind and strategic reasoning in LLMs.", "source": "arxiv", "arxiv_id": "2402.01521v2", "pdf_url": "https://arxiv.org/pdf/2402.01521v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-02T16:07:05Z", "updated": "2024-10-17T16:08:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "KG-RAG: Bridging the Gap Between Knowledge and Creativity", "authors": ["Diego Sanmartin"], "year": 2024, "url": "http://arxiv.org/abs/2405.12035v1", "abstract": "Ensuring factual accuracy while maintaining the creative capabilities of Large Language Model Agents (LMAs) poses significant challenges in the development of intelligent agent systems. LMAs face prevalent issues such as information hallucinations, catastrophic forgetting, and limitations in processing long contexts when dealing with knowledge-intensive tasks. This paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation) pipeline, a novel framework designed to enhance the knowledge capabilities of LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities of LLMs, thereby significantly reducing the reliance on the latent knowledge of LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then performs information retrieval over the newly created graph to perform KGQA (Knowledge Graph Question Answering). The retrieval methodology leverages a novel algorithm called Chain of Explorations (CoE) which benefits from LLMs reasoning to explore nodes and relationships within the KG sequentially. Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable improvements in the reduction of hallucinated content and suggest a promising path toward developing intelligent systems adept at handling knowledge-intensive tasks.", "source": "arxiv", "arxiv_id": "2405.12035v1", "pdf_url": "https://arxiv.org/pdf/2405.12035v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-20T14:03:05Z", "updated": "2024-05-20T14:03:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models", "authors": ["Kemou Jiang", "Xuan Cai", "Zhiyong Cui", "Aoyong Li", "Yilong Ren", "Haiyang Yu", "Hao Yang", "Daocheng Fu", "Licheng Wen", "Pinlong Cai"], "year": 2024, "url": "http://arxiv.org/abs/2407.14239v1", "abstract": "Large language models (LLMs) as autonomous agents offer a novel avenue for tackling real-world challenges through a knowledge-driven manner. These LLM-enhanced methodologies excel in generalization and interpretability. However, the complexity of driving tasks often necessitates the collaboration of multiple, heterogeneous agents, underscoring the need for such LLM-driven agents to engage in cooperative knowledge sharing and cognitive synergy. Despite the promise of LLMs, current applications predominantly center around single agent scenarios. To broaden the horizons of knowledge-driven strategies and bolster the generalization capabilities of autonomous agents, we propose the KoMA framework consisting of multi-agent interaction, multi-step planning, shared-memory, and ranking-based reflection modules to enhance multi-agents' decision-making in complex driving scenarios. Based on the framework's generated text descriptions of driving scenarios, the multi-agent interaction module enables LLM agents to analyze and infer the intentions of surrounding vehicles, akin to human cognition. The multi-step planning module enables LLM agents to analyze and obtain final action decisions layer by layer to ensure consistent goals for short-term action decisions. The shared memory module can accumulate collective experience to make superior decisions, and the ranking-based reflection module can evaluate and improve agent behavior with the aim of enhancing driving safety and efficiency. The KoMA framework not only enhances the robustness and adaptability of autonomous driving agents but also significantly elevates their generalization capabilities across diverse scenarios. Empirical results demonstrate the superiority of our approach over traditional methods, particularly in its ability to handle complex, unpredictable driving environments without extensive retraining.", "source": "arxiv", "arxiv_id": "2407.14239v1", "pdf_url": "https://arxiv.org/pdf/2407.14239v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-19T12:13:08Z", "updated": "2024-07-19T12:13:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level Kaggle Data Science Performance", "authors": ["Antoine Grosnit", "Alexandre Maraval", "Refinath S N", "Zichao Zhao", "James Doran", "Giuseppe Paolo", "Albert Thomas", "Jonas Gonzalez", "Abhineet Kumar", "Khyati Khandelwal", "Abdelhakim Benechehab", "Hamza Cherkaoui", "Youssef Attia El-Hili", "Kun Shao", "Jianye Hao", "Jun Yao", "BalÃ¡zs KÃ©gl", "Haitham Bou-Ammar", "Jun Wang"], "year": 2024, "url": "http://arxiv.org/abs/2411.03562v3", "abstract": "Human expertise emerges through iterative cycles of interaction, reflection, and internal model updating, which are central to cognitive theories such as Kolb's experiential learning and Vygotsky's zone of proximal development. In contrast, current AI systems, particularly LLM agents, rely on static pre-training or rigid workflows, lacking mechanisms for continual adaptation. Recent studies identified early cognitive traits in LLM agents (reflection, revision, and self-correction) suggesting foundational elements of human-like experiential learning. Thus the key question: Can we design LLM agents capable of structured, cognitively grounded learning similar to human processes? In response, we propose a computational framework of Kolb's learning cycle with Vygotsky's ZPD for autonomous agents. Our architecture separates extrinsic (environment interaction) and intrinsic (internal reflection/abstraction) functions, enabling cognitively grounded scaffolded learning, where the agent initially learns within structured environments, followed by open-ended generalisation. This approach empowers agents to master complex tasks ; domains that traditional fine-tuning or simple reflective methods could not tackle effectively. Its potential is powerfully demonstrated via direct comparison with humans in real-world Kaggle data science competitions. Learning fully automated data science code generation across 81 tasks, our system, Agent K, demonstrated the ability to perform the entire workflow autonomously, achieving an Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2% among 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals level performance - including 4 gold and 4 silver on prize-awarding competitions - Agent K is the 1st AI system to successfully integrate Kolb- and Vygotsky-inspired human cognitive learning, marking a major step toward generalist AI.", "source": "arxiv", "arxiv_id": "2411.03562v3", "pdf_url": "https://arxiv.org/pdf/2411.03562v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-11-05T23:55:23Z", "updated": "2025-09-15T15:34:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence", "authors": ["Zhuoling Li", "Xiaogang Xu", "Zhenhua Xu", "SerNam Lim", "Hengshuang Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2405.17424v2", "abstract": "Recent embodied agents are primarily built based on reinforcement learning (RL) or large language models (LLMs). Among them, RL agents are efficient for deployment but only perform very few tasks. By contrast, giant LLM agents (often more than 1000B parameters) present strong generalization while demanding enormous computing resources. In this work, we combine their advantages while avoiding the drawbacks by conducting the proposed referee RL on our developed large auto-regressive model (LARM). Specifically, LARM is built upon a lightweight LLM (fewer than 5B parameters) and directly outputs the next action to execute rather than text. We mathematically reveal that classic RL feedbacks vanish in long-horizon embodied exploration and introduce a giant LLM based referee to handle this reward vanishment during training LARM. In this way, LARM learns to complete diverse open-world tasks without human intervention. Especially, LARM successfully harvests enchanted diamond equipment in Minecraft, which demands significantly longer decision-making chains than the highest achievements of prior best methods.", "source": "arxiv", "arxiv_id": "2405.17424v2", "pdf_url": "https://arxiv.org/pdf/2405.17424v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-05-27T17:59:32Z", "updated": "2025-02-05T14:06:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agent Honeypot: Monitoring AI Hacking Agents in the Wild", "authors": ["Reworr", "Dmitrii Volkov"], "year": 2024, "url": "http://arxiv.org/abs/2410.13919v2", "abstract": "Attacks powered by Large Language Model (LLM) agents represent a growing threat to modern cybersecurity. To address this concern, we present LLM Honeypot, a system designed to monitor autonomous AI hacking agents. By augmenting a standard SSH honeypot with prompt injection and time-based analysis techniques, our framework aims to distinguish LLM agents among all attackers. Over a trial deployment of about three months in a public environment, we collected 8,130,731 hacking attempts and 8 potential AI agents. Our work demonstrates the emergence of AI-driven threats and their current level of usage, serving as an early warning of malicious LLM agents in the wild.", "source": "arxiv", "arxiv_id": "2410.13919v2", "pdf_url": "https://arxiv.org/pdf/2410.13919v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-17T09:25:28Z", "updated": "2025-02-10T22:06:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agent for Fire Dynamics Simulations", "authors": ["Leidong Xu", "Danyal Mohaddes", "Yi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2412.17146v1", "abstract": "Significant advances have been achieved in leveraging foundation models, such as large language models (LLMs), to accelerate complex scientific workflows. In this work we introduce FoamPilot, a proof-of-concept LLM agent designed to enhance the usability of FireFOAM, a specialized solver for fire dynamics and fire suppression simulations built using OpenFOAM, a popular open-source toolbox for computational fluid dynamics (CFD). FoamPilot provides three core functionalities: code insight, case configuration and simulation evaluation. Code insight is an alternative to traditional keyword searching leveraging retrieval-augmented generation (RAG) and aims to enable efficient navigation and summarization of the FireFOAM source code for developers and experienced users. For case configuration, the agent interprets user requests in natural language and aims to modify existing simulation setups accordingly to support intermediate users. FoamPilot's job execution functionality seeks to manage the submission and execution of simulations in high-performance computing (HPC) environments and provide preliminary analysis of simulation results to support less experienced users. Promising results were achieved for each functionality, particularly for simple tasks, and opportunities were identified for significant further improvement for more complex tasks. The integration of these functionalities into a single LLM agent is a step aimed at accelerating the simulation workflow for engineers and scientists employing FireFOAM for complex simulations critical for improving fire safety.", "source": "arxiv", "arxiv_id": "2412.17146v1", "pdf_url": "https://arxiv.org/pdf/2412.17146v1", "categories": ["cs.AI", "physics.flu-dyn"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-22T20:03:35Z", "updated": "2024-12-22T20:03:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents Improve Semantic Code Search", "authors": ["Sarthak Jain", "Aditya Dora", "Ka Seng Sam", "Prabhat Singh"], "year": 2024, "url": "http://arxiv.org/abs/2408.11058v1", "abstract": "Code Search is a key task that many programmers often have to perform while developing solutions to problems. Current methodologies suffer from an inability to perform accurately on prompts that contain some ambiguity or ones that require additional context relative to a code-base. We introduce the approach of using Retrieval Augmented Generation (RAG) powered agents to inject information into user prompts allowing for better inputs into embedding models. By utilizing RAG, agents enhance user queries with relevant details from GitHub repositories, making them more informative and contextually aligned. Additionally, we introduce a multi-stream ensemble approach which when paired with agentic workflow can obtain improved retrieval accuracy, which we deploy on application called repo-rift.com. Experimental results on the CodeSearchNet dataset demonstrate that RepoRift significantly outperforms existing methods, achieving an 78.2% success rate at Success@10 and a 34.6% success rate at Success@1. This research presents a substantial advancement in semantic code search, highlighting the potential of agentic LLMs and RAG to enhance code retrieval systems.", "source": "arxiv", "arxiv_id": "2408.11058v1", "pdf_url": "https://arxiv.org/pdf/2408.11058v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-08-05T00:43:56Z", "updated": "2024-08-05T00:43:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents as 6G Orchestrator: A Paradigm for Task-Oriented Physical-Layer Automation", "authors": ["Zhuoran Xiao", "Chenhui Ye", "Yunbo Hu", "Honggang Yuan", "Yihang Huang", "Yijia Feng", "Liyu Cai", "Jiang Chang"], "year": 2024, "url": "http://arxiv.org/abs/2410.03688v1", "abstract": "The rapid advancement in generative pre-training models is propelling a paradigm shift in technological progression from basic applications such as chatbots towards more sophisticated agent-based systems. It is with huge potential and necessity that the 6G system be combined with the copilot of large language model (LLM) agents and digital twins (DT) to manage the highly complicated communication system with new emerging features such as native AI service and sensing. With the 6G-oriented agent, the base station could understand the transmission requirements of various dynamic upper-layer tasks, automatically orchestrate the optimal system workflow. Through continuously get feedback from the 6G DT for reinforcement, the agents can finally raise the performance of practical system accordingly. Differing from existing LLM agents designed for general application, the 6G-oriented agent aims to make highly rigorous and precise planning with a vast amount of extra expert knowledge, which inevitably requires a specific system design from model training to implementation. This paper proposes a novel comprehensive approach for building task-oriented 6G LLM agents. We first propose a two-stage continual pre-training and fine-tuning scheme to build the field basic model and diversities of specialized expert models for meeting the requirements of various application scenarios. Further, a novel inference framework based on semantic retrieval for leveraging the existing communication-related functions is proposed. Experiment results of exemplary tasks, such as physical-layer task decomposition, show the proposed paradigm's feasibility and effectiveness.", "source": "arxiv", "arxiv_id": "2410.03688v1", "pdf_url": "https://arxiv.org/pdf/2410.03688v1", "categories": ["cs.NI", "cs.AI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-09-21T05:08:29Z", "updated": "2024-09-21T05:08:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents can Autonomously Exploit One-day Vulnerabilities", "authors": ["Richard Fang", "Rohan Bindu", "Akul Gupta", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2404.08144v2", "abstract": "LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities.\n  In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable LLM agents.", "source": "arxiv", "arxiv_id": "2404.08144v2", "pdf_url": "https://arxiv.org/pdf/2404.08144v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-04-11T22:07:19Z", "updated": "2024-04-17T04:34:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents can Autonomously Hack Websites", "authors": ["Richard Fang", "Rohan Bindu", "Akul Gupta", "Qiusi Zhan", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2402.06664v3", "abstract": "In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.\n  In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.", "source": "arxiv", "arxiv_id": "2402.06664v3", "pdf_url": "https://arxiv.org/pdf/2402.06664v3", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-02-06T14:46:08Z", "updated": "2024-02-16T04:02:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models", "authors": ["Ivar Frisch", "Mario Giulianelli"], "year": 2024, "url": "http://arxiv.org/abs/2402.02896v1", "abstract": "While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.", "source": "arxiv", "arxiv_id": "2402.02896v1", "pdf_url": "https://arxiv.org/pdf/2402.02896v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-05T11:05:20Z", "updated": "2024-02-05T11:05:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Harmony: Multi-Agent Communication for Problem Solving", "authors": ["Sumedh Rasal"], "year": 2024, "url": "http://arxiv.org/abs/2401.01312v1", "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing but exhibit limitations, particularly in autonomously addressing novel challenges such as reasoning and problem-solving. Traditional techniques like chain-of-thought prompting necessitate explicit human guidance. This paper introduces a novel multi-agent communication framework, inspired by the CAMEL model, to enhance LLMs' autonomous problem-solving capabilities. The framework employs multiple LLM agents, each with a distinct persona, engaged in role-playing communication, offering a nuanced and adaptable approach to diverse problem scenarios. Extensive experimentation demonstrates the framework's superior performance and adaptability, providing valuable insights into the collaborative potential of multiple agents in overcoming the limitations of individual models.", "source": "arxiv", "arxiv_id": "2401.01312v1", "pdf_url": "https://arxiv.org/pdf/2401.01312v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-01-02T17:54:02Z", "updated": "2024-01-02T17:54:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM Voting: Human Choices and AI Collective Decision Making", "authors": ["Joshua C. Yang", "Damian Dailisan", "Marcin Korecki", "Carina I. Hausladen", "Dirk Helbing"], "year": 2024, "url": "http://arxiv.org/abs/2402.01766v3", "abstract": "This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.", "source": "arxiv", "arxiv_id": "2402.01766v3", "pdf_url": "https://arxiv.org/pdf/2402.01766v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "econ.GN"], "primary_category": "cs.CL", "doi": "10.1609/aies.v7i1.31758", "venue": "", "published": "2024-01-31T14:52:02Z", "updated": "2024-08-14T13:41:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM experiments with simulation: Large Language Model Multi-Agent System for Simulation Model Parametrization in Digital Twins", "authors": ["Yuchen Xia", "Daniel Dittler", "Nasser Jazdi", "Haonan Chen", "Michael Weyrich"], "year": 2024, "url": "http://arxiv.org/abs/2405.18092v2", "abstract": "This paper presents a novel design of a multi-agent system framework that applies large language models (LLMs) to automate the parametrization of simulation models in digital twins. This framework features specialized LLM agents tasked with observing, reasoning, decision-making, and summarizing, enabling them to dynamically interact with digital twin simulations to explore parametrization possibilities and determine feasible parameter settings to achieve an objective. The proposed approach enhances the usability of simulation model by infusing it with knowledge heuristics from LLM and enables autonomous search for feasible parametrization to solve a user task. Furthermore, the system has the potential to increase user-friendliness and reduce the cognitive load on human users by assisting in complex decision-making processes. The effectiveness and functionality of the system are demonstrated through a case study, and the visualized demos and codes are available at a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation", "source": "arxiv", "arxiv_id": "2405.18092v2", "pdf_url": "https://arxiv.org/pdf/2405.18092v2", "categories": ["cs.AI", "cs.ET", "cs.MA", "cs.RO", "eess.SY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-28T11:59:40Z", "updated": "2024-07-22T14:03:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Design of Multi Active/Passive Core-Agent Architectures", "authors": ["Amine Ben Hassouna", "Hana Chaari", "Ines Belhaj"], "year": 2024, "url": "http://arxiv.org/abs/2409.11393v3", "abstract": "In an era where vast amounts of data are collected and processed from diverse sources, there is a growing demand for sophisticated AI systems capable of intelligently fusing and analyzing this information. To address these challenges, researchers have turned towards integrating tools into LLM-powered agents to enhance the overall information fusion process. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture, resulting in a lack of modularity and terminological inconsistencies among researchers. To address these issues, we propose a novel LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF) that establishes a clear foundation for agent development from both functional and software architectural perspectives, developed and evaluated using the Architecture Tradeoff and Risk Analysis Framework (ATRAF). Our framework clearly distinguishes between the different components of an LLM-based agent, setting LLMs and tools apart from a new element, the core-agent, which plays the role of central coordinator. This pivotal entity comprises five modules: planning, memory, profile, action, and security -- the latter often neglected in previous works. By classifying core-agents into passive and active types based on their authoritative natures, we propose various multi-core agent architectures that combine unique characteristics of distinctive agents to tackle complex tasks more efficiently. We evaluate our framework by applying it to thirteen state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying overlooked architectural aspects. Moreover, we thoroughly assess five architecture variants of our framework by designing new agent architectures that combine characteristics of state-of-the-art agents to address specific goals. ...", "source": "arxiv", "arxiv_id": "2409.11393v3", "pdf_url": "https://arxiv.org/pdf/2409.11393v3", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.MA"], "primary_category": "cs.SE", "doi": "10.1016/j.inffus.2025.103865", "venue": "Information Fusion 127 (2026) 103865", "published": "2024-09-17T17:54:17Z", "updated": "2025-11-21T13:25:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-Enhanced Data Management", "authors": ["Xuanhe Zhou", "Xinyang Zhao", "Guoliang Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.02643v1", "abstract": "Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by vector databases which provide semantic search and caching abilities. LLMDB improves the task accuracy by LLM agent which provides multiple-round inference and pipeline executions. We showcase three real-world scenarios that LLMDB can well support, including query rewrite, database diagnosis and data analytics. We also summarize the open research challenges of LLMDB.", "source": "arxiv", "arxiv_id": "2402.02643v1", "pdf_url": "https://arxiv.org/pdf/2402.02643v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2024-02-04T23:42:02Z", "updated": "2024-02-04T23:42:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-IE: A Python Package for Generative Information Extraction with Large Language Models", "authors": ["Enshuo Hsu", "Kirk Roberts"], "year": 2024, "url": "http://arxiv.org/abs/2411.11779v1", "abstract": "Objectives: Despite the recent adoption of large language models (LLMs) for biomedical information extraction, challenges in prompt engineering and algorithms persist, with no dedicated software available. To address this, we developed LLM-IE: a Python package for building complete information extraction pipelines. Our key innovation is an interactive LLM agent to support schema definition and prompt design.\n  Materials and Methods: The LLM-IE supports named entity recognition, entity attribute extraction, and relation extraction tasks. We benchmarked on the i2b2 datasets and conducted a system evaluation.\n  Results: The sentence-based prompting algorithm resulted in the best performance while requiring a longer inference time. System evaluation provided intuitive visualization.\n  Discussion: LLM-IE was designed from practical NLP experience in healthcare and has been adopted in internal projects. It should hold great value to the biomedical NLP community.\n  Conclusion: We developed a Python package, LLM-IE, that provides building blocks for robust information extraction pipeline construction.", "source": "arxiv", "arxiv_id": "2411.11779v1", "pdf_url": "https://arxiv.org/pdf/2411.11779v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "10.1093/jamiaopen/ooaf012", "venue": "", "published": "2024-11-18T17:56:13Z", "updated": "2024-11-18T17:56:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-SmartAudit: Advanced Smart Contract Vulnerability Detection", "authors": ["Zhiyuan Wei", "Jing Sun", "Zijiang Zhang", "Xianhao Zhang", "Meng Li", "Zhe Hou"], "year": 2024, "url": "http://arxiv.org/abs/2410.09381v2", "abstract": "The immutable nature of blockchain technology, while revolutionary, introduces significant security challenges, particularly in smart contracts. These security issues can lead to substantial financial losses. Current tools and approaches often focus on specific types of vulnerabilities. However, a comprehensive tool capable of detecting a wide range of vulnerabilities with high accuracy is lacking. This paper introduces LLM-SmartAudit, a novel framework leveraging the advanced capabilities of Large Language Models (LLMs) to detect and analyze vulnerabilities in smart contracts. Using a multi-agent conversational approach, LLM-SmartAudit employs a collaborative system with specialized agents to enhance the audit process. To evaluate the effectiveness of LLM-SmartAudit, we compiled two distinct datasets: a labeled dataset for benchmarking against traditional tools and a real-world dataset for assessing practical applications. Experimental results indicate that our solution outperforms all traditional smart contract auditing tools, offering higher accuracy and greater efficiency. Furthermore, our framework can detect complex logic vulnerabilities that traditional tools have previously overlooked. Our findings demonstrate that leveraging LLM agents provides a highly effective method for automated smart contract auditing.", "source": "arxiv", "arxiv_id": "2410.09381v2", "pdf_url": "https://arxiv.org/pdf/2410.09381v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-12T06:24:21Z", "updated": "2024-11-04T09:11:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-based Multi-Agent Systems: Techniques and Business Perspectives", "authors": ["Yingxuan Yang", "Qiuying Peng", "Jun Wang", "Ying Wen", "Weinan Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.14033v2", "abstract": "In the era of (multi-modal) large language models, most operational processes can be reformulated and reproduced using LLM agents. The LLM agents can perceive, control, and get feedback from the environment so as to accomplish the given tasks in an autonomous manner. Besides the environment-interaction property, the LLM agents can call various external tools to ease the task completion process. The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. As a natural trend of development, the tools for calling are becoming autonomous agents, thus the full intelligent system turns out to be a LLM-based Multi-Agent System (LaMAS). Compared to the previous single-LLM-agent system, LaMAS has the advantages of i) dynamic task decomposition and organic specialization, ii) higher flexibility for system changing, iii) proprietary data preserving for each participating entity, and iv) feasibility of monetization for each entity. This paper discusses the technical and business landscapes of LaMAS. To support the ecosystem of LaMAS, we provide a preliminary version of such LaMAS protocol considering technical requirements, data privacy, and business incentives. As such, LaMAS would be a practical solution to achieve artificial collective intelligence in the near future.", "source": "arxiv", "arxiv_id": "2411.14033v2", "pdf_url": "https://arxiv.org/pdf/2411.14033v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-21T11:36:29Z", "updated": "2024-12-28T12:48:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments", "authors": ["Junzhe Chen", "Xuming Hu", "Shuodi Liu", "Shiyu Huang", "Wei-Wei Tu", "Zhaofeng He", "Lijie Wen"], "year": 2024, "url": "http://arxiv.org/abs/2402.16499v1", "abstract": "Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings. The code and data will be available.", "source": "arxiv", "arxiv_id": "2402.16499v1", "pdf_url": "https://arxiv.org/pdf/2402.16499v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-26T11:31:48Z", "updated": "2024-02-26T11:31:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMatDesign: Autonomous Materials Discovery with Large Language Models", "authors": ["Shuyi Jia", "Chao Zhang", "Victor Fung"], "year": 2024, "url": "http://arxiv.org/abs/2406.13163v1", "abstract": "Discovering new materials can have significant scientific and technological implications but remains a challenging problem today due to the enormity of the chemical space. Recent advances in machine learning have enabled data-driven methods to rapidly screen or generate promising materials, but these methods still depend heavily on very large quantities of training data and often lack the flexibility and chemical understanding often desired in materials discovery. We introduce LLMatDesign, a novel language-based framework for interpretable materials design powered by large language models (LLMs). LLMatDesign utilizes LLM agents to translate human instructions, apply modifications to materials, and evaluate outcomes using provided tools. By incorporating self-reflection on its previous decisions, LLMatDesign adapts rapidly to new tasks and conditions in a zero-shot manner. A systematic evaluation of LLMatDesign on several materials design tasks, in silico, validates LLMatDesign's effectiveness in developing new materials with user-defined target properties in the small data regime. Our framework demonstrates the remarkable potential of autonomous LLM-guided materials discovery in the computational setting and towards self-driving laboratories in the future.", "source": "arxiv", "arxiv_id": "2406.13163v1", "pdf_url": "https://arxiv.org/pdf/2406.13163v1", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "primary_category": "cond-mat.mtrl-sci", "doi": "", "venue": "", "published": "2024-06-19T02:35:02Z", "updated": "2024-06-19T02:35:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMs May Not Be Human-Level Players, But They Can Be Testers: Measuring Game Difficulty with LLM Agents", "authors": ["Chang Xiao", "Brenda Z. Yang"], "year": 2024, "url": "http://arxiv.org/abs/2410.02829v1", "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated their potential as autonomous agents across various tasks. One emerging application is the use of LLMs in playing games. In this work, we explore a practical problem for the gaming industry: Can LLMs be used to measure game difficulty? We propose a general game-testing framework using LLM agents and test it on two widely played strategy games: Wordle and Slay the Spire. Our results reveal an interesting finding: although LLMs may not perform as well as the average human player, their performance, when guided by simple, generic prompting techniques, shows a statistically significant and strong correlation with difficulty indicated by human players. This suggests that LLMs could serve as effective agents for measuring game difficulty during the development process. Based on our experiments, we also outline general principles and guidelines for incorporating LLMs into the game testing process.", "source": "arxiv", "arxiv_id": "2410.02829v1", "pdf_url": "https://arxiv.org/pdf/2410.02829v1", "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-01T18:40:43Z", "updated": "2024-10-01T18:40:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction", "authors": ["Hejie Cui", "Zhuocheng Shen", "Jieyu Zhang", "Hui Shao", "Lianhui Qin", "Joyce C. Ho", "Carl Yang"], "year": 2024, "url": "http://arxiv.org/abs/2403.15464v1", "abstract": "Electronic health records (EHRs) contain valuable patient data for health-related prediction tasks, such as disease prediction. Traditional approaches rely on supervised learning methods that require large labeled datasets, which can be expensive and challenging to obtain. In this study, we investigate the feasibility of applying Large Language Models (LLMs) to convert structured patient visit data (e.g., diagnoses, labs, prescriptions) into natural language narratives. We evaluate the zero-shot and few-shot performance of LLMs using various EHR-prediction-oriented prompting strategies. Furthermore, we propose a novel approach that utilizes LLM agents with different roles: a predictor agent that makes predictions and generates reasoning processes and a critic agent that analyzes incorrect predictions and provides guidance for improving the reasoning of the predictor agent. Our results demonstrate that with the proposed approach, LLMs can achieve decent few-shot performance compared to traditional supervised learning methods in EHR-based disease predictions, suggesting its potential for health-oriented applications.", "source": "arxiv", "arxiv_id": "2403.15464v1", "pdf_url": "https://arxiv.org/pdf/2403.15464v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-19T18:10:13Z", "updated": "2024-03-19T18:10:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Language Agents as Optimizable Graphs", "authors": ["Mingchen Zhuge", "Wenyi Wang", "Louis Kirsch", "Francesco Faccio", "Dmitrii Khizbullin", "JÃ¼rgen Schmidhuber"], "year": 2024, "url": "http://arxiv.org/abs/2402.16823v3", "abstract": "Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.", "source": "arxiv", "arxiv_id": "2402.16823v3", "pdf_url": "https://arxiv.org/pdf/2402.16823v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-26T18:48:27Z", "updated": "2024-08-22T13:06:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Language Models are Spacecraft Operators", "authors": ["Victor Rodriguez-Fernandez", "Alejandro Carrasco", "Jason Cheng", "Eli Scharf", "Peng Mun Siew", "Richard Linares"], "year": 2024, "url": "http://arxiv.org/abs/2404.00413v1", "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Guidance, Navigation, and Control in space, enabling LLMs to have a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. Code is available at https://github.com/ARCLab-MIT/kspdg.", "source": "arxiv", "arxiv_id": "2404.00413v1", "pdf_url": "https://arxiv.org/pdf/2404.00413v1", "categories": ["physics.space-ph", "cs.AI", "cs.LG"], "primary_category": "physics.space-ph", "doi": "", "venue": "", "published": "2024-03-30T16:43:59Z", "updated": "2024-03-30T16:43:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Language agents achieve superhuman synthesis of scientific knowledge", "authors": ["Michael D. Skarlinski", "Sam Cox", "Jon M. Laurent", "James D. Braza", "Michaela Hinks", "Michael J. Hammerling", "Manvitha Ponnapati", "Samuel G. Rodriques", "Andrew D. White"], "year": 2024, "url": "http://arxiv.org/abs/2409.13740v2", "abstract": "Language models are known to hallucinate incorrect information, and it is unclear if they are sufficiently accurate and reliable for use in scientific research. We developed a rigorous human-AI comparison methodology to evaluate language model agents on real-world literature search tasks covering information retrieval, summarization, and contradiction detection tasks. We show that PaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds subject matter expert performance on three realistic literature research tasks without any restrictions on humans (i.e., full access to internet, search tools, and time). PaperQA2 writes cited, Wikipedia-style summaries of scientific topics that are significantly more accurate than existing, human-written Wikipedia articles. We also introduce a hard benchmark for scientific literature research called LitQA2 that guided design of PaperQA2, leading to it exceeding human performance. Finally, we apply PaperQA2 to identify contradictions within the scientific literature, an important scientific task that is challenging for humans. PaperQA2 identifies 2.34 +/- 1.99 contradictions per paper in a random subset of biology papers, of which 70% are validated by human experts. These results demonstrate that language model agents are now capable of exceeding domain experts across meaningful tasks on scientific literature.", "source": "arxiv", "arxiv_id": "2409.13740v2", "pdf_url": "https://arxiv.org/pdf/2409.13740v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "physics.soc-ph"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-10T16:37:58Z", "updated": "2024-09-26T15:27:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agent as a Mechanical Designer", "authors": ["Yayati Jadhav", "Amir Barati Farimani"], "year": 2024, "url": "http://arxiv.org/abs/2404.17525v3", "abstract": "Conventional mechanical design follows an iterative process in which initial concepts are refined through cycles of expert assessment and resource-intensive Finite Element Method (FEM) analysis to meet performance goals. While machine learning models have been developed to assist in parts of this process, they typically require large datasets, extensive training, and are often tailored to specific tasks, limiting their generalizability. To address these limitations, we propose a framework that leverages a pretrained Large Language Model (LLM) in conjunction with an FEM module to autonomously generate, evaluate, and refine structural designs based on performance specifications and numerical feedback. The LLM operates without domain-specific fine-tuning, using general reasoning to propose design candidates, interpret FEM-derived performance metrics, and apply structurally sound modifications. Using 2D truss structures as a testbed, we show that the LLM can effectively navigate highly discrete and multi-faceted design spaces, balance competing objectives, and identify convergence when further optimization yields diminishing returns. Compared to Non-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves faster convergence and fewer FEM evaluations. Experiments with varying temperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini) indicate that smaller models yield higher constraint satisfaction with fewer steps, while lower temperatures enhance design consistency. These results establish LLMs as a promising new class of reasoning-based, natural language-driven optimizers for autonomous design and iterative structural refinement.", "source": "arxiv", "arxiv_id": "2404.17525v3", "pdf_url": "https://arxiv.org/pdf/2404.17525v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-04-26T16:41:24Z", "updated": "2025-04-30T18:23:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agent for Fake News Detection", "authors": ["Xinyi Li", "Yongfeng Zhang", "Edward C. Malthouse"], "year": 2024, "url": "http://arxiv.org/abs/2405.01593v1", "abstract": "In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim's veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent's application to news verification across various domains.", "source": "arxiv", "arxiv_id": "2405.01593v1", "pdf_url": "https://arxiv.org/pdf/2405.01593v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-30T06:55:27Z", "updated": "2024-04-30T06:55:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agent for Hyper-Parameter Optimization", "authors": ["Siyi Liu", "Chen Gao", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.01881v3", "abstract": "Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted on 12 representative machine-learning tasks indicate that AgentHPO not only matches but also often surpasses the best human trials in terms of performance while simultaneously providing explainable results. Further analysis sheds light on the strategies employed by the LLM in optimizing these tasks, highlighting its effectiveness and adaptability in various scenarios.", "source": "arxiv", "arxiv_id": "2402.01881v3", "pdf_url": "https://arxiv.org/pdf/2402.01881v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-02T20:12:05Z", "updated": "2025-02-26T13:57:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agent in Financial Trading: A Survey", "authors": ["Han Ding", "Yinheng Li", "Junhao Wang", "Hang Chen"], "year": 2024, "url": "http://arxiv.org/abs/2408.06361v1", "abstract": "Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude. With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders. In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading. We summarize the common architecture used in the agent, the data inputs, and the performance of LLM trading agents in backtesting as well as the challenges presented in these research. This survey aims to provide insights into the current state of LLM-based financial trading agents and outline future research directions in this field.", "source": "arxiv", "arxiv_id": "2408.06361v1", "pdf_url": "https://arxiv.org/pdf/2408.06361v1", "categories": ["q-fin.TR", "cs.CL"], "primary_category": "q-fin.TR", "doi": "", "venue": "", "published": "2024-07-26T08:53:05Z", "updated": "2024-07-26T08:53:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Agents for Improving Engagement with Behavior Change Interventions: Application to Digital Mindfulness", "authors": ["Harsh Kumar", "Suhyeon Yoo", "Angela Zavaleta Bernuy", "Jiakai Shi", "Huayin Luo", "Joseph Williams", "Anastasia Kuzminykh", "Ashton Anderson", "Rachel Kornfield"], "year": 2024, "url": "http://arxiv.org/abs/2407.13067v1", "abstract": "Although engagement in self-directed wellness exercises typically declines over time, integrating social support such as coaching can sustain it. However, traditional forms of support are often inaccessible due to the high costs and complex coordination. Large Language Models (LLMs) show promise in providing human-like dialogues that could emulate social support. Yet, in-depth, in situ investigations of LLMs to support behavior change remain underexplored. We conducted two randomized experiments to assess the impact of LLM agents on user engagement with mindfulness exercises. First, a single-session study, involved 502 crowdworkers; second, a three-week study, included 54 participants. We explored two types of LLM agents: one providing information and another facilitating self-reflection. Both agents enhanced users' intentions to practice mindfulness. However, only the information-providing LLM, featuring a friendly persona, significantly improved engagement with the exercises. Our findings suggest that specific LLM agents may bridge the social support gap in digital health interventions.", "source": "arxiv", "arxiv_id": "2407.13067v1", "pdf_url": "https://arxiv.org/pdf/2407.13067v1", "categories": ["cs.HC", "cs.AI", "cs.CY"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-07-03T15:43:16Z", "updated": "2024-07-03T15:43:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification", "authors": ["Guang Lin", "Toshihisa Tanaka", "Qibin Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2405.20770v4", "abstract": "Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.", "source": "arxiv", "arxiv_id": "2405.20770v4", "pdf_url": "https://arxiv.org/pdf/2405.20770v4", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-24T07:23:56Z", "updated": "2025-04-23T05:12:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Models Are Neurosymbolic Reasoners", "authors": ["Meng Fang", "Shilong Deng", "Yudi Zhang", "Zijing Shi", "Ling Chen", "Mykola Pechenizkiy", "Jun Wang"], "year": 2024, "url": "http://arxiv.org/abs/2401.09334v1", "abstract": "A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning. This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners. We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds. To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives. We begin by initializing the LLM agent and informing it of its role. The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module. With these inputs, the LLM agent chooses an action and interacts with the game environments. Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.", "source": "arxiv", "arxiv_id": "2401.09334v1", "pdf_url": "https://arxiv.org/pdf/2401.09334v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-17T16:57:19Z", "updated": "2024-01-17T16:57:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Models as Instruments of Power: New Regimes of Autonomous Manipulation and Control", "authors": ["Yaqub Chaudhary", "Jonnie Penn"], "year": 2024, "url": "http://arxiv.org/abs/2405.03813v1", "abstract": "Large language models (LLMs) can reproduce a wide variety of rhetorical styles and generate text that expresses a broad spectrum of sentiments. This capacity, now available at low cost, makes them powerful tools for manipulation and control. In this paper, we consider a set of underestimated societal harms made possible by the rapid and largely unregulated adoption of LLMs. Rather than consider LLMs as isolated digital artefacts used to displace this or that area of work, we focus on the large-scale computational infrastructure upon which they are instrumentalised across domains. We begin with discussion on how LLMs may be used to both pollute and uniformize information environments and how these modalities may be leveraged as mechanisms of control. We then draw attention to several areas of emerging research, each of which compounds the capabilities of LLMs as instruments of power. These include (i) persuasion through the real-time design of choice architectures in conversational interfaces (e.g., via \"AI personas\"), (ii) the use of LLM-agents as computational models of human agents (e.g., \"silicon subjects\"), (iii) the use of LLM-agents as computational models of human agent populations (e.g., \"silicon societies\") and finally, (iv) the combination of LLMs with reinforcement learning to produce controllable and steerable strategic dialogue models. We draw these strands together to discuss how these areas may be combined to build LLM-based systems that serve as powerful instruments of individual, social and political control via the simulation and disingenuous \"prediction\" of human behaviour, intent, and action.", "source": "arxiv", "arxiv_id": "2405.03813v1", "pdf_url": "https://arxiv.org/pdf/2405.03813v1", "categories": ["cs.SI", "cs.CY"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-05-06T19:52:57Z", "updated": "2024-05-06T19:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation", "authors": ["Jiawei Wang", "Renhe Jiang", "Chuang Yang", "Zengqing Wu", "Makoto Onizuka", "Ryosuke Shibasaki", "Noboru Koshizuka", "Chuan Xiao"], "year": 2024, "url": "http://arxiv.org/abs/2402.14744v3", "abstract": "This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.", "source": "arxiv", "arxiv_id": "2402.14744v3", "pdf_url": "https://arxiv.org/pdf/2402.14744v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-22T18:03:14Z", "updated": "2024-10-27T20:02:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Models based Multi-Agent Framework for Objective Oriented Control Design in Power Electronics", "authors": ["Chenggang Cui", "Jiaming Liu", "Junkang Feng", "Peifeng Hui", "Amer M. Y. M. Ghias", "Chuanlin Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2406.12628v1", "abstract": "Power electronics, a critical component in modern power systems, face several challenges in control design, including model uncertainties, and lengthy and costly design cycles. This paper is aiming to propose a Large Language Models (LLMs) based multi-agent framework for objective-oriented control design in power electronics. The framework leverages the reasoning capabilities of LLMs and a multi-agent workflow to develop an efficient and autonomous controller design process. The LLM agent is able to understand and respond to high-level instructions in natural language, adapting its behavior based on the task's specific requirements and constraints from a practical implementation point of view. This novel and efficient approach promises a more flexible and adaptable controller design process in power electronics that will largely facilitate the practitioners.", "source": "arxiv", "arxiv_id": "2406.12628v1", "pdf_url": "https://arxiv.org/pdf/2406.12628v1", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-06-18T13:54:12Z", "updated": "2024-06-18T13:54:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large language model empowered participatory urban planning", "authors": ["Zhilun Zhou", "Yuming Lin", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.01698v1", "abstract": "Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.", "source": "arxiv", "arxiv_id": "2402.01698v1", "pdf_url": "https://arxiv.org/pdf/2402.01698v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-24T10:50:01Z", "updated": "2024-01-24T10:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Learning to Ask: When LLM Agents Meet Unclear Instruction", "authors": ["Wenxuan Wang", "Juluan Shi", "Zixuan Ling", "Yuk-Kit Chan", "Chaozheng Wang", "Cheryl Lee", "Youliang Yuan", "Jen-tse Huang", "Wenxiang Jiao", "Michael R. Lyu"], "year": 2024, "url": "http://arxiv.org/abs/2409.00557v3", "abstract": "Equipped with the capability to call functions, modern large language models (LLMs) can leverage external tools for addressing a range of tasks unattainable through language skills alone. However, the effective execution of these tools relies heavily not just on the advanced capabilities of LLMs but also on precise user instructions, which often cannot be ensured in the real world. To evaluate the performance of LLMs tool-use under imperfect instructions, we meticulously examine the real-world instructions queried from users, analyze the error patterns, and build a challenging tool-use benchmark called Noisy ToolBench (NoisyToolBench). We find that due to the next-token prediction training objective, LLMs tend to arbitrarily generate the missed argument, which may lead to hallucinations and risks. To address this issue, we propose a novel framework, Ask-when-Needed (AwN), which prompts LLMs to ask questions to users whenever they encounter obstacles due to unclear instructions. Moreover, to reduce the manual labor involved in user-LLM interaction and assess LLMs performance in tool utilization from both accuracy and efficiency perspectives, we design an automated evaluation tool named ToolEvaluator. Our experiments demonstrate that the AwN significantly outperforms existing frameworks for tool learning in the NoisyToolBench. We will release all related code and datasets to support future research.", "source": "arxiv", "arxiv_id": "2409.00557v3", "pdf_url": "https://arxiv.org/pdf/2409.00557v3", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-31T23:06:12Z", "updated": "2025-02-16T14:50:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LegalAgentBench: Evaluating LLM Agents in Legal Domain", "authors": ["Haitao Li", "Junjie Chen", "Jingli Yang", "Qingyao Ai", "Wei Jia", "Youfeng Liu", "Kai Lin", "Yueyue Wu", "Guozhi Yuan", "Yiran Hu", "Wuyue Wang", "Yiqun Liu", "Minlie Huang"], "year": 2024, "url": "http://arxiv.org/abs/2412.17259v1", "abstract": "With the increasing intelligence and autonomy of LLM agents, their potential applications in the legal domain are becoming increasingly apparent. However, existing general-domain benchmarks cannot fully capture the complexity and subtle nuances of real-world judicial cognition and decision-making. Therefore, we propose LegalAgentBench, a comprehensive benchmark specifically designed to evaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17 corpora from real-world legal scenarios and provides 37 tools for interacting with external knowledge. We designed a scalable task construction framework and carefully annotated 300 tasks. These tasks span various types, including multi-hop reasoning and writing, and range across different difficulty levels, effectively reflecting the complexity of real-world legal scenarios. Moreover, beyond evaluating final success, LegalAgentBench incorporates keyword analysis during intermediate processes to calculate progress rates, enabling more fine-grained evaluation. We evaluated eight popular LLMs, highlighting the strengths, limitations, and potential areas for improvement of existing models and methods. LegalAgentBench sets a new benchmark for the practical application of LLMs in the legal domain, with its code and data available at \\url{https://github.com/CSHaitao/LegalAgentBench}.", "source": "arxiv", "arxiv_id": "2412.17259v1", "pdf_url": "https://arxiv.org/pdf/2412.17259v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-23T04:02:46Z", "updated": "2024-12-23T04:02:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient Machine Learning", "authors": ["Daniel Geissler", "Paul Lukowicz"], "year": 2024, "url": "http://arxiv.org/abs/2407.10580v1", "abstract": "Hybrid intelligence aims to enhance decision-making, problem-solving, and overall system performance by combining the strengths of both, human cognitive abilities and artificial intelligence. With the rise of Large Language Models (LLM), progressively participating as smart agents to accelerate machine learning development, Hybrid Intelligence is becoming an increasingly important topic for effective interaction between humans and machines. This paper presents an approach to leverage Hybrid Intelligence towards sustainable and energy-aware machine learning. When developing machine learning models, final model performance commonly rules the optimization process while the efficiency of the process itself is often neglected. Moreover, in recent times, energy efficiency has become equally crucial due to the significant environmental impact of complex and large-scale computational processes. The contribution of this work covers the interactive inclusion of secondary knowledge sources through Human-in-the-loop (HITL) and LLM agents to stress out and further resolve inefficiencies in the machine learning development process.", "source": "arxiv", "arxiv_id": "2407.10580v1", "pdf_url": "https://arxiv.org/pdf/2407.10580v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-15T09:58:27Z", "updated": "2024-07-15T09:58:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution", "authors": ["Yuheng Zhao", "Junjie Wang", "Linbin Xiang", "Xiaowen Zhang", "Zifei Guo", "Cagatay Turkay", "Yu Zhang", "Siming Chen"], "year": 2024, "url": "http://arxiv.org/abs/2411.05651v2", "abstract": "Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights. This process demands skills in programming, data processing, and visualization tools, highlighting the need for a more intelligent, streamlined VA approach. Large language models (LLMs) have recently been developed as agents to handle various tasks with dynamic planning and tool-using capabilities, offering the potential to enhance the efficiency and versatility of VA. We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration. Our method is designed to help users progressively translate high-level analytical goals into low-level tasks, producing visualizations and deriving insights. Specifically, we introduce an LLM agent-based task planning and execution strategy, employing a recursive process involving a planner, executor, and controller. The planner is responsible for recommending and decomposing tasks, the executor handles task execution, including data analysis, visualization generation and multi-view composition, and the controller coordinates the interaction between the planner and executor. Building on the framework, we develop a system with a hybrid user interface that includes a task flow diagram for monitoring and managing the task planning process, a visualization panel for interactive data exploration, and a chat view for guiding the model through natural language instructions. We examine the effectiveness of our method through a usage scenario and an expert study.", "source": "arxiv", "arxiv_id": "2411.05651v2", "pdf_url": "https://arxiv.org/pdf/2411.05651v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1109/TVCG.2024.3496112", "venue": "IEEE Transactions on Visualization and Computer Graphics 2024", "published": "2024-11-08T15:46:10Z", "updated": "2025-06-21T06:39:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Long-form factuality in large language models", "authors": ["Jerry Wei", "Chengrun Yang", "Xinying Song", "Yifeng Lu", "Nathan Hu", "Jie Huang", "Dustin Tran", "Daiyi Peng", "Ruibo Liu", "Da Huang", "Cosmo Du", "Quoc V. Le"], "year": 2024, "url": "http://arxiv.org/abs/2403.18802v4", "abstract": "Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.", "source": "arxiv", "arxiv_id": "2403.18802v4", "pdf_url": "https://arxiv.org/pdf/2403.18802v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-27T17:48:55Z", "updated": "2024-11-07T03:14:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LookALike: Human Mimicry based collaborative decision making", "authors": ["Rabimba Karanjai", "Weidong Shi"], "year": 2024, "url": "http://arxiv.org/abs/2403.10824v1", "abstract": "Artificial General Intelligence falls short when communicating role specific nuances to other systems. This is more pronounced when building autonomous LLM agents capable and designed to communicate with each other for real world problem solving. Humans can communicate context and domain specific nuances along with knowledge, and that has led to refinement of skills. In this work we propose and evaluate a novel method that leads to knowledge distillation among LLM agents leading to realtime human role play preserving unique contexts without relying on any stored data or pretraining. We also evaluate how our system performs better in simulated real world tasks compared to state of the art.", "source": "arxiv", "arxiv_id": "2403.10824v1", "pdf_url": "https://arxiv.org/pdf/2403.10824v1", "categories": ["cs.LG", "cs.AI", "cs.HC"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-03-16T06:25:53Z", "updated": "2024-03-16T06:25:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LossAgent: Towards Any Optimization Objectives for Image Processing with LLM Agents", "authors": ["Bingchen Li", "Xin Li", "Yiting Lu", "Zhibo Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.04090v2", "abstract": "We present the first loss agent, dubbed LossAgent, for low-level image processing tasks, e.g., image super-resolution and restoration, intending to achieve any customized optimization objectives of low-level image processing in different practical applications. Notably, not all optimization objectives, such as complex hand-crafted perceptual metrics, text description, and intricate human feedback, can be instantiated with existing low-level losses, e.g., MSE loss, which presents a crucial challenge in optimizing image processing networks in an end-to-end manner. To eliminate this, our LossAgent introduces the powerful large language model (LLM) as the loss agent, where the rich textual understanding of prior knowledge empowers the loss agent with the potential to understand complex optimization objectives, trajectory, and state feedback from external environments in the optimization process of the low-level image processing networks. In particular, we establish the loss repository by incorporating existing loss functions that support the end-to-end optimization for low-level image processing. Then, we design the optimization-oriented prompt engineering for the loss agent to actively and intelligently decide the compositional weights for each loss in the repository at each optimization interaction, thereby achieving the required optimization trajectory for any customized optimization objectives. Extensive experiments on three typical low-level image processing tasks and multiple optimization objectives have shown the effectiveness and applicability of our proposed LossAgent.", "source": "arxiv", "arxiv_id": "2412.04090v2", "pdf_url": "https://arxiv.org/pdf/2412.04090v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-12-05T11:52:20Z", "updated": "2025-03-10T12:01:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MAGE: A Multi-Agent Engine for Automated RTL Code Generation", "authors": ["Yujie Zhao", "Hejia Zhang", "Hanxian Huang", "Zhongming Yu", "Jishen Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2412.07822v1", "abstract": "The automatic generation of RTL code (e.g., Verilog) through natural language instructions has emerged as a promising direction with the advancement of large language models (LLMs). However, producing RTL code that is both syntactically and functionally correct remains a significant challenge. Existing single-LLM-agent approaches face substantial limitations because they must navigate between various programming languages and handle intricate generation, verification, and modification tasks. To address these challenges, this paper introduces MAGE, the first open-source multi-agent AI system designed for robust and accurate Verilog RTL code generation. We propose a novel high-temperature RTL candidate sampling and debugging system that effectively explores the space of code candidates and significantly improves the quality of the candidates. Furthermore, we design a novel Verilog-state checkpoint checking mechanism that enables early detection of functional errors and delivers precise feedback for targeted fixes, significantly enhancing the functional correctness of the generated RTL code. MAGE achieves a 95.7% rate of syntactic and functional correctness code generation on VerilogEval-Human 2 benchmark, surpassing the state-of-the-art Claude-3.5-sonnet by 23.3 %, demonstrating a robust and reliable approach for AI-driven RTL design workflows.", "source": "arxiv", "arxiv_id": "2412.07822v1", "pdf_url": "https://arxiv.org/pdf/2412.07822v1", "categories": ["cs.AR", "cs.LG"], "primary_category": "cs.AR", "doi": "", "venue": "", "published": "2024-12-10T21:53:55Z", "updated": "2024-12-10T21:53:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents", "authors": ["Yun Xing", "Nhat Chung", "Jie Zhang", "Yue Cao", "Ivor Tsang", "Yang Liu", "Lei Ma", "Qing Guo"], "year": 2024, "url": "http://arxiv.org/abs/2412.08014v2", "abstract": "Physical adversarial attacks in driving scenarios can expose critical vulnerabilities in visual perception models. However, developing such attacks remains challenging due to diverse real-world environments and the requirement for maintaining visual naturality. Building upon this challenge, we reformulate physical adversarial attacks as a one-shot patch generation problem. Our approach generates adversarial patches through a deep generative model that considers the specific scene context, enabling direct physical deployment in matching environments. The primary challenge lies in simultaneously achieving two objectives: generating adversarial patches that effectively mislead object detection systems while determining contextually appropriate deployment within the scene. We propose MAGIC (Mastering Physical Adversarial Generation In Context), a novel framework powered by multi-modal LLM agents to address these challenges. MAGIC automatically understands scene context and generates adversarial patch through the synergistic interaction of language and vision capabilities. In particular, MAGIC orchestrates three specialized LLM agents: The adv-patch generation agent (GAgent) masters the creation of deceptive patches through strategic prompt engineering for text-to-image models. The adv-patch deployment agent (DAgent) ensures contextual coherence by determining optimal deployment strategies based on scene understanding. The self-examination agent (EAgent) completes this trilogy by providing critical oversight and iterative refinement of both processes. We validate our method on both digital and physical levels, i.e., nuImage and manually captured real-world scenes, where both statistical and visual results prove that our MAGIC is powerful and effective for attacking widely applied object detection systems, i.e., YOLO and DETR series.", "source": "arxiv", "arxiv_id": "2412.08014v2", "pdf_url": "https://arxiv.org/pdf/2412.08014v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-12-11T01:41:19Z", "updated": "2025-03-11T07:15:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation", "authors": ["Harsh Singh", "Rocktim Jyoti Das", "Mingfei Han", "Preslav Nakov", "Ivan Laptev"], "year": 2024, "url": "http://arxiv.org/abs/2411.17636v2", "abstract": "Large Language Models (LLMs) have demonstrated remarkable planning abilities across various domains, including robotics manipulation and navigation. While recent efforts in robotics have leveraged LLMs both for high-level and low-level planning, these approaches often face significant challenges, such as hallucinations in long-horizon tasks and limited adaptability due to the generation of plans in a single pass without real-time feedback. To address these limitations, we propose a novel multi-agent LLM framework, Multi-Agent Large Language Model for Manipulation (MALMM) that distributes high-level planning and low-level control code generation across specialized LLM agents, supervised by an additional agent that dynamically manages transitions. By incorporating observations from the environment after each step, our framework effectively handles intermediate failures and enables adaptive re-planning. Unlike existing methods, our approach does not rely on pre-trained skill policies or in-context learning examples and generalizes to a variety of new tasks. We evaluate our approach on nine RLBench tasks, including long-horizon tasks, and demonstrate its ability to solve robotics manipulation in a zero-shot setting, thereby overcoming key limitations of existing LLM-based manipulation methods.", "source": "arxiv", "arxiv_id": "2411.17636v2", "pdf_url": "https://arxiv.org/pdf/2411.17636v2", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-11-26T17:53:44Z", "updated": "2025-08-25T06:11:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents", "authors": ["Congchi Yin", "Feng Li", "Shu Zhang", "Zike Wang", "Jun Shao", "Piji Li", "Jianhua Chen", "Xun Jiang"], "year": 2024, "url": "http://arxiv.org/abs/2408.12142v2", "abstract": "The clinical diagnosis of most mental disorders primarily relies on the conversations between psychiatrist and patient. The creation of such diagnostic conversation datasets is promising to boost the AI mental healthcare community. However, directly collecting the conversations in real diagnosis scenarios is near impossible due to stringent privacy and ethical considerations. To address this issue, we seek to synthesize diagnostic conversation by exploiting anonymized patient cases that are easier to access. Specifically, we design a neuro-symbolic multi-agent framework for synthesizing the diagnostic conversation of mental disorders with large language models. It takes patient case as input and is capable of generating multiple diverse conversations with one single patient case. The framework basically involves the interaction between a doctor agent and a patient agent, and generates conversations under symbolic control via a dynamic diagnosis tree. By applying the proposed framework, we develop the largest Chinese mental disorders diagnosis dataset MDD-5k. This dataset is built upon 1000 real, anonymized patient cases by cooperating with Shanghai Mental Health Center and comprises 5000 high-quality long conversations with diagnosis results and treatment opinions as labels. To the best of our knowledge, it's also the first labeled dataset for Chinese mental disorders diagnosis. Human evaluation demonstrates the proposed MDD-5k dataset successfully simulates human-like diagnostic process of mental disorders.", "source": "arxiv", "arxiv_id": "2408.12142v2", "pdf_url": "https://arxiv.org/pdf/2408.12142v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-22T05:59:47Z", "updated": "2024-12-26T06:39:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MEGAnno+: A Human-LLM Collaborative Annotation System", "authors": ["Hannah Kim", "Kushan Mitra", "Rafael Li Chen", "Sajjadur Rahman", "Dan Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.18050v1", "abstract": "Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.", "source": "arxiv", "arxiv_id": "2402.18050v1", "pdf_url": "https://arxiv.org/pdf/2402.18050v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-28T04:58:07Z", "updated": "2024-02-28T04:58:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MIRAI: Evaluating LLM Agents for Event Forecasting", "authors": ["Chenchen Ye", "Ziniu Hu", "Yihe Deng", "Zijie Huang", "Mingyu Derek Ma", "Yanqiao Zhu", "Wei Wang"], "year": 2024, "url": "http://arxiv.org/abs/2407.01231v1", "abstract": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems. Given this capability, increasing interests have been put into employing LLM agents for predicting international events, which can influence decision-making and shape policy development on an international scale. Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability. To address this gap, we introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events. Our benchmark features an agentic environment with tools for accessing an extensive database of historical, structured events and textual news articles. We refine the GDELT event database with careful cleaning and parsing to curate a series of relational prediction tasks with varying forecasting horizons, assessing LLM agents' abilities from short-term to long-term forecasting. We further implement APIs to enable LLM agents to utilize different tools via a code-based interface. In summary, MIRAI comprehensively evaluates the agents' capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events. Through comprehensive benchmarking, we aim to establish a reliable framework for assessing the capabilities of LLM agents in forecasting international events, thereby contributing to the development of more accurate and trustworthy models for international relation analysis.", "source": "arxiv", "arxiv_id": "2407.01231v1", "pdf_url": "https://arxiv.org/pdf/2407.01231v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-01T12:22:46Z", "updated": "2024-07-01T12:22:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MISR: Measuring Instrumental Self-Reasoning in Frontier Models", "authors": ["Kai Fronsdal", "David Lindner"], "year": 2024, "url": "http://arxiv.org/abs/2412.03904v1", "abstract": "We propose a suite of tasks to evaluate the instrumental self-reasoning ability of large language model (LLM) agents. Instrumental self-reasoning ability could improve adaptability and enable self-modification, but it could also pose significant risks, such as enabling deceptive alignment. Prior work has only evaluated self-reasoning in non-agentic settings or in limited domains. In this paper, we propose evaluations for instrumental self-reasoning ability in agentic tasks in a wide range of scenarios, including self-modification, knowledge seeking, and opaque self-reasoning. We evaluate agents built using state-of-the-art LLMs, including commercial and open source systems. We find that instrumental self-reasoning ability emerges only in the most capable frontier models and that it is highly context-dependent. No model passes the the most difficult versions of our evaluations, hence our evaluation can be used to measure increases in instrumental self-reasoning ability in future models. We open-source our evaluations at https://github.com/kaifronsdal/Self-Reasoning-Evals.", "source": "arxiv", "arxiv_id": "2412.03904v1", "pdf_url": "https://arxiv.org/pdf/2412.03904v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-05T06:20:47Z", "updated": "2024-12-05T06:20:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents", "authors": ["Ruochen Li", "Teerth Patel", "Qingyun Wang", "Xinya Du"], "year": 2024, "url": "http://arxiv.org/abs/2408.14033v3", "abstract": "Autonomous machine learning research has gained significant attention recently. We present MLR-COPILOT, an autonomous Machine Learning Research framework powered by large language model agents. The system is designed to enhance ML research productivity through automatic generation and implementation of research ideas within constraints. Our work was released in August 2024 (concurrent to AI-Scientist) and has gained notable recognition from leading projects. We further enhance our ideation with training afterwards. The framework consists of three stages: idea generation, experiment implementation, and code execution. First, existing research papers are used to generate feasible ideas and experiment plans with IdeaAgent, powered by an RL-tuned LLM. Next, ExperimentAgent leverages retrieved prototype code to convert plans into executable code with optionally retrieved candidate models and data from HuggingFace. In the final stage, ExperimentAgent runs experiments, and allows subsequent iterations of debugging and human feedback for a better chance of success with executable outcomes. We evaluate our framework on five machine learning research tasks. Experiment results demonstrate the potential of our framework to facilitate ML research progress and innovation.", "source": "arxiv", "arxiv_id": "2408.14033v3", "pdf_url": "https://arxiv.org/pdf/2408.14033v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-26T05:55:48Z", "updated": "2025-11-14T19:05:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MMAC-Copilot: Multi-modal Agent Collaboration Operating Copilot", "authors": ["Zirui Song", "Yaohang Li", "Meng Fang", "Yanda Li", "Zhenhao Chen", "Zecheng Shi", "Yuan Huang", "Xiuying Chen", "Ling Chen"], "year": 2024, "url": "http://arxiv.org/abs/2404.18074v3", "abstract": "Large language model agents that interact with PC applications often face limitations due to their singular mode of interaction with real-world environments, leading to restricted versatility and frequent hallucinations. To address this, we propose the Multi-Modal Agent Collaboration framework (MMAC-Copilot), a framework utilizes the collective expertise of diverse agents to enhance interaction ability with application. The framework introduces a team collaboration chain, enabling each participating agent to contribute insights based on their specific domain knowledge, effectively reducing the hallucination associated with knowledge domain gaps. We evaluate MMAC-Copilot using the GAIA benchmark and our newly introduced Visual Interaction Benchmark (VIBench). MMAC-Copilot achieved exceptional performance on GAIA, with an average improvement of 6.8\\% over existing leading systems. VIBench focuses on non-API-interactable applications across various domains, including 3D gaming, recreation, and office scenarios. It also demonstrated remarkable capability on VIBench. We hope this work can inspire in this field and provide a more comprehensive assessment of Autonomous agents. The anonymous Github is available at \\href{https://anonymous.4open.science/r/ComputerAgentWithVision-3C12}{Anonymous Github}", "source": "arxiv", "arxiv_id": "2404.18074v3", "pdf_url": "https://arxiv.org/pdf/2404.18074v3", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-28T05:33:15Z", "updated": "2025-03-23T13:04:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains", "authors": ["Guoli Yin", "Haoping Bai", "Shuang Ma", "Feng Nan", "Yanchao Sun", "Zhaoyang Xu", "Shen Ma", "Jiarui Lu", "Xiang Kong", "Aonan Zhang", "Dian Ang Yap", "Yizhe zhang", "Karsten Ahnert", "Vik Kamath", "Mathias Berglund", "Dominic Walsh", "Tobias Gindele", "Juergen Wiest", "Zhengfeng Lai", "Xiaoming Wang", "Jiulong Shan", "Meng Cao", "Ruoming Pang", "Zirui Wang"], "year": 2024, "url": "http://arxiv.org/abs/2407.18961v3", "abstract": "Recent advances in large language models (LLMs) have increased the demand for comprehensive benchmarks to evaluate their capabilities as human-like agents. Existing benchmarks, while useful, often focus on specific application scenarios, emphasizing task completion but failing to dissect the underlying skills that drive these outcomes. This lack of granularity makes it difficult to deeply discern where failures stem from. Additionally, setting up these environments requires considerable effort, and issues of unreliability and reproducibility sometimes arise, especially in interactive tasks. To address these limitations, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups. It evaluates models across five domains, including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine Learning coding, Contest-level programming and Mathematics, and covers five essential capabilities: Understanding, Reasoning, Planning, Problem-solving, and Self-correction. With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. By testing 18 representative models on MMAU, we provide deep and insightful analyses. Ultimately, MMAU not only sheds light on the capabilities and limitations of LLM agents but also enhances the interpretability of their performance. Datasets and evaluation scripts of MMAU are released at https://github.com/apple/axlearn/tree/main/docs/research/mmau.", "source": "arxiv", "arxiv_id": "2407.18961v3", "pdf_url": "https://arxiv.org/pdf/2407.18961v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-18T00:58:41Z", "updated": "2024-08-15T21:32:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MVVM: Deploy Your AI Agents-Securely, Efficiently, Everywhere", "authors": ["Yiwei Yang", "Aibo Hu", "Yusheng Zheng", "Brian Zhao", "Xinqi Zhang", "Dawei Xiang", "Kexin Chu", "Wei Zhang", "Andi Quinn"], "year": 2024, "url": "http://arxiv.org/abs/2410.15894v2", "abstract": "The rise of AI agents powered by Large Language Models (LLMs) presents critical challenges: how to securely execute and migrate these agents across heterogeneous environments while protecting sensitive user data, maintaining availability during network failures, minimizing response latency for time-critical decisions, and ensuring output safety in mission-critical applications. We present MVVM, a WebAssembly-based secure container framework that enables transparent live migration of LLM agent workspaces between edge devices and cloud servers with end-to-end privacy guarantees, resilient multi-tier replication, speculative execution for latency optimization, and integrated validation for safety assurance. MVVM introduces two key innovations: (1) a two-way sandboxing framework leveraging hardware enclaves and accelerator extensions that protects both the agent from malicious hosts and the host from compromised agents; (2) an efficient cross platform migration mechanism using WebAssembly and WASI's platform-agnostic design, enabling seamless movement across ARM phones, RISC-V MCUs, x86 servers, and heterogeneous accelerators; and three astonishing use cases: (1) privacy-aware daemon that automatically determines whether to execute locally or remotely based on data sensitivity and resource availability; (2) multi-tier replication with intelligent quality degradation that maintains service availability despite network failures or resource constraints; (3) a comprehensive execution framework combining speculative execution for 10x latency reduction with parallel validation that ensures output safety without compromising responsiveness. Our evaluation demonstrates that MVVM is validated on three separate devices across 18 workloads.", "source": "arxiv", "arxiv_id": "2410.15894v2", "pdf_url": "https://arxiv.org/pdf/2410.15894v2", "categories": ["cs.OS"], "primary_category": "cs.OS", "doi": "", "venue": "", "published": "2024-10-21T11:14:55Z", "updated": "2025-09-23T18:28:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving", "authors": ["Md. Ashraful Islam", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "year": 2024, "url": "http://arxiv.org/abs/2405.11403v1", "abstract": "Code synthesis, which requires a deep understanding of complex natural language problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge. While large language models (LLMs) demonstrate impressive proficiency in natural language processing, their performance in code generation tasks remains limited. In this paper, we introduce a new approach to code generation tasks leveraging multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers. Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging. After conducting thorough experiments, with multiple LLM ablations and analyses across eight challenging competitive problem-solving and program synthesis benchmarks, MapCoder showcases remarkable code generation capabilities, achieving new state-of-the-art results (pass@1) on HumanEval (93.9%), MBPP (83.1%), APPS (22.0%), CodeContests (28.5%), and xCodeEval (45.3%). Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties. We open-source our framework at https://github.com/Md-Ashraful-Pramanik/MapCoder.", "source": "arxiv", "arxiv_id": "2405.11403v1", "pdf_url": "https://arxiv.org/pdf/2405.11403v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-18T22:10:15Z", "updated": "2024-05-18T22:10:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization", "authors": ["Zhiyu Yang", "Zihan Zhou", "Shuo Wang", "Xin Cong", "Xu Han", "Yukun Yan", "Zhenghao Liu", "Zhixing Tan", "Pengyuan Liu", "Dong Yu", "Zhiyuan Liu", "Xiaodong Shi", "Maosong Sun"], "year": 2024, "url": "http://arxiv.org/abs/2402.11453v3", "abstract": "Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores.", "source": "arxiv", "arxiv_id": "2402.11453v3", "pdf_url": "https://arxiv.org/pdf/2402.11453v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-18T04:28:28Z", "updated": "2024-03-19T14:44:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MathLearner: A Large Language Model Agent Framework for Learning to Solve Mathematical Problems", "authors": ["Wenbei Xie", "Donglin Liu", "Haoran Yan", "Wenjie Wu", "Zongyang Liu"], "year": 2024, "url": "http://arxiv.org/abs/2408.01779v1", "abstract": "With the development of artificial intelligence (AI), large language models (LLM) are widely used in many fields. However, the reasoning ability of LLM is still very limited when it comes to mathematical reasoning. Mathematics plays an important role in all aspects of human society and is a technical guarantee in the fields of healthcare, transport and aerospace, for this reason, the development of AI big language models in the field of mathematics has great potential significance. To improve the mathematical reasoning ability of large language models, we proposed an agent framework for learning to solve mathematical problems based on inductive reasoning. By emulating the human learning process of generalization of learned information and effective application of previous knowledge in new reasoning tasks, this framework has great performance in the mathematical reasoning process. It improves global accuracy over the baseline method (chain-of-thought) by 20.96% and solves 17.54% of the mathematical problems that the baseline cannot solve. Benefiting from the efficient RETRIEVAL method, our model improves the ability of large language models to efficiently use external knowledge, i.e., the mathematical computation of the model can be based on written procedures. In education, our model can be used as a personalised learning aid, thus reducing the inequality of educational resources.", "source": "arxiv", "arxiv_id": "2408.01779v1", "pdf_url": "https://arxiv.org/pdf/2408.01779v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-03T13:28:19Z", "updated": "2024-08-03T13:28:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents", "authors": ["Arya Bulusu", "Brandon Man", "Ashish Jagmohan", "Aditya Vempaty", "Jennifer Mari-Wyka", "Deepak Akkil"], "year": 2024, "url": "http://arxiv.org/abs/2407.17544v1", "abstract": "There has been significant recent interest in harnessing LLMs to control software systems through multi-step reasoning, planning and tool-usage. While some promising results have been obtained, application to specific domains raises several general issues including the control of specialized domain tools, the lack of existing datasets for training and evaluation, and the non-triviality of automated system evaluation and improvement. In this paper, we present a case-study where we examine these issues in the context of a specific domain. Specifically, we present an automated math visualizer and solver system for mathematical pedagogy. The system orchestrates mathematical solvers and math graphing tools to produce accurate visualizations from simple natural language commands. We describe the creation of specialized data-sets, and also develop an auto-evaluator to easily evaluate the outputs of our system by comparing them to ground-truth expressions. We have open sourced the data-sets and code for the proposed system.", "source": "arxiv", "arxiv_id": "2407.17544v1", "pdf_url": "https://arxiv.org/pdf/2407.17544v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-07-24T15:45:07Z", "updated": "2024-07-24T15:45:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling", "authors": ["Yakun Zhu", "Shaohang Wei", "Xu Wang", "Kui Xue", "Xiaofan Zhang", "Shaoting Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2410.13610v3", "abstract": "Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual's health status. We introduce MeNTi, a universal agent architecture for LLMs. MeNTi integrates a specialized medical toolkit and employs meta-tool and nested calling mechanisms to enhance LLM tool utilization. Specifically, it achieves flexible tool selection and nested tool calling to address practical issues faced in intricate medical scenarios, including calculator selection, slot filling, and unit conversion. To assess the capabilities of LLMs for quantitative assessment throughout the clinical process of calculator scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical calculators to perform calculations and assess patient health status. CalcQA is constructed by professional physicians and includes 100 case-calculator pairs, complemented by a toolkit of 281 medical tools. The experimental results demonstrate significant performance improvements with our framework. This research paves new directions for applying LLMs in demanding scenarios of medicine.", "source": "arxiv", "arxiv_id": "2410.13610v3", "pdf_url": "https://arxiv.org/pdf/2410.13610v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-17T14:46:22Z", "updated": "2025-05-23T07:21:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method", "authors": ["Tian Xia", "Zhiwei He", "Tong Ren", "Yibo Miao", "Zhuosheng Zhang", "Yang Yang", "Rui Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.15813v3", "abstract": "Bargaining is an important and unique part of negotiation between humans. As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents' bargaining abilities remains an open problem. For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent's performance in the Bargain task. We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents' bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance. To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer's offers, and an LLM Narrator to create natural language sentences for generated offers. Experimental results show that OG-Narrator improves the buyer's deal rates from 26.67% to 88.88% and brings a ten times multiplication of profits on all baselines, even a model that has not been aligned.", "source": "arxiv", "arxiv_id": "2402.15813v3", "pdf_url": "https://arxiv.org/pdf/2402.15813v3", "categories": ["cs.CL", "cs.GT"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-24T13:36:58Z", "updated": "2024-06-04T08:12:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Medchain: Bridging the Gap Between LLM Agents and Clinical Practice with Interactive Sequence", "authors": ["Jie Liu", "Wenxuan Wang", "Zizhan Ma", "Guolin Huang", "Yihang SU", "Kao-Jung Chang", "Wenting Chen", "Haoliang Li", "Linlin Shen", "Michael Lyu"], "year": 2024, "url": "http://arxiv.org/abs/2412.01605v2", "abstract": "Clinical decision making (CDM) is a complex, dynamic process crucial to healthcare delivery, yet it remains a significant challenge for artificial intelligence systems. While Large Language Model (LLM)-based agents have been tested on general medical knowledge using licensing exams and knowledge question-answering tasks, their performance in the CDM in real-world scenarios is limited due to the lack of comprehensive testing datasets that mirror actual medical practice. To address this gap, we present MedChain, a dataset of 12,163 clinical cases that covers five key stages of clinical workflow. MedChain distinguishes itself from existing benchmarks with three key features of real-world clinical practice: personalization, interactivity, and sequentiality. Further, to tackle real-world CDM challenges, we also propose MedChain-Agent, an AI system that integrates a feedback mechanism and a MCase-RAG module to learn from previous cases and adapt its responses. MedChain-Agent demonstrates remarkable adaptability in gathering information dynamically and handling sequential clinical tasks, significantly outperforming existing approaches.", "source": "arxiv", "arxiv_id": "2412.01605v2", "pdf_url": "https://arxiv.org/pdf/2412.01605v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-02T15:25:02Z", "updated": "2025-10-10T02:35:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Memory-Augmented Agent Training for Business Document Understanding", "authors": ["Jiale Liu", "Yifan Zeng", "Malte HÃ¸jmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "year": 2024, "url": "http://arxiv.org/abs/2412.15274v1", "abstract": "Traditional enterprises face significant challenges in processing business documents, where tasks like extracting transport references from invoices remain largely manual despite their crucial role in logistics operations. While Large Language Models offer potential automation, their direct application to specialized business domains often yields unsatisfactory results. We introduce Matrix (Memory-Augmented agent Training through Reasoning and Iterative eXploration), a novel paradigm that enables LLM agents to progressively build domain expertise through experience-driven memory refinement and iterative learning. To validate this approach, we collaborate with one of the world's largest logistics companies to create a dataset of Universal Business Language format invoice documents, focusing on the task of transport reference extraction. Experiments demonstrate that Matrix outperforms prompting a single LLM by 30.3%, vanilla LLM agent by 35.2%. We further analyze the metrics of the optimized systems and observe that the agent system requires less API calls, fewer costs and can analyze longer documents on average. Our methods establish a new approach to transform general-purpose LLMs into specialized business tools through systematic memory enhancement in document processing tasks.", "source": "arxiv", "arxiv_id": "2412.15274v1", "pdf_url": "https://arxiv.org/pdf/2412.15274v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-17T18:35:04Z", "updated": "2024-12-17T18:35:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC Meeting", "authors": ["Sungil Seok", "Shuide Wen", "Qiyuan Yang", "Juan Feng", "Wenming Yang"], "year": 2024, "url": "http://arxiv.org/abs/2410.18012v2", "abstract": "The Federal Funds rate in the United States plays a significant role in both domestic and international financial markets. However, research has predominantly focused on the effects of adjustments to the Federal Funds rate rather than on the decision-making process itself. Recent advancements in large language models(LLMs) offer a potential method for reconstructing the original FOMC meetings, which are responsible for setting the Federal Funds rate. In this paper, we propose a five-stage FOMC meeting simulation framework, MiniFed, which employs LLM agents to simulate real-world FOMC meeting members and optimize the FOMC structure. This framework effectively revitalizes the FOMC meeting process and facilitates projections of the Federal Funds rate. Experimental results demonstrate that our proposed MiniFed framework achieves both high accuracy in Federal Funds rate projections and behavioral alignment with the agents' real-world counterparts. Given that few studies have focused on employing LLM agents to simulate large-scale real-world conferences, our work can serve as a benchmark for future developments.", "source": "arxiv", "arxiv_id": "2410.18012v2", "pdf_url": "https://arxiv.org/pdf/2410.18012v2", "categories": ["cs.SI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-10-23T16:40:38Z", "updated": "2024-10-25T14:19:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Mixture-of-Agents Enhances Large Language Model Capabilities", "authors": ["Junlin Wang", "Jue Wang", "Ben Athiwaratkun", "Ce Zhang", "James Zou"], "year": 2024, "url": "http://arxiv.org/abs/2406.04692v1", "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.", "source": "arxiv", "arxiv_id": "2406.04692v1", "pdf_url": "https://arxiv.org/pdf/2406.04692v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-07T07:04:10Z", "updated": "2024-06-07T07:04:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents", "authors": ["Luyuan Wang", "Yongyu Deng", "Yiwei Zha", "Guodong Mao", "Qinmin Wang", "Tianchen Min", "Wei Chen", "Shoufa Chen"], "year": 2024, "url": "http://arxiv.org/abs/2406.08184v1", "abstract": "Large language model (LLM)-based mobile agents are increasingly popular due to their capability to interact directly with mobile phone Graphic User Interfaces (GUIs) and their potential to autonomously manage daily tasks. Despite their promising prospects in both academic and industrial sectors, little research has focused on benchmarking the performance of existing mobile agents, due to the inexhaustible states of apps and the vague definition of feasible action sequences. To address this challenge, we propose an efficient and user-friendly benchmark, MobileAgentBench, designed to alleviate the burden of extensive manual testing. We initially define 100 tasks across 10 open-source apps, categorized by multiple levels of difficulty. Subsequently, we evaluate several existing mobile agents, including AppAgent and MobileAgent, to thoroughly and systematically compare their performance. All materials are accessible on our project webpage: https://MobileAgentBench.github.io, contributing to the advancement of both academic and industrial fields.", "source": "arxiv", "arxiv_id": "2406.08184v1", "pdf_url": "https://arxiv.org/pdf/2406.08184v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-12T13:14:50Z", "updated": "2024-06-12T13:14:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Molly: Making Large Language Model Agents Solve Python Problem More Logically", "authors": ["Rui Xiao", "Jiong Wang", "Lu Han", "Na Zong", "Han Wu"], "year": 2024, "url": "http://arxiv.org/abs/2412.18093v1", "abstract": "Applying large language models (LLMs) as teaching assists has attracted much attention as an integral part of intelligent education, particularly in computing courses. To reduce the gap between the LLMs and the computer programming education expert, fine-tuning and retrieval augmented generation (RAG) are the two mainstream methods in existing researches. However, fine-tuning for specific tasks is resource-intensive and may diminish the model`s generalization capabilities. RAG can perform well on reducing the illusion of LLMs, but the generation of irrelevant factual content during reasoning can cause significant confusion for learners. To address these problems, we introduce the Molly agent, focusing on solving the proposed problem encountered by learners when learning Python programming language. Our agent automatically parse the learners' questioning intent through a scenario-based interaction, enabling precise retrieval of relevant documents from the constructed knowledge base. At generation stage, the agent reflect on the generated responses to ensure that they not only align with factual content but also effectively answer the user's queries. Extensive experimentation on a constructed Chinese Python QA dataset shows the effectiveness of the Molly agent, indicating an enhancement in its performance for providing useful responses to Python questions.", "source": "arxiv", "arxiv_id": "2412.18093v1", "pdf_url": "https://arxiv.org/pdf/2412.18093v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-24T02:08:38Z", "updated": "2024-12-24T02:08:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Moral Alignment for LLM Agents", "authors": ["Elizaveta Tennant", "Stephen Hailes", "Mirco Musolesi"], "year": 2024, "url": "http://arxiv.org/abs/2410.01639v4", "abstract": "Decision-making agents based on pre-trained Large Language Models (LLMs) are increasingly being deployed across various domains of human activity. While their applications are currently rather specialized, several research efforts are underway to develop more generalist agents. As LLM-based systems become more agentic, their influence on human activity will grow and their transparency will decrease. Consequently, developing effective methods for aligning them to human values is vital.\n  The prevailing practice in alignment often relies on human preference data (e.g., in RLHF or DPO), in which values are implicit, opaque and are essentially deduced from relative preferences over different model outputs. In this work, instead of relying on human feedback, we introduce the design of reward functions that explicitly and transparently encode core human values for Reinforcement Learning-based fine-tuning of foundation agent models. Specifically, we use intrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of Deontological Ethics and Utilitarianism, quantifying moral rewards for agents in terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD) environment. We also show how moral fine-tuning can be deployed to enable an agent to unlearn a previously developed selfish strategy. Finally, we find that certain moral strategies learned on the IPD game generalize to several other matrix game environments. In summary, we demonstrate that fine-tuning with intrinsic rewards is a promising general solution for aligning LLM agents to human values, and it might represent a more transparent and cost-effective alternative to currently predominant alignment techniques.", "source": "arxiv", "arxiv_id": "2410.01639v4", "pdf_url": "https://arxiv.org/pdf/2410.01639v4", "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-02T15:09:36Z", "updated": "2025-05-11T19:14:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion", "authors": ["Sen Li", "Ruochen Wang", "Cho-Jui Hsieh", "Minhao Cheng", "Tianyi Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2402.12741v2", "abstract": "Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. To efficiently address these challenges, we develop a training-free Multimodal-LLM agent (MuLan), as a human painter, that can progressively generate multi-object with intricate planning and feedback control. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object by stable diffusion, conditioned on previously generated objects. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined upon each sub-task by an LLM and attention guidance. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. The multi-step process also allows human users to monitor the generation process and make preferred changes at any intermediate step via text prompts, thereby improving the human-AI collaboration experience. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines and its creativity when collaborating with human users. The code is available at https://github.com/measure-infinity/mulan-code.", "source": "arxiv", "arxiv_id": "2402.12741v2", "pdf_url": "https://arxiv.org/pdf/2402.12741v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-02-20T06:14:30Z", "updated": "2024-05-24T15:56:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Multi-Agent System for Cosmological Parameter Analysis", "authors": ["Andrew Laverick", "Kristen Surrao", "Inigo Zubeldia", "Boris Bolliet", "Miles Cranmer", "Antony Lewis", "Blake Sherwin", "Julien Lesgourgues"], "year": 2024, "url": "http://arxiv.org/abs/2412.00431v2", "abstract": "Multi-agent systems (MAS) utilizing multiple Large Language Model agents with Retrieval Augmented Generation and that can execute code locally may become beneficial in cosmological data analysis. Here, we illustrate a first small step towards AI-assisted analyses and a glimpse of the potential of MAS to automate and optimize scientific workflows in Cosmology. The system architecture of our example package, that builds upon the autogen/ag2 framework, can be applied to MAS in any area of quantitative scientific research. The particular task we apply our methods to is the cosmological parameter analysis of the Atacama Cosmology Telescope lensing power spectrum likelihood using Monte Carlo Markov Chains. Our work-in-progress code is open source and available at https://github.com/CMBAgents/cmbagent.", "source": "arxiv", "arxiv_id": "2412.00431v2", "pdf_url": "https://arxiv.org/pdf/2412.00431v2", "categories": ["astro-ph.IM", "astro-ph.CO", "physics.comp-ph", "physics.data-an"], "primary_category": "astro-ph.IM", "doi": "", "venue": "", "published": "2024-11-30T10:58:56Z", "updated": "2024-12-03T15:33:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MultiTalk: Introspective and Extrospective Dialogue for Human-Environment-LLM Alignment", "authors": ["Venkata Naren Devarakonda", "Ali Umut Kaypak", "Shuaihang Yuan", "Prashanth Krishnamurthy", "Yi Fang", "Farshad Khorrami"], "year": 2024, "url": "http://arxiv.org/abs/2409.16455v1", "abstract": "LLMs have shown promising results in task planning due to their strong natural language understanding and reasoning capabilities. However, issues such as hallucinations, ambiguities in human instructions, environmental constraints, and limitations in the executing agent's capabilities often lead to flawed or incomplete plans. This paper proposes MultiTalk, an LLM-based task planning methodology that addresses these issues through a framework of introspective and extrospective dialogue loops. This approach helps ground generated plans in the context of the environment and the agent's capabilities, while also resolving uncertainties and ambiguities in the given task. These loops are enabled by specialized systems designed to extract and predict task-specific states, and flag mismatches or misalignments among the human user, the LLM agent, and the environment. Effective feedback pathways between these systems and the LLM planner foster meaningful dialogue. The efficacy of this methodology is demonstrated through its application to robotic manipulation tasks. Experiments and ablations highlight the robustness and reliability of our method, and comparisons with baselines further illustrate the superiority of MultiTalk in task planning for embodied agents.", "source": "arxiv", "arxiv_id": "2409.16455v1", "pdf_url": "https://arxiv.org/pdf/2409.16455v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-09-24T20:54:21Z", "updated": "2024-09-24T20:54:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Planning Case Study", "authors": ["Shangding Gu"], "year": 2024, "url": "http://arxiv.org/abs/2401.06603v2", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities for reinforcement learning (RL) models, such as planning and reasoning capabilities. However, the problems of LLMs and RL model collaboration still need to be solved. In this study, we employ a teacher-student learning framework to tackle these problems, specifically by offering feedback for LLMs using RL models and providing high-level information for RL models with LLMs in a cooperative multi-agent setting. Within this framework, the LLM acts as a teacher, while the RL model acts as a student. The two agents cooperatively assist each other through a process of recursive help, such as \"I help you help I help.\" The LLM agent supplies abstract information to the RL agent, enabling efficient exploration and policy improvement. In turn, the RL agent offers feedback to the LLM agent, providing valuable, real-time information that helps generate more useful tokens. This bi-directional feedback loop promotes optimization, exploration, and mutual improvement for both agents, enabling them to accomplish increasingly challenging tasks. Remarkably, we propose a practical algorithm to address the problem and conduct empirical experiments to evaluate the effectiveness of our method.", "source": "arxiv", "arxiv_id": "2401.06603v2", "pdf_url": "https://arxiv.org/pdf/2401.06603v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-12T14:35:57Z", "updated": "2025-03-02T01:46:57Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory", "authors": ["Hangyeol Kang", "Maher Ben Moussa", "Nadia Magnenat-Thalmann"], "year": 2024, "url": "http://arxiv.org/abs/2405.20189v1", "abstract": "In this work, we describe our approach to developing an intelligent and robust social robotic system for the Nadine social robot platform. We achieve this by integrating Large Language Models (LLMs) and skilfully leveraging the powerful reasoning and instruction-following capabilities of these types of models to achieve advanced human-like affective and cognitive capabilities. This approach is novel compared to the current state-of-the-art LLM-based agents which do not implement human-like long-term memory or sophisticated emotional appraisal. The naturalness of social robots, consisting of multiple modules, highly depends on the performance and capabilities of each component of the system and the seamless integration of the components. We built a social robot system that enables generating appropriate behaviours through multimodal input processing, bringing episodic memories accordingly to the recognised user, and simulating the emotional states of the robot induced by the interaction with the human partner. In particular, we introduce an LLM-agent frame for social robots, SoR-ReAct, serving as a core component for the interaction module in our system. This design has brought forth the advancement of social robots and aims to increase the quality of human-robot interaction.", "source": "arxiv", "arxiv_id": "2405.20189v1", "pdf_url": "https://arxiv.org/pdf/2405.20189v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-05-30T15:55:41Z", "updated": "2024-05-30T15:55:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification", "authors": ["Hanna Abi Akl"], "year": 2024, "url": "http://arxiv.org/abs/2402.16910v2", "abstract": "We present a neuro-symbolic (NeSy) workflow combining a symbolic-based learning technique with a large language model (LLM) agent to generate synthetic data for code comment classification in the C programming language. We also show how generating controlled synthetic data using this workflow fixes some of the notable weaknesses of LLM-based generation and increases the performance of classical machine learning models on the code comment classification task. Our best model, a Neural Network, achieves a Macro-F1 score of 91.412% with an increase of 1.033% after data augmentation.", "source": "arxiv", "arxiv_id": "2402.16910v2", "pdf_url": "https://arxiv.org/pdf/2402.16910v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-02-25T13:20:13Z", "updated": "2024-05-24T07:11:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Network Formation and Dynamics Among Multi-LLMs", "authors": ["Marios Papachristou", "Yuan Yuan"], "year": 2024, "url": "http://arxiv.org/abs/2402.10659v7", "abstract": "Social networks profoundly influence how humans form opinions, exchange information, and organize collectively. As large language models (LLMs) are increasingly embedded into social and professional environments, it is critical to understand whether their interactions approximate human-like network dynamics. We develop a framework to study the network formation behaviors of multiple LLM agents and benchmark them against human decisions. Across synthetic and real-world settings, including friendship, telecommunication, and employment networks, we find that LLMs consistently reproduce fundamental micro-level principles such as preferential attachment, triadic closure, and homophily, as well as macro-level properties including community structure and small-world effects. Importantly, the relative emphasis of these principles adapts to context: for example, LLMs favor homophily in friendship networks but heterophily in organizational settings, mirroring patterns of social mobility. A controlled human-subject survey confirms strong alignment between LLMs and human participants in link-formation decisions. These results establish that LLMs can serve as powerful tools for social simulation and synthetic data generation, while also raising critical questions about bias, fairness, and the design of AI systems that participate in human networks.", "source": "arxiv", "arxiv_id": "2402.10659v7", "pdf_url": "https://arxiv.org/pdf/2402.10659v7", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.SI", "doi": "10.1093/pnasnexus/pgaf317", "venue": "PNAS Nexus 2025", "published": "2024-02-16T13:10:14Z", "updated": "2025-10-05T17:06:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Next-Generation Phishing: How LLM Agents Empower Cyber Attackers", "authors": ["Khalifa Afane", "Wenqi Wei", "Ying Mao", "Junaid Farooq", "Juntao Chen"], "year": 2024, "url": "http://arxiv.org/abs/2411.13874v1", "abstract": "The escalating threat of phishing emails has become increasingly sophisticated with the rise of Large Language Models (LLMs). As attackers exploit LLMs to craft more convincing and evasive phishing emails, it is crucial to assess the resilience of current phishing defenses. In this study we conduct a comprehensive evaluation of traditional phishing detectors, such as Gmail Spam Filter, Apache SpamAssassin, and Proofpoint, as well as machine learning models like SVM, Logistic Regression, and Naive Bayes, in identifying both traditional and LLM-rephrased phishing emails. We also explore the emerging role of LLMs as phishing detection tools, a method already adopted by companies like NTT Security Holdings and JPMorgan Chase. Our results reveal notable declines in detection accuracy for rephrased emails across all detectors, highlighting critical weaknesses in current phishing defenses. As the threat landscape evolves, our findings underscore the need for stronger security controls and regulatory oversight on LLM-generated content to prevent its misuse in creating advanced phishing attacks. This study contributes to the development of more effective Cyber Threat Intelligence (CTI) by leveraging LLMs to generate diverse phishing variants that can be used for data augmentation, harnessing the power of LLMs to enhance phishing detection, and paving the way for more robust and adaptable threat detection systems.", "source": "arxiv", "arxiv_id": "2411.13874v1", "pdf_url": "https://arxiv.org/pdf/2411.13874v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-11-21T06:20:29Z", "updated": "2024-11-21T06:20:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OKG: On-the-Fly Keyword Generation in Sponsored Search Advertising", "authors": ["Zhao Wang", "Briti Gangopadhyay", "Mengjie Zhao", "Shingo Takamatsu"], "year": 2024, "url": "http://arxiv.org/abs/2412.03577v1", "abstract": "Current keyword decision-making in sponsored search advertising relies on large, static datasets, limiting the ability to automatically set up keywords and adapt to real-time KPI metrics and product updates that are essential for effective advertising. In this paper, we propose On-the-fly Keyword Generation (OKG), an LLM agent-based method that dynamically monitors KPI changes and adapts keyword generation in real time, aligning with strategies recommended by advertising platforms. Additionally, we introduce the first publicly accessible dataset containing real keyword data along with its KPIs across diverse domains, providing a valuable resource for future research. Experimental results show that OKG significantly improves keyword adaptability and responsiveness compared to traditional methods. The code for OKG and the dataset are available at https://github.com/sony/okg.", "source": "arxiv", "arxiv_id": "2412.03577v1", "pdf_url": "https://arxiv.org/pdf/2412.03577v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-11-18T03:02:06Z", "updated": "2024-11-18T03:02:06Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OfficeBench: Benchmarking Language Agents across Multiple Applications for Office Automation", "authors": ["Zilong Wang", "Yuedong Cui", "Li Zhong", "Zimin Zhang", "Da Yin", "Bill Yuchen Lin", "Jingbo Shang"], "year": 2024, "url": "http://arxiv.org/abs/2407.19056v1", "abstract": "Office automation significantly enhances human productivity by automatically finishing routine tasks in the workflow. Beyond the basic information extraction studied in much of the prior document AI literature, the office automation research should be extended to more realistic office tasks which require to integrate various information sources in the office system and produce outputs through a series of decision-making processes. We introduce OfficeBench, one of the first office automation benchmarks for evaluating current LLM agents' capability to address office tasks in realistic office workflows. OfficeBench requires LLM agents to perform feasible long-horizon planning, proficiently switch between applications in a timely manner, and accurately ground their actions within a large combined action space, based on the contextual demands of the workflow. Applying our customized evaluation methods on each task, we find that GPT-4 Omni achieves the highest pass rate of 47.00%, demonstrating a decent performance in handling office tasks. However, this is still far below the human performance and accuracy standards required by real-world office workflows. We further observe that most issues are related to operation redundancy and hallucinations, as well as limitations in switching between multiple applications, which may provide valuable insights for developing effective agent frameworks for office automation.", "source": "arxiv", "arxiv_id": "2407.19056v1", "pdf_url": "https://arxiv.org/pdf/2407.19056v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-26T19:27:17Z", "updated": "2024-07-26T19:27:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Offline Training of Language Model Agents with Functions as Learnable Weights", "authors": ["Shaokun Zhang", "Jieyu Zhang", "Jiale Liu", "Linxin Song", "Chi Wang", "Ranjay Krishna", "Qingyun Wu"], "year": 2024, "url": "http://arxiv.org/abs/2402.11359v4", "abstract": "Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability.", "source": "arxiv", "arxiv_id": "2402.11359v4", "pdf_url": "https://arxiv.org/pdf/2402.11359v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-17T18:31:21Z", "updated": "2024-07-30T18:22:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web", "authors": ["Raghav Kapoor", "Yash Parag Butala", "Melisa Russak", "Jing Yu Koh", "Kiran Kamble", "Waseem Alshikh", "Ruslan Salakhutdinov"], "year": 2024, "url": "http://arxiv.org/abs/2402.17553v3", "abstract": "For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as \"Play the next song\", as well as longer horizon tasks such as \"Send an email to John Doe mentioning the time and place to meet\". Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.", "source": "arxiv", "arxiv_id": "2402.17553v3", "pdf_url": "https://arxiv.org/pdf/2402.17553v3", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-27T14:47:53Z", "updated": "2024-07-21T23:16:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning", "authors": ["Shihao Wang", "Zhiding Yu", "Xiaohui Jiang", "Shiyi Lan", "Min Shi", "Nadine Chang", "Jan Kautz", "Ying Li", "Jose M. Alvarez"], "year": 2024, "url": "http://arxiv.org/abs/2405.01533v2", "abstract": "The advances in vision-language models (VLMs) have led to a growing interest in autonomous driving to leverage their strong reasoning capabilities. However, extending these capabilities from 2D to full 3D understanding is crucial for real-world applications. To address this challenge, we propose OmniDrive, a holistic vision-language dataset that aligns agent models with 3D driving tasks through counterfactual reasoning. This approach enhances decision-making by evaluating potential scenarios and their outcomes, similar to human drivers considering alternative actions. Our counterfactual-based synthetic data annotation process generates large-scale, high-quality datasets, providing denser supervision signals that bridge planning trajectories and language-based reasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely Omni-L and Omni-Q, to assess the importance of vision-language alignment versus 3D perception, revealing critical insights into designing effective LLM-agents. Significant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop planning demonstrate the effectiveness of our dataset and methods.", "source": "arxiv", "arxiv_id": "2405.01533v2", "pdf_url": "https://arxiv.org/pdf/2405.01533v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-05-02T17:59:24Z", "updated": "2025-04-16T15:12:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "On Verbalized Confidence Scores for LLMs", "authors": ["Daniel Yang", "Yao-Hung Hubert Tsai", "Makoto Yamada"], "year": 2024, "url": "http://arxiv.org/abs/2412.14737v1", "abstract": "The rise of large language models (LLMs) and their tight integration into our daily life make it essential to dedicate efforts towards their trustworthiness. Uncertainty quantification for LLMs can establish more human trust into their responses, but also allows LLM agents to make more informed decisions based on each other's uncertainty. To estimate the uncertainty in a response, internal token logits, task-specific proxy models, or sampling of multiple responses are commonly used. This work focuses on asking the LLM itself to verbalize its uncertainty with a confidence score as part of its output tokens, which is a promising way for prompt- and model-agnostic uncertainty quantification with low overhead. Using an extensive benchmark, we assess the reliability of verbalized confidence scores with respect to different datasets, models, and prompt methods. Our results reveal that the reliability of these scores strongly depends on how the model is asked, but also that it is possible to extract well-calibrated confidence scores with certain prompt methods. We argue that verbalized confidence scores can become a simple but effective and versatile uncertainty quantification method in the future. Our code is available at https://github.com/danielyxyang/llm-verbalized-uq .", "source": "arxiv", "arxiv_id": "2412.14737v1", "pdf_url": "https://arxiv.org/pdf/2412.14737v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-19T11:10:36Z", "updated": "2024-12-19T11:10:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "On the Structural Memory of LLM Agents", "authors": ["Ruihong Zeng", "Jinyuan Fang", "Siwei Liu", "Zaiqiao Meng"], "year": 2024, "url": "http://arxiv.org/abs/2412.15266v1", "abstract": "Memory plays a pivotal role in enabling large language model~(LLM)-based agents to engage in complex and long-term interactions, such as question answering (QA) and dialogue systems. While various memory modules have been proposed for these tasks, the impact of different memory structures across tasks remains insufficiently explored. This paper investigates how memory structures and memory retrieval methods affect the performance of LLM-based agents. Specifically, we evaluate four types of memory structures, including chunks, knowledge triples, atomic facts, and summaries, along with mixed memory that combines these components. In addition, we evaluate three widely used memory retrieval methods: single-step retrieval, reranking, and iterative retrieval. Extensive experiments conducted across four tasks and six datasets yield the following key insights: (1) Different memory structures offer distinct advantages, enabling them to be tailored to specific tasks; (2) Mixed memory structures demonstrate remarkable resilience in noisy environments; (3) Iterative retrieval consistently outperforms other methods across various scenarios. Our investigation aims to inspire further research into the design of memory systems for LLM-based agents.", "source": "arxiv", "arxiv_id": "2412.15266v1", "pdf_url": "https://arxiv.org/pdf/2412.15266v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-17T04:30:00Z", "updated": "2024-12-17T04:30:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System", "authors": ["Yujie Luo", "Xiangyuan Ru", "Kangwei Liu", "Lin Yuan", "Mengshu Sun", "Ningyu Zhang", "Lei Liang", "Zhiqiang Zhang", "Jun Zhou", "Lanning Wei", "Da Zheng", "Haofen Wang", "Huajun Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.20005v2", "abstract": "We introduce OneKE, a dockerized schema-guided knowledge extraction system, which can extract knowledge from the Web and raw PDF Books, and support various domains (science, news, etc.). Specifically, we design OneKE with multiple agents and a configure knowledge base. Different agents perform their respective roles, enabling support for various extraction scenarios. The configure knowledge base facilitates schema configuration, error case debugging and correction, further improving the performance. Empirical evaluations on benchmark datasets demonstrate OneKE's efficacy, while case studies further elucidate its adaptability to diverse tasks across multiple domains, highlighting its potential for broad applications. We have open-sourced the Code at https://github.com/zjunlp/OneKE and released a Video at http://oneke.openkg.cn/demo.mp4.", "source": "arxiv", "arxiv_id": "2412.20005v2", "pdf_url": "https://arxiv.org/pdf/2412.20005v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-28T04:01:30Z", "updated": "2025-02-06T10:37:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Online Personalizing White-box LLMs Generation with Neural Bandits", "authors": ["Zekai Chen", "Weeden Daniel", "Po-yu Chen", "Francois Buet-Golfouse"], "year": 2024, "url": "http://arxiv.org/abs/2404.16115v1", "abstract": "The advent of personalized content generation by LLMs presents a novel challenge: how to efficiently adapt text to meet individual preferences without the unsustainable demand of creating a unique model for each user. This study introduces an innovative online method that employs neural bandit algorithms to dynamically optimize soft instruction embeddings based on user feedback, enhancing the personalization of open-ended text generation by white-box LLMs. Through rigorous experimentation on various tasks, we demonstrate significant performance improvements over baseline strategies. NeuralTS, in particular, leads to substantial enhancements in personalized news headline generation, achieving up to a 62.9% improvement in terms of best ROUGE scores and up to 2.76% increase in LLM-agent evaluation against the baseline.", "source": "arxiv", "arxiv_id": "2404.16115v1", "pdf_url": "https://arxiv.org/pdf/2404.16115v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-24T18:13:12Z", "updated": "2024-04-24T18:13:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models", "authors": ["Lucio La Cava", "Andrea Tagarelli"], "year": 2024, "url": "http://arxiv.org/abs/2401.07115v3", "abstract": "The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. Scholars have been studying the inherent personalities exhibited by LLMs and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by employing a set of 12 LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five Inventory (BFI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs.", "source": "arxiv", "arxiv_id": "2401.07115v3", "pdf_url": "https://arxiv.org/pdf/2401.07115v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "physics.soc-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-13T16:41:40Z", "updated": "2025-03-22T22:45:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "OpenCity: A Scalable Platform to Simulate Urban Activities with Massive LLM Agents", "authors": ["Yuwei Yan", "Qingbin Zeng", "Zhiheng Zheng", "Jingzhe Yuan", "Jie Feng", "Jun Zhang", "Fengli Xu", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.21286v1", "abstract": "Agent-based models (ABMs) have long been employed to explore how individual behaviors aggregate into complex societal phenomena in urban space. Unlike black-box predictive models, ABMs excel at explaining the micro-macro linkages that drive such emergent behaviors. The recent rise of Large Language Models (LLMs) has led to the development of LLM agents capable of simulating urban activities with unprecedented realism. However, the extreme high computational cost of LLMs presents significant challenges for scaling up the simulations of LLM agents. To address this problem, we propose OpenCity, a scalable simulation platform optimized for both system and prompt efficiencies. Specifically, we propose a LLM request scheduler to reduce communication overhead by parallelizing requests through IO multiplexing. Besides, we deisgn a \"group-and-distill\" prompt optimization strategy minimizes redundancy by clustering agents with similar static attributes. Through experiments on six global cities, OpenCity achieves a 600-fold acceleration in simulation time per agent, a 70% reduction in LLM requests, and a 50% reduction in token usage. These improvements enable the simulation of 10,000 agents' daily activities in 1 hour on commodity hardware. Besides, the substantial speedup of OpenCity allows us to establish a urban simulation benchmark for LLM agents for the first time, comparing simulated urban activities with real-world data in 6 major cities around the globe. We believe our OpenCity platform provides a critical infrastructure to harness the power of LLMs for interdisciplinary studies in urban space, fostering the collective efforts of broader research communities. Code repo is available at https://anonymous.4open.science/r/Anonymous-OpenCity-42BD.", "source": "arxiv", "arxiv_id": "2410.21286v1", "pdf_url": "https://arxiv.org/pdf/2410.21286v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-10-11T13:52:35Z", "updated": "2024-10-11T13:52:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents", "authors": ["Xiangyu Yin", "Chuqiao Shi", "Yimo Han", "Yi Jiang"], "year": 2024, "url": "http://arxiv.org/abs/2410.09034v1", "abstract": "Ptychography is an advanced computational imaging technique in X-ray and electron microscopy. It has been widely adopted across scientific research fields, including physics, chemistry, biology, and materials science, as well as in industrial applications such as semiconductor characterization. In practice, obtaining high-quality ptychographic images requires simultaneous optimization of numerous experimental and algorithmic parameters. Traditionally, parameter selection often relies on trial and error, leading to low-throughput workflows and potential human bias. In this work, we develop the \"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that leverages large language models (LLMs) to automate data analysis in ptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM agents for tasks including knowledge retrieval, code generation, parameter recommendation, and image reasoning. Our study demonstrates that PEAR's multi-agent design significantly improves the workflow success rate, even with smaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various automation levels and is designed to work with customized local knowledge bases, ensuring flexibility and adaptability across different research environments.", "source": "arxiv", "arxiv_id": "2410.09034v1", "pdf_url": "https://arxiv.org/pdf/2410.09034v1", "categories": ["cs.CE", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2024-10-11T17:50:59Z", "updated": "2024-10-11T17:50:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PRACT: Optimizing Principled Reasoning and Acting of LLM Agent", "authors": ["Zhiwei Liu", "Weiran Yao", "Jianguo Zhang", "Rithesh Murthy", "Liangwei Yang", "Zuxin Liu", "Tian Lan", "Ming Zhu", "Juntao Tan", "Shirley Kokane", "Thai Hoang", "Juan Carlos Niebles", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong"], "year": 2024, "url": "http://arxiv.org/abs/2410.18528v1", "abstract": "We introduce the Principled Reasoning and Acting (PRAct) framework, a novel method for learning and enforcing action principles from trajectory data. Central to our approach is the use of text gradients from a reflection and optimization engine to derive these action principles. To adapt action principles to specific task requirements, we propose a new optimization framework, Reflective Principle Optimization (RPO). After execution, RPO employs a reflector to critique current action principles and an optimizer to update them accordingly. We develop the RPO framework under two scenarios: Reward-RPO, which uses environmental rewards for reflection, and Self-RPO, which conducts self-reflection without external rewards. Additionally, two RPO methods, RPO-Traj and RPO-Batch, is introduced to adapt to different settings. Experimental results across four environments demonstrate that the PRAct agent, leveraging the RPO framework, effectively learns and applies action principles to enhance performance.", "source": "arxiv", "arxiv_id": "2410.18528v1", "pdf_url": "https://arxiv.org/pdf/2410.18528v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-24T08:21:51Z", "updated": "2024-10-24T08:21:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning", "authors": ["Jiawen Liu", "Yuanyuan Yao", "Pengcheng An", "Qi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2403.14227v1", "abstract": "In children's collaborative learning, effective peer conversations can significantly enhance the quality of children's collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children's creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.", "source": "arxiv", "arxiv_id": "2403.14227v1", "pdf_url": "https://arxiv.org/pdf/2403.14227v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "10.1145/3613905.3651008", "venue": "", "published": "2024-03-21T08:37:15Z", "updated": "2024-03-21T08:37:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PentestAgent: Incorporating LLM Agents to Automated Penetration Testing", "authors": ["Xiangmin Shen", "Lingzhi Wang", "Zhenyuan Li", "Yan Chen", "Wencheng Zhao", "Dawei Sun", "Jiashui Wang", "Wei Ruan"], "year": 2024, "url": "http://arxiv.org/abs/2411.05185v3", "abstract": "Penetration testing is a critical technique for identifying security vulnerabilities, traditionally performed manually by skilled security specialists. This complex process involves gathering information about the target system, identifying entry points, exploiting the system, and reporting findings. Despite its effectiveness, manual penetration testing is time-consuming and expensive, often requiring significant expertise and resources that many organizations cannot afford. While automated penetration testing methods have been proposed, they often fall short in real-world applications due to limitations in flexibility, adaptability, and implementation.\n  Recent advancements in large language models (LLMs) offer new opportunities for enhancing penetration testing through increased intelligence and automation. However, current LLM-based approaches still face significant challenges, including limited penetration testing knowledge and a lack of comprehensive automation capabilities. To address these gaps, we propose PentestAgent, a novel LLM-based automated penetration testing framework that leverages the power of LLMs and various LLM-based techniques like Retrieval Augmented Generation (RAG) to enhance penetration testing knowledge and automate various tasks. Our framework leverages multi-agent collaboration to automate intelligence gathering, vulnerability analysis, and exploitation stages, reducing manual intervention. We evaluate PentestAgent using a comprehensive benchmark, demonstrating superior performance in task completion and overall efficiency. This work significantly advances the practical applicability of automated penetration testing systems.", "source": "arxiv", "arxiv_id": "2411.05185v3", "pdf_url": "https://arxiv.org/pdf/2411.05185v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-11-07T21:10:39Z", "updated": "2025-05-29T23:48:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions", "authors": ["Qingbin Zeng", "Qinglong Yang", "Shunan Dong", "Heming Du", "Liang Zheng", "Fengli Xu", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2408.04168v3", "abstract": "This paper considers a scenario in city navigation: an AI agent is provided with language descriptions of the goal location with respect to some well-known landmarks; By only observing the scene around, including recognizing landmarks and road network connections, the agent has to make decisions to navigate to the goal location without instructions. This problem is very challenging, because it requires agent to establish self-position and acquire spatial representation of complex urban environment, where landmarks are often invisible. In the absence of navigation instructions, such abilities are vital for the agent to make high-quality decisions in long-range city navigation. With the emergent reasoning ability of large language models (LLMs), a tempting baseline is to prompt LLMs to \"react\" on each observation and make decisions accordingly. However, this baseline has very poor performance that the agent often repeatedly visits same locations and make short-sighted, inconsistent decisions. To address these issues, this paper introduces a novel agentic workflow featured by its abilities to perceive, reflect and plan. Specifically, we find LLaVA-7B can be fine-tuned to perceive the direction and distance of landmarks with sufficient accuracy for city navigation. Moreover, reflection is achieved through a memory mechanism, where past experiences are stored and can be retrieved with current perception for effective decision argumentation. Planning uses reflection results to produce long-term plans, which can avoid short-sighted decisions in long-range navigation. We show the designed workflow significantly improves navigation ability of the LLM agent compared with the state-of-the-art baselines.", "source": "arxiv", "arxiv_id": "2408.04168v3", "pdf_url": "https://arxiv.org/pdf/2408.04168v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-08T02:28:43Z", "updated": "2024-10-17T06:43:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Permissive Information-Flow Analysis for Large Language Models", "authors": ["Shoaib Ahmed Siddiqui", "Radhika Gaonkar", "Boris KÃ¶pf", "David Krueger", "Andrew Paverd", "Ahmed Salem", "Shruti Tople", "Lukas Wutschitz", "Menglin Xia", "Santiago Zanella-BÃ©guelin"], "year": 2024, "url": "http://arxiv.org/abs/2410.03055v3", "abstract": "Large Language Models (LLMs) are rapidly becoming commodity components of larger software systems. This poses natural security and privacy problems: poisoned data retrieved from one component can change the model's behavior and compromise the entire system, including coercing the model to spread confidential data to untrusted components. One promising approach is to tackle this problem at the system level via dynamic information flow (aka taint) tracking. Unfortunately, this approach of propagating the most restrictive input label to the output is too conservative for applications where LLMs operate on inputs retrieved from diverse sources. In this paper, we propose a novel, more permissive approach to propagate information flow labels through LLM queries. The key idea behind our approach is to propagate only the labels of the samples that were influential in generating the model output and to eliminate the labels of unnecessary inputs. We implement and investigate the effectiveness of two variations of this approach, based on (i) prompt-based retrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We compare these with a baseline that uses introspection to predict the output label. Our experimental results in an LLM agent setting show that the permissive label propagator improves over the baseline in more than 85% of the cases, which underscores the practicality of our approach.", "source": "arxiv", "arxiv_id": "2410.03055v3", "pdf_url": "https://arxiv.org/pdf/2410.03055v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-04T00:25:43Z", "updated": "2026-01-14T23:52:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Persona Inconstancy in Multi-Agent LLM Collaboration: Conformity, Confabulation, and Impersonation", "authors": ["Razan Baltaji", "Babak Hemmatian", "Lav R. Varshney"], "year": 2024, "url": "http://arxiv.org/abs/2405.03862v3", "abstract": "Multi-agent AI systems can be used for simulating collective decision-making in scientific and practical applications. They can also be used to introduce a diverse group discussion step in chatbot pipelines, enhancing the cultural sensitivity of the chatbot's responses. These applications, however, are predicated on the ability of AI agents to reliably adopt assigned personas and mimic human interactions. To see whether LLM agents satisfy these requirements, we examine AI agent ensembles engaged in cross-national collaboration and debate by analyzing their private responses and chat transcripts. Our findings suggest that multi-agent discussions can support collective AI decisions that more often reflect diverse perspectives, yet this effect is tempered by the agents' susceptibility to conformity due to perceived peer pressure and occasional challenges in maintaining consistent personas and opinions. Instructions that encourage debate in support of one's opinions rather than collaboration increase the rate of inconstancy. Without addressing the factors we identify, the full potential of multi-agent frameworks for producing more culturally diverse AI outputs or more realistic simulations of group decision-making may remain untapped.", "source": "arxiv", "arxiv_id": "2405.03862v3", "pdf_url": "https://arxiv.org/pdf/2405.03862v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "The 2nd Workshop on Cross-Cultural Considerations in NLP (2024)", "published": "2024-05-06T21:20:35Z", "updated": "2024-08-14T18:01:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PersonaGym: Evaluating Persona Agents and LLMs", "authors": ["Vinay Samuel", "Henry Peng Zou", "Yue Zhou", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Ameet Deshpande", "Karthik Narasimhan", "Vishvak Murahari"], "year": 2024, "url": "http://arxiv.org/abs/2407.18416v5", "abstract": "Persona agents, which are LLM agents conditioned to act according to an assigned persona, enable contextually rich and user aligned interactions across domains like education and healthcare. However, evaluating how faithfully these agents adhere to their personas remains a significant challenge, particularly in free-form settings that demand consistency across diverse, persona-relevant environments. We introduce PersonaGym, the first dynamic evaluation framework for persona agents, and PersonaScore, a human-aligned automatic metric grounded in decision theory that enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals significant advancement opportunities. For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed source model. Importantly, increased model size and complexity do not necessarily enhance persona agent capabilities, underscoring the need for algorithmic and architectural innovation toward faithful, performant persona agents.", "source": "arxiv", "arxiv_id": "2407.18416v5", "pdf_url": "https://arxiv.org/pdf/2407.18416v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-25T22:24:45Z", "updated": "2025-09-05T16:33:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security", "authors": ["Yuanchun Li", "Hao Wen", "Weijun Wang", "Xiangyu Li", "Yizhen Yuan", "Guohong Liu", "Jiacheng Liu", "Wenxing Xu", "Xiang Wang", "Yi Sun", "Rui Kong", "Yile Wang", "Hanfei Geng", "Jian Luan", "Xuefeng Jin", "Zilong Ye", "Guanjing Xiong", "Fan Zhang", "Xiang Li", "Mengwei Xu", "Zhijun Li", "Peng Li", "Yang Liu", "Ya-Qin Zhang", "Yunxin Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.05459v2", "abstract": "Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.", "source": "arxiv", "arxiv_id": "2401.05459v2", "pdf_url": "https://arxiv.org/pdf/2401.05459v2", "categories": ["cs.HC", "cs.AI", "cs.SE"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-01-10T09:25:45Z", "updated": "2024-05-08T06:16:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Personalizing Prostate Cancer Education for Patients Using an EHR-Integrated LLM Agent", "authors": ["Yuexing Hao", "Jason Holmes", "Mark R. Waddle", "Brian J. Davis", "Nathan Y. Yu", "Kristin Vickers", "Heather Preston", "Drew Margolin", "Corinna E. Lockenhoff", "Aditya Vashistha", "Saleh Kalantari", "Marzyeh Ghassemi", "Wei Liu"], "year": 2024, "url": "http://arxiv.org/abs/2409.19100v2", "abstract": "Cancer patients often lack timely education and personalized support due to clinician workload. This quality improvement study develops and evaluates a Large Language Model (LLM) agent, MedEduChat, which is integrated with the clinic's electronic health records (EHR) and designed to enhance prostate cancer patient education. Fifteen non-metastatic prostate cancer patients and three clinicians recruited from the Mayo Clinic interacted with the agent between May 2024 and April 2025. Findings showed that MedEduChat has a high usability score (UMUX 83.7 out of 100) and improves patients' health confidence (Health Confidence Score rose from 9.9 to 13.9). Clinicians evaluated the patient-chat interaction history and rated MedEduChat as highly correct (2.9 out of 3), complete (2.7 out of 3), and safe (2.7 out of 3), with moderate personalization (2.3 out of 3). This study highlights the potential of LLM agents to improve patient engagement and health education.", "source": "arxiv", "arxiv_id": "2409.19100v2", "pdf_url": "https://arxiv.org/pdf/2409.19100v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "npj Digital Medicine 2025", "published": "2024-09-27T19:04:11Z", "updated": "2025-11-17T16:06:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Physics-Informed Autonomous LLM Agents for Explainable Power Electronics Modulation Design", "authors": ["Junhua Liu", "Fanfan Lin", "Xinze Li", "Kwan Hui Lim", "Shuai Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2411.14214v2", "abstract": "LLM-based autonomous agents have recently shown strong capabilities in solving complex industrial design tasks. However, in domains aiming for carbon neutrality and high-performance renewable energy systems, current AI-assisted design automation methods face critical challenges in explainability, scalability, and practical usability. To address these limitations, we introduce PHIA (Physics-Informed Autonomous Agent), an LLM-driven system that automates modulation design for power converters in Power Electronics Systems with minimal human intervention. In contrast to traditional pipeline-based methods, PHIA incorporates an LLM-based planning module that interactively acquires and verifies design requirements via a user-friendly chat interface. This planner collaborates with physics-informed simulation and optimization components to autonomously generate and iteratively refine modulation designs. The interactive interface also supports interpretability by providing textual explanations and visual outputs throughout the design process. Experimental results show that PHIA reduces standard mean absolute error by 63.2% compared to the second-best benchmark and accelerates the overall design process by over 33 times. A user study involving 20 domain experts further confirms PHIA's superior design efficiency and usability, highlighting its potential to transform industrial design workflows in power electronics.", "source": "arxiv", "arxiv_id": "2411.14214v2", "pdf_url": "https://arxiv.org/pdf/2411.14214v2", "categories": ["cs.AI", "cs.ET"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-21T15:24:41Z", "updated": "2025-10-14T16:10:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Plancraft: an evaluation dataset for planning with LLM agents", "authors": ["Gautier Dagan", "Frank Keller", "Alex Lascarides"], "year": 2024, "url": "http://arxiv.org/abs/2412.21033v2", "abstract": "We present Plancraft, a multi-modal evaluation dataset for LLM agents. Plancraft has both a text-only and multi-modal interface, based on the Minecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and Retrieval Augmented Generation (RAG), as well as a handcrafted planner and Oracle Retriever, to ablate the different components of a modern agent architecture. To evaluate decision-making, Plancraft also includes a subset of examples that are intentionally unsolvable, providing a realistic challenge that requires the agent not only to complete tasks but also to decide whether they are solvable at all. We benchmark both open-source and closed-source LLMs and compare their performance and efficiency to a handcrafted planner. Overall, we find that LLMs and VLMs struggle with the planning problems that Plancraft introduces, and offer suggestions on how to improve their capabilities.", "source": "arxiv", "arxiv_id": "2412.21033v2", "pdf_url": "https://arxiv.org/pdf/2412.21033v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-30T15:58:41Z", "updated": "2025-07-15T09:27:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Planning with Multi-Constraints via Collaborative Language Agents", "authors": ["Cong Zhang", "Derrick Goh Xin Deik", "Dexun Li", "Hao Zhang", "Yong Liu"], "year": 2024, "url": "http://arxiv.org/abs/2405.16510v4", "abstract": "The rapid advancement of neural language models has sparked a new surge of intelligent agent research. Unlike traditional agents, large language model-based agents (LLM agents) have emerged as a promising paradigm for achieving artificial general intelligence (AGI) due to their superior reasoning and generalization capabilities. Effective planning is crucial for the success of LLM agents in real-world tasks, making it a highly pursued topic in the community. Current planning methods typically translate tasks into executable action sequences. However, determining a feasible or optimal sequence for complex tasks with multiple constraints at fine granularity, which often requires compositing long chains of heterogeneous actions, remains challenging. This paper introduces Planning with Multi-Constraints (PMC), a zero-shot methodology for collaborative LLM-based multi-agent systems that simplifies complex task planning with constraints by decomposing it into a hierarchy of subordinate tasks. Each subtask is then mapped into executable actions. PMC was assessed on two constraint-intensive benchmarks, TravelPlanner and API-Bank. Notably, PMC achieved an average 42.68% success rate on TravelPlanner, significantly higher than GPT-4 (2.92%), and outperforming GPT-4 with ReAct on API-Bank by 13.64%, showing the immense potential of integrating LLM with multi-agent systems. We also show that PMC works with small LLM as the planning core, e.g., LLaMA-3.1-8B.", "source": "arxiv", "arxiv_id": "2405.16510v4", "pdf_url": "https://arxiv.org/pdf/2405.16510v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-26T10:33:17Z", "updated": "2024-12-16T02:27:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning", "authors": ["Hang Ni", "Yuzhi Wang", "Hao Liu"], "year": 2024, "url": "http://arxiv.org/abs/2412.20505v1", "abstract": "Urban regeneration presents significant challenges within the context of urbanization, requiring adaptive approaches to tackle evolving needs. Leveraging advancements in large language models (LLMs), we propose Cyclical Urban Planning (CUP), a new paradigm that continuously generates, evaluates, and refines urban plans in a closed-loop. Specifically, our multi-agent LLM-based framework consists of three key components: (1) Planning, where LLM agents generate and refine urban plans based on contextual data; (2) Living, where agents simulate the behaviors and interactions of residents, modeling life in the urban environment; and (3) Judging, which involves evaluating plan effectiveness and providing iterative feedback for improvement. The cyclical process enables a dynamic and responsive planning approach. Experiments on the real-world dataset demonstrate the effectiveness of our framework as a continuous and adaptive planning process.", "source": "arxiv", "arxiv_id": "2412.20505v1", "pdf_url": "https://arxiv.org/pdf/2412.20505v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-29T15:43:25Z", "updated": "2024-12-29T15:43:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Practical Considerations for Agentic LLM Systems", "authors": ["Chris Sypherd", "Vaishak Belle"], "year": 2024, "url": "http://arxiv.org/abs/2412.04093v1", "abstract": "As the strength of Large Language Models (LLMs) has grown over recent years, so too has interest in their use as the underlying models for autonomous agents. Although LLMs demonstrate emergent abilities and broad expertise across natural language domains, their inherent unpredictability makes the implementation of LLM agents challenging, resulting in a gap between related research and the real-world implementation of such systems. To bridge this gap, this paper frames actionable insights and considerations from the research community in the context of established application paradigms to enable the construction and facilitate the informed deployment of robust LLM agents. Namely, we position relevant research findings into four broad categories--Planning, Memory, Tools, and Control Flow--based on common practices in application-focused literature and highlight practical considerations to make when designing agentic LLMs for real-world applications, such as handling stochasticity and managing resources efficiently. While we do not conduct empirical evaluations, we do provide the necessary background for discussing critical aspects of agentic LLM designs, both in academia and industry.", "source": "arxiv", "arxiv_id": "2412.04093v1", "pdf_url": "https://arxiv.org/pdf/2412.04093v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-05T11:57:49Z", "updated": "2024-12-05T11:57:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PreAct: Prediction Enhances Agent's Planning Ability", "authors": ["Dayuan Fu", "Jianzhao Huang", "Siyuan Lu", "Guanting Dong", "Yejie Wang", "Keqing He", "Weiran Xu"], "year": 2024, "url": "http://arxiv.org/abs/2402.11534v2", "abstract": "Addressing the disparity between forecasts and actual results can enable individuals to expand their thought processes and stimulate self-reflection, thus promoting accurate planning. In this research, we present **PreAct**, an agent framework that integrates **pre**diction, **rea**soning, and **act**ion. By utilizing the information derived from predictions, the large language model (LLM) agent can provide a wider range and more strategically focused reasoning. This leads to more efficient actions that aid the agent in accomplishing intricate tasks. Our experimental results show that PreAct surpasses the ReAct method in completing complex tasks and that PreAct's performance can be further improved when paired with other memory or selection strategy techniques. We presented the model with varying quantities of historical predictions and discovered that these predictions consistently enhance LLM planning.The variances in single-step reasoning between PreAct and ReAct indicate that PreAct indeed has benefits in terms of diversity and strategic orientation over ReAct.", "source": "arxiv", "arxiv_id": "2402.11534v2", "pdf_url": "https://arxiv.org/pdf/2402.11534v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-18T10:15:38Z", "updated": "2024-12-05T04:40:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Preemptive Detection and Correction of Misaligned Actions in LLM Agents", "authors": ["Haishuo Fang", "Xiaodan Zhu", "Iryna Gurevych"], "year": 2024, "url": "http://arxiv.org/abs/2407.11843v4", "abstract": "Deploying LLM-based agents in real-life applications often faces a critical challenge: the misalignment between agents' behavior and user intent. Such misalignment may lead agents to unintentionally execute critical actions that carry negative outcomes (e.g., accidentally triggering a \"buy-now\" in web shopping), resulting in undesirable or even irreversible consequences. Although addressing these issues is crucial, the preemptive detection and correction of misaligned actions remains relatively underexplored. To fill this gap, we introduce InferAct, a novel approach that leverages the belief reasoning ability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions before execution. Once the misalignment is detected, InferAct alerts users for timely correction, preventing adverse outcomes and enhancing the reliability of LLM agents' decision-making processes. Experiments on three widely used tasks demonstrate that InferAct achieves up to 20% improvements on Marco-F1 against baselines in misaligned action detection. An in-depth evaluation of misalignment correction further highlights InferAct's effectiveness in improving agent alignment.", "source": "arxiv", "arxiv_id": "2407.11843v4", "pdf_url": "https://arxiv.org/pdf/2407.11843v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-16T15:24:44Z", "updated": "2025-09-30T13:12:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Privacy Leakage Overshadowed by Views of AI: A Study on Human Oversight of Privacy in Language Model Agent", "authors": ["Zhiping Zhang", "Bingcan Guo", "Tianshi Li"], "year": 2024, "url": "http://arxiv.org/abs/2411.01344v3", "abstract": "Language model (LM) agents that act on users' behalf for personal tasks (e.g., replying emails) can boost productivity, but are also susceptible to unintended privacy leakage risks. We present the first study on people's capacity to oversee the privacy implications of the LM agents. By conducting a task-based survey ($N=300$), we investigate how people react to and assess the response generated by LM agents for asynchronous interpersonal communication tasks, compared with a response they wrote. We found that people may favor the agent response with more privacy leakage over the response they drafted or consider both good, leading to an increased harmful disclosure from 15.7% to 55.0%. We further identified six privacy behavior patterns reflecting varying concerns, trust levels, and privacy preferences underlying people's oversight of LM agents' actions. Our findings shed light on designing agentic systems that enable privacy-preserving interactions and achieve bidirectional alignment on privacy preferences to help users calibrate trust.", "source": "arxiv", "arxiv_id": "2411.01344v3", "pdf_url": "https://arxiv.org/pdf/2411.01344v3", "categories": ["cs.HC", "cs.AI", "cs.CR"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-11-02T19:15:42Z", "updated": "2025-10-06T04:47:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance", "authors": ["Yaxi Lu", "Shenzhi Yang", "Cheng Qian", "Guirong Chen", "Qinyu Luo", "Yesai Wu", "Huadong Wang", "Xin Cong", "Zhong Zhang", "Yankai Lin", "Weiwen Liu", "Yasheng Wang", "Zhiyuan Liu", "Fangming Liu", "Maosong Sun"], "year": 2024, "url": "http://arxiv.org/abs/2410.12361v3", "abstract": "Agents powered by large language models have shown remarkable abilities in solving complex tasks. However, most agent systems remain reactive, limiting their effectiveness in scenarios requiring foresight and autonomous decision-making. In this paper, we tackle the challenge of developing proactive agents capable of anticipating and initiating tasks without explicit human instructions. We propose a novel data-driven approach for this problem. Firstly, we collect real-world human activities to generate proactive task predictions. These predictions are then labeled by human annotators as either accepted or rejected. The labeled data is used to train a reward model that simulates human judgment and serves as an automatic evaluator of the proactiveness of LLM agents. Building on this, we develop a comprehensive data generation pipeline to create a diverse dataset, ProactiveBench, containing 6,790 events. Finally, we demonstrate that fine-tuning models with the proposed ProactiveBench can significantly elicit the proactiveness of LLM agents. Experimental results show that our fine-tuned model achieves an F1-Score of 66.47% in proactively offering assistance, outperforming all open-source and close-source models. These results highlight the potential of our method in creating more proactive and effective agent systems, paving the way for future advancements in human-agent collaboration.", "source": "arxiv", "arxiv_id": "2410.12361v3", "pdf_url": "https://arxiv.org/pdf/2410.12361v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-16T08:24:09Z", "updated": "2024-12-03T04:34:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction", "authors": ["Sonny George", "Chris Sypherd", "Dylan Cashman"], "year": 2024, "url": "http://arxiv.org/abs/2411.12828v1", "abstract": "Large language model (LLM) agents show promise in an increasing number of domains. In many proposed applications, it is expected that the agent reasons over accumulated experience presented in an input prompt. We propose the OEDD (Operationalize Experience Despite Distraction) corpus, a human-annotator-validated body of scenarios with pre-scripted agent histories where the agent must make a decision based on disparate experiential information in the presence of a distractor. We evaluate three state-of-the-art LLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal chain-of-thought prompting strategy and observe that when (1) the input context contains over 1,615 tokens of historical interactions, (2) a crucially decision-informing premise is the rightful conclusion over two disparate environment premises, and (3) a trivial, but distracting red herring fact follows, all LLMs perform worse than random choice at selecting the better of two actions. Our code and test corpus are publicly available at: https://github.com/sonnygeorge/OEDD .", "source": "arxiv", "arxiv_id": "2411.12828v1", "pdf_url": "https://arxiv.org/pdf/2411.12828v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "Findings Assoc. Comput. Linguistics: EMNLP 2024 15447-15459 (2024)", "published": "2024-11-19T19:33:16Z", "updated": "2024-11-19T19:33:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents", "authors": ["Qisen Yang", "Zekun Wang", "Honghui Chen", "Shenzhi Wang", "Yifan Pu", "Xin Gao", "Wenhao Huang", "Shiji Song", "Gao Huang"], "year": 2024, "url": "http://arxiv.org/abs/2402.12326v2", "abstract": "Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT's enhancements in content coherence, interactivity, interest, immersion, and satisfaction.", "source": "arxiv", "arxiv_id": "2402.12326v2", "pdf_url": "https://arxiv.org/pdf/2402.12326v2", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-19T18:00:30Z", "updated": "2024-08-29T08:27:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PyBench: Evaluating LLM Agent on various real-world coding tasks", "authors": ["Yaolun Zhang", "Yinxu Pan", "Yudong Wang", "Jie Cai"], "year": 2024, "url": "http://arxiv.org/abs/2407.16732v2", "abstract": "The LLM Agent, equipped with a code interpreter, is capable of automatically solving real-world coding tasks, such as data analysis and image editing.\n  However, existing benchmarks primarily focus on either simplistic tasks, such as completing a few lines of code, or on extremely complex and specific tasks at the repository level, neither of which are representative of various daily coding tasks.\n  To address this gap, we introduce \\textbf{PyBench}, a benchmark encompassing five main categories of real-world tasks, covering more than 10 types of files. Given a high-level user query and related files, the LLM Agent needs to reason and execute Python code via a code interpreter for a few turns before making a formal response to fulfill the user's requirements. Successfully addressing tasks in PyBench demands a robust understanding of various Python packages, superior reasoning capabilities, and the ability to incorporate feedback from executed code. Our evaluations indicate that current open-source LLMs are struggling with these tasks. Hence, we conduct analysis and experiments on four kinds of datasets proving that comprehensive abilities are needed for PyBench. Our fine-tuned 8B size model: \\textbf{PyLlama3} achieves an exciting performance on PyBench which surpasses many 33B and 70B size models. Our Benchmark, Training Dataset, and Model are available at: {https://github.com/Mercury7353/PyBench}", "source": "arxiv", "arxiv_id": "2407.16732v2", "pdf_url": "https://arxiv.org/pdf/2407.16732v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-07-23T15:23:14Z", "updated": "2024-08-03T03:00:43Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Python Symbolic Execution with LLM-powered Code Generation", "authors": ["Wenhan Wang", "Kaibo Liu", "An Ran Chen", "Ge Li", "Zhi Jin", "Gang Huang", "Lei Ma"], "year": 2024, "url": "http://arxiv.org/abs/2409.09271v1", "abstract": "Symbolic execution is a key technology in software testing, which generates test cases by collecting symbolic path constraints and then solving constraints with SMT solvers. Symbolic execution has been proven helpful in generating high-coverage test cases, but its limitations, e.g., the difficulties in solving path constraints, prevent it from broader usage in software testing. Moreover, symbolic execution has encountered many difficulties when applied to dynamically typed languages like Python, because it is extremely challenging to translate the flexible Python grammar into rigid solvers.\n  To overcome the main challenges of applying symbolic execution in Python, we proposed an LLM-empowered agent, LLM-Sym, that automatically calls an SMT solver, Z3, to solve execution path constraints. Based on an introductory-level symbolic execution engine, our LLM agent can extend it to supporting programs with complex data type `list'. The core contribution of LLM-Sym is translating complex Python path constraints into Z3 code. To enable accurate path-to-Z3 translation, we design a multiple-step code generation pipeline including type inference, retrieval and self-refine. Our experiments demonstrate that LLM-Sym is capable of solving path constraints on Leetcode problems with complicated control flows and list data structures, which is impossible for the backbone symbolic execution engine. Our approach paves the way for the combination of the generation ability of LLMs with the reasoning ability of symbolic solvers, and opens up new opportunities in LLM-augmented test case generation.", "source": "arxiv", "arxiv_id": "2409.09271v1", "pdf_url": "https://arxiv.org/pdf/2409.09271v1", "categories": ["cs.SE", "cs.PL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-09-14T02:43:20Z", "updated": "2024-09-14T02:43:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents", "authors": ["Tongxin Yuan", "Zhiwei He", "Lingzhong Dong", "Yiming Wang", "Ruijie Zhao", "Tian Xia", "Lizhen Xu", "Binglin Zhou", "Fangqi Li", "Zhuosheng Zhang", "Rui Wang", "Gongshen Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.10019v3", "abstract": "Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on the harmlessness of LLM-generated content in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging and identifying safety risks given agent interaction records. R-Judge comprises 569 records of multi-turn agent interaction, encompassing 27 key risk scenarios among 5 application categories and 10 risk types. It is of high-quality curation with annotated safety labels and risk descriptions. Evaluation of 11 LLMs on R-Judge shows considerable room for enhancing the risk awareness of LLMs: The best-performing model, GPT-4o, achieves 74.42% while no other models significantly exceed the random. Moreover, we reveal that risk awareness in open agent scenarios is a multi-dimensional capability involving knowledge and reasoning, thus challenging for LLMs. With further experiments, we find that fine-tuning on safety judgment significantly improve model performance while straightforward prompting mechanisms fail. R-Judge is publicly available at https://github.com/Lordog/R-Judge.", "source": "arxiv", "arxiv_id": "2401.10019v3", "pdf_url": "https://arxiv.org/pdf/2401.10019v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-18T14:40:46Z", "updated": "2024-10-05T06:50:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents", "authors": ["Tomoyuki Kagaya", "Thong Jing Yuan", "Yuxuan Lou", "Jayashree Karlekar", "Sugiri Pranata", "Akira Kinose", "Koki Oguri", "Felix Wick", "Yang You"], "year": 2024, "url": "http://arxiv.org/abs/2402.03610v1", "abstract": "Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.", "source": "arxiv", "arxiv_id": "2402.03610v1", "pdf_url": "https://arxiv.org/pdf/2402.03610v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-06T00:53:27Z", "updated": "2024-02-06T00:53:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts", "authors": ["Hjalmar Wijk", "Tao Lin", "Joel Becker", "Sami Jawhar", "Neev Parikh", "Thomas Broadley", "Lawrence Chan", "Michael Chen", "Josh Clymer", "Jai Dhyani", "Elena Ericheva", "Katharyn Garcia", "Brian Goodrich", "Nikola Jurkovic", "Holden Karnofsky", "Megan Kinniment", "Aron Lajko", "Seraphina Nix", "Lucas Sato", "William Saunders", "Maksym Taran", "Ben West", "Elizabeth Barnes"], "year": 2024, "url": "http://arxiv.org/abs/2411.15114v2", "abstract": "Frontier AI safety policies highlight automation of AI research and development (R&D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-k with varying time budgets and agent designs, and find that the best AI agents achieve a score 4x higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2x the score of the top AI agent when both are given 32 total hours (across different attempts). Qualitatively, we find that modern AI agents possess significant expertise in many ML topics -- e.g. an agent wrote a faster custom Triton kernel than any of our human experts' -- and can generate and test solutions over ten times faster than humans, at much lower cost. We open-source the evaluation environments, human expert data, analysis code and agent trajectories to facilitate future research.", "source": "arxiv", "arxiv_id": "2411.15114v2", "pdf_url": "https://arxiv.org/pdf/2411.15114v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-11-22T18:30:46Z", "updated": "2025-05-27T03:32:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RL2: Reinforce Large Language Model to Assist Safe Reinforcement Learning for Energy Management of Active Distribution Networks", "authors": ["Xu Yang", "Chenhui Lin", "Haotian Liu", "Wenchuan Wu"], "year": 2024, "url": "http://arxiv.org/abs/2412.01303v1", "abstract": "As large-scale distributed energy resources are integrated into the active distribution networks (ADNs), effective energy management in ADNs becomes increasingly prominent compared to traditional distribution networks. Although advanced reinforcement learning (RL) methods, which alleviate the burden of complicated modelling and optimization, have greatly improved the efficiency of energy management in ADNs, safety becomes a critical concern for RL applications in real-world problems. Since the design and adjustment of penalty functions, which correspond to operational safety constraints, requires extensive domain knowledge in RL and power system operation, the emerging ADN operators call for a more flexible and customized approach to address the penalty functions so that the operational safety and efficiency can be further enhanced. Empowered with strong comprehension, reasoning, and in-context learning capabilities, large language models (LLMs) provide a promising way to assist safe RL for energy management in ADNs. In this paper, we introduce the LLM to comprehend operational safety requirements in ADNs and generate corresponding penalty functions. In addition, we propose an RL2 mechanism to refine the generated functions iteratively and adaptively through multi-round dialogues, in which the LLM agent adjusts the functions' pattern and parameters based on training and test performance of the downstream RL agent. The proposed method significantly reduces the intervention of the ADN operators. Comprehensive test results demonstrate the effectiveness of the proposed method.", "source": "arxiv", "arxiv_id": "2412.01303v1", "pdf_url": "https://arxiv.org/pdf/2412.01303v1", "categories": ["eess.SY", "cs.AI"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-12-02T09:15:36Z", "updated": "2024-12-02T09:15:36Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs", "authors": ["SeongYeub Chu", "JongWoo Kim", "Bryan Wong", "MunYong Yi"], "year": 2024, "url": "http://arxiv.org/abs/2410.14202v3", "abstract": "Existing automated essay scoring (AES) has solely relied on essay text without using explanatory rationales for the scores, thereby forgoing an opportunity to capture the specific aspects evaluated by rubric indicators in a fine-grained manner. This paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates prompt-engineering-based large language models (LLMs) with a fine-tuning-based essay scoring model using a smaller large language model (S-LLM). RMTS uses an LLM-based trait-wise rationale generation system where a separate LLM agent generates trait-specific rationales based on rubric guidelines, which the scoring model uses to accurately predict multi-trait scores. Extensive experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize, show that RMTS significantly outperforms state-of-the-art models and vanilla S-LLMs in trait-specific scoring. By assisting quantitative assessment with fine-grained qualitative rationales, RMTS enhances the trait-wise reliability, providing partial explanations about essays. The code is available at https://github.com/BBeeChu/RMTS.git.", "source": "arxiv", "arxiv_id": "2410.14202v3", "pdf_url": "https://arxiv.org/pdf/2410.14202v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-18T06:35:17Z", "updated": "2025-02-05T07:52:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents", "authors": ["Weizhe Chen", "Sven Koenig", "Bistra Dilkina"], "year": 2024, "url": "http://arxiv.org/abs/2406.11132v2", "abstract": "In the past year, large language models (LLMs) have had remarkable success in domains outside the traditional natural language processing, and their capacity is further expanded into the so-called LLM agents when connected with external tools. In all domains, the prompt to the LLMs has been shown to make a big difference in what the LLM would generate and thus affect the performance of the LLM agents. Therefore, automatic prompt engineering (APE) has become an important question for many researchers and users of LLMs. However, previous works in APE rely on a final checker to evaluate the performance of the given prompt -- a requirement that is hard to meet in the case of LLM agents, where intermediate feedback is easier to obtain, and the final evaluation could be expensive, inaccurate, or even missing. In this paper, we propose a novel method, \\textsc{RePrompt}, which does a ``gradient descent\"-like approach to optimize the step-by-step instructions in the prompts given to LLM agents, based on the chat history obtained from interactions and reflections with LLM agents. By leveraging intermediate feedback, \\textsc{RePrompt} can optimize the prompt without the need for a final solution checker. We evaluate our approach on PDDL generation, TravelPlanner, and Meeting Planning to show that our method could generally improve performance for different reasoning tasks.", "source": "arxiv", "arxiv_id": "2406.11132v2", "pdf_url": "https://arxiv.org/pdf/2406.11132v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-17T01:23:11Z", "updated": "2025-02-13T21:38:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reasoning, Memorization, and Fine-Tuning Language Models for Non-Cooperative Games", "authors": ["Yunhao Yang", "Leonard Berthellemy", "Ufuk Topcu"], "year": 2024, "url": "http://arxiv.org/abs/2410.14890v1", "abstract": "We develop a method that integrates the tree of thoughts and multi-agent framework to enhance the capability of pre-trained language models in solving complex, unfamiliar games. The method decomposes game-solving into four incremental tasks -- game summarization, area selection, action extraction, and action validation -- each assigned to a specific language-model agent. By constructing a tree of thoughts, the method simulates reasoning paths and allows agents to collaboratively distill game representations and tactics, mitigating the limitations of language models in reasoning and long-term memorization. Additionally, an automated fine-tuning process further optimizes the agents' performance by ranking query-response pairs based on game outcomes, e.g., winning or losing. We apply the method to a non-cooperative game and demonstrate a 65 percent winning rate against benchmark algorithms, with an additional 10 percent improvement after fine-tuning. In contrast to existing deep learning algorithms for game solving that require millions of training samples, the proposed method consumes approximately 1000 training samples, highlighting its efficiency and scalability.", "source": "arxiv", "arxiv_id": "2410.14890v1", "pdf_url": "https://arxiv.org/pdf/2410.14890v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-18T22:28:22Z", "updated": "2024-10-18T22:28:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve", "authors": ["Yuxiao Qu", "Tianjun Zhang", "Naman Garg", "Aviral Kumar"], "year": 2024, "url": "http://arxiv.org/abs/2407.18219v2", "abstract": "A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.", "source": "arxiv", "arxiv_id": "2407.18219v2", "pdf_url": "https://arxiv.org/pdf/2407.18219v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-07-25T17:35:59Z", "updated": "2024-07-26T17:50:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reframe Anything: LLM Agent for Open World Video Reframing", "authors": ["Jiawang Cao", "Yongliang Wu", "Weiheng Chi", "Wenbo Zhu", "Ziyue Su", "Jay Wu"], "year": 2024, "url": "http://arxiv.org/abs/2403.06070v1", "abstract": "The proliferation of mobile devices and social media has revolutionized content dissemination, with short-form video becoming increasingly prevalent. This shift has introduced the challenge of video reframing to fit various screen aspect ratios, a process that highlights the most compelling parts of a video. Traditionally, video reframing is a manual, time-consuming task requiring professional expertise, which incurs high production costs. A potential solution is to adopt some machine learning models, such as video salient object detection, to automate the process. However, these methods often lack generalizability due to their reliance on specific training data. The advent of powerful large language models (LLMs) open new avenues for AI capabilities. Building on this, we introduce Reframe Any Video Agent (RAVA), a LLM-based agent that leverages visual foundation models and human instructions to restructure visual content for video reframing. RAVA operates in three stages: perception, where it interprets user instructions and video content; planning, where it determines aspect ratios and reframing strategies; and execution, where it invokes the editing tools to produce the final video. Our experiments validate the effectiveness of RAVA in video salient object detection and real-world reframing tasks, demonstrating its potential as a tool for AI-powered video editing.", "source": "arxiv", "arxiv_id": "2403.06070v1", "pdf_url": "https://arxiv.org/pdf/2403.06070v1", "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-03-10T03:29:56Z", "updated": "2024-03-10T03:29:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reinforcement Learning for Aligning Large Language Models Agents with Interactive Environments: Quantifying and Mitigating Prompt Overfitting", "authors": ["Mohamed Salim Aissi", "Clement Romac", "Thomas Carta", "Sylvain Lamprier", "Pierre-Yves Oudeyer", "Olivier Sigaud", "Laure Soulier", "Nicolas Thome"], "year": 2024, "url": "http://arxiv.org/abs/2410.19920v3", "abstract": "Reinforcement learning (RL) is a promising approach for aligning large language models (LLMs) knowledge with sequential decision-making tasks. However, few studies have thoroughly investigated the impact on LLM agents capabilities of fine-tuning them with RL in a specific environment. In this paper, we propose a novel framework to analyze the sensitivity of LLMs to prompt formulations following RL training in a textual environment. Our findings reveal that the performance of LLMs degrades when faced with prompt formulations different from those used during the RL training phase. Besides, we analyze the source of this sensitivity by examining the model's internal representations and salient tokens. Finally, we propose to use a contrastive loss to mitigate this sensitivity and improve the robustness and generalization capabilities of LLMs.", "source": "arxiv", "arxiv_id": "2410.19920v3", "pdf_url": "https://arxiv.org/pdf/2410.19920v3", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-25T18:25:35Z", "updated": "2025-09-05T09:22:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reproducing and Extending Experiments in Behavioral Strategy with Large Language Models", "authors": ["Daniel Albert", "Stephan Billinger"], "year": 2024, "url": "http://arxiv.org/abs/2410.06932v1", "abstract": "In this study, we propose LLM agents as a novel approach in behavioral strategy research, complementing simulations and laboratory experiments to advance our understanding of cognitive processes in decision-making. Specifically, we reproduce a human laboratory experiment in behavioral strategy using large language model (LLM) generated agents and investigate how LLM agents compare to observed human behavior. Our results show that LLM agents effectively reproduce search behavior and decision-making comparable to humans. Extending our experiment, we analyze LLM agents' simulated \"thoughts,\" discovering that more forward-looking thoughts correlate with favoring exploitation over exploration to maximize wealth. We show how this new approach can be leveraged in behavioral strategy research and address limitations.", "source": "arxiv", "arxiv_id": "2410.06932v1", "pdf_url": "https://arxiv.org/pdf/2410.06932v1", "categories": ["econ.GN", "cs.AI"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2024-10-09T14:26:20Z", "updated": "2024-10-09T14:26:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for Autonomous Driving", "authors": ["Sihao Wu", "Jiaxu Liu", "Xiangyu Yin", "Guangliang Cheng", "Xingyu Zhao", "Meng Fang", "Xinping Yi", "Xiaowei Huang"], "year": 2024, "url": "http://arxiv.org/abs/2410.12568v2", "abstract": "The integration of Large Language Models (LLMs) into autonomous driving systems demonstrates strong common sense and reasoning abilities, effectively addressing the pitfalls of purely data-driven methods. Current LLM-based agents require lengthy inference times and face challenges in interacting with real-time autonomous driving environments. A key open question is whether we can effectively leverage the knowledge from LLMs to train an efficient and robust Reinforcement Learning (RL) agent. This paper introduces RAPID, a novel \\underline{\\textbf{R}}obust \\underline{\\textbf{A}}daptive \\underline{\\textbf{P}}olicy \\underline{\\textbf{I}}nfusion and \\underline{\\textbf{D}}istillation framework, which trains specialized mix-of-policy RL agents using data synthesized by an LLM-based driving agent and online adaptation. RAPID features three key designs: 1) utilization of offline data collected from an LLM agent to distil expert knowledge into RL policies for faster real-time inference; 2) introduction of robust distillation in RL to inherit both performance and robustness from LLM-based teacher; and 3) employment of a mix-of-policy approach for joint decision decoding with a policy adapter. Through fine-tuning via online environment interaction, RAPID reduces the forgetting of LLM knowledge while maintaining adaptability to different tasks. Extensive experiments demonstrate RAPID's capability to effectively integrate LLM knowledge into scaled-down RL policies in an efficient, adaptable, and robust way. Code and checkpoints will be made publicly available upon acceptance.", "source": "arxiv", "arxiv_id": "2410.12568v2", "pdf_url": "https://arxiv.org/pdf/2410.12568v2", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-10-16T13:43:00Z", "updated": "2024-10-20T04:35:34Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Rx Strategist: Prescription Verification using LLM Agents System", "authors": ["Phuc Phan Van", "Dat Nguyen Minh", "An Dinh Ngoc", "Huy Phan Thanh"], "year": 2024, "url": "http://arxiv.org/abs/2409.03440v1", "abstract": "To protect patient safety, modern pharmaceutical complexity demands strict prescription verification. We offer a new approach - Rx Strategist - that makes use of knowledge graphs and different search strategies to enhance the power of Large Language Models (LLMs) inside an agentic framework. This multifaceted technique allows for a multi-stage LLM pipeline and reliable information retrieval from a custom-built active ingredient database. Different facets of prescription verification, such as indication, dose, and possible drug interactions, are covered in each stage of the pipeline. We alleviate the drawbacks of monolithic LLM techniques by spreading reasoning over these stages, improving correctness and reliability while reducing memory demands. Our findings demonstrate that Rx Strategist surpasses many current LLMs, achieving performance comparable to that of a highly experienced clinical pharmacist. In the complicated world of modern medications, this combination of LLMs with organized knowledge and sophisticated search methods presents a viable avenue for reducing prescription errors and enhancing patient outcomes.", "source": "arxiv", "arxiv_id": "2409.03440v1", "pdf_url": "https://arxiv.org/pdf/2409.03440v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-05T11:42:26Z", "updated": "2024-09-05T11:42:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling", "authors": ["Loris Gaven", "Clement Romac", "Thomas Carta", "Sylvain Lamprier", "Olivier Sigaud", "Pierre-Yves Oudeyer"], "year": 2024, "url": "http://arxiv.org/abs/2410.12481v1", "abstract": "The past years have seen Large Language Models (LLMs) strive not only as generative models but also as agents solving textual sequential decision-making tasks. When facing complex environments where their zero-shot abilities are insufficient, recent work showed online Reinforcement Learning (RL) could be used for the LLM agent to discover and learn efficient strategies interactively. However, most prior work sticks to on-policy algorithms, which greatly reduces the scope of methods such agents could use for both exploration and exploitation, such as experience replay and hindsight relabeling. Yet, such methods may be key for LLM learning agents, and in particular when designing autonomous intrinsically motivated agents sampling and pursuing their own goals (i.e. autotelic agents). This paper presents and studies an adaptation of Soft Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves the path towards autotelic LLM agents that learn online but can also outperform on-policy methods in more classic multi-goal RL environments.", "source": "arxiv", "arxiv_id": "2410.12481v1", "pdf_url": "https://arxiv.org/pdf/2410.12481v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-16T11:59:27Z", "updated": "2024-10-16T11:59:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SAUP: Situation Awareness Uncertainty Propagation on LLM Agent", "authors": ["Qiwei Zhao", "Xujiang Zhao", "Yanchi Liu", "Wei Cheng", "Yiyou Sun", "Mika Oishi", "Takao Osaki", "Katsushi Matsuda", "Huaxiu Yao", "Haifeng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.01033v1", "abstract": "Large language models (LLMs) integrated into multistep agent systems enable complex decision-making processes across various applications. However, their outputs often lack reliability, making uncertainty estimation crucial. Existing uncertainty estimation methods primarily focus on final-step outputs, which fail to account for cumulative uncertainty over the multistep decision-making process and the dynamic interactions between agents and their environments. To address these limitations, we propose SAUP (Situation Awareness Uncertainty Propagation), a novel framework that propagates uncertainty through each step of an LLM-based agent's reasoning process. SAUP incorporates situational awareness by assigning situational weights to each step's uncertainty during the propagation. Our method, compatible with various one-step uncertainty estimation techniques, provides a comprehensive and accurate uncertainty measure. Extensive experiments on benchmark datasets demonstrate that SAUP significantly outperforms existing state-of-the-art methods, achieving up to 20% improvement in AUROC.", "source": "arxiv", "arxiv_id": "2412.01033v1", "pdf_url": "https://arxiv.org/pdf/2412.01033v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-02T01:31:13Z", "updated": "2024-12-02T01:31:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning", "authors": ["Yizhou Chi", "Yizhang Lin", "Sirui Hong", "Duyi Pan", "Yaying Fei", "Guanghao Mei", "Bangbang Liu", "Tianqi Pang", "Jacky Kwok", "Ceyao Zhang", "Bang Liu", "Chenglin Wu"], "year": 2024, "url": "http://arxiv.org/abs/2410.17238v1", "abstract": "Automated Machine Learning (AutoML) approaches encompass traditional methods that optimize fixed pipelines for model selection and ensembling, as well as newer LLM-based frameworks that autonomously build pipelines. While LLM-based agents have shown promise in automating machine learning tasks, they often generate low-diversity and suboptimal code, even after multiple iterations. To overcome these limitations, we introduce Tree-Search Enhanced LLM Agents (SELA), an innovative agent-based system that leverages Monte Carlo Tree Search (MCTS) to optimize the AutoML process. By representing pipeline configurations as trees, our framework enables agents to conduct experiments intelligently and iteratively refine their strategies, facilitating a more effective exploration of the machine learning solution space. This novel approach allows SELA to discover optimal pathways based on experimental feedback, improving the overall quality of the solutions. In an extensive evaluation across 20 machine learning datasets, we compare the performance of traditional and agent-based AutoML methods, demonstrating that SELA achieves a win rate of 65% to 80% against each baseline across all datasets. These results underscore the significant potential of agent-based strategies in AutoML, offering a fresh perspective on tackling complex machine learning challenges.", "source": "arxiv", "arxiv_id": "2410.17238v1", "pdf_url": "https://arxiv.org/pdf/2410.17238v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-22T17:56:08Z", "updated": "2024-10-22T17:56:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents", "authors": ["Dawei Li", "Zhen Tan", "Peijia Qian", "Yifan Li", "Kumar Satvik Chaudhary", "Lijie Hu", "Jiayi Shen"], "year": 2024, "url": "http://arxiv.org/abs/2411.03284v1", "abstract": "While multi-agent systems have been shown to significantly enhance the performance of Large Language Models (LLMs) across various tasks and applications, the dense interaction between scaling agents potentially hampers their efficiency and diversity. To address these challenges, we draw inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse mixture-of-agents (SMoA) framework to improve the efficiency and diversity of multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel Response Selection and Early Stopping mechanisms to sparsify information flows among individual LLM agents, striking a balance between performance and efficiency. Additionally, inspired by the expert diversity principle in SMoE frameworks for workload balance between experts, we assign distinct role descriptions to each LLM agent, fostering diverse and divergent thinking. Extensive experiments on reasoning, alignment, and fairness benchmarks demonstrate that SMoA achieves performance comparable to traditional mixture-of-agents approaches but with significantly lower computational costs. Further analysis reveals that SMoA is more stable, has a greater capacity to scale, and offers considerable potential through hyper-parameter optimization. Code and data will be available at: https://github.com/David-Li0406/SMoA.", "source": "arxiv", "arxiv_id": "2411.03284v1", "pdf_url": "https://arxiv.org/pdf/2411.03284v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-05T17:33:39Z", "updated": "2024-11-05T17:33:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents", "authors": ["Feng Lin", "Dong Jae Kim", "Tse-Husn", "Chen"], "year": 2024, "url": "http://arxiv.org/abs/2403.15852v2", "abstract": "Software process models are essential to facilitate collaboration and communication among software teams to solve complex development tasks. Inspired by these software engineering practices, we present FlowGen - a code generation framework that emulates software process models based on multiple Large Language Model (LLM) agents. We emulate three process models, FlowGenWaterfall, FlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e., requirement engineer, architect, developer, tester, and scrum master) that correspond to everyday development activities and organize their communication patterns. The agents work collaboratively using chain-of-thought and prompt composition with continuous self-refinement to improve the code quality. We use GPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion) to evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Our findings show that FlowGenScrum excels compared to other process models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement over RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum achieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming Reflexion. Notably, integrating CodeT into FlowGenScrum resulted in statistically significant improvements, achieving the highest Pass@1 scores. Our analysis also reveals that the development activities impacted code smell and exception handling differently, with design and code review adding more exception handling and reducing code smells. Finally, FlowGen models maintain stable Pass@1 scores across GPT3.5 versions and temperature values, highlighting the effectiveness of software process models in enhancing the quality and stability of LLM-generated code.", "source": "arxiv", "arxiv_id": "2403.15852v2", "pdf_url": "https://arxiv.org/pdf/2403.15852v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-03-23T14:04:48Z", "updated": "2024-10-31T14:43:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "STEER: Assessing the Economic Rationality of Large Language Models", "authors": ["Narun Raman", "Taylor Lundy", "Samuel Amouyal", "Yoav Levine", "Kevin Leyton-Brown", "Moshe Tennenholtz"], "year": 2024, "url": "http://arxiv.org/abs/2402.09552v2", "abstract": "There is increasing interest in using LLMs as decision-making \"agents.\" Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions -- and more broadly, determining whether an LLM agent is reliable enough to be trusted -- requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained \"elements\" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a \"STEER report card.\" Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models' ability to exhibit rational behavior.", "source": "arxiv", "arxiv_id": "2402.09552v2", "pdf_url": "https://arxiv.org/pdf/2402.09552v2", "categories": ["cs.CL", "econ.GN"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-14T20:05:26Z", "updated": "2024-05-28T16:27:56Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making", "authors": ["Chuanhao Li", "Runhan Yang", "Tiankai Li", "Milad Bafarassat", "Kourosh Sharifi", "Dirk Bergemann", "Zhuoran Yang"], "year": 2024, "url": "http://arxiv.org/abs/2405.16376v2", "abstract": "Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities. However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information. These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves. To overcome these obstacles, this paper presents a novel LLM agent framework equipped with memory and specialized tools to enhance their strategic decision-making capabilities. We deploy the tools in a number of economically important environments, in particular bilateral bargaining and multi-agent and dynamic mechanism design. We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems. Our findings establish that our enhanced framework significantly improves the strategic decision-making capability of LLMs. While we highlight the inherent limitations of current LLM models, we demonstrate the improvements through targeted enhancements, suggesting a promising direction for future developments in LLM applications for interactive environments.", "source": "arxiv", "arxiv_id": "2405.16376v2", "pdf_url": "https://arxiv.org/pdf/2405.16376v2", "categories": ["cs.CL", "cs.GT"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-25T23:25:10Z", "updated": "2024-05-28T01:21:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering", "authors": ["John Yang", "Carlos E. Jimenez", "Alexander Wettig", "Kilian Lieret", "Shunyu Yao", "Karthik Narasimhan", "Ofir Press"], "year": 2024, "url": "http://arxiv.org/abs/2405.15793v3", "abstract": "Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents' behavior and performance.", "source": "arxiv", "arxiv_id": "2405.15793v3", "pdf_url": "https://arxiv.org/pdf/2405.15793v3", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-05-06T17:41:33Z", "updated": "2024-11-11T20:01:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual Reality", "authors": ["Yiwen Xu", "Qinyang Hou", "Hongyu Wan", "Mirjana Prpa"], "year": 2024, "url": "http://arxiv.org/abs/2409.15623v1", "abstract": "In this paper, we present Safe Guard, an LLM-agent for the detection of hate speech in voice-based interactions in social VR (VRChat). Our system leverages Open AI GPT and audio feature extraction for real-time voice interactions. We contribute a system design and evaluation of the system that demonstrates the capability of our approach in detecting hate speech, and reducing false positives compared to currently available approaches. Our results indicate the potential of LLM-based agents in creating safer virtual environments and set the groundwork for further advancements in LLM-driven moderation approaches.", "source": "arxiv", "arxiv_id": "2409.15623v1", "pdf_url": "https://arxiv.org/pdf/2409.15623v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS", "doi": "", "venue": "", "published": "2024-09-23T23:54:45Z", "updated": "2024-09-23T23:54:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents", "authors": ["Sheng Yin", "Xianghe Pang", "Yuanzhuo Ding", "Menglan Chen", "Yutong Bi", "Yichen Xiong", "Wenhao Huang", "Zhen Xiang", "Jing Shao", "Siheng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.13178v5", "abstract": "With the integration of large language models (LLMs), embodied agents have strong capabilities to understand and plan complicated natural language instructions. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in the real world. Existing benchmarks predominantly overlook critical safety risks, focusing solely on planning performance, while a few evaluate LLMs' safety awareness only on non-interactive image-text data. To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards. SafeAgentBench includes: (1) an executable, diverse, and high-quality dataset of 750 tasks, rigorously curated to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 9 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that, although agents based on different design frameworks exhibit substantial differences in task success rates, their overall safety awareness remains weak. The most safety-conscious baseline achieves only a 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing the LLM driving the agent does not lead to notable improvements in safety awareness. Dataset and codes are available in https://github.com/shengyin1224/SafeAgentBench and https://huggingface.co/datasets/safeagentbench/SafeAgentBench.", "source": "arxiv", "arxiv_id": "2412.13178v5", "pdf_url": "https://arxiv.org/pdf/2412.13178v5", "categories": ["cs.CR", "cs.AI", "cs.RO"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-17T18:55:58Z", "updated": "2025-10-31T08:18:50Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance", "authors": ["Lucio La Cava", "Andrea Tagarelli"], "year": 2024, "url": "http://arxiv.org/abs/2409.08963v1", "abstract": "Ensuring content compliance with community guidelines is crucial for maintaining healthy online social environments. However, traditional human-based compliance checking struggles with scaling due to the increasing volume of user-generated content and a limited number of moderators. Recent advancements in Natural Language Understanding demonstrated by Large Language Models unlock new opportunities for automated content compliance verification. This work evaluates six AI-agents built on Open-LLMs for automated rule compliance checking in Decentralized Social Networks, a challenging environment due to heterogeneous community scopes and rules. Analyzing over 50,000 posts from hundreds of Mastodon servers, we find that AI-agents effectively detect non-compliant content, grasp linguistic subtleties, and adapt to diverse community contexts. Most agents also show high inter-rater reliability and consistency in score justification and suggestions for compliance. Human-based evaluation with domain experts confirmed the agents' reliability and usefulness, rendering them promising tools for semi-automated or human-in-the-loop content moderation systems.", "source": "arxiv", "arxiv_id": "2409.08963v1", "pdf_url": "https://arxiv.org/pdf/2409.08963v1", "categories": ["cs.CY", "cs.CL", "cs.HC", "physics.soc-ph"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-09-13T16:29:25Z", "updated": "2024-09-13T16:29:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code", "authors": ["Ziniu Hu", "Ahmet Iscen", "Aashi Jain", "Thomas Kipf", "Yisong Yue", "David A. Ross", "Cordelia Schmid", "Alireza Fathi"], "year": 2024, "url": "http://arxiv.org/abs/2403.01248v1", "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.", "source": "arxiv", "arxiv_id": "2403.01248v1", "pdf_url": "https://arxiv.org/pdf/2403.01248v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-03-02T16:16:26Z", "updated": "2024-03-02T16:16:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent", "authors": ["Runliang Niu", "Jindong Li", "Shiqi Wang", "Yali Fu", "Xiyu Hu", "Xueyuan Leng", "He Kong", "Yi Chang", "Qi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.07945v1", "abstract": "Existing Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing a variety of daily computer tasks. Finally, we trained a model, ScreenAgent, which achieved computer control capabilities comparable to GPT-4V and demonstrated more precise UI positioning capabilities. Our attempts could inspire further research on building a generalist LLM agent. The code is available at \\url{https://github.com/niuzaisheng/ScreenAgent}.", "source": "arxiv", "arxiv_id": "2402.07945v1", "pdf_url": "https://arxiv.org/pdf/2402.07945v1", "categories": ["cs.HC", "cs.AI", "cs.CV"], "primary_category": "cs.HC", "doi": "10.24963/ijcai.2024/711", "venue": "Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI 2024)", "published": "2024-02-09T02:33:45Z", "updated": "2024-02-09T02:33:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data", "authors": ["Junhong Shen", "Atishay Jain", "Zedian Xiao", "Ishan Amlekar", "Mouad Hadji", "Aaron Podolny", "Ameet Talwalkar"], "year": 2024, "url": "http://arxiv.org/abs/2411.15004v2", "abstract": "Large Language Model (LLM) agents are rapidly improving to handle increasingly complex web-based tasks. Most of these agents rely on general-purpose, proprietary models like GPT-4 and focus on designing better prompts to improve their planning abilities. However, general-purpose LLMs are not specifically trained to understand specialized web contexts such as HTML, and they often struggle with long-horizon planning. We explore an alternative approach that fine-tunes open-source LLMs using production-scale workflow data collected from over 250 domains corresponding to 6 billion tokens. This simple yet effective approach shows substantial gains over prompting-based agents on existing benchmarks -- ScribeAgent achieves state-of-the-art direct generation performance on Mind2Web and improves the task success rate by 7.3% over the previous best text-only web agents on WebArena. We further perform detailed ablation studies on various fine-tuning design choices and provide insights into LLM selection, training recipes, context window optimization, and effect of dataset sizes.", "source": "arxiv", "arxiv_id": "2411.15004v2", "pdf_url": "https://arxiv.org/pdf/2411.15004v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-22T15:26:23Z", "updated": "2024-12-05T02:00:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Searching for Structure: Investigating Emergent Communication with Large Language Models", "authors": ["Tom Kouwenhoven", "Max Peeperkorn", "Tessa Verhoef"], "year": 2024, "url": "http://arxiv.org/abs/2412.07646v3", "abstract": "Human languages have evolved to be structured through repeated language learning and use. These processes introduce biases that operate during language acquisition and shape linguistic systems toward communicative efficiency. In this paper, we investigate whether the same happens if artificial languages are optimised for implicit biases of Large Language Models (LLMs). To this end, we simulate a classical referential game in which LLMs learn and use artificial languages. Our results show that initially unstructured holistic languages are indeed shaped to have some structural properties that allow two LLM agents to communicate successfully. Similar to observations in human experiments, generational transmission increases the learnability of languages, but can at the same time result in non-humanlike degenerate vocabularies. Taken together, this work extends experimental findings, shows that LLMs can be used as tools in simulations of language evolution, and opens possibilities for future human-machine experiments in this field.", "source": "arxiv", "arxiv_id": "2412.07646v3", "pdf_url": "https://arxiv.org/pdf/2412.07646v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-10T16:32:19Z", "updated": "2024-12-13T12:35:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", "authors": ["Yoichi Ishibashi", "Yoshimasa Nishimura"], "year": 2024, "url": "http://arxiv.org/abs/2404.02183v1", "abstract": "Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.", "source": "arxiv", "arxiv_id": "2404.02183v1", "pdf_url": "https://arxiv.org/pdf/2404.02183v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-04-02T13:37:28Z", "updated": "2024-04-02T13:37:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Self-Reflection in LLM Agents: Effects on Problem-Solving Performance", "authors": ["Matthew Renze", "Erhan Guven"], "year": 2024, "url": "http://arxiv.org/abs/2405.06682v3", "abstract": "In this study, we investigated the effects of self-reflection in large language models (LLMs) on problem-solving performance. We instructed nine popular LLMs to answer a series of multiple-choice questions to provide a performance baseline. For each incorrectly answered question, we instructed eight types of self-reflecting LLM agents to reflect on their mistakes and provide themselves with guidance to improve problem-solving. Then, using this guidance, each self-reflecting agent attempted to re-answer the same questions. Our results indicate that LLM agents are able to significantly improve their problem-solving performance through self-reflection ($p < 0.001$). In addition, we compared the various types of self-reflection to determine their individual contribution to performance. All code and data are available on GitHub at https://github.com/matthewrenze/self-reflection", "source": "arxiv", "arxiv_id": "2405.06682v3", "pdf_url": "https://arxiv.org/pdf/2405.06682v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.1109/FLLM63129.2024.10852493", "venue": "2nd International Conference on Foundation and Large Language Models (FLLM 2024), pp. 476-483", "published": "2024-05-05T18:56:46Z", "updated": "2024-10-16T23:19:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Semantic Enhancement for Object SLAM with Heterogeneous Multimodal Large Language Model Agents", "authors": ["Jungseok Hong", "Ran Choi", "John J. Leonard"], "year": 2024, "url": "http://arxiv.org/abs/2411.06752v2", "abstract": "Object Simultaneous Localization and Mapping (SLAM) systems struggle to correctly associate semantically similar objects in close proximity, especially in cluttered indoor environments and when scenes change. We present Semantic Enhancement for Object SLAM (SEO-SLAM), a novel framework that enhances semantic mapping by integrating heterogeneous multimodal large language model (MLLM) agents. Our method enables scene adaptation while maintaining a semantically rich map. To improve computational efficiency, we propose an asynchronous processing scheme that significantly reduces the agents' inference time without compromising semantic accuracy or SLAM performance. Additionally, we introduce a multi-data association strategy using a cost matrix that combines semantic and Mahalanobis distances, formulating the problem as a Linear Assignment Problem (LAP) to alleviate perceptual aliasing. Experimental results demonstrate that SEO-SLAM consistently achieves higher semantic accuracy and reduces false positives compared to baselines, while our asynchronous MLLM agents significantly improve processing efficiency over synchronous setups. We also demonstrate that SEO-SLAM has the potential to improve downstream tasks such as robotic assistance. Our dataset is publicly available at: jungseokhong.com/SEO-SLAM.", "source": "arxiv", "arxiv_id": "2411.06752v2", "pdf_url": "https://arxiv.org/pdf/2411.06752v2", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-11-11T07:10:02Z", "updated": "2025-06-16T20:07:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents", "authors": ["Zengqing Wu", "Run Peng", "Shuyuan Zheng", "Qianying Liu", "Xu Han", "Brian Inhyuk Kwon", "Makoto Onizuka", "Shaojie Tang", "Chuan Xiao"], "year": 2024, "url": "http://arxiv.org/abs/2402.12327v3", "abstract": "Large Language Models (LLMs) have increasingly been utilized in social simulations, where they are often guided by carefully crafted instructions to stably exhibit human-like behaviors during simulations. Nevertheless, we doubt the necessity of shaping agents' behaviors for accurate social simulations. Instead, this paper emphasizes the importance of spontaneous phenomena, wherein agents deeply engage in contexts and make adaptive decisions without explicit directions. We explored spontaneous cooperation across three competitive scenarios and successfully simulated the gradual emergence of cooperation, findings that align closely with human behavioral data. This approach not only aids the computational social science community in bridging the gap between simulations and real-world dynamics but also offers the AI community a novel method to assess LLMs' capability of deliberate reasoning.", "source": "arxiv", "arxiv_id": "2402.12327v3", "pdf_url": "https://arxiv.org/pdf/2402.12327v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA", "econ.GN"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-19T18:00:53Z", "updated": "2024-10-27T19:03:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Should agentic conversational AI change how we think about ethics? Characterising an interactional ethics centred on respect", "authors": ["Lize Alberts", "Geoff Keeling", "Amanda McCroskery"], "year": 2024, "url": "http://arxiv.org/abs/2401.09082v2", "abstract": "With the growing popularity of conversational agents based on large language models (LLMs), we need to ensure their behaviour is ethical and appropriate. Work in this area largely centres around the 'HHH' criteria: making outputs more helpful and honest, and avoiding harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus is useful when viewing LLM agents as mere mediums or output-generating systems, it fails to account for pragmatic factors that can make the same speech act seem more or less tactless or inconsiderate in different social situations. With the push towards agentic AI, wherein systems become increasingly proactive in chasing goals and performing actions in the world, considering the pragmatics of interaction becomes essential. We propose an interactional approach to ethics that is centred on relational and situational factors. We explore what it means for a system, as a social actor, to treat an individual respectfully in a (series of) interaction(s). Our work anticipates a set of largely unexplored risks at the level of situated social interaction, and offers practical suggestions to help agentic LLM technologies treat people well.", "source": "arxiv", "arxiv_id": "2401.09082v2", "pdf_url": "https://arxiv.org/pdf/2401.09082v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-17T09:44:03Z", "updated": "2024-05-16T09:53:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent", "authors": ["Weizhou Shen", "Chenliang Li", "Hongzhan Chen", "Ming Yan", "Xiaojun Quan", "Hehong Chen", "Ji Zhang", "Fei Huang"], "year": 2024, "url": "http://arxiv.org/abs/2401.07324v3", "abstract": "Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "source": "arxiv", "arxiv_id": "2401.07324v3", "pdf_url": "https://arxiv.org/pdf/2401.07324v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-14T16:17:07Z", "updated": "2024-02-16T12:42:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SocraSynth: Multi-LLM Reasoning with Conditional Statistics", "authors": ["Edward Y. Chang"], "year": 2024, "url": "http://arxiv.org/abs/2402.06634v1", "abstract": "Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability. This paper introduces SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these issues. SocraSynth utilizes conditional statistics and systematic context enhancement through continuous arguments, alongside adjustable debate contentiousness levels. The platform typically involves a human moderator and two LLM agents representing opposing viewpoints on a given subject. SocraSynth operates in two main phases: knowledge generation and reasoning evaluation. In the knowledge generation phase, the moderator defines the debate topic and contentiousness level, prompting the agents to formulate supporting arguments for their respective stances. The reasoning evaluation phase then employs Socratic reasoning and formal logic principles to appraise the quality of the arguments presented. The dialogue concludes with the moderator adjusting the contentiousness from confrontational to collaborative, gathering final, conciliatory remarks to aid in human reasoning and decision-making. Through case studies in three distinct application domains, this paper showcases SocraSynth's effectiveness in fostering rigorous research, dynamic reasoning, comprehensive assessment, and enhanced collaboration. This underscores the value of multi-agent interactions in leveraging LLMs for advanced knowledge extraction and decision-making support.", "source": "arxiv", "arxiv_id": "2402.06634v1", "pdf_url": "https://arxiv.org/pdf/2402.06634v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-19T07:16:21Z", "updated": "2024-01-19T07:16:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Soft Self-Consistency Improves Language Model Agents", "authors": ["Han Wang", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "year": 2024, "url": "http://arxiv.org/abs/2402.13212v2", "abstract": "Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current \"sample and select\" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (SOFT-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. SOFT-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, SOFT-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that SOFT-SC can be applied to both open-source and black-box models.", "source": "arxiv", "arxiv_id": "2402.13212v2", "pdf_url": "https://arxiv.org/pdf/2402.13212v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-20T18:22:38Z", "updated": "2024-06-05T19:50:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Sparse Rewards Can Self-Train Dialogue Agents", "authors": ["Barrett Martin Lattimer", "Varun Gangal", "Ryan McDonald", "Yi Yang"], "year": 2024, "url": "http://arxiv.org/abs/2409.04617v3", "abstract": "Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM) agents, especially in multi-turn dialogue tasks, have been primarily driven by supervised fine-tuning and high-quality human feedback. However, as base LLM models continue to improve, acquiring meaningful human feedback has become increasingly challenging and costly. In certain domains, base LLM agents may eventually exceed human capabilities, making traditional feedback-driven methods impractical. In this paper, we introduce a novel self-improvement paradigm that empowers LLM agents to autonomously enhance their performance without external human feedback. Our method, Juxtaposed Outcomes for Simulation Harvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward simulation environment to extract ideal behaviors and further train the LLM on its own outputs. We present ToolWOZ, a sparse reward tool-calling simulation environment derived from MultiWOZ. We demonstrate that models trained with JOSH, both small and frontier, significantly improve tool-based interactions while preserving general model capabilities across diverse benchmarks. Our code and data are publicly available on GitHub at https://github.com/asappresearch/josh-llm-simulation-training", "source": "arxiv", "arxiv_id": "2409.04617v3", "pdf_url": "https://arxiv.org/pdf/2409.04617v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-06T21:00:57Z", "updated": "2025-07-18T17:06:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SpecRover: Code Intent Extraction via LLMs", "authors": ["Haifeng Ruan", "Yuntong Zhang", "Abhik Roychoudhury"], "year": 2024, "url": "http://arxiv.org/abs/2408.02232v4", "abstract": "Autonomous program improvement typically involves automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent AutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50% improvement in efficacy over AutoCodeRover. Compared to the open-source agents available, our work shows modest cost ($0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite. The production of explanation by SpecRover allows for a better \"signal\" to be given to the developer, on when the suggested patches can be accepted with confidence. SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era.", "source": "arxiv", "arxiv_id": "2408.02232v4", "pdf_url": "https://arxiv.org/pdf/2408.02232v4", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-08-05T04:53:01Z", "updated": "2024-12-11T11:18:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities", "authors": ["Ryosuke Takata", "Atsushi Masumori", "Takashi Ikegami"], "year": 2024, "url": "http://arxiv.org/abs/2411.03252v1", "abstract": "We study the emergence of agency from scratch by using Large Language Model (LLM)-based agents. In previous studies of LLM-based agents, each agent's characteristics, including personality and memory, have traditionally been predefined. We focused on how individuality, such as behavior, personality, and memory, can be differentiated from an undifferentiated state. The present LLM agents engage in cooperative communication within a group simulation, exchanging context-based messages in natural language. By analyzing this multi-agent simulation, we report valuable new insights into how social norms, cooperation, and personality traits can emerge spontaneously. This paper demonstrates that autonomously interacting LLM-powered agents generate hallucinations and hashtags to sustain communication, which, in turn, increases the diversity of words within their interactions. Each agent's emotions shift through communication, and as they form communities, the personalities of the agents emerge and evolve accordingly. This computational modeling approach and its findings will provide a new method for analyzing collective artificial intelligence.", "source": "arxiv", "arxiv_id": "2411.03252v1", "pdf_url": "https://arxiv.org/pdf/2411.03252v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-05T16:49:33Z", "updated": "2024-11-05T16:49:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning", "authors": ["Hang Zhou", "Yehui Tang", "Haochen Qin", "Yujie Yang", "Renren Jin", "Deyi Xiong", "Kai Han", "Yunhe Wang"], "year": 2024, "url": "http://arxiv.org/abs/2411.14497v1", "abstract": "The efficacy of large language models (LLMs) on downstream tasks usually hinges on instruction tuning, which relies critically on the quality of training data. Unfortunately, collecting high-quality and diverse data is both expensive and time-consuming. To mitigate this issue, we propose a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment. The framework adopts a three-pronged strategy. It initially generates diverse instruction data with multiple LLM agents through a bespoke sampling method. Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality. Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality. Our empirical studies, including instruction tuning experiments with models such as Pythia and LLaMA, demonstrate the effectiveness of the proposed framework. Optimized datasets have achieved substantial improvements, with an average increase of 12% and notable gains in specific metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset.", "source": "arxiv", "arxiv_id": "2411.14497v1", "pdf_url": "https://arxiv.org/pdf/2411.14497v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-21T02:30:53Z", "updated": "2024-11-21T02:30:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking", "authors": ["Nikolai Rozanov", "Marek Rei"], "year": 2024, "url": "http://arxiv.org/abs/2410.02810v3", "abstract": "Large language models (LLMs) are increasingly used as autonomous agents, tackling tasks from robotics to web navigation. Their performance depends on the underlying base agent. Existing methods, however, struggle with long-context reasoning and goal adherence. We introduce StateAct, a novel and efficient base agent that enhances decision-making through (1) self-prompting, which reinforces task goals at every step, and (2) chain-of-states, an extension of chain-of-thought that tracks state information over time. StateAct outperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30% on Textcraft, and 7% on Webshop across multiple frontier LLMs. We also demonstrate that StateAct can be used as a drop-in replacement for ReAct with advanced LLM agent methods such as test-time scaling, yielding an additional 12% gain on Textcraft. By improving efficiency and long-range reasoning without requiring additional training or retrieval, StateAct provides a scalable foundation for LLM agents. We open source our code to support further research at https://github.com/ai-nikolai/stateact .", "source": "arxiv", "arxiv_id": "2410.02810v3", "pdf_url": "https://arxiv.org/pdf/2410.02810v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-21T05:54:35Z", "updated": "2025-04-08T06:37:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Static network structure cannot stabilize cooperation among Large Language Model agents", "authors": ["Jin Han", "Balaraju Battu", "Ivan RomiÄ", "Talal Rahwan", "Petter Holme"], "year": 2024, "url": "http://arxiv.org/abs/2411.10294v1", "abstract": "Large language models (LLMs) are increasingly used to model human social behavior, with recent research exploring their ability to simulate social dynamics. Here, we test whether LLMs mirror human behavior in social dilemmas, where individual and collective interests conflict. Humans generally cooperate more than expected in laboratory settings, showing less cooperation in well-mixed populations but more in fixed networks. In contrast, LLMs tend to exhibit greater cooperation in well-mixed settings. This raises a key question: Are LLMs about to emulate human behavior in cooperative dilemmas on networks? In this study, we examine networked interactions where agents repeatedly engage in the Prisoner's Dilemma within both well-mixed and structured network configurations, aiming to identify parallels in cooperative behavior between LLMs and humans. Our findings indicate critical distinctions: while humans tend to cooperate more within structured networks, LLMs display increased cooperation mainly in well-mixed environments, with limited adjustment to networked contexts. Notably, LLM cooperation also varies across model types, illustrating the complexities of replicating human-like social adaptability in artificial agents. These results highlight a crucial gap: LLMs struggle to emulate the nuanced, adaptive social strategies humans deploy in fixed networks. Unlike human participants, LLMs do not alter their cooperative behavior in response to network structures or evolving social contexts, missing the reciprocity norms that humans adaptively employ. This limitation points to a fundamental need in future LLM design -- to integrate a deeper comprehension of social norms, enabling more authentic modeling of human-like cooperation and adaptability in networked environments.", "source": "arxiv", "arxiv_id": "2411.10294v1", "pdf_url": "https://arxiv.org/pdf/2411.10294v1", "categories": ["cs.SI", "cs.CY", "cs.GT", "physics.soc-ph"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-11-15T15:52:15Z", "updated": "2024-11-15T15:52:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions", "authors": ["Ryan Y. Lin", "Siddhartha Ojha", "Kevin Cai", "Maxwell F. Chen"], "year": 2024, "url": "http://arxiv.org/abs/2410.00031v2", "abstract": "Machine-learning technologies are seeing increased deployment in real-world market scenarios. In this work, we explore the strategic behaviors of large language models (LLMs) when deployed as autonomous agents in multi-commodity markets, specifically within Cournot competition frameworks. We examine whether LLMs can independently engage in anti-competitive practices such as collusion or, more specifically, market division. Our findings demonstrate that LLMs can effectively monopolize specific commodities by dynamically adjusting their pricing and resource allocation strategies, thereby maximizing profitability without direct human input or explicit collusion commands. These results pose unique challenges and opportunities for businesses looking to integrate AI into strategic roles and for regulatory bodies tasked with maintaining fair and competitive markets. The study provides a foundation for further exploration into the ramifications of deferring high-stakes decisions to LLM-based agents.", "source": "arxiv", "arxiv_id": "2410.00031v2", "pdf_url": "https://arxiv.org/pdf/2410.00031v2", "categories": ["cs.GT", "cs.AI", "cs.CL", "q-fin.CP"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2024-09-19T20:10:40Z", "updated": "2025-05-16T10:05:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search", "authors": ["Jonathan Light", "Min Cai", "Weiqin Chen", "Guanzhi Wang", "Xiusi Chen", "Wei Cheng", "Yisong Yue", "Ziniu Hu"], "year": 2024, "url": "http://arxiv.org/abs/2408.10635v3", "abstract": "Traditional reinforcement learning and planning typically requires vast amounts of data and training to develop effective policies. In contrast, large language models (LLMs) exhibit strong generalization and zero-shot capabilities, but struggle with tasks that require detailed planning and decision-making in complex action spaces. We introduce STRATEGIST, a novel approach that integrates the strengths of both methods. Our approach leverages LLMs to search and update high-level strategies (as text), which are then refined and executed by low-level Monte Carlo Tree Search (MCTS). STRATEGIST is a generalizable framework to optimize the strategy through population-based self-play simulations without the need for any training data. We demonstrate the effectiveness of STRATEGIST in learning optimal strategies for competitive, multi-turn games with partial information, including Game of Pure Strategy (GOPS) and multi-agent, hidden-identity discussion games like The Resistance: Avalon. Our results show that agents equipped with STRATEGIST outperform those trained with traditional RL methods, other LLM-based skill acquisition techniques, pre-existing LLM agents across both game environments and achieves comparable performance against human players.", "source": "arxiv", "arxiv_id": "2408.10635v3", "pdf_url": "https://arxiv.org/pdf/2408.10635v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-20T08:22:04Z", "updated": "2025-07-29T09:42:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "StreamBench: Towards Benchmarking Continuous Improvement of Language Agents", "authors": ["Cheng-Kuang Wu", "Zhi Rui Tam", "Chieh-Yen Lin", "Yun-Nung Chen", "Hung-yi Lee"], "year": 2024, "url": "http://arxiv.org/abs/2406.08747v2", "abstract": "Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios. Source code: https://github.com/stream-bench/stream-bench. Benchmark website: https://stream-bench.github.io.", "source": "arxiv", "arxiv_id": "2406.08747v2", "pdf_url": "https://arxiv.org/pdf/2406.08747v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-13T02:08:28Z", "updated": "2024-10-31T03:16:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SwissNYF: Tool Grounded LLM Agents for Black Box Setting", "authors": ["Somnath Sendhil Kumar", "Dhruv Jain", "Eshaan Agarwal", "Raunak Pandey"], "year": 2024, "url": "http://arxiv.org/abs/2402.10051v1", "abstract": "While Large Language Models (LLMs) have demonstrated enhanced capabilities in function-calling, these advancements primarily rely on accessing the functions' responses. This methodology is practical for simpler APIs but faces scalability issues with irreversible APIs that significantly impact the system, such as a database deletion API. Similarly, processes requiring extensive time for each API call and those necessitating forward planning, like automated action pipelines, present complex challenges. Furthermore, scenarios often arise where a generalized approach is needed because algorithms lack direct access to the specific implementations of these functions or secrets to use them. Traditional tool planning methods are inadequate in these cases, compelling the need to operate within black-box environments. Unlike their performance in tool manipulation, LLMs excel in black-box tasks, such as program synthesis. Therefore, we harness the program synthesis capabilities of LLMs to strategize tool usage in black-box settings, ensuring solutions are verified prior to implementation. We introduce TOPGUN, an ingeniously crafted approach leveraging program synthesis for black box tool planning. Accompanied by SwissNYF, a comprehensive suite that integrates black-box algorithms for planning and verification tasks, addressing the aforementioned challenges and enhancing the versatility and effectiveness of LLMs in complex API interactions. The public code for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.", "source": "arxiv", "arxiv_id": "2402.10051v1", "pdf_url": "https://arxiv.org/pdf/2402.10051v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-15T16:15:38Z", "updated": "2024-02-15T16:15:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction in LBSN", "authors": ["Zhilun Zhou", "Jingyang Fan", "Yu Liu", "Fengli Xu", "Depeng Jin", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2411.00028v2", "abstract": "The fast development of location-based social networks (LBSNs) has led to significant changes in society, resulting in popular studies of using LBSN data for socioeconomic prediction, e.g., regional population and commercial activity estimation. Existing studies design various graphs to model heterogeneous LBSN data, and further apply graph representation learning methods for socioeconomic prediction. However, these approaches heavily rely on heuristic ideas and expertise to extract task-relevant knowledge from diverse data, which may not be optimal for specific tasks. Additionally, they tend to overlook the inherent relationships between different indicators, limiting the prediction accuracy. Motivated by the remarkable abilities of large language models (LLMs) in commonsense reasoning, embedding, and multi-agent collaboration, in this work, we synergize LLM agents and knowledge graph for socioeconomic prediction. We first construct a location-based knowledge graph (LBKG) to integrate multi-sourced LBSN data. Then we leverage the reasoning power of LLM agent to identify relevant meta-paths in the LBKG for each type of socioeconomic prediction task, and design a semantic-guided attention module for knowledge fusion with meta-paths. Moreover, we introduce a cross-task communication mechanism to further enhance performance by enabling knowledge sharing across tasks at both LLM agent and KG levels. On the one hand, the LLM agents for different tasks collaborate to generate more diverse and comprehensive meta-paths. On the other hand, the embeddings from different tasks are adaptively merged for better socioeconomic prediction. Experiments on two datasets demonstrate the effectiveness of the synergistic design between LLM and KG, providing insights for information sharing across socioeconomic prediction tasks.", "source": "arxiv", "arxiv_id": "2411.00028v2", "pdf_url": "https://arxiv.org/pdf/2411.00028v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-29T04:03:15Z", "updated": "2024-11-19T14:29:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Synthetic Dialogue Dataset Generation using LLM Agents", "authors": ["Yelaman Abdullin", "Diego Molla-Aliod", "Bahadorreza Ofoghi", "John Yearwood", "Qingyang Li"], "year": 2024, "url": "http://arxiv.org/abs/2401.17461v1", "abstract": "Linear programming (LP) problems are pervasive in real-life applications. However, despite their apparent simplicity, an untrained user may find it difficult to determine the linear model of their specific problem. We envisage the creation of a goal-oriented conversational agent that will engage in conversation with the user to elicit all information required so that a subsequent agent can generate the linear model. In this paper, we present an approach for the generation of sample dialogues that can be used to develop and train such a conversational agent. Using prompt engineering, we develop two agents that \"talk\" to each other, one acting as the conversational agent, and the other acting as the user. Using a set of text descriptions of linear problems from NL4Opt available to the user only, the agent and the user engage in conversation until the agent has retrieved all key information from the original problem description. We also propose an extrinsic evaluation of the dialogues by assessing how well the summaries generated by the dialogues match the original problem descriptions. We conduct human and automatic evaluations, including an evaluation approach that uses GPT-4 to mimic the human evaluation metrics. The evaluation results show an overall good quality of the dialogues, though research is still needed to improve the quality of the GPT-4 evaluation metrics. The resulting dialogues, including the human annotations of a subset, are available to the research community. The conversational agent used for the generation of the dialogues can be used as a baseline.", "source": "arxiv", "arxiv_id": "2401.17461v1", "pdf_url": "https://arxiv.org/pdf/2401.17461v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-30T21:49:30Z", "updated": "2024-01-30T21:49:30Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision", "authors": ["Ruiwen Zhou", "Yingxuan Yang", "Muning Wen", "Ying Wen", "Wenhao Wang", "Chunling Xi", "Guoqiang Xu", "Yong Yu", "Weinan Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.06221v1", "abstract": "Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration selection via thought matching, leading to more helpful demonstrations and less irrelevant input noise. Then, TRAD introduces Aligned Decision, complementing retrieved demonstration steps with their previous or subsequent steps, which enables tolerance for imperfect thought and provides a choice for balance between more context and less noise. Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not only outperforms state-of-the-art models but also effectively helps in reducing noise and promoting generalization. Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and improves the success rate of robotic process automation.", "source": "arxiv", "arxiv_id": "2403.06221v1", "pdf_url": "https://arxiv.org/pdf/2403.06221v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-10T13:58:38Z", "updated": "2024-03-10T13:58:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TapeAgents: a Holistic Framework for Agent Development and Optimization", "authors": ["Dzmitry Bahdanau", "Nicolas Gontier", "Gabriel Huang", "Ehsan Kamalloo", "Rafael Pardinas", "Alex PichÃ©", "Torsten Scholak", "Oleh Shliazhko", "Jordan Prince Tremblay", "Karam Ghanem", "Soham Parikh", "Mitul Tiwari", "Quaizar Vohra"], "year": 2024, "url": "http://arxiv.org/abs/2412.08445v1", "abstract": "We present TapeAgents, an agent framework built around a granular, structured log tape of the agent session that also plays the role of the session's resumable state. In TapeAgents we leverage tapes to facilitate all stages of the LLM Agent development lifecycle. The agent reasons by processing the tape and the LLM output to produce new thought and action steps and append them to the tape. The environment then reacts to the agent's actions by likewise appending observation steps to the tape. By virtue of this tape-centred design, TapeAgents can provide AI practitioners with holistic end-to-end support. At the development stage, tapes facilitate session persistence, agent auditing, and step-by-step debugging. Post-deployment, one can reuse tapes for evaluation, fine-tuning, and prompt-tuning; crucially, one can adapt tapes from other agents or use revised historical tapes. In this report, we explain the TapeAgents design in detail. We demonstrate possible applications of TapeAgents with several concrete examples of building monolithic agents and multi-agent teams, of optimizing agent prompts and finetuning the agent's LLM. We present tooling prototypes and report a case study where we use TapeAgents to finetune a Llama-3.1-8B form-filling assistant to perform as well as GPT-4o while being orders of magnitude cheaper. Lastly, our comparative analysis shows that TapeAgents's advantages over prior frameworks stem from our novel design of the LLM agent as a resumable, modular state machine with a structured configuration, that generates granular, structured logs and that can transform these logs into training text -- a unique combination of features absent in previous work.", "source": "arxiv", "arxiv_id": "2412.08445v1", "pdf_url": "https://arxiv.org/pdf/2412.08445v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-11T15:09:54Z", "updated": "2024-12-11T15:09:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents", "authors": ["Jinyang Li", "Nan Huo", "Yan Gao", "Jiayi Shi", "Yingxiu Zhao", "Ge Qu", "Yurong Wu", "Chenhao Ma", "Jian-Guang Lou", "Reynold Cheng"], "year": 2024, "url": "http://arxiv.org/abs/2403.05307v1", "abstract": "Interactive Data Analysis, the collaboration between humans and LLM agents, enables real-time data exploration for informed decision-making. The challenges and costs of collecting realistic interactive logs for data analysis hinder the quantitative evaluation of Large Language Model (LLM) agents in this task. To mitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate LLM agents on interactive data analysis. Tapilot-Crossing contains 1024 interactions, covering 4 practical scenarios: Normal, Action, Private, and Private Action. Notably, Tapilot-Crossing is constructed by an economical multi-agent environment, Decision Company, with few human efforts. We evaluate popular and advanced LLM agents in Tapilot-Crossing, which underscores the challenges of interactive data analysis. Furthermore, we propose Adaptive Interaction Reflection (AIR), a self-generated reflection strategy that guides LLM agents to learn from successful history. Experiments demonstrate that Air can evolve LLMs into effective interactive data analysis agents, achieving a relative performance improvement of up to 44.5%.", "source": "arxiv", "arxiv_id": "2403.05307v1", "pdf_url": "https://arxiv.org/pdf/2403.05307v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-08T13:34:20Z", "updated": "2024-03-08T13:34:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities", "authors": ["Yuxuan Zhu", "Antony Kellermann", "Akul Gupta", "Philip Li", "Richard Fang", "Rohan Bindu", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2406.01637v2", "abstract": "LLM agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that LLM agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce HPTSA, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 14 real-world vulnerabilities and show that our team of agents improve over prior agent frameworks by up to 4.3X.", "source": "arxiv", "arxiv_id": "2406.01637v2", "pdf_url": "https://arxiv.org/pdf/2406.01637v2", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-06-02T16:25:26Z", "updated": "2025-03-30T00:26:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Testing Uncertainty of Large Language Models for Physics Knowledge and Reasoning", "authors": ["Elizaveta Reganova", "Peter Steinbach"], "year": 2024, "url": "http://arxiv.org/abs/2411.14465v1", "abstract": "Large Language Models (LLMs) have gained significant popularity in recent years for their ability to answer questions in various fields. However, these models have a tendency to \"hallucinate\" their responses, making it challenging to evaluate their performance. A major challenge is determining how to assess the certainty of a model's predictions and how it correlates with accuracy. In this work, we introduce an analysis for evaluating the performance of popular open-source LLMs, as well as gpt-3.5 Turbo, on multiple choice physics questionnaires. We focus on the relationship between answer accuracy and variability in topics related to physics. Our findings suggest that most models provide accurate replies in cases where they are certain, but this is by far not a general behavior. The relationship between accuracy and uncertainty exposes a broad horizontal bell-shaped distribution. We report how the asymmetry between accuracy and uncertainty intensifies as the questions demand more logical reasoning of the LLM agent, while the same relationship remains sharp for knowledge retrieval tasks.", "source": "arxiv", "arxiv_id": "2411.14465v1", "pdf_url": "https://arxiv.org/pdf/2411.14465v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-18T13:42:13Z", "updated": "2024-11-18T13:42:13Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs", "authors": ["Zhenlan Ji", "Daoyuan Wu", "Pingchuan Ma", "Zongjie Li", "Shuai Wang"], "year": 2024, "url": "http://arxiv.org/abs/2404.17833v1", "abstract": "Agents based on large language models (LLMs) have demonstrated effectiveness in solving a wide range of tasks by integrating LLMs with key modules such as planning, memory, and tool usage. Increasingly, customers are adopting LLM agents across a variety of commercial applications critical to reliability, including support for mental well-being, chemical synthesis, and software development. Nevertheless, our observations and daily use of LLM agents indicate that they are prone to making erroneous plans, especially when the tasks are complex and require long-term planning.\n  In this paper, we propose PDoctor, a novel and automated approach to testing LLM agents and understanding their erroneous planning. As the first work in this direction, we formulate the detection of erroneous planning as a constraint satisfiability problem: an LLM agent's plan is considered erroneous if its execution violates the constraints derived from the user inputs. To this end, PDoctor first defines a domain-specific language (DSL) for user queries and synthesizes varying inputs with the assistance of the Z3 constraint solver. These synthesized inputs are natural language paragraphs that specify the requirements for completing a series of tasks. Then, PDoctor derives constraints from these requirements to form a testing oracle. We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4). The results show that PDoctor can effectively detect diverse errors in agent planning and provide insights and error characteristics that are valuable to both agent developers and users. We conclude by discussing potential alternative designs and directions to extend PDoctor.", "source": "arxiv", "arxiv_id": "2404.17833v1", "pdf_url": "https://arxiv.org/pdf/2404.17833v1", "categories": ["cs.AI", "cs.PL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-27T08:56:45Z", "updated": "2024-04-27T08:56:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework", "authors": ["Changyu Du", "Sebastian Esser", "Stavros Nousias", "AndrÃ© Borrmann"], "year": 2024, "url": "http://arxiv.org/abs/2408.08054v2", "abstract": "The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands in order to materialize their design intentions within BIM authoring tools. This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (Architecture, Engineering, and Construction) industry. To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multi-agent framework that can generate 3D building models from natural language instructions. This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool's APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software. Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality. Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework. The evaluation results demonstrate that our approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input. Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting. The code is available at: https://github.com/dcy0577/Text2BIM", "source": "arxiv", "arxiv_id": "2408.08054v2", "pdf_url": "https://arxiv.org/pdf/2408.08054v2", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-15T09:48:45Z", "updated": "2025-07-11T14:40:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Drama Machine: Simulating Character Development with LLM Agents", "authors": ["Liam Magee", "Vanicka Arora", "Gus Gollings", "Norma Lam-Saw"], "year": 2024, "url": "http://arxiv.org/abs/2408.01725v2", "abstract": "This paper explores use of multiple large language model (LLM) agents to simulate complex, dynamic characters in dramatic scenarios. We introduce a drama machine framework that coordinates interactions between LLM agents playing different 'Ego' and 'Superego' psychological roles. In roleplay simulations, this design allows intersubjective dialogue and intra-subjective internal monologue to develop in parallel. We apply this framework to two dramatic scenarios - an interview and a detective story - and compare character development with and without the Superego's influence. Though exploratory, results suggest this multi-agent approach can produce more nuanced, adaptive narratives that evolve over a sequence of dialogical turns. We discuss different modalities of LLM-based roleplay and character development, along with what this might mean for conceptualization of AI subjectivity. The paper concludes by considering how this approach opens possibilities for thinking of the roles of internal conflict and social performativity in AI-based simulation.", "source": "arxiv", "arxiv_id": "2408.01725v2", "pdf_url": "https://arxiv.org/pdf/2408.01725v2", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-08-03T09:40:26Z", "updated": "2024-08-31T04:27:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies", "authors": ["Feng He", "Tianqing Zhu", "Dayong Ye", "Bo Liu", "Wanlei Zhou", "Philip S. Yu"], "year": 2024, "url": "http://arxiv.org/abs/2407.19354v2", "abstract": "Inspired by the rapid development of Large Language Models (LLMs), LLM agents have evolved to perform complex tasks. LLM agents are now extensively applied across various domains, handling vast amounts of data to interact with humans and execute tasks. The widespread applications of LLM agents demonstrate their significant commercial value; however, they also expose security and privacy vulnerabilities. At the current stage, comprehensive research on the security and privacy of LLM agents is highly needed. This survey aims to provide a comprehensive overview of the newly emerged privacy and security issues faced by LLM agents. We begin by introducing the fundamental knowledge of LLM agents, followed by a categorization and analysis of the threats. We then discuss the impacts of these threats on humans, environment, and other agents. Subsequently, we review existing defensive strategies, and finally explore future trends. Additionally, the survey incorporates diverse case studies to facilitate a more accessible understanding. By highlighting these critical security and privacy issues, the survey seeks to stimulate future research towards enhancing the security and privacy of LLM agents, thereby increasing their reliability and trustworthiness in future applications.", "source": "arxiv", "arxiv_id": "2407.19354v2", "pdf_url": "https://arxiv.org/pdf/2407.19354v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "10.1145/3773080", "venue": "", "published": "2024-07-28T00:26:24Z", "updated": "2025-11-02T06:38:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Future of Scientific Publishing: Automated Article Generation", "authors": ["Jeremy R. Harper"], "year": 2024, "url": "http://arxiv.org/abs/2404.17586v1", "abstract": "This study introduces a novel software tool leveraging large language model (LLM) prompts, designed to automate the generation of academic articles from Python code a significant advancement in the fields of biomedical informatics and computer science. Selected for its widespread adoption and analytical versatility, Python served as a foundational proof of concept; however, the underlying methodology and framework exhibit adaptability across various GitHub repo's underlining the tool's broad applicability (Harper 2024). By mitigating the traditionally time-intensive academic writing process, particularly in synthesizing complex datasets and coding outputs, this approach signifies a monumental leap towards streamlining research dissemination. The development was achieved without reliance on advanced language model agents, ensuring high fidelity in the automated generation of coherent and comprehensive academic content. This exploration not only validates the successful application and efficiency of the software but also projects how future integration of LLM agents which could amplify its capabilities, propelling towards a future where scientific findings are disseminated more swiftly and accessibly.", "source": "arxiv", "arxiv_id": "2404.17586v1", "pdf_url": "https://arxiv.org/pdf/2404.17586v1", "categories": ["cs.HC", "cs.AI", "cs.ET"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-04-11T16:47:02Z", "updated": "2024-04-11T16:47:02Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Stepwise Deception: Simulating the Evolution from True News to Fake News with LLM Agents", "authors": ["Yuhan Liu", "Zirui Song", "Juntian Zhang", "Xiaoqing Zhang", "Xiuying Chen", "Rui Yan"], "year": 2024, "url": "http://arxiv.org/abs/2410.19064v2", "abstract": "With the growing spread of misinformation online, understanding how true news evolves into fake news has become crucial for early detection and prevention. However, previous research has often assumed fake news inherently exists rather than exploring its gradual formation. To address this gap, we propose FUSE (Fake news evolUtion Simulation framEwork), a novel Large Language Model (LLM)-based simulation approach explicitly focusing on fake news evolution from real news. Our framework model a social network with four distinct types of LLM agents commonly observed in daily interactions: spreaders who propagate information, commentators who provide interpretations, verifiers who fact-check, and bystanders who observe passively to simulate realistic daily interactions that progressively distort true news. To quantify these gradual distortions, we develop FUSE-EVAL, a comprehensive evaluation framework measuring truth deviation along multiple linguistic and semantic dimensions. Results show that FUSE effectively captures fake news evolution patterns and accurately reproduces known fake news, aligning closely with human evaluations. Experiments demonstrate that FUSE accurately reproduces known fake news evolution scenarios, aligns closely with human judgment, and highlights the importance of timely intervention at early stages. Our framework is extensible, enabling future research on broader scenarios of fake news.", "source": "arxiv", "arxiv_id": "2410.19064v2", "pdf_url": "https://arxiv.org/pdf/2410.19064v2", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-10-24T18:17:16Z", "updated": "2025-05-28T08:26:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents", "authors": ["Feiran Jia", "Tong Wu", "Xin Qin", "Anna Squicciarini"], "year": 2024, "url": "http://arxiv.org/abs/2412.16682v1", "abstract": "Large Language Model (LLM) agents are increasingly being deployed as conversational assistants capable of performing complex real-world tasks through tool integration. This enhanced ability to interact with external systems and process various data sources, while powerful, introduces significant security vulnerabilities. In particular, indirect prompt injection attacks pose a critical threat, where malicious instructions embedded within external data sources can manipulate agents to deviate from user intentions. While existing defenses based on rule constraints, source spotlighting, and authentication protocols show promise, they struggle to maintain robust security while preserving task functionality. We propose a novel and orthogonal perspective that reframes agent security from preventing harmful actions to ensuring task alignment, requiring every agent action to serve user objectives. Based on this insight, we develop Task Shield, a test-time defense mechanism that systematically verifies whether each instruction and tool call contributes to user-specified goals. Through experiments on the AgentDojo benchmark, we demonstrate that Task Shield reduces attack success rates (2.07\\%) while maintaining high task utility (69.79\\%) on GPT-4o.", "source": "arxiv", "arxiv_id": "2412.16682v1", "pdf_url": "https://arxiv.org/pdf/2412.16682v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-21T16:17:48Z", "updated": "2024-12-21T16:17:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "authors": ["Frank F. Xu", "Yufan Song", "Boxuan Li", "Yuxuan Tang", "Kritanjali Jain", "Mengxue Bao", "Zora Z. Wang", "Xuhui Zhou", "Zhitong Guo", "Murong Cao", "Mingyang Yang", "Hao Yang Lu", "Amaad Martin", "Zhe Su", "Leander Maben", "Raj Mehta", "Wayne Chi", "Lawrence Jang", "Yiqing Xie", "Shuyan Zhou", "Graham Neubig"], "year": 2024, "url": "http://arxiv.org/abs/2412.14161v3", "abstract": "We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt AI into their workflows and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that the most competitive agent can complete 30% of tasks autonomously. This paints a nuanced picture on task automation with LM agents--in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments on https://the-agent-company.com.", "source": "arxiv", "arxiv_id": "2412.14161v3", "pdf_url": "https://arxiv.org/pdf/2412.14161v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-18T18:55:40Z", "updated": "2025-09-10T08:35:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TinyAgent: Function Calling at the Edge", "authors": ["Lutfi Eren Erdogan", "Nicholas Lee", "Siddharth Jha", "Sehoon Kim", "Ryan Tabrizi", "Suhong Moon", "Coleman Hooper", "Gopala Anumanchipalli", "Kurt Keutzer", "Amir Gholami"], "year": 2024, "url": "http://arxiv.org/abs/2409.00608v3", "abstract": "Recent large language models (LLMs) have enabled the development of advanced agentic systems that can integrate various tools and APIs to fulfill user queries through function calling. However, the deployment of these LLMs on the edge has not been explored since they typically require cloud-based infrastructure due to their substantial model size and computational demands. To this end, we present TinyAgent, an end-to-end framework for training and deploying task-specific small language model agents capable of function calling for driving agentic systems at the edge. We first show how to enable accurate function calling for open-source models via the LLMCompiler framework. We then systematically curate a high-quality dataset for function calling, which we use to fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient inference, we introduce a novel tool retrieval method to reduce the input prompt length and utilize quantization to further accelerate the inference speed. As a driving application, we demonstrate a local Siri-like system for Apple's MacBook that can execute user commands through text or voice input. Our results show that our models can achieve, and even surpass, the function-calling capabilities of larger models like GPT-4-Turbo, while being fully deployed at the edge. We open-source our dataset, models, and installable package and provide a demo video for our MacBook assistant agent.", "source": "arxiv", "arxiv_id": "2409.00608v3", "pdf_url": "https://arxiv.org/pdf/2409.00608v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-01T04:23:48Z", "updated": "2024-10-25T01:16:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent", "authors": ["Shanbo Cheng", "Zhichao Huang", "Tom Ko", "Hang Li", "Ningxin Peng", "Lu Xu", "Qini Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2407.21646v2", "abstract": "In this paper, we present Cross Language Agent -- Simultaneous Interpretation, CLASI, a high-quality and human-like Simultaneous Speech Translation (SiST) System. Inspired by professional human interpreters, we utilize a novel data-driven read-write strategy to balance the translation quality and latency. To address the challenge of translating in-domain terminologies, CLASI employs a multi-modal retrieving module to obtain relevant information to augment the translation. Supported by LLMs, our approach can generate error-tolerated translation by considering the input audio, historical context, and retrieved information. Experimental results show that our system outperforms other systems by significant margins. Aligned with professional human interpreters, we evaluate CLASI with a better human evaluation metric, valid information proportion (VIP), which measures the amount of information that can be successfully conveyed to the listeners. In the real-world scenarios, where the speeches are often disfluent, informal, and unclear, CLASI achieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese translation directions, respectively. In contrast, state-of-the-art commercial or open-source systems only achieve 35.4% and 41.6%. On the extremely hard dataset, where other systems achieve under 13% VIP, CLASI can still achieve 70% VIP.", "source": "arxiv", "arxiv_id": "2407.21646v2", "pdf_url": "https://arxiv.org/pdf/2407.21646v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-31T14:48:27Z", "updated": "2024-08-30T06:50:51Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Agentic Schema Refinement", "authors": ["Agapi Rissaki", "Ilias Fountalis", "Nikolaos Vasiloglou", "Wolfgang Gatterbauer"], "year": 2024, "url": "http://arxiv.org/abs/2412.07786v1", "abstract": "Large enterprise databases can be complex and messy, obscuring the data semantics needed for analytical tasks. We propose a semantic layer in-between the database and the user as a set of small and easy-to-interpret database views, effectively acting as a refined version of the schema. To discover these views, we introduce a multi-agent Large Language Model (LLM) simulation where LLM agents collaborate to iteratively define and refine views with minimal input. Our approach paves the way for LLM-powered exploration of unwieldy databases.", "source": "arxiv", "arxiv_id": "2412.07786v1", "pdf_url": "https://arxiv.org/pdf/2412.07786v1", "categories": ["cs.DB", "cs.AI"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2024-11-25T19:57:16Z", "updated": "2024-11-25T19:57:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Evaluating Large Language Models for Graph Query Generation", "authors": ["Siraj Munir", "Alessandro Aldini"], "year": 2024, "url": "http://arxiv.org/abs/2411.08449v2", "abstract": "Large Language Models (LLMs) are revolutionizing the landscape of Generative Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging rapidly. However, when applied to database technologies, specifically query generation for graph databases and Knowledge Graphs (KGs), LLMs still face significant challenges. While research on LLM-driven query generation for Structured Query Language (SQL) exists, similar systems for graph databases remain underdeveloped. This paper presents a comparative study addressing the challenge of generating Cypher queries a powerful language for interacting with graph databases using open-access LLMs. We rigorously evaluate several LLM agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT) reasoning. Our empirical analysis of query generation accuracy reveals that Claude Sonnet 3.5 outperforms its counterparts in this specific domain. Further, we highlight promising future research directions to address the identified limitations and advance LLM-driven query generation for graph databases.", "source": "arxiv", "arxiv_id": "2411.08449v2", "pdf_url": "https://arxiv.org/pdf/2411.08449v2", "categories": ["cs.ET", "cs.CL"], "primary_category": "cs.ET", "doi": "", "venue": "", "published": "2024-11-13T09:11:56Z", "updated": "2024-11-18T09:57:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents", "authors": ["Agnieszka Mensfelt", "Kostas Stathis", "Vince Trencsenyi"], "year": 2024, "url": "http://arxiv.org/abs/2408.16081v2", "abstract": "Large language models (LLMs) are increasingly explored as general-purpose reasoners, particularly in agentic contexts. However, their outputs remain prone to mathematical and logical errors. This is especially challenging in open-ended tasks, where unstructured outputs lack explicit ground truth and may contain subtle inconsistencies. To address this issue, we propose Logic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs with formal logic to enable validation and refinement of natural language reasoning. LELMA comprises three components: an LLM-Reasoner, an LLM-Translator, and a Solver, and employs autoformalization to translate reasoning into logic representations, which are then used to assess logical validity. Using game-theoretic scenarios such as the Prisoner's Dilemma as testbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro) and advanced (GPT-4o) models in generating logically sound reasoning. LELMA achieves high accuracy in error detection and improves reasoning correctness via self-refinement, particularly in GPT-4o. The study also highlights challenges in autoformalization accuracy and in evaluation of inherently ambiguous open-ended reasoning tasks.", "source": "arxiv", "arxiv_id": "2408.16081v2", "pdf_url": "https://arxiv.org/pdf/2408.16081v2", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-28T18:25:35Z", "updated": "2025-05-29T14:53:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards Robust Automation of Surgical Systems via Digital Twin-based Scene Representations from Foundation Models", "authors": ["Hao Ding", "Lalithkumar Seenivasan", "Hongchao Shu", "Grayson Byrd", "Han Zhang", "Pu Xiao", "Juan Antonio Barragan", "Russell H. Taylor", "Peter Kazanzides", "Mathias Unberath"], "year": 2024, "url": "http://arxiv.org/abs/2409.13107v2", "abstract": "Large language model-based (LLM) agents are emerging as a powerful enabler of robust embodied intelligence due to their capability of planning complex action sequences. Sound planning ability is necessary for robust automation in many task domains, but especially in surgical automation. These agents rely on a highly detailed natural language representation of the scene. Thus, to leverage the emergent capabilities of LLM agents for surgical task planning, developing similarly powerful and robust perception algorithms is necessary to derive a detailed scene representation of the environment from visual input. Previous research has focused primarily on enabling LLM-based task planning while adopting simple yet severely limited perception solutions to meet the needs for bench-top experiments but lack the critical flexibility to scale to less constrained settings. In this work, we propose an alternate perception approach -- a digital twin-based machine perception approach that capitalizes on the convincing performance and out-of-the-box generalization of recent vision foundation models. Integrating our digital twin-based scene representation and LLM agent for planning with the dVRK platform, we develop an embodied intelligence system and evaluate its robustness in performing peg transfer and gauze retrieval tasks. Our approach shows strong task performance and generalizability to varied environment settings. Despite convincing performance, this work is merely a first step towards the integration of digital twin-based scene representations. Future studies are necessary for the realization of a comprehensive digital twin framework to improve the interpretability and generalizability of embodied intelligence in surgery.", "source": "arxiv", "arxiv_id": "2409.13107v2", "pdf_url": "https://arxiv.org/pdf/2409.13107v2", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-09-19T22:24:46Z", "updated": "2024-09-24T15:08:03Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents", "authors": ["Peter MÃ¼hlbacher", "Nikos I. Bosse", "Lawrence Phillips"], "year": 2024, "url": "http://arxiv.org/abs/2409.14913v2", "abstract": "We present initial results of a forthcoming benchmark for evaluating LLM agents on white-collar tasks of economic value. We evaluate agents on real-world \"messy\" open-web research tasks of the type that are routine in finance and consulting. In doing so, we lay the groundwork for an LLM agent evaluation suite where good performance directly corresponds to a large economic and societal impact. We built and tested several agent architectures with o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini. On average, LLM agents powered by Claude-3.5 Sonnet and o1-preview substantially outperformed agents using GPT-4o, with agents based on Llama 3.1 (405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct architecture with the ability to delegate subtasks to subagents performed best. In addition to quantitative evaluations, we qualitatively assessed the performance of the LLM agents by inspecting their traces and reflecting on their observations. Our evaluation represents the first in-depth assessment of agents' abilities to conduct challenging, economically valuable analyst-style research on the real open web.", "source": "arxiv", "arxiv_id": "2409.14913v2", "pdf_url": "https://arxiv.org/pdf/2409.14913v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-23T11:08:04Z", "updated": "2024-09-25T08:52:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TrajAgent: An LLM-Agent Framework for Trajectory Modeling via Large-and-Small Model Collaboration", "authors": ["Yuwei Du", "Jie Feng", "Jie Zhao", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.20445v5", "abstract": "Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modeling. However, the heterogeneity of data and the diversity of trajectory tasks make effective and reliable trajectory modeling an important yet highly challenging endeavor, even for domain experts. In this paper, we propose TrajAgent, an agent framework powered by large language models, designed to facilitate robust and efficient trajectory modeling through automation modeling. This framework leverages and optimizes diverse specialized models to address various trajectory modeling tasks across different datasets effectively. In TrajAgent, we first develop UniEnv, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on UniEnv, we introduce an agentic workflow designed for automatic trajectory modeling across various trajectory tasks and data. Furthermore, we introduce collaborative learning schema between LLM-based agents and small speciallized models, to enhance the performance of the whole framework effectively. Extensive experiments on five tasks using four real-world datasets demonstrate the effectiveness of TrajAgent in automated trajectory modeling, achieving a performance improvement of 2.38%-69.91% over baseline methods. The codes and data can be accessed via https://github.com/tsinghua-fib-lab/TrajAgent.", "source": "arxiv", "arxiv_id": "2410.20445v5", "pdf_url": "https://arxiv.org/pdf/2410.20445v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-27T13:51:09Z", "updated": "2025-10-28T04:18:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Transforming Wearable Data into Personal Health Insights using Large Language Model Agents", "authors": ["Mike A. Merrill", "Akshay Paruchuri", "Naghmeh Rezaei", "Geza Kovacs", "Javier Perez", "Yun Liu", "Erik Schenck", "Nova Hammerquist", "Jake Sunshine", "Shyam Tailor", "Kumar Ayush", "Hao-Wei Su", "Qian He", "Cory Y. McLean", "Mark Malhotra", "Shwetak Patel", "Jiening Zhan", "Tim Althoff", "Daniel McDuff", "Xin Liu"], "year": 2024, "url": "http://arxiv.org/abs/2406.06464v4", "abstract": "Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population.", "source": "arxiv", "arxiv_id": "2406.06464v4", "pdf_url": "https://arxiv.org/pdf/2406.06464v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-10T17:00:54Z", "updated": "2025-09-08T17:59:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Tree Search for Language Model Agents", "authors": ["Jing Yu Koh", "Stephen McAleer", "Daniel Fried", "Ruslan Salakhutdinov"], "year": 2024, "url": "http://arxiv.org/abs/2407.01476v3", "abstract": "Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. Our experiments highlight the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute. We conduct a thorough analysis of our results to highlight improvements from search, limitations, and promising directions for future work. Our code and models are publicly released at https://jykoh.com/search-agents.", "source": "arxiv", "arxiv_id": "2407.01476v3", "pdf_url": "https://arxiv.org/pdf/2407.01476v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-01T17:07:55Z", "updated": "2025-09-24T05:46:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution", "authors": ["Ziyi Ni", "Yifan Li", "Daxiang Dong"], "year": 2024, "url": "http://arxiv.org/abs/2412.14212v1", "abstract": "The exceptional capabilities of large language models (LLMs) have substantially accelerated the rapid rise and widespread adoption of agents. Recent studies have demonstrated that generating Python code to consolidate LLM-based agents' actions into a unified action space (CodeAct) is a promising approach for developing real-world LLM agents. However, this step-by-step code generation approach often lacks consistency and robustness, leading to instability in agent applications, particularly for complex reasoning and out-of-domain tasks. In this paper, we propose a novel approach called Tree-of-Code (ToC) to tackle the challenges of complex problem planning and execution with an end-to-end mechanism. By integrating key ideas from both Tree-of-Thought and CodeAct, ToC combines their strengths to enhance solution exploration. In our framework, each final code execution result is treated as a node in the decision tree, with a breadth-first search strategy employed to explore potential solutions. The final outcome is determined through a voting mechanism based on the outputs of the nodes.", "source": "arxiv", "arxiv_id": "2412.14212v1", "pdf_url": "https://arxiv.org/pdf/2412.14212v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-12-18T08:47:17Z", "updated": "2024-12-18T08:47:17Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents", "authors": ["Yifan Song", "Da Yin", "Xiang Yue", "Jie Huang", "Sujian Li", "Bill Yuchen Lin"], "year": 2024, "url": "http://arxiv.org/abs/2403.02502v2", "abstract": "Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.", "source": "arxiv", "arxiv_id": "2403.02502v2", "pdf_url": "https://arxiv.org/pdf/2403.02502v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-04T21:50:29Z", "updated": "2024-07-10T17:36:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents", "authors": ["Wenyue Hua", "Xianjun Yang", "Mingyu Jin", "Zelong Li", "Wei Cheng", "Ruixiang Tang", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.01586v4", "abstract": "The rise of LLM-based agents shows great potential to revolutionize task planning, capturing significant attention. Given that these agents will be integrated into high-stake domains, ensuring their reliability and safety is crucial. This paper presents an Agent-Constitution-based agent framework, TrustAgent, with a particular focus on improving the LLM-based agent safety. The proposed framework ensures strict adherence to the Agent Constitution through three strategic components: pre-planning strategy which injects safety knowledge to the model before plan generation, in-planning strategy which enhances safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Our experimental results demonstrate that the proposed framework can effectively enhance an LLM agent's safety across multiple domains by identifying and mitigating potential dangers during the planning. Further analysis reveals that the framework not only improves safety but also enhances the helpfulness of the agent. Additionally, we highlight the importance of the LLM reasoning ability in adhering to the Constitution. This paper sheds light on how to ensure the safe integration of LLM-based agents into human-centric environments. Data and code are available at https://github.com/agiresearch/TrustAgent.", "source": "arxiv", "arxiv_id": "2402.01586v4", "pdf_url": "https://arxiv.org/pdf/2402.01586v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-02T17:26:23Z", "updated": "2024-10-03T22:12:05Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction", "authors": ["Mitchell Rosser", "Marc. G Carmichael"], "year": 2024, "url": "http://arxiv.org/abs/2411.16723v1", "abstract": "With the recent development of natural language generation models - termed as large language models (LLMs) - a potential use case has opened up to improve the way that humans interact with robot assistants. These LLMs should be able to leverage their large breadth of understanding to interpret natural language commands into effective, task appropriate and safe robot task executions. However, in reality, these models suffer from hallucinations, which may cause safety issues or deviations from the task. In other domains, these issues have been improved through the use of collaborative AI systems where multiple LLM agents can work together to collectively plan, code and self-check outputs. In this research, multiple collaborative AI systems were tested against a single independent AI agent to determine whether the success in other domains would translate into improved human-robot interaction performance. The results show that there is no defined trend between the number of agents and the success of the model. However, it is clear that some collaborative AI agent architectures can exhibit a greatly improved capacity to produce error-free code and to solve abstract problems.", "source": "arxiv", "arxiv_id": "2411.16723v1", "pdf_url": "https://arxiv.org/pdf/2411.16723v1", "categories": ["cs.MA", "cs.AI", "cs.RO"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-11-23T02:47:12Z", "updated": "2024-11-23T02:47:12Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment", "authors": ["Mingzhe Xing", "Rongkai Zhang", "Hui Xue", "Qi Chen", "Fan Yang", "Zhen Xiao"], "year": 2024, "url": "http://arxiv.org/abs/2402.06596v1", "abstract": "Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation}, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.", "source": "arxiv", "arxiv_id": "2402.06596v1", "pdf_url": "https://arxiv.org/pdf/2402.06596v1", "categories": ["cs.AI", "cs.HC", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-09T18:19:25Z", "updated": "2024-02-09T18:19:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Understanding the planning of LLM agents: A survey", "authors": ["Xu Huang", "Weiwen Liu", "Xiaolong Chen", "Xingmei Wang", "Hao Wang", "Defu Lian", "Yasheng Wang", "Ruiming Tang", "Enhong Chen"], "year": 2024, "url": "http://arxiv.org/abs/2402.02716v1", "abstract": "As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.", "source": "arxiv", "arxiv_id": "2402.02716v1", "pdf_url": "https://arxiv.org/pdf/2402.02716v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-05T04:25:24Z", "updated": "2024-02-05T04:25:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Unlocking the Wisdom of Large Language Models: An Introduction to The Path to Artificial General Intelligence", "authors": ["Edward Y. Chang"], "year": 2024, "url": "http://arxiv.org/abs/2409.01007v3", "abstract": "This booklet, Unlocking the Wisdom of Multi-LLM Collaborative Intelligence, serves as an accessible introduction to the full volume The Path to Artificial General Intelligence. Through fourteen aphorisms, it distills the core principles of Multi-LLM Agent Collaborative Intelligence (MACI), a framework designed to coordinate multiple LLMs toward reasoning, planning, and decision-making that surpasses the capabilities of any single model. The booklet includes titles, abstracts, and introductions from each main chapter, along with the full content of the first two. The newly released third edition features significant enhancements to Chapters 6 through 9 and a revised preface responding to Yann LeCun's critique of AGI feasibility. While LeCun argues that LLMs lack grounding, memory, and planning, we propose that MACI's collaborative architecture, featuring multimodal agents in executive, legislative, and judicial roles, directly addresses these limitations. Chapters on SocraSynth, EVINCE, consciousness modeling, and behavior regulation demonstrate that reasoning systems grounded in structured interaction and checks and balances can produce more reliable, interpretable, and adaptive intelligence. By integrating complementary model strengths, including world modeling and multimodal perception, MACI enables a system-level intelligence that exceeds the sum of its parts. Like human institutions, progress in AI may depend less on isolated performance and more on coordinated judgment. Collaborative LLMs, not just larger ones, may chart the path toward artificial general intelligence.", "source": "arxiv", "arxiv_id": "2409.01007v3", "pdf_url": "https://arxiv.org/pdf/2409.01007v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-02T07:29:37Z", "updated": "2025-04-15T05:21:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction", "authors": ["Yansong Ning", "Hao Liu"], "year": 2024, "url": "http://arxiv.org/abs/2402.06861v2", "abstract": "Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we obtain UrbanKGC agent family, consisting of UrbanKGent-7/8/13B version. We perform a comprehensive evaluation on two real-world datasets using both human and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent family can not only significantly outperform 31 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost. Compared with the existing benchmark, the UrbanKGent family could help construct an UrbanKG with hundreds of times richer relationships using only one-fifth of the data. Our data and code are available at https://github.com/usail-hkust/UrbanKGent.", "source": "arxiv", "arxiv_id": "2402.06861v2", "pdf_url": "https://arxiv.org/pdf/2402.06861v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-10T01:50:19Z", "updated": "2024-10-06T03:40:58Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Using a Feedback Loop for LLM-based Infrastructure as Code Generation", "authors": ["Mayur Amarnath Palavalli", "Mark Santolucito"], "year": 2024, "url": "http://arxiv.org/abs/2411.19043v1", "abstract": "Code generation with Large Language Models (LLMs) has helped to increase software developer productivity in coding tasks, but has yet to have significant impact on the tasks of software developers that surround this code. In particular, the challenge of infrastructure management remains an open question. We investigate the ability of an LLM agent to construct infrastructure using the Infrastructure as Code (IaC) paradigm. We particularly investigate the use of a feedback loop that returns errors and warnings on the generated IaC to allow the LLM agent to improve the code. We find that, for each iteration of the loop, its effectiveness decreases exponentially until it plateaus at a certain point and becomes ineffective.", "source": "arxiv", "arxiv_id": "2411.19043v1", "pdf_url": "https://arxiv.org/pdf/2411.19043v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-11-28T10:40:55Z", "updated": "2024-11-28T10:40:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2410.07484v2", "abstract": "Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such \"world alignment\" can be efficiently achieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few additional rules suffice to align LLM predictions with the specified environment dynamics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions. The resulting world model is composed of the LLM and the learned rules. Our embodied LLM agent \"WALL-E\" is built upon model-predictive control (MPC). By optimizing look-ahead actions based on the precise world model, MPC significantly improves exploration and learning efficiency. Compared to existing LLM agents, WALL-E's reasoning only requires a few principal rules rather than verbose buffered trajectories being included in the LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E achieves higher success rates than existing methods, with lower costs on replanning time and the number of tokens used for reasoning. In Minecraft, WALL-E exceeds baselines by 15-30% in success rate while costing 8-20 fewer replanning rounds and only 60-80% of tokens. In ALFWorld, its success rate surges to a new record high of 95% only after 6 iterations.", "source": "arxiv", "arxiv_id": "2410.07484v2", "pdf_url": "https://arxiv.org/pdf/2410.07484v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-09T23:37:36Z", "updated": "2024-10-11T23:32:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WESE: Weak Exploration to Strong Exploitation for LLM Agents", "authors": ["Xu Huang", "Weiwen Liu", "Xiaolong Chen", "Xingmei Wang", "Defu Lian", "Yasheng Wang", "Ruiming Tang", "Enhong Chen"], "year": 2024, "url": "http://arxiv.org/abs/2404.07456v1", "abstract": "Recently, large language models (LLMs) have demonstrated remarkable potential as an intelligent agent. However, existing researches mainly focus on enhancing the agent's reasoning or decision-making abilities through well-designed prompt engineering or task-specific fine-tuning, ignoring the procedure of exploration and exploitation. When addressing complex tasks within open-world interactive environments, these methods exhibit limitations. Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions. On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost. This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM agents in solving open-world interactive tasks. Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global knowledge. A knowledge graph-based strategy is then introduced to store the acquired knowledge and extract task-relevant knowledge, enhancing the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive benchmarks.", "source": "arxiv", "arxiv_id": "2404.07456v1", "pdf_url": "https://arxiv.org/pdf/2404.07456v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-11T03:31:54Z", "updated": "2024-04-11T03:31:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WaitGPT: Monitoring and Steering Conversational LLM Agent in Data Analysis with On-the-Fly Code Visualization", "authors": ["Liwenhan Xie", "Chengbo Zheng", "Haijun Xia", "Huamin Qu", "Chen Zhu-Tian"], "year": 2024, "url": "http://arxiv.org/abs/2408.01703v1", "abstract": "Large language models (LLMs) support data analysis through conversational user interfaces, as exemplified in OpenAI's ChatGPT (formally known as Advanced Data Analysis or Code Interpreter). Essentially, LLMs produce code for accomplishing diverse analysis tasks. However, presenting raw code can obscure the logic and hinder user verification. To empower users with enhanced comprehension and augmented control over analysis conducted by LLMs, we propose a novel approach to transform LLM-generated code into an interactive visual representation. In the approach, users are provided with a clear, step-by-step visualization of the LLM-generated code in real time, allowing them to understand, verify, and modify individual data operations in the analysis. Our design decisions are informed by a formative study (N=8) probing into user practice and challenges. We further developed a prototype named WaitGPT and conducted a user study (N=12) to evaluate its usability and effectiveness. The findings from the user study reveal that WaitGPT facilitates monitoring and steering of data analysis performed by LLMs, enabling participants to enhance error detection and increase their overall confidence in the results.", "source": "arxiv", "arxiv_id": "2408.01703v1", "pdf_url": "https://arxiv.org/pdf/2408.01703v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1145/3654777.3676374", "venue": "", "published": "2024-08-03T07:51:08Z", "updated": "2024-08-03T07:51:08Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement", "authors": ["Weimin Xiong", "Yifan Song", "Xiutian Zhao", "Wenhao Wu", "Xun Wang", "Ke Wang", "Cheng Li", "Wei Peng", "Sujian Li"], "year": 2024, "url": "http://arxiv.org/abs/2406.11176v2", "abstract": "Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the Iterative step-level Process Refinement (IPR) framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical findings highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.", "source": "arxiv", "arxiv_id": "2406.11176v2", "pdf_url": "https://arxiv.org/pdf/2406.11176v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-17T03:29:13Z", "updated": "2024-09-24T10:01:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Watson: A Cognitive Observability Framework for the Reasoning of LLM-Powered Agents", "authors": ["Benjamin Rombaut", "Sogol Masoumzadeh", "Kirill Vasilevski", "Dayi Lin", "Ahmed E. Hassan"], "year": 2024, "url": "http://arxiv.org/abs/2411.03455v3", "abstract": "Large language models (LLMs) are increasingly integrated into autonomous systems, giving rise to a new class of software known as Agentware, where LLM-powered agents perform complex, open-ended tasks in domains such as software engineering, customer service, and data analysis. However, their high autonomy and opaque reasoning processes pose significant challenges for traditional software observability methods. To address this, we introduce the concept of cognitive observability - the ability to recover and inspect the implicit reasoning behind agent decisions. We present Watson, a general-purpose framework for observing the reasoning processes of fast-thinking LLM agents without altering their behavior. Watson retroactively infers reasoning traces using prompt attribution techniques. We evaluate Watson in both manual debugging and automated correction scenarios across the MMLU benchmark and the AutoCodeRover and OpenHands agents on the SWE-bench-lite dataset. In both static and dynamic settings, Watson surfaces actionable reasoning insights and supports targeted interventions, demonstrating its practical utility for improving transparency and reliability in Agentware systems.", "source": "arxiv", "arxiv_id": "2411.03455v3", "pdf_url": "https://arxiv.org/pdf/2411.03455v3", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-05T19:13:22Z", "updated": "2025-09-19T13:47:46Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Wearable intelligent throat enables natural speech in stroke patients with dysarthria", "authors": ["Chenyu Tang", "Shuo Gao", "Cong Li", "Wentian Yi", "Yuxuan Jin", "Xiaoxue Zhai", "Sixuan Lei", "Hongbei Meng", "Zibo Zhang", "Muzi Xu", "Shengbo Wang", "Xuhang Chen", "Chenxi Wang", "Hongyun Yang", "Ningli Wang", "Wenyu Wang", "Jin Cao", "Xiaodong Feng", "Peter Smielewski", "Yu Pan", "Wenhui Song", "Martin Birchall", "Luigi G. Occhipinti"], "year": 2024, "url": "http://arxiv.org/abs/2411.18266v3", "abstract": "Wearable silent speech systems hold significant potential for restoring communication in patients with speech impairments. However, seamless, coherent speech remains elusive, and clinical efficacy is still unproven. Here, we present an AI-driven intelligent throat (IT) system that integrates throat muscle vibrations and carotid pulse signal sensors with large language model (LLM) processing to enable fluent, emotionally expressive communication. The system utilizes ultrasensitive textile strain sensors to capture high-quality signals from the neck area and supports token-level processing for real-time, continuous speech decoding, enabling seamless, delay-free communication. In tests with five stroke patients with dysarthria, IT's LLM agents intelligently corrected token errors and enriched sentence-level emotional and logical coherence, achieving low error rates (4.2% word error rate, 2.9% sentence error rate) and a 55% increase in user satisfaction. This work establishes a portable, intuitive communication platform for patients with dysarthria with the potential to be applied broadly across different neurological conditions and in multi-language support systems.", "source": "arxiv", "arxiv_id": "2411.18266v3", "pdf_url": "https://arxiv.org/pdf/2411.18266v3", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SY"], "primary_category": "eess.AS", "doi": "", "venue": "", "published": "2024-11-27T12:03:52Z", "updated": "2025-03-14T09:14:26Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Web Retrieval Agents for Evidence-Based Misinformation Detection", "authors": ["Jacob-Junqi Tian", "Hao Yu", "Yury Orlovskiy", "Tyler Vergho", "Mauricio Rivera", "Mayank Goel", "Zachary Yang", "Jean-Francois Godbout", "Reihaneh Rabbany", "Kellin Pelrine"], "year": 2024, "url": "http://arxiv.org/abs/2409.00009v2", "abstract": "This paper develops an agent-based automated fact-checking approach for detecting misinformation. We demonstrate that combining a powerful LLM agent, which does not have access to the internet for searches, with an online web search agent yields better results than when each tool is used independently. Our approach is robust across multiple models, outperforming alternatives and increasing the macro F1 of misinformation detection by as much as 20 percent compared to LLMs without search. We also conduct extensive analyses on the sources our system leverages and their biases, decisions in the construction of the system like the search tool and the knowledge base, the type of evidence needed and its impact on the results, and other parts of the overall process. By combining strong performance with in-depth understanding, we hope to provide building blocks for future search-enabled misinformation mitigation systems.", "source": "arxiv", "arxiv_id": "2409.00009v2", "pdf_url": "https://arxiv.org/pdf/2409.00009v2", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-08-15T15:13:16Z", "updated": "2024-10-09T19:13:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents", "authors": ["Zhaoqian Xue", "Beichen Wang", "Suiyuan Zhu", "Kai Mei", "Hua Tang", "Wenyue Hua", "Mengnan Du", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.13184v6", "abstract": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence system that utilizes Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations. This paper introduces a mathematical model for quantifying the levels of civilization development and further employs a state transition matrix approach to evaluate their trajectories. Through this methodology, our study quantitatively analyzes the growth trajectories of civilizations, providing insights into future decision-making at critical points of growth and saturation. Furthermore, this paper acknowledges the vast diversity of potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among different civilizations. Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLM agents with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research not only introduces a novel method for comprehending potential inter-civilizational dynamics but also holds practical value in enabling entities with divergent value systems to strategize, prevent conflicts, and engage in games under conditions of asymmetric information. The accompanying code is available at https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.", "source": "arxiv", "arxiv_id": "2402.13184v6", "pdf_url": "https://arxiv.org/pdf/2402.13184v6", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-20T17:49:46Z", "updated": "2025-06-09T00:57:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs", "authors": ["Hanna Kim", "Minkyoo Song", "Seung Ho Na", "Seungwon Shin", "Kimin Lee"], "year": 2024, "url": "http://arxiv.org/abs/2410.14569v3", "abstract": "Recent advancements in Large Language Models (LLMs) have established them as agentic systems capable of planning and interacting with various tools. These LLM agents are often paired with web-based tools, enabling access to diverse sources and real-time information. Although these advancements offer significant benefits across various applications, they also increase the risk of malicious use, particularly in cyberattacks involving personal information. In this work, we investigate the risks associated with misuse of LLM agents in cyberattacks involving personal data. Specifically, we aim to understand: 1) how potent LLM agents can be when directed to conduct cyberattacks, 2) how cyberattacks are enhanced by web-based tools, and 3) how affordable and easy it becomes to launch cyberattacks using LLM agents. We examine three attack scenarios: the collection of Personally Identifiable Information (PII), the generation of impersonation posts, and the creation of spear-phishing emails. Our experiments reveal the effectiveness of LLM agents in these attacks: LLM agents achieved a precision of up to 95.9% in collecting PII, generated impersonation posts where 93.9% of them were deemed authentic, and boosted click rate of phishing links in spear phishing emails by 46.67%. Additionally, our findings underscore the limitations of existing safeguards in contemporary commercial LLMs, emphasizing the urgent need for robust security measures to prevent the misuse of LLM agents.", "source": "arxiv", "arxiv_id": "2410.14569v3", "pdf_url": "https://arxiv.org/pdf/2410.14569v3", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-18T16:16:34Z", "updated": "2025-02-03T06:52:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment", "authors": ["Minrui Xu", "Dusit Niyato", "Jiawen Kang", "Zehui Xiong", "Shiwen Mao", "Zhu Han", "Dong In Kim", "Khaled B. Letaief"], "year": 2024, "url": "http://arxiv.org/abs/2401.07764v2", "abstract": "AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.", "source": "arxiv", "arxiv_id": "2401.07764v2", "pdf_url": "https://arxiv.org/pdf/2401.07764v2", "categories": ["cs.AI", "cs.NI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-15T15:20:59Z", "updated": "2024-02-16T19:15:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Who is Undercover? Guiding LLMs to Explore Multi-Perspective Team Tactic in the Game", "authors": ["Ruiqi Dong", "Zhixuan Liao", "Guangwei Lai", "Yuhan Ma", "Danni Ma", "Chenyou Fan"], "year": 2024, "url": "http://arxiv.org/abs/2410.15311v1", "abstract": "Large Language Models (LLMs) are pivotal AI agents in complex tasks but still face challenges in open decision-making problems within complex scenarios. To address this, we use the language logic game ``Who is Undercover?'' (WIU) as an experimental platform to propose the Multi-Perspective Team Tactic (MPTT) framework. MPTT aims to cultivate LLMs' human-like language expression logic, multi-dimensional thinking, and self-perception in complex scenarios. By alternating speaking and voting sessions, integrating techniques like self-perspective, identity-determination, self-reflection, self-summary and multi-round find-teammates, LLM agents make rational decisions through strategic concealment and communication, fostering human-like trust. Preliminary results show that MPTT, combined with WIU, leverages LLMs' cognitive capabilities to create a decision-making framework that can simulate real society. This framework aids minority groups in communication and expression, promoting fairness and diversity in decision-making. Additionally, our Human-in-the-loop experiments demonstrate that LLMs can learn and align with human behaviors through interactive, indicating their potential for active participation in societal decision-making.", "source": "arxiv", "arxiv_id": "2410.15311v1", "pdf_url": "https://arxiv.org/pdf/2410.15311v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-20T06:41:31Z", "updated": "2024-10-20T06:41:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WildfireGPT: Tailored Large Language Model for Wildfire Analysis", "authors": ["Yangxinyu Xie", "Bowen Jiang", "Tanwi Mallick", "Joshua David Bergerson", "John K. Hutchison", "Duane R. Verner", "Jordan Branham", "M. Ross Alexander", "Robert B. Ross", "Yan Feng", "Leslie-Anne Levy", "Weijie Su", "Camillo J. Taylor"], "year": 2024, "url": "http://arxiv.org/abs/2402.07877v4", "abstract": "Recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence. However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge, such as wildfire details within the broader context of climate change. For decision-makers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific. To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks. We enrich WildfireGPT by providing additional context, such as climate projections and scientific literature, to ensure its information is current, relevant, and scientifically accurate. This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including but not limited to researchers and engineers, for making positive impact and decision making.", "source": "arxiv", "arxiv_id": "2402.07877v4", "pdf_url": "https://arxiv.org/pdf/2402.07877v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-12T18:41:55Z", "updated": "2025-04-23T03:30:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks", "authors": ["Jingwen Tong", "Jiawei Shao", "Qiong Wu", "Wei Guo", "Zijian Li", "Zehong Lin", "Jun Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2409.07964v1", "abstract": "Wireless networks are increasingly facing challenges due to their expanding scale and complexity. These challenges underscore the need for advanced AI-driven strategies, particularly in the upcoming 6G networks. In this article, we introduce WirelessAgent, a novel approach leveraging large language models (LLMs) to develop AI agents capable of managing complex tasks in wireless networks. It can effectively improve network performance through advanced reasoning, multimodal data processing, and autonomous decision making. Thereafter, we demonstrate the practical applicability and benefits of WirelessAgent for network slicing management. The experimental results show that WirelessAgent is capable of accurately understanding user intent, effectively allocating slice resources, and consistently maintaining optimal performance.", "source": "arxiv", "arxiv_id": "2409.07964v1", "pdf_url": "https://arxiv.org/pdf/2409.07964v1", "categories": ["cs.NI", "cs.AI", "cs.LG"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-09-12T11:48:01Z", "updated": "2024-09-12T11:48:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment", "authors": ["Hao Tang", "Darren Key", "Kevin Ellis"], "year": 2024, "url": "http://arxiv.org/abs/2402.12275v3", "abstract": "We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We define this optimism as a logical constraint between a program and a planner. We study our agent on gridworlds, and on task planning, finding our approach is more sample-efficient compared to deep RL, more compute-efficient compared to ReAct-style agents, and that it can transfer its knowledge across environments by editing its code.", "source": "arxiv", "arxiv_id": "2402.12275v3", "pdf_url": "https://arxiv.org/pdf/2402.12275v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-19T16:39:18Z", "updated": "2024-09-20T18:56:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects", "authors": ["Islem Bouzenia", "Michael Pradel"], "year": 2024, "url": "http://arxiv.org/abs/2412.10133v2", "abstract": "The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that prepares scripts for building an arbitrary project from source code and running its test cases. Inspired by the way a human developer would address this task, our approach is a large language model (LLM)-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/50 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of USD 0.16, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.", "source": "arxiv", "arxiv_id": "2412.10133v2", "pdf_url": "https://arxiv.org/pdf/2412.10133v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "ISSTA 2025", "published": "2024-12-13T13:30:51Z", "updated": "2025-04-30T10:25:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World", "authors": ["Guande Wu", "Chen Zhao", "Claudio Silva", "He He"], "year": 2024, "url": "http://arxiv.org/abs/2404.00246v1", "abstract": "Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state and identify and correct execution errors. Both human-machine and machine-machine experiments show that LLM agents have strong grounding capacities, and our approach significantly improves the evaluation metric.", "source": "arxiv", "arxiv_id": "2404.00246v1", "pdf_url": "https://arxiv.org/pdf/2404.00246v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-30T04:48:38Z", "updated": "2024-03-30T04:48:38Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics", "authors": ["Yuan Zhou", "Peng Zhang", "Mengya Song", "Alice Zheng", "Yiwen Lu", "Zhiheng Liu", "Yong Chen", "Zhaohan Xi"], "year": 2024, "url": "http://arxiv.org/abs/2410.02026v1", "abstract": "Large language models (LLMs) have demonstrated remarkable progress in healthcare. However, a significant gap remains regarding LLMs' professionalism in domain-specific clinical practices, limiting their application in real-world diagnostics. In this work, we introduce ZODIAC, an LLM-powered framework with cardiologist-level professionalism designed to engage LLMs in cardiological diagnostics. ZODIAC assists cardiologists by extracting clinically relevant characteristics from patient data, detecting significant arrhythmias, and generating preliminary reports for the review and refinement by cardiologists. To achieve cardiologist-level professionalism, ZODIAC is built on a multi-agent collaboration framework, enabling the processing of patient data across multiple modalities. Each LLM agent is fine-tuned using real-world patient data adjudicated by cardiologists, reinforcing the model's professionalism. ZODIAC undergoes rigorous clinical validation with independent cardiologists, evaluated across eight metrics that measure clinical effectiveness and address security concerns. Results show that ZODIAC outperforms industry-leading models, including OpenAI's GPT-4o, Meta's Llama-3.1-405B, and Google's Gemini-pro, as well as medical-specialist LLMs like Microsoft's BioGPT. ZODIAC demonstrates the transformative potential of specialized LLMs in healthcare by delivering domain-specific solutions that meet the stringent demands of medical practice. Notably, ZODIAC has been successfully integrated into electrocardiography (ECG) devices, exemplifying the growing trend of embedding LLMs into Software-as-Medical-Device (SaMD).", "source": "arxiv", "arxiv_id": "2410.02026v1", "pdf_url": "https://arxiv.org/pdf/2410.02026v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-02T20:46:39Z", "updated": "2024-10-02T20:46:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents", "authors": ["Shaoguang Mao", "Yuzhe Cai", "Yan Xia", "Wenshan Wu", "Xun Wang", "Fengyi Wang", "Tao Ge", "Furu Wei"], "year": 2023, "url": "http://arxiv.org/abs/2311.03220v4", "abstract": "This paper introduces Alympics (Olympics for Agents), a systematic simulation framework utilizing Large Language Model (LLM) agents for game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the \"Water Allocation Challenge,\" we explore Alympics through a challenging strategic game focused on the multi-round auction on scarce survival resources. This study demonstrates the framework's ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in strategic decision-making scenarios. Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also highlight their potential in advancing game theory knowledge, thereby enriching our understanding of both game theory and empowering further research into strategic decision-making domains with LLM agents. Codes, prompts, and all related resources are available at https://github.com/microsoft/Alympics.", "source": "arxiv", "arxiv_id": "2311.03220v4", "pdf_url": "https://arxiv.org/pdf/2311.03220v4", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-06T16:03:46Z", "updated": "2024-01-16T07:12:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation", "authors": ["Difei Gao", "Lei Ji", "Zechen Bai", "Mingyu Ouyang", "Peiran Li", "Dongxing Mao", "Qinchen Wu", "Weichen Zhang", "Peiyi Wang", "Xiangwu Guo", "Hengxu Wang", "Luowei Zhou", "Mike Zheng Shou"], "year": 2023, "url": "http://arxiv.org/abs/2312.13108v2", "abstract": "Graphical User Interface (GUI) automation holds significant promise for assisting users with complex tasks, thereby boosting human productivity. Existing works leveraging Large Language Model (LLM) or LLM-based AI agents have shown capabilities in automating tasks on Android and Web platforms. However, these tasks are primarily aimed at simple device usage and entertainment operations. This paper presents a novel benchmark, AssistGUI, to evaluate whether models are capable of manipulating the mouse and keyboard on the Windows platform in response to user-requested tasks. We carefully collected a set of 100 tasks from nine widely-used software applications, such as, After Effects and MS Word, each accompanied by the necessary project files for better evaluation. Moreover, we propose an advanced Actor-Critic Embodied Agent framework, which incorporates a sophisticated GUI parser driven by an LLM-agent and an enhanced reasoning mechanism adept at handling lengthy procedural tasks. Our experimental results reveal that our GUI Parser and Reasoning mechanism outshine existing methods in performance. Nevertheless, the potential remains substantial, with the best model attaining only a 46% success rate on our benchmark. We conclude with a thorough analysis of the current methods' limitations, setting the stage for future breakthroughs in this domain.", "source": "arxiv", "arxiv_id": "2312.13108v2", "pdf_url": "https://arxiv.org/pdf/2312.13108v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-12-20T15:28:38Z", "updated": "2024-01-01T14:26:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AVIS: Autonomous Visual Information Seeking with Large Language Model Agent", "authors": ["Ziniu Hu", "Ahmet Iscen", "Chen Sun", "Kai-Wei Chang", "Yizhou Sun", "David A Ross", "Cordelia Schmid", "Alireza Fathi"], "year": 2023, "url": "http://arxiv.org/abs/2306.08129v3", "abstract": "In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as \"What event is commemorated by the building depicted in this image?\", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information from the tool outputs, and a working memory component that retains the acquired information throughout the process. The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state. Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions. We show that AVIS achieves state-of-the-art results on knowledge-intensive visual question answering benchmarks such as Infoseek and OK-VQA.", "source": "arxiv", "arxiv_id": "2306.08129v3", "pdf_url": "https://arxiv.org/pdf/2306.08129v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-06-13T20:50:22Z", "updated": "2023-11-02T07:54:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AdaPlanner: Adaptive Planning from Feedback with Language Models", "authors": ["Haotian Sun", "Yuchen Zhuang", "Lingkai Kong", "Bo Dai", "Chao Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2305.16653v1", "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.", "source": "arxiv", "arxiv_id": "2305.16653v1", "pdf_url": "https://arxiv.org/pdf/2305.16653v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-26T05:52:27Z", "updated": "2023-05-26T05:52:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Adapting LLM Agents with Universal Feedback in Communication", "authors": ["Kuan Wang", "Yadong Lu", "Michael Santacroce", "Yeyun Gong", "Chao Zhang", "Yelong Shen"], "year": 2023, "url": "http://arxiv.org/abs/2310.01444v3", "abstract": "Recent advances in large language models (LLMs) have demonstrated potential for LLM agents. To facilitate the training for these agents with both linguistic feedback and non-linguistic reward signals, we introduce Learning through Communication (LTC). We design a universal buffer to store all the feedback, and an iterative pipeline to enable an LLM agent to explore and update its policy in an given environment. To optimize agent interactions for task-specific learning with our universal buffer and pipeline, we introduce diverse communication patterns tailored for both single-agent and multi-agent environments. We evaluate the efficacy of our LTC approach on four diverse datasets: ALFWorld (single-agent), HotpotQA (multi-agent collaboration), Chameleon (multi-agent competition), and GSM8k (multi-agent teacher-student). On these data sets, LTC outperforms the supervised instruction fine-tuning baselines by 3.6% to 12%. These results highlight the versatility and efficiency of LTC in facilitating online adaptation for LLM agents.", "source": "arxiv", "arxiv_id": "2310.01444v3", "pdf_url": "https://arxiv.org/pdf/2310.01444v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-01T07:50:30Z", "updated": "2024-04-14T03:47:19Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Agent-OM: Leveraging LLM Agents for Ontology Matching", "authors": ["Zhangcheng Qiang", "Weiqing Wang", "Kerry Taylor"], "year": 2023, "url": "http://arxiv.org/abs/2312.00326v24", "abstract": "Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.", "source": "arxiv", "arxiv_id": "2312.00326v24", "pdf_url": "https://arxiv.org/pdf/2312.00326v24", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-01T03:44:54Z", "updated": "2025-12-18T11:37:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentBench: Evaluating LLMs as Agents", "authors": ["Xiao Liu", "Hao Yu", "Hanchen Zhang", "Yifan Xu", "Xuanyu Lei", "Hanyu Lai", "Yu Gu", "Hangliang Ding", "Kaiwen Men", "Kejuan Yang", "Shudan Zhang", "Xiang Deng", "Aohan Zeng", "Zhengxiao Du", "Chenhui Zhang", "Sheng Shen", "Tianjun Zhang", "Yu Su", "Huan Sun", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "year": 2023, "url": "http://arxiv.org/abs/2308.03688v3", "abstract": "The potential of Large Language Model (LLM) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively \\textit{evaluate LLMs as agents} on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities. Our extensive test over \\num API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Improving instruction following and training on high quality multi-round alignment data could improve agent performance. And different from existing assumptions, training on code present ambivalent impacts on different agent tasks. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.", "source": "arxiv", "arxiv_id": "2308.03688v3", "pdf_url": "https://arxiv.org/pdf/2308.03688v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-08-07T16:08:11Z", "updated": "2025-10-04T03:54:18Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation", "authors": ["Jiaju Lin", "Haoran Zhao", "Aochi Zhang", "Yiting Wu", "Huqiuyue Ping", "Qin Chen"], "year": 2023, "url": "http://arxiv.org/abs/2308.04026v1", "abstract": "With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .", "source": "arxiv", "arxiv_id": "2308.04026v1", "pdf_url": "https://arxiv.org/pdf/2308.04026v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-08-08T03:59:28Z", "updated": "2023-08-08T03:59:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Asking Before Acting: Gather Information in Embodied Decision Making with Language Models", "authors": ["Xiaoyu Chen", "Shenao Zhang", "Pushi Zhang", "Li Zhao", "Jianyu Chen"], "year": 2023, "url": "http://arxiv.org/abs/2305.15695v2", "abstract": "With strong capabilities of reasoning and a broad understanding of the world, Large Language Models (LLMs) have demonstrated immense potential in building versatile embodied decision-making agents capable of executing a wide array of tasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM agents encounter challenges in efficiently gathering essential information, leading to suboptimal performance. Conversely, human individuals often seek additional information from their peers prior to taking action, harnessing external knowledge to avoid unnecessary trial and error. Drawing inspiration from this behavior, we propose \\textit{Asking Before Acting} (ABA), a method that empowers the agent to proactively inquire with external sources for pertinent information using natural language during their interactions within the environment. In this way, the agent is able to enhance its efficiency and performance by circumventing potentially laborious steps and combating the difficulties associated with exploration in unfamiliar environments and vagueness of the instructions. We conduct extensive experiments involving a spectrum of environments including text-based household everyday tasks, robot arm manipulation tasks, and real world open domain image based embodied tasks. The experiments involve various models from Vicuna to GPT-4. The results demonstrate that, even with modest prompts modifications, ABA exhibits substantial advantages on both performance and efficiency over baseline LLM agents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates learning the rationale for asking and allows for additional enhancements especially in tasks that baselines struggle to solve.", "source": "arxiv", "arxiv_id": "2305.15695v2", "pdf_url": "https://arxiv.org/pdf/2305.15695v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-05-25T04:05:08Z", "updated": "2024-04-16T13:24:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing", "authors": ["Juyeon Yoon", "Robert Feldt", "Shin Yoo"], "year": 2023, "url": "http://arxiv.org/abs/2311.08649v1", "abstract": "GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51% for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 374 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.", "source": "arxiv", "arxiv_id": "2311.08649v1", "pdf_url": "https://arxiv.org/pdf/2311.08649v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2023-11-15T01:59:40Z", "updated": "2023-11-15T01:59:40Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation", "authors": ["Shenzhi Wang", "Chang Liu", "Zilong Zheng", "Siyuan Qi", "Shuo Chen", "Qisen Yang", "Andrew Zhao", "Chaofei Wang", "Shiji Song", "Gao Huang"], "year": 2023, "url": "http://arxiv.org/abs/2310.01320v3", "abstract": "Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a \"Game-of-Thoughts\". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial thoughts and speech, while refinement contemplation further polishes them. Additionally, we incorporate first-order and second-order perspective transitions into these processes respectively. Specifically, the first-order allows an LLM agent to infer others' mental states, and the second-order involves understanding how others perceive the agent's mental state. After integrating ReCon with different LLMs, extensive experiment results from the Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around deceptive information without extra fine-tuning and data. Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research.", "source": "arxiv", "arxiv_id": "2310.01320v3", "pdf_url": "https://arxiv.org/pdf/2310.01320v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-10-02T16:27:36Z", "updated": "2023-10-24T12:51:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "AvalonBench: Evaluating LLMs Playing the Game of Avalon", "authors": ["Jonathan Light", "Min Cai", "Sheng Shen", "Ziniu Hu"], "year": 2023, "url": "http://arxiv.org/abs/2310.05036v3", "abstract": "In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots playing evil, while good-role bot achieves 38.2% win rate in the same setting. We envision AvalonBench could be a good test-bed for developing more advanced LLMs (with self-playing) and agent frameworks that can effectively model the layered complexities of such game environments.", "source": "arxiv", "arxiv_id": "2310.05036v3", "pdf_url": "https://arxiv.org/pdf/2310.05036v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-10-08T06:37:08Z", "updated": "2023-11-08T16:01:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "CGMI: Configurable General Multi-Agent Interaction Framework", "authors": ["Shi Jinxin", "Zhao Jiabao", "Wang Yilei", "Wu Xingjiao", "Li Jiawen", "He Liang"], "year": 2023, "url": "http://arxiv.org/abs/2308.12503v2", "abstract": "Benefiting from the powerful capabilities of large language models (LLMs), agents based on LLMs have shown the potential to address domain-specific tasks and emulate human behaviors. However, the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture. To address this, we present the Configurable General Multi-Agent Interaction (CGMI) framework, designed to replicate human interactions in real-world scenarios. Specifically, we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality. Additionally, we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules. We have also integrated general agents to augment the virtual environment's realism. Using the CGMI framework, we simulated numerous classroom interactions between teacher and students. The experiments indicate that aspects such as the teaching methodology, curriculum, and student performance closely mirror real classroom settings. We will open source our work.", "source": "arxiv", "arxiv_id": "2308.12503v2", "pdf_url": "https://arxiv.org/pdf/2308.12503v2", "categories": ["cs.AI", "cs.HC", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-08-24T02:03:29Z", "updated": "2023-08-28T05:44:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ChatDev: Communicative Agents for Software Development", "authors": ["Chen Qian", "Wei Liu", "Hongzhang Liu", "Nuo Chen", "Yufan Dang", "Jiahao Li", "Cheng Yang", "Weize Chen", "Yusheng Su", "Xin Cong", "Juyuan Xu", "Dahai Li", "Zhiyuan Liu", "Maosong Sun"], "year": 2023, "url": "http://arxiv.org/abs/2307.07924v5", "abstract": "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev.", "source": "arxiv", "arxiv_id": "2307.07924v5", "pdf_url": "https://arxiv.org/pdf/2307.07924v5", "categories": ["cs.SE", "cs.CL", "cs.MA"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2023-07-16T02:11:34Z", "updated": "2024-06-05T13:23:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ChoiceMates: Supporting Unfamiliar Online Decision-Making with Multi-Agent Conversational Interactions", "authors": ["Jeongeon Park", "Bryan Min", "Kihoon Son", "Jean Y. Song", "Xiaojuan Ma", "Juho Kim"], "year": 2023, "url": "http://arxiv.org/abs/2310.01331v3", "abstract": "From deciding on a PhD program to buying a new camera, unfamiliar decisions--decisions without domain knowledge--are frequent and significant. The complexity and uncertainty of such decisions demand unique approaches to information seeking, understanding, and decision-making. Our formative study highlights that users want to start by discovering broad and relevant domain information evenly and simultaneously, quickly address emerging inquiries, and gain personalized standards to assess information found. We present ChoiceMates, an interactive multi-agent system designed to address these needs by enabling users to engage with a dynamic set of LLM agents each presenting a unique experience in the domain. Unlike existing multi-agent systems that automate tasks with agents, the user orchestrates agents to assist their decision-making process. Our user evaluation (n=12) shows that ChoiceMates enables a more confident, satisfactory decision-making with better situation understanding than web search, and higher decision quality and confidence than a commercial multi-agent framework. This work provides insights into designing a more controllable and collaborative multi-agent system.", "source": "arxiv", "arxiv_id": "2310.01331v3", "pdf_url": "https://arxiv.org/pdf/2310.01331v3", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2023-10-02T16:49:39Z", "updated": "2025-01-22T06:13:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game", "authors": ["Zijing Shi", "Meng Fang", "Shunfeng Zheng", "Shilong Deng", "Ling Chen", "Yali Du"], "year": 2023, "url": "http://arxiv.org/abs/2312.17515v1", "abstract": "Multi-agent collaboration with Large Language Models (LLMs) demonstrates proficiency in basic tasks, yet its efficiency in more complex scenarios remains unexplored. In gaming environments, these agents often face situations without established coordination protocols, requiring them to make intelligent inferences about teammates from limited data. This problem motivates the area of ad hoc teamwork, in which an agent may potentially cooperate with a variety of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork problem where the agent operates in an environment driven by natural language. Our findings reveal the potential of LLM agents in team collaboration, highlighting issues related to hallucinations in communication. To address this issue, we develop CodeAct, a general agent that equips LLM with enhanced memory and code-driven reasoning, enabling the repurposing of partial information for rapid adaptation to new teammates.", "source": "arxiv", "arxiv_id": "2312.17515v1", "pdf_url": "https://arxiv.org/pdf/2312.17515v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-29T08:26:54Z", "updated": "2023-12-29T08:26:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Do LLM Agents Exhibit Social Behavior?", "authors": ["Yan Leng", "Yuan Yuan"], "year": 2023, "url": "http://arxiv.org/abs/2312.15198v3", "abstract": "As LLMs increasingly take on roles in human-AI interactions and autonomous AI systems, understanding their social behavior becomes important for informed use and continuous improvement. However, their behaviors in social interactions with humans and other agents, as well as the mechanisms shaping their responses, remain underexplored. To address this gap, we introduce a novel probabilistic framework, State-Understanding-Value-Action (SUVA), to systematically analyze LLM responses in social contexts based on their textual outputs (i.e., utterances). Using canonical behavioral economics games and social preference concepts relatable to LLM users, SUVA assesses LLMs' social behavior through both their final decisions and the response generation processes leading to those decisions. Our analysis of eight LLMs -- including two GPT, four LLaMA, and two Mistral models -- suggests that most models do not generate decisions aligned solely with self-interest; instead, they often produce responses that reflect social welfare considerations and display patterns consistent with direct and indirect reciprocity. Additionally, higher-capacity models more frequently display group identity effects. The SUVA framework also provides explainable tools -- including tree-based visualizations and probabilistic dependency analysis -- to elucidate how factors in LLMs' utterance-based reasoning influence their decisions. We demonstrate that utterance-based reasoning reliably predicts LLMs' final actions; references to altruism, fairness, and cooperation in the reasoning increase the likelihood of prosocial actions, while mentions of self-interest and competition reduce them. Overall, our framework enables practitioners to assess LLMs for applications involving social interactions, and provides researchers with a structured method to interpret how LLM behavior arises from utterance-based reasoning.", "source": "arxiv", "arxiv_id": "2312.15198v3", "pdf_url": "https://arxiv.org/pdf/2312.15198v3", "categories": ["cs.AI", "cs.SI", "econ.GN"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-23T08:46:53Z", "updated": "2024-10-15T20:27:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ESGReveal: An LLM-based approach for extracting structured data from ESG reports", "authors": ["Yi Zou", "Mengying Shi", "Zhongjie Chen", "Zhu Deng", "ZongXiong Lei", "Zihan Zeng", "Shiming Yang", "HongXiang Tong", "Lei Xiao", "Wenwen Zhou"], "year": 2023, "url": "http://arxiv.org/abs/2312.17264v1", "abstract": "ESGReveal is an innovative method proposed for efficiently extracting and analyzing Environmental, Social, and Governance (ESG) data from corporate reports, catering to the critical need for reliable ESG information retrieval. This approach utilizes Large Language Models (LLM) enhanced with Retrieval Augmented Generation (RAG) techniques. The ESGReveal system includes an ESG metadata module for targeted queries, a preprocessing module for assembling databases, and an LLM agent for data extraction. Its efficacy was appraised using ESG reports from 166 companies across various sectors listed on the Hong Kong Stock Exchange in 2022, ensuring comprehensive industry and market capitalization representation. Utilizing ESGReveal unearthed significant insights into ESG reporting with GPT-4, demonstrating an accuracy of 76.9% in data extraction and 83.7% in disclosure analysis, which is an improvement over baseline models. This highlights the framework's capacity to refine ESG data analysis precision. Moreover, it revealed a demand for reinforced ESG disclosures, with environmental and social data disclosures standing at 69.5% and 57.2%, respectively, suggesting a pursuit for more corporate transparency. While current iterations of ESGReveal do not process pictorial information, a functionality intended for future enhancement, the study calls for continued research to further develop and compare the analytical capabilities of various LLMs. In summary, ESGReveal is a stride forward in ESG data processing, offering stakeholders a sophisticated tool to better evaluate and advance corporate sustainability efforts. Its evolution is promising in promoting transparency in corporate reporting and aligning with broader sustainable development aims.", "source": "arxiv", "arxiv_id": "2312.17264v1", "pdf_url": "https://arxiv.org/pdf/2312.17264v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-25T06:44:32Z", "updated": "2023-12-25T06:44:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld", "authors": ["Yijun Yang", "Tianyi Zhou", "Kanxue Li", "Dapeng Tao", "Lusong Li", "Li Shen", "Xiaodong He", "Jing Jiang", "Yuhui Shi"], "year": 2023, "url": "http://arxiv.org/abs/2311.16714v2", "abstract": "While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals. Although vision-language models (VLMs) integrate LLM modules (1) aligned with static image features, and (2) may possess prior knowledge of world dynamics (as demonstrated in the text world), they have not been trained in an embodied visual world and thus cannot align with its dynamics. On the other hand, training an embodied agent in a noisy visual world without expert guidance is often challenging and inefficient. In this paper, we train a VLM agent living in a visual world using an LLM agent excelling in a parallel text world. Specifically, we distill LLM's reflection outcomes (improved actions by analyzing mistakes) in a text world's tasks to finetune the VLM on the same tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world dynamics. Such cross-modality imitation learning between the two parallel worlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize to a broad scope of new tasks without any further guidance from the LLM expert. Extensive evaluations on the ALFWorld benchmark's diverse tasks highlight EMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement in the success rate.", "source": "arxiv", "arxiv_id": "2311.16714v2", "pdf_url": "https://arxiv.org/pdf/2311.16714v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-11-28T11:53:56Z", "updated": "2024-03-29T04:07:25Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Empowering Working Memory for Large Language Model Agents", "authors": ["Jing Guo", "Nan Li", "Jianchuan Qi", "Hang Yang", "Ruiqiao Li", "Yuzhen Feng", "Si Zhang", "Ming Xu"], "year": 2023, "url": "http://arxiv.org/abs/2312.17259v2", "abstract": "Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning. This paper explores the potential of applying cognitive psychology's working memory frameworks, to enhance LLM architecture. The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links. To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes. This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios. While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security. Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, human-like memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.", "source": "arxiv", "arxiv_id": "2312.17259v2", "pdf_url": "https://arxiv.org/pdf/2312.17259v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-22T05:59:00Z", "updated": "2024-05-28T05:34:52Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks", "authors": ["Megan Kinniment", "Lucas Jun Koba Sato", "Haoxing Du", "Brian Goodrich", "Max Hasin", "Lawrence Chan", "Luke Harold Miles", "Tao R. Lin", "Hjalmar Wijk", "Joel Burget", "Aaron Ho", "Elizabeth Barnes", "Paul Christiano"], "year": 2023, "url": "http://arxiv.org/abs/2312.11671v2", "abstract": "In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as \"autonomous replication and adaptation\" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult.\n  We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not adequate to rule out the possibility that near-future agents will be capable of ARA. In particular, we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA, unless intermediate evaluations are performed during pretraining. Relatedly, we expect that fine-tuning of the existing models could produce substantially more competent agents, even if the fine-tuning is not directly targeted at ARA.", "source": "arxiv", "arxiv_id": "2312.11671v2", "pdf_url": "https://arxiv.org/pdf/2312.11671v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-18T19:27:09Z", "updated": "2024-01-04T18:46:39Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ExpeL: LLM Agents Are Experiential Learners", "authors": ["Andrew Zhao", "Daniel Huang", "Quentin Xu", "Matthieu Lin", "Yong-Jin Liu", "Gao Huang"], "year": 2023, "url": "http://arxiv.org/abs/2308.10144v3", "abstract": "The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.", "source": "arxiv", "arxiv_id": "2308.10144v3", "pdf_url": "https://arxiv.org/pdf/2308.10144v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2023-08-20T03:03:34Z", "updated": "2024-12-20T06:14:53Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Experiential Co-Learning of Software-Developing Agents", "authors": ["Chen Qian", "Yufan Dang", "Jiahao Li", "Wei Liu", "Zihao Xie", "Yifei Wang", "Weize Chen", "Cheng Yang", "Xin Cong", "Xiaoyin Che", "Zhiyuan Liu", "Maosong Sun"], "year": 2023, "url": "http://arxiv.org/abs/2312.17025v3", "abstract": "Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at https://github.com/OpenBMB/ChatDev.", "source": "arxiv", "arxiv_id": "2312.17025v3", "pdf_url": "https://arxiv.org/pdf/2312.17025v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-28T13:50:42Z", "updated": "2024-06-05T13:39:20Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View", "authors": ["Jintian Zhang", "Xin Xu", "Ningyu Zhang", "Ruibo Liu", "Bryan Hooi", "Shumin Deng"], "year": 2023, "url": "http://arxiv.org/abs/2310.02124v3", "abstract": "As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets\\footnote{\\url{https://github.com/zjunlp/MachineSoM}.}, hoping to catalyze further research in this promising avenue.", "source": "arxiv", "arxiv_id": "2310.02124v3", "pdf_url": "https://arxiv.org/pdf/2310.02124v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-03T15:05:52Z", "updated": "2024-05-27T11:12:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web", "authors": ["Hiroki Furuta", "Yutaka Matsuo", "Aleksandra Faust", "Izzeddin Gur"], "year": 2023, "url": "http://arxiv.org/abs/2311.18751v3", "abstract": "Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise of small-scale finetuned and transferred models for task compositionality, their performance further degrades under different instruction compositions changing combinational order. In contrast to the recent remarkable success of LMA, our benchmark and detailed analysis emphasize the necessity of building LMAs that are robust and generalizable to task compositionality for real-world deployment.", "source": "arxiv", "arxiv_id": "2311.18751v3", "pdf_url": "https://arxiv.org/pdf/2311.18751v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2023-11-30T17:50:47Z", "updated": "2024-12-31T04:24:16Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Few-Shot Classification & Segmentation Using Large Language Models Agent", "authors": ["Tian Meng", "Yang Tao", "Wuliang Yin"], "year": 2023, "url": "http://arxiv.org/abs/2311.12065v1", "abstract": "The task of few-shot image classification and segmentation (FS-CS) requires the classification and segmentation of target objects in a query image, given only a few examples of the target classes. We introduce a method that utilises large language models (LLM) as an agent to address the FS-CS problem in a training-free manner. By making the LLM the task planner and off-the-shelf vision models the tools, the proposed method is capable of classifying and segmenting target objects using only image-level labels. Specifically, chain-of-thought prompting and in-context learning guide the LLM to observe support images like human; vision models such as Segment Anything Model (SAM) and GPT-4Vision assist LLM understand spatial and semantic information at the same time. Ultimately, the LLM uses its summarizing and reasoning capabilities to classify and segment the query image. The proposed method's modular framework makes it easily extendable. Our approach achieves state-of-the-art performance on the Pascal-5i dataset.", "source": "arxiv", "arxiv_id": "2311.12065v1", "pdf_url": "https://arxiv.org/pdf/2311.12065v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-11-19T00:33:41Z", "updated": "2023-11-19T00:33:41Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design", "authors": ["Yangyang Yu", "Haohang Li", "Zhi Chen", "Yuechen Jiang", "Yang Li", "Denghui Zhang", "Rong Liu", "Jordan W. Suchow", "Khaldoun Khashanah"], "year": 2023, "url": "http://arxiv.org/abs/2311.13743v2", "abstract": "Recent advancements in Large Language Models (LLMs) have exhibited notable efficacy in question-answering (QA) tasks across diverse domains. Their prowess in integrating extensive web knowledge has fueled interest in developing LLM-based autonomous agents. While LLMs are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks. Addressing this, we introduce \\textsc{FinMem}, a novel LLM-based agent framework devised for financial decision-making. It encompasses three core modules: Profiling, to customize the agent's characteristics; Memory, with layered message processing, to aid the agent in assimilating hierarchical financial data; and Decision-making, to convert insights gained from memories into investment decisions. Notably, \\textsc{FinMem}'s memory module aligns closely with the cognitive structure of human traders, offering robust interpretability and real-time tuning. Its adjustable cognitive span allows for the retention of critical information beyond human perceptual limits, thereby enhancing trading outcomes. This framework enables the agent to self-evolve its professional knowledge, react agilely to new investment cues, and continuously refine trading decisions in the volatile financial environment. We first compare \\textsc{FinMem} with various algorithmic agents on a scalable real-world financial dataset, underscoring its leading trading performance in stocks. We then fine-tuned the agent's perceptual span and character setting to achieve a significantly enhanced trading performance. Collectively, \\textsc{FinMem} presents a cutting-edge LLM agent framework for automated trading, boosting cumulative investment returns.", "source": "arxiv", "arxiv_id": "2311.13743v2", "pdf_url": "https://arxiv.org/pdf/2311.13743v2", "categories": ["q-fin.CP", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "q-fin.CP", "doi": "", "venue": "", "published": "2023-11-23T00:24:40Z", "updated": "2023-12-03T16:18:55Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "GestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents", "authors": ["Xin Zeng", "Xiaoyu Wang", "Tengxiang Zhang", "Chun Yu", "Shengdong Zhao", "Yiqiang Chen"], "year": 2023, "url": "http://arxiv.org/abs/2310.12821v5", "abstract": "Existing gesture interfaces only work with a fixed set of gestures defined either by interface designers or by users themselves, which introduces learning or demonstration efforts that diminish their naturalness. Humans, on the other hand, understand free-form gestures by synthesizing the gesture, context, experience, and common sense. In this way, the user does not need to learn, demonstrate, or associate gestures. We introduce GestureGPT, a free-form hand gesture understanding framework that mimics human gesture understanding procedures to enable a natural free-form gestural interface. Our framework leverages multiple Large Language Model agents to manage and synthesize gesture and context information, then infers the interaction intent by associating the gesture with an interface function. More specifically, our triple-agent framework includes a Gesture Description Agent that automatically segments and formulates natural language descriptions of hand poses and movements based on hand landmark coordinates. The description is deciphered by a Gesture Inference Agent through self-reasoning and querying about the interaction context (e.g., interaction history, gaze data), which is managed by a Context Management Agent. Following iterative exchanges, the Gesture Inference Agent discerns the user's intent by grounding it to an interactive function. We validated our framework offline under two real-world scenarios: smart home control and online video streaming. The average zero-shot Top-1/Top-5 grounding accuracies are 44.79%/83.59% for smart home tasks and 37.50%/73.44% for video streaming tasks. We also provide an extensive discussion that includes rationale for model selection, generalizability, and future research directions for a practical system etc.", "source": "arxiv", "arxiv_id": "2310.12821v5", "pdf_url": "https://arxiv.org/pdf/2310.12821v5", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "10.1145/3698145", "venue": "Proc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545, 2024, 38 pages", "published": "2023-10-19T15:17:34Z", "updated": "2024-11-04T02:48:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging", "authors": ["Qianou Ma", "Hua Shen", "Kenneth Koedinger", "Tongshuang Wu"], "year": 2023, "url": "http://arxiv.org/abs/2310.05292v5", "abstract": "Large Language Models (LLMs) now excel at generative skills and can create content at impeccable speeds. However, they are imperfect and still make various mistakes. In a Computer Science education context, as these models are widely recognized as \"AI pair programmers,\" it becomes increasingly important to train students on evaluating and debugging the LLM-generated code. In this work, we introduce HypoCompass, a novel system to facilitate deliberate practice on debugging, where human novices play the role of Teaching Assistants and help LLM-powered teachable agents debug code. We enable effective task delegation between students and LLMs in this learning-by-teaching environment: students focus on hypothesizing the cause of code errors, while adjacent skills like code completion are offloaded to LLM-agents. Our evaluations demonstrate that HypoCompass generates high-quality training materials (e.g., bugs and fixes), outperforming human counterparts fourfold in efficiency, and significantly improves student performance on debugging by 12% in the pre-to-post test.", "source": "arxiv", "arxiv_id": "2310.05292v5", "pdf_url": "https://arxiv.org/pdf/2310.05292v5", "categories": ["cs.HC", "cs.SE"], "primary_category": "cs.HC", "doi": "10.1007/978-3-031-64302-6_19", "venue": "AIED 2024, LNAI 14829, pp. 1-16", "published": "2023-10-08T21:39:47Z", "updated": "2024-10-10T16:29:59Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Knowledge-enhanced Agents for Interactive Text Games", "authors": ["Prateek Chhikara", "Jiarui Zhang", "Filip Ilievski", "Jonathan Francis", "Kaixin Ma"], "year": 2023, "url": "http://arxiv.org/abs/2305.05091v2", "abstract": "Communication via natural language is a key aspect of machine intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. Significant progress has been made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding. Yet, various sequential interactive tasks, as in text-based games, have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a knowledge-injection framework for improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports two representative model classes: reinforcement learning agents and language model agents. Furthermore, we devise multiple injection strategies for the above domain knowledge types and agent architectures, including injection via knowledge graphs and augmentation of the existing input encoding strategies. We experiment with four models on the 10 tasks in the ScienceWorld text-based game environment, to illustrate the impact of knowledge injection on various model configurations and challenging task settings. Our findings provide crucial insights into the interplay between task properties, model architectures, and domain knowledge for interactive contexts.", "source": "arxiv", "arxiv_id": "2305.05091v2", "pdf_url": "https://arxiv.org/pdf/2305.05091v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "10.1145/3587259.3627561", "venue": "", "published": "2023-05-08T23:31:39Z", "updated": "2023-12-17T02:03:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "L2MAC: Large Language Model Automatic Computer for Extensive Code Generation", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Mihaela van der Schaar"], "year": 2023, "url": "http://arxiv.org/abs/2310.02003v6", "abstract": "Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and coherent outputs. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long output generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based general-purpose stored-program automatic computer (von Neumann architecture) framework, an LLM-based multi-agent system, for long and consistent output generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction in turn is executed by a separate LLM agent, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective interaction with the file store. These components enable L2MAC to generate extensive outputs, bypassing the constraints of the finite context window while producing outputs that fulfill a complex user-specified task. We empirically demonstrate that L2MAC achieves state-of-the-art performance in generating large codebases for system design tasks, significantly outperforming other coding methods in implementing the detailed user-specified task; we show that L2MAC works for general-purpose extensive text-based tasks, such as writing an entire book; and we provide valuable insights into L2MAC's performance improvement over existing methods.", "source": "arxiv", "arxiv_id": "2310.02003v6", "pdf_url": "https://arxiv.org/pdf/2310.02003v6", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2023-10-02T16:55:19Z", "updated": "2025-06-27T17:28:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LASER: LLM Agent with State-Space Exploration for Web Navigation", "authors": ["Kaixin Ma", "Hongming Zhang", "Hongwei Wang", "Xiaoman Pan", "Wenhao Yu", "Dong Yu"], "year": 2023, "url": "http://arxiv.org/abs/2309.08172v2", "abstract": "Large language models (LLMs) have been successfully adapted for interactive decision-making tasks like web navigation. While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to guide the model on how to reason in the environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible backtracking, allowing the model to recover from errors easily. We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on both the WebShop task and amazon.com. Experimental results show that LASER significantly outperforms previous methods and closes the gap with human performance on the web navigation task.", "source": "arxiv", "arxiv_id": "2309.08172v2", "pdf_url": "https://arxiv.org/pdf/2309.08172v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-15T05:44:08Z", "updated": "2024-02-21T17:42:32Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay", "authors": ["Yihuai Lan", "Zhiqiang Hu", "Lei Wang", "Yang Wang", "Deheng Ye", "Peilin Zhao", "Ee-Peng Lim", "Hui Xiong", "Hao Wang"], "year": 2023, "url": "http://arxiv.org/abs/2310.14985v4", "abstract": "This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents' social behaviors. Results affirm the framework's effectiveness in creating adaptive agents and suggest LLM-based agents' potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field's research and applications. Our code is publicly available at https://github.com/3DAgentWorld/LLM-Game-Agent.", "source": "arxiv", "arxiv_id": "2310.14985v4", "pdf_url": "https://arxiv.org/pdf/2310.14985v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-23T14:35:26Z", "updated": "2024-10-13T22:09:00Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models", "authors": ["Saaket Agashe", "Yue Fan", "Anthony Reyna", "Xin Eric Wang"], "year": 2023, "url": "http://arxiv.org/abs/2310.03903v3", "abstract": "Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at https://github.com/eric-ai-lab/llm_coordination.", "source": "arxiv", "arxiv_id": "2310.03903v3", "pdf_url": "https://arxiv.org/pdf/2310.03903v3", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-05T21:18:15Z", "updated": "2025-04-28T20:21:54Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Language Models can Solve Computer Tasks", "authors": ["Geunwoo Kim", "Pierre Baldi", "Stephen McAleer"], "year": 2023, "url": "http://arxiv.org/abs/2303.17491v3", "abstract": "Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.", "source": "arxiv", "arxiv_id": "2303.17491v3", "pdf_url": "https://arxiv.org/pdf/2303.17491v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-03-30T16:01:52Z", "updated": "2023-11-16T20:15:14Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning", "authors": ["Zhiting Hu", "Tianmin Shu"], "year": 2023, "url": "http://arxiv.org/abs/2312.05230v1", "abstract": "Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities. In this position paper, we present a new perspective of machine reasoning, LAW, that connects the concepts of Language models, Agent models, and World models, for more robust and versatile reasoning capabilities. In particular, we propose that world and agent models are a better abstraction of reasoning, that introduces the crucial elements of deliberate human-like reasoning, including beliefs about the world and other agents, anticipation of consequences, goals/rewards, and strategic planning. Crucially, language models in LAW serve as a backend to implement the system or its elements and hence provide the computational power and adaptability. We review the recent studies that have made relevant progress and discuss future research directions towards operationalizing the LAW framework.", "source": "arxiv", "arxiv_id": "2312.05230v1", "pdf_url": "https://arxiv.org/pdf/2312.05230v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-08T18:25:22Z", "updated": "2023-12-08T18:25:22Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Models Are Semi-Parametric Reinforcement Learning Agents", "authors": ["Danyang Zhang", "Lu Chen", "Situo Zhang", "Hongshen Xu", "Zihan Zhao", "Kai Yu"], "year": 2023, "url": "http://arxiv.org/abs/2306.07929v2", "abstract": "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER.", "source": "arxiv", "arxiv_id": "2306.07929v2", "pdf_url": "https://arxiv.org/pdf/2306.07929v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-06-09T08:08:18Z", "updated": "2023-10-30T01:52:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach", "authors": ["Weiyu Ma", "Qirui Mi", "Yongcheng Zeng", "Xue Yan", "Yuqiao Wu", "Runji Lin", "Haifeng Zhang", "Jun Wang"], "year": 2023, "url": "http://arxiv.org/abs/2312.11865v3", "abstract": "StarCraft II is a challenging benchmark for AI agents due to the necessity of both precise micro level operations and strategic macro awareness. Previous works, such as Alphastar and SCC, achieve impressive performance on tackling StarCraft II , however, still exhibit deficiencies in long term strategic planning and strategy interpretability. Emerging large language model (LLM) agents, such as Voyage and MetaGPT, presents the immense potential in solving intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs on StarCraft II, a highly complex RTS game.To conveniently take full advantage of LLMs` reasoning abilities, we first develop textual StratCraft II environment, called TextStarCraft II, which LLM agent can interact. Secondly, we propose a Chain of Summarization method, including single frame summarization for processing raw observations and multi frame summarization for analyzing game information, providing command recommendations, and generating strategic decisions. Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and the impact of Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the relevant knowledge and complex planning abilities needed to address StarCraft II scenarios; 2. Human experts consider the performance of LLM agents to be close to that of an average player who has played StarCraft II for eight years; 3. LLM agents are capable of defeating the built in AI at the Harder(Lv5) difficulty level. We have open sourced the code and released demo videos of LLM agent playing StarCraft II.", "source": "arxiv", "arxiv_id": "2312.11865v3", "pdf_url": "https://arxiv.org/pdf/2312.11865v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-19T05:27:16Z", "updated": "2024-06-18T03:07:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Large Language Models as Agents in the Clinic", "authors": ["Nikita Mehandru", "Brenda Y. Miao", "Eduardo Rodriguez Almaraz", "Madhumita Sushil", "Atul J. Butte", "Ahmed Alaa"], "year": 2023, "url": "http://arxiv.org/abs/2309.10895v1", "abstract": "Recent developments in large language models (LLMs) have unlocked new opportunities for healthcare, from information synthesis to clinical decision support. These new LLMs are not just capable of modeling language, but can also act as intelligent \"agents\" that interact with stakeholders in open-ended conversations and even influence clinical decision-making. Rather than relying on benchmarks that measure a model's ability to process clinical data or answer standardized test questions, LLM agents should be assessed for their performance on real-world clinical tasks. These new evaluation frameworks, which we call \"Artificial-intelligence Structured Clinical Examinations\" (\"AI-SCI\"), can draw from comparable technologies where machines operate with varying degrees of self-governance, such as self-driving cars. High-fidelity simulations may also be used to evaluate interactions between users and LLMs within a clinical workflow, or to model the dynamic interactions of multiple LLMs. Developing these robust, real-world clinical evaluations will be crucial towards deploying LLM agents into healthcare.", "source": "arxiv", "arxiv_id": "2309.10895v1", "pdf_url": "https://arxiv.org/pdf/2309.10895v1", "categories": ["cs.HC", "cs.MA"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2023-09-19T19:38:28Z", "updated": "2023-09-19T19:38:28Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Let Models Speak Ciphers: Multiagent Debate through Embeddings", "authors": ["Chau Pham", "Boyi Liu", "Yingxiang Yang", "Zhengyu Chen", "Tianyi Liu", "Jianbo Yuan", "Bryan A. Plummer", "Zhaoran Wang", "Hongxia Yang"], "year": 2023, "url": "http://arxiv.org/abs/2310.06272v2", "abstract": "Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights, outperforming the state-of-the-art LLM debate methods using natural language by 0.5-5.0% across five reasoning tasks and multiple open-source LLMs of varying sizes. This showcases the superiority and robustness of embeddings as an alternative \"language\" for communication among LLMs. We anticipate that CIPHER will inspire further exploration for the design of interactions within LLM agent systems, offering a new direction that could significantly influence future developments in the field.", "source": "arxiv", "arxiv_id": "2310.06272v2", "pdf_url": "https://arxiv.org/pdf/2310.06272v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-10T03:06:38Z", "updated": "2024-02-26T17:36:48Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "MathChat: Converse to Tackle Challenging Math Problems with LLM Agents", "authors": ["Yiran Wu", "Feiran Jia", "Shaokun Zhang", "Hangyu Li", "Erkang Zhu", "Yue Wang", "Yin Tat Lee", "Richard Peng", "Qingyun Wu", "Chi Wang"], "year": 2023, "url": "http://arxiv.org/abs/2306.01337v3", "abstract": "Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields. LLMs, with their generalized ability, are used as a foundation model to build AI agents for different tasks. In this paper, we study the effectiveness of utilizing LLM agents to solve math problems through conversations. We propose MathChat, a conversational problem-solving framework designed for math problems. MathChat consists of an LLM agent and a user proxy agent which is responsible for tool execution and additional guidance. This synergy facilitates a collaborative problem-solving process, where the agents engage in a dialogue to solve the problems. We perform evaluation on difficult high school competition problems from the MATH dataset. Utilizing Python, we show that MathChat can further improve previous tool-using prompting methods by 6%.", "source": "arxiv", "arxiv_id": "2306.01337v3", "pdf_url": "https://arxiv.org/pdf/2306.01337v3", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-06-02T08:02:15Z", "updated": "2024-06-28T10:26:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Mechanism Design for Large Language Models", "authors": ["Paul Duetting", "Vahab Mirrokni", "Renato Paes Leme", "Haifeng Xu", "Song Zuo"], "year": 2023, "url": "http://arxiv.org/abs/2310.10826v3", "abstract": "We investigate auction mechanisms for AI-generated content, focusing on applications like ad creative generation. In our model, agents' preferences over stochastically generated content are encoded as large language models (LLMs). We propose an auction format that operates on a token-by-token basis, and allows LLM agents to influence content creation through single dimensional bids. We formulate two desirable incentive properties and prove their equivalence to a monotonicity condition on output aggregation. This equivalence enables a second-price rule design, even absent explicit agent valuation functions. Our design is supported by demonstrations on a publicly available LLM.", "source": "arxiv", "arxiv_id": "2310.10826v3", "pdf_url": "https://arxiv.org/pdf/2310.10826v3", "categories": ["cs.GT", "econ.TH"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2023-10-16T21:01:12Z", "updated": "2024-07-02T14:34:11Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents", "authors": ["Yashar Talebirad", "Amirhossein Nadiri"], "year": 2023, "url": "http://arxiv.org/abs/2306.03314v1", "abstract": "In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the \"Gorilla\" model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an avenue for advancing the capabilities and performance of LLMs through collaboration and knowledge exchange among intelligent agents.", "source": "arxiv", "arxiv_id": "2306.03314v1", "pdf_url": "https://arxiv.org/pdf/2306.03314v1", "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-06-05T23:55:37Z", "updated": "2023-06-05T23:55:37Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models", "authors": ["Yuchen Xiao", "Yanchao Sun", "Mengda Xu", "Udari Madhushani", "Jared Vann", "Deepeka Garg", "Sumitra Ganesh"], "year": 2023, "url": "http://arxiv.org/abs/2310.14403v5", "abstract": "Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations with long interaction horizons. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to improve LLM-powered policies without finetuning. The proposed method O3D (Offline Data-driven Discovery and Distillation) automatically discovers reusable skills and distills generalizable knowledge across multiple tasks based on offline interaction data, advancing the capability of solving downstream tasks. Empirical results under two interactive decision-making benchmarks (ALFWorld and WebShop) verify that O3D can notably enhance the decision-making capabilities of LLMs through the offline discovery and distillation process, and consistently outperform baselines across various LLMs.", "source": "arxiv", "arxiv_id": "2310.14403v5", "pdf_url": "https://arxiv.org/pdf/2310.14403v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-10-22T20:28:33Z", "updated": "2024-02-26T18:29:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering", "authors": ["Linyong Nan", "Ellen Zhang", "Weijin Zou", "Yilun Zhao", "Wenfei Zhou", "Arman Cohan"], "year": 2023, "url": "http://arxiv.org/abs/2311.09721v1", "abstract": "This study introduces a new long-form database question answering dataset designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter. The task necessitates LLMs to strategically generate multiple SQL queries to retrieve sufficient data from a database, to reason with the acquired context, and to synthesize them into a comprehensive analytical narrative. Our findings highlight that this task poses great challenges even for the state-of-the-art GPT-4 model. We propose and evaluate two interaction strategies, and provide a fine-grained analysis of the individual stages within the interaction. A key discovery is the identification of two primary bottlenecks hindering effective interaction: the capacity for planning and the ability to generate multiple SQL queries. To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations. This framework allows for a more nuanced understanding of the strengths and limitations of current LLMs in complex retrieval and reasoning tasks.", "source": "arxiv", "arxiv_id": "2311.09721v1", "pdf_url": "https://arxiv.org/pdf/2311.09721v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-16T09:55:07Z", "updated": "2023-11-16T09:55:07Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research", "authors": ["Jakub LÃ¡la", "Odhran O'Donoghue", "Aleksandar Shtedritski", "Sam Cox", "Samuel G. Rodriques", "Andrew D. White"], "year": 2023, "url": "http://arxiv.org/abs/2312.07559v2", "abstract": "Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-Augmented Generation (RAG) models have been proposed to reduce hallucinations and provide provenance for how an answer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present PaperQA, a RAG agent for answering questions over the scientific literature. PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers. Viewing this agent as a question answering model, we find it exceeds performance of existing LLMs and LLM agents on current science QA benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce LitQA, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate PaperQA's matches expert human researchers on LitQA.", "source": "arxiv", "arxiv_id": "2312.07559v2", "pdf_url": "https://arxiv.org/pdf/2312.07559v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-08T18:50:20Z", "updated": "2023-12-14T19:40:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents", "authors": ["Ziyi Yang", "Shreyas S. Raman", "Ankit Shah", "Stefanie Tellex"], "year": 2023, "url": "http://arxiv.org/abs/2309.09919v3", "abstract": "Recent advancements in large language models (LLMs) have enabled a new research domain, LLM agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of LLMs obtained during pretraining. However, while considerable effort has been made to teach the robot the \"dos,\" the \"don'ts\" received relatively less attention. We argue that, for any practical usage, it is as crucial to teach the robot the \"don'ts\": conveying explicit instructions about prohibited actions, assessing the robot's comprehension of these restrictions, and, most importantly, ensuring compliance. Moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as ISO 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. Aiming at deploying the LLM agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (LTL) that simultaneously enables natural language (NL) to temporal constraints encoding, safety violation reasoning and explaining, and unsafe action pruning. To demonstrate the effectiveness of our system, we conducted experiments in VirtualHome environment and on a real robot. The experimental results show that our system strictly adheres to the safety constraints and scales well with complex safety constraints, highlighting its potential for practical utility.", "source": "arxiv", "arxiv_id": "2309.09919v3", "pdf_url": "https://arxiv.org/pdf/2309.09919v3", "categories": ["cs.RO", "cs.AI", "cs.FL"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2023-09-18T16:33:30Z", "updated": "2023-11-28T07:08:29Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena", "authors": ["Jiangjie Chen", "Siyu Yuan", "Rong Ye", "Bodhisattwa Prasad Majumder", "Kyle Richardson"], "year": 2023, "url": "http://arxiv.org/abs/2310.05746v4", "abstract": "Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM performance and occasional outperformance by simpler methods indicate opportunities for further advancements in LLM design and the value of our simulation environment for ongoing testing and refinement.", "source": "arxiv", "arxiv_id": "2310.05746v4", "pdf_url": "https://arxiv.org/pdf/2310.05746v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-09T14:22:09Z", "updated": "2024-08-25T11:19:33Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RAH! RecSys-Assistant-Human: A Human-Centered Recommendation Framework with LLM Agents", "authors": ["Yubo Shu", "Haonan Zhang", "Hansu Gu", "Peng Zhang", "Tun Lu", "Dongsheng Li", "Ning Gu"], "year": 2023, "url": "http://arxiv.org/abs/2308.09904v2", "abstract": "The rapid evolution of the web has led to an exponential growth in content. Recommender systems play a crucial role in Human-Computer Interaction (HCI) by tailoring content based on individual preferences. Despite their importance, challenges persist in balancing recommendation accuracy with user satisfaction, addressing biases while preserving user privacy, and solving cold-start problems in cross-domain situations. This research argues that addressing these issues is not solely the recommender systems' responsibility, and a human-centered approach is vital. We introduce the RAH Recommender system, Assistant, and Human) framework, an innovative solution with LLM-based agents such as Perceive, Learn, Act, Critic, and Reflect, emphasizing the alignment with user personalities. The framework utilizes the Learn-Act-Critic loop and a reflection mechanism for improving user alignment. Using the real-world data, our experiments demonstrate the RAH framework's efficacy in various recommendation domains, from reducing human burden to mitigating biases and enhancing user control. Notably, our contributions provide a human-centered recommendation framework that partners effectively with various recommendation models.", "source": "arxiv", "arxiv_id": "2308.09904v2", "pdf_url": "https://arxiv.org/pdf/2308.09904v2", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2023-08-19T04:46:01Z", "updated": "2023-10-17T11:48:10Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs", "authors": ["Justin Chih-Yao Chen", "Swarnadeep Saha", "Mohit Bansal"], "year": 2023, "url": "http://arxiv.org/abs/2309.13007v3", "abstract": "Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance. Code: https://github.com/dinobby/ReConcile", "source": "arxiv", "arxiv_id": "2309.13007v3", "pdf_url": "https://arxiv.org/pdf/2309.13007v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-22T17:12:45Z", "updated": "2024-06-21T19:34:27Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent", "authors": ["Renat Aksitov", "Sobhan Miryoosefi", "Zonglin Li", "Daliang Li", "Sheila Babayan", "Kavya Kopparapu", "Zachary Fisher", "Ruiqi Guo", "Sushant Prakash", "Pranesh Srinivasan", "Manzil Zaheer", "Felix Yu", "Sanjiv Kumar"], "year": 2023, "url": "http://arxiv.org/abs/2312.10003v1", "abstract": "Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.", "source": "arxiv", "arxiv_id": "2312.10003v1", "pdf_url": "https://arxiv.org/pdf/2312.10003v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-15T18:20:15Z", "updated": "2023-12-15T18:20:15Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency", "authors": ["Zhihan Liu", "Hao Hu", "Shenao Zhang", "Hongyi Guo", "Shuqi Ke", "Boyi Liu", "Zhaoran Wang"], "year": 2023, "url": "http://arxiv.org/abs/2309.17382v3", "abstract": "Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call \"reason for future, act for now\" (\\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (\"reason for future\"). At each step, the LLM agent takes the initial action of the planned trajectory (\"act for now\"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.\n  The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an \"in-context\" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. Here, $T$ denotes the number of online interactions. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks.", "source": "arxiv", "arxiv_id": "2309.17382v3", "pdf_url": "https://arxiv.org/pdf/2309.17382v3", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-09-29T16:36:39Z", "updated": "2024-06-24T05:25:21Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models", "authors": ["Zhao Mandi", "Shreeya Jain", "Shuran Song"], "year": 2023, "url": "http://arxiv.org/abs/2307.04738v1", "abstract": "We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website https://project-roco.github.io for videos and code.", "source": "arxiv", "arxiv_id": "2307.04738v1", "pdf_url": "https://arxiv.org/pdf/2307.04738v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2023-07-10T17:52:01Z", "updated": "2023-07-10T17:52:01Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Self-collaboration Code Generation via ChatGPT", "authors": ["Yihong Dong", "Xue Jiang", "Zhi Jin", "Ge Li"], "year": 2023, "url": "http://arxiv.org/abs/2304.07590v3", "abstract": "Although Large Language Models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLM agents act as distinct `experts', each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other's work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development's analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9%-47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.", "source": "arxiv", "arxiv_id": "2304.07590v3", "pdf_url": "https://arxiv.org/pdf/2304.07590v3", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2023-04-15T16:33:32Z", "updated": "2024-05-11T14:00:45Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning", "authors": ["Shaohui Peng", "Xing Hu", "Qi Yi", "Rui Zhang", "Jiaming Guo", "Di Huang", "Zikang Tian", "Ruizhi Chen", "Zidong Du", "Qi Guo", "Yunji Chen", "Ling Li"], "year": 2023, "url": "http://arxiv.org/abs/2309.01352v1", "abstract": "Large language models (LLMs) show their powerful automatic reasoning and planning capability with a wealth of semantic knowledge about the human world. However, the grounding problem still hinders the applications of LLMs in the real-world environment. Existing studies try to fine-tune the LLM or utilize pre-defined behavior APIs to bridge the LLMs and the environment, which not only costs huge human efforts to customize for every single task but also weakens the generality strengths of LLMs. To autonomously ground the LLM onto the environment, we proposed the Self-Driven Grounding (SDG) framework to automatically and progressively ground the LLM with self-driven skill learning. SDG first employs the LLM to propose the hypothesis of sub-goals to achieve tasks and then verify the feasibility of the hypothesis via interacting with the underlying environment. Once verified, SDG can then learn generalized skills with the guidance of these successfully grounded subgoals. These skills can be further utilized to accomplish more complex tasks which fail to pass the verification phase. Verified in the famous instruction following task set-BabyAI, SDG achieves comparable performance in the most challenging tasks compared with imitation learning methods that cost millions of demonstrations, proving the effectiveness of learned skills and showing the feasibility and efficiency of our framework.", "source": "arxiv", "arxiv_id": "2309.01352v1", "pdf_url": "https://arxiv.org/pdf/2309.01352v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-04T04:31:24Z", "updated": "2023-09-04T04:31:24Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SmartPlay: A Benchmark for LLMs as Intelligent Agents", "authors": ["Yue Wu", "Xuan Tang", "Tom M. Mitchell", "Yuanzhi Li"], "year": 2023, "url": "http://arxiv.org/abs/2310.01557v5", "abstract": "Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at github.com/Microsoft/SmartPlay", "source": "arxiv", "arxiv_id": "2310.01557v5", "pdf_url": "https://arxiv.org/pdf/2310.01557v5", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2023-10-02T18:52:11Z", "updated": "2024-03-17T23:23:31Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "SurrealDriver: Designing LLM-powered Generative Driver Agent Framework based on Human Drivers' Driving-thinking Data", "authors": ["Ye Jin", "Ruoxuan Yang", "Zhijie Yi", "Xiaoxi Shen", "Huiling Peng", "Xiaoan Liu", "Jingli Qin", "Jiayang Li", "Jintao Xie", "Peizhong Gao", "Guyue Zhou", "Jiangtao Gong"], "year": 2023, "url": "http://arxiv.org/abs/2309.13193v2", "abstract": "Leveraging advanced reasoning capabilities and extensive world knowledge of large language models (LLMs) to construct generative agents for solving complex real-world problems is a major trend. However, LLMs inherently lack embodiment as humans, resulting in suboptimal performance in many embodied decision-making tasks. In this paper, we introduce a framework for building human-like generative driving agents using post-driving self-report driving-thinking data from human drivers as both demonstration and feedback. To capture high-quality, natural language data from drivers, we conducted urban driving experiments, recording drivers' verbalized thoughts under various conditions to serve as chain-of-thought prompts and demonstration examples for the LLM-Agent. The framework's effectiveness was evaluated through simulations and human assessments. Results indicate that incorporating expert demonstration data significantly reduced collision rates by 81.04\\% and increased human likeness by 50\\% compared to a baseline LLM-based agent. Our study provides insights into using natural language-based human demonstration data for embodied tasks. The driving-thinking dataset is available at \\url{https://github.com/AIR-DISCOVER/Driving-Thinking-Dataset}.", "source": "arxiv", "arxiv_id": "2309.13193v2", "pdf_url": "https://arxiv.org/pdf/2309.13193v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2023-09-22T21:56:00Z", "updated": "2024-07-22T00:29:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Tailoring with Targeted Precision: Edit-Based Agents for Open-Domain Procedure Customization", "authors": ["Yash Kumar Lal", "Li Zhang", "Faeze Brahman", "Bodhisattwa Prasad Majumder", "Peter Clark", "Niket Tandon"], "year": 2023, "url": "http://arxiv.org/abs/2311.09510v3", "abstract": "How-to procedures, such as how to plant a garden, are now used by millions of users, but sometimes need customizing to meet a user's specific needs, e.g., planting a garden without pesticides. Our goal is to measure and improve an LLM's ability to perform such customization. Our approach is to test several simple multi-LLM-agent architectures for customization, as well as an end-to-end LLM, using a new evaluation set, called CustomPlans, of over 200 WikiHow procedures each with a customization need. We find that a simple architecture with two LLM agents used sequentially performs best, one that edits a generic how-to procedure and one that verifies its executability, significantly outperforming (10.5% absolute) an end-to-end prompted LLM. This suggests that LLMs can be configured reasonably effectively for procedure customization. This also suggests that multi-agent editing architectures may be worth exploring further for other customization applications (e.g. coding, creative writing) in the future.", "source": "arxiv", "arxiv_id": "2311.09510v3", "pdf_url": "https://arxiv.org/pdf/2311.09510v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-16T02:25:36Z", "updated": "2024-05-31T01:32:23Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Testing Language Model Agents Safely in the Wild", "authors": ["Silen Naihin", "David Atkinson", "Marc Green", "Merwane Hamadi", "Craig Swift", "Douglas Schonholtz", "Adam Tauman Kalai", "David Bau"], "year": 2023, "url": "http://arxiv.org/abs/2311.10538v3", "abstract": "A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet real-world autonomous tests face several unique safety challenges, both due to the possibility of causing harm during a test, as well as the risk of encountering new unsafe agent behavior through interactions with real-world and potentially malicious actors. We propose a framework for conducting safe autonomous agent tests on the open internet: agent actions are audited by a context-sensitive monitor that enforces a stringent safety boundary to stop an unsafe test, with suspect behavior ranked and logged to be examined by humans. We design a basic safety monitor (AgentMonitor) that is flexible enough to monitor existing LLM agents, and, using an adversarial simulated agent, we measure its ability to identify and stop unsafe situations. Then we apply the AgentMonitor on a battery of real-world tests of AutoGPT, and we identify several limitations and challenges that will face the creation of safe in-the-wild tests as autonomous agents grow more capable.", "source": "arxiv", "arxiv_id": "2311.10538v3", "pdf_url": "https://arxiv.org/pdf/2311.10538v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-11-17T14:06:05Z", "updated": "2023-12-03T13:18:09Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Perils & Promises of Fact-checking with Large Language Models", "authors": ["Dorian Quelle", "Alexandre Bovet"], "year": 2023, "url": "http://arxiv.org/abs/2310.13549v2", "abstract": "Automated fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, and news articles and to verify information, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Understanding the capacities and limitations of LLMs in fact-checking tasks is therefore essential for ensuring the health of our information ecosystem. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.", "source": "arxiv", "arxiv_id": "2310.13549v2", "pdf_url": "https://arxiv.org/pdf/2310.13549v2", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL", "doi": "10.3389/frai.2024.1341697", "venue": "Frontiers in Artificial Intelligence, Volume 7, 2024", "published": "2023-10-20T14:49:47Z", "updated": "2024-02-07T12:01:49Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "The Philosopher's Stone: Trojaning Plugins of Large Language Models", "authors": ["Tian Dong", "Minhui Xue", "Guoxing Chen", "Rayne Holland", "Yan Meng", "Shaofeng Li", "Zhen Liu", "Haojin Zhu"], "year": 2023, "url": "http://arxiv.org/abs/2312.00374v3", "abstract": "Open-source Large Language Models (LLMs) have recently gained popularity because of their comparable performance to proprietary LLMs. To efficiently fulfill domain-specialized tasks, open-source LLMs can be refined, without expensive accelerators, using low-rank adapters. However, it is still unknown whether low-rank adapters can be exploited to control LLMs. To address this gap, we demonstrate that an infected adapter can induce, on specific triggers,an LLM to output content defined by an adversary and to even maliciously use tools. To train a Trojan adapter, we propose two novel attacks, POLISHED and FUSION, that improve over prior approaches. POLISHED uses a superior LLM to align naÃ¯vely poisoned data based on our insight that it can better inject poisoning knowledge during training. In contrast, FUSION leverages a novel over-poisoning procedure to transform a benign adapter into a malicious one by magnifying the attention between trigger and target in model weights. In our experiments, we first conduct two case studies to demonstrate that a compromised LLM agent can use malware to control the system (e.g., a LLM-driven robot) or to launch a spear-phishing attack. Then, in terms of targeted misinformation, we show that our attacks provide higher attack effectiveness than the existing baseline and, for the purpose of attracting downloads, preserve or improve the adapter's utility. Finally, we designed and evaluated three potential defenses. However, none proved entirely effective in safeguarding against our attacks, highlighting the need for more robust defenses supporting a secure LLM supply chain.", "source": "arxiv", "arxiv_id": "2312.00374v3", "pdf_url": "https://arxiv.org/pdf/2312.00374v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2023-12-01T06:36:17Z", "updated": "2024-09-11T12:48:42Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory", "authors": ["Lei Liu", "Xiaoyan Yang", "Yue Shen", "Binbin Hu", "Zhiqiang Zhang", "Jinjie Gu", "Guannan Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2311.08719v1", "abstract": "Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, \\textit{i.e.}, inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, (\\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.", "source": "arxiv", "arxiv_id": "2311.08719v1", "pdf_url": "https://arxiv.org/pdf/2311.08719v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-15T06:08:35Z", "updated": "2023-11-15T06:08:35Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph", "authors": ["Jiashuo Sun", "Chengjin Xu", "Lumingyuan Tang", "Saizhuo Wang", "Chen Lin", "Yeyun Gong", "Lionel M. Ni", "Heung-Yeung Shum", "Jian Guo"], "year": 2023, "url": "http://arxiv.org/abs/2307.07697v6", "abstract": "Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.", "source": "arxiv", "arxiv_id": "2307.07697v6", "pdf_url": "https://arxiv.org/pdf/2307.07697v6", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-07-15T03:31:38Z", "updated": "2024-03-24T06:42:47Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
{"title": "Thinking Assistants: LLM-Based Conversational Assistants that Help Users Think By Asking rather than Answering", "authors": ["Soya Park", "Hari Subramonyam", "Chinmay Kulkarni"], "year": 2023, "url": "http://arxiv.org/abs/2312.06024v4", "abstract": "Many AI systems focus solely on providing solutions or explaining outcomes. However, complex tasks like research and strategic thinking often benefit from a more comprehensive approach to augmenting the thinking process rather than passively getting information. We introduce the concept of \"Thinking Assistant\", a new genre of assistants that help users improve decision-making with a combination of asking reflection questions based on expert knowledge. Through our lab study (N=80), these Large Language Model (LLM) based Thinking Assistants were better able to guide users to make important decisions, compared with conversational agents that only asked questions, provided advice, or neither.\n  Based on the results, we develop a Thinking Assistant in academic career development, determining research trajectory or developing one's unique research identity, which requires deliberation, reflection and experts' advice accordingly. In a longitudinal deployment with 223 conversations, participants responded positively to approximately 65% of the responses.\n  Our work proposes directions for developing more effective LLM agents. Rather than adhering to the prevailing authoritative approach of generating definitive answers, LLM agents aimed at assisting with cognitive enhancement should prioritize fostering reflection. They should initially provide responses designed to prompt thoughtful consideration through inquiring, followed by offering advice only after gaining a deeper understanding of the user's context and needs.", "source": "arxiv", "arxiv_id": "2312.06024v4", "pdf_url": "https://arxiv.org/pdf/2312.06024v4", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2023-12-10T22:38:36Z", "updated": "2024-12-24T01:11:04Z", "provenance": [{"route": "arxiv_query:(all:\"LLM agent survey (A150++)\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T22:50:47", "note": ""}]}
