**Index Table 1. Subsection map (axes + representative works).**

| Subsection | Axes | Representative works |
|---|---|---|
| 3.1 Agent loop and action spaces | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Song2025Agent; @Liu2025Mcpagentbench; @Tomaevi2025Towards; @Feng2025Group; @Chowa2025From] |
| 3.2 Tool interfaces and orchestration | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Zhou2026Beyond; @Dong2025Etom; @Yang2025Toolmind; @Mohammadi2025Evaluation; @Liu2025Toolscope] |
| 4.1 Planning and reasoning loops | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Shang2024Agentsquare; @Luo2025Universe; @Nakano2025Guided; @Zhou2025Siraj; @Hu2025Training] |
| 4.2 Memory and retrieval (RAG) | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Shi2025Progent; @Dong2025Etom; @Shang2024Agentsquare; @Li2025Agentswift; @Maragheh2025Future] |
| 5.1 Self-improvement and adaptation | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>communication protocol / roles<br>aggregation (vote / debate / referee)<br>stability / robustness | [@Zhou2025Self; @Li2026Autonomous; @Du2024Anytool; @Zhang2026Evoroute; @Guo2025Comprehensive] |
| 5.2 Multi-agent coordination | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Feng2025Group; @Zhang2025Datascibench; @Yang2024Based; @Lumer2025Memtool; @Lu2025Just] |
| 6.1 Benchmarks and evaluation protocols | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Fu2025Eval; @Kwon2025Agentnet; @Shi2025Progent; @Mohammadi2025Evaluation; @Zhou2025Self] |
| 6.2 Safety, security, and governance | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Zhang2025Security; @Luo2025Universe; @Fu2025Eval; @Kale2025Reliable; @Erdogan2024Tinyagent] |

**Index Table 2. Concrete anchors (benchmarks / numbers / caveats).**

| Subsection | Anchor facts | Representative works |
|---|---|---|
| 3.1 Agent loop and action spaces | Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datas<br>Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability w<br>Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the | [@Chowa2025From; @Gasmi2025Bridging; @Zhang2026Evoroute] |
| 3.2 Tool interfaces and orchestration | Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs,<br>Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datas<br>To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data ins | [@Nandi2025Bench; @Chowa2025From; @Yang2025Toolmind] |
| 4.1 Planning and reasoning loops | To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4)<br>LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than base<br>Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show | [@Nakano2025Guided; @Gao2024Efficient; @Shang2024Agentsquare] |
| 4.2 Memory and retrieval (RAG) | To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Re<br>Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show<br>Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discover | [@Abbineni2025Muallm; @Shang2024Agentsquare; @Li2025Agentswift] |
| 5.1 Self-improvement and adaptation | Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework ach<br>Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution st<br>Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration div | [@Zhou2025Self; @Guo2025Comprehensive; @Yang2025Bioverge] |
| 5.2 Multi-agent coordination | Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competi<br>Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interac<br>We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search- | [@Lu2025Just; @Lumer2025Memtool; @Feng2025Group] |
| 6.1 Benchmarks and evaluation protocols | Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework ach<br>Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of t<br>This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy th | [@Zhou2025Self; @Zhong2025Rtbas; @Mohammadi2025Evaluation] |
| 6.2 Safety, security, and governance | Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability w<br>Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invo<br>MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in too | [@Gasmi2025Bridging; @Mou2026Toolsafe; @Zhang2025Security] |
