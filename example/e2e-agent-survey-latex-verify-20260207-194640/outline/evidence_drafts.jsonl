{"sub_id": "3.1", "title": "Agent loop and action spaces", "evidence_ids": ["E-P0121-44cc8fbce0", "E-P0191-312a342670", "E-P0232-1232593f0e", "E-P0177-4b027dfb27", "E-P0012-e0bcca0d9e", "E-P0040-8e34a29629", "E-P0103-60cc0d458f", "E-P0156-0b34b55332", "E-P0236-1ca9e70d75", "E-P0159-39281e5083", "E-P0001-ca4a00b5cf", "E-P0201-1d5f67b08e", "E-P0092-ebb97f6129", "E-P0047-a06d9a2787", "E-P0173-66900cdb54", "E-P0230-15e523063d", "E-P0023-83f8d55860", "E-P0045-8a32e8598f", "E-P0030-4c14763fd0", "E-P0206-aa504d163c", "E-P0230-171b93237a", "E-P0291-3728b2e26e", "E-P0295-f1a25e82c1", "E-P0206-50bf74b151"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0121-44cc8fbce0", "text": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "paper_id": "P0121", "citations": ["Song2025Agent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}}, {"evidence_id": "E-P0191-312a342670", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "paper_id": "P0191", "citations": ["Liu2025Mcpagentbench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method"}}, {"evidence_id": "E-P0232-1232593f0e", "text": "Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates.", "paper_id": "P0232", "citations": ["Tomaevi2025Towards"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0232#limitations[1]"}}, {"evidence_id": "E-P0177-4b027dfb27", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "paper_id": "P0177", "citations": ["Feng2025Group"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}}, {"evidence_id": "E-P0012-e0bcca0d9e", "text": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "paper_id": "P0012", "citations": ["Chowa2025From"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}}, {"evidence_id": "E-P0040-8e34a29629", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "paper_id": "P0040", "citations": ["Gasmi2025Bridging"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}}, {"evidence_id": "E-P0103-60cc0d458f", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "paper_id": "P0103", "citations": ["Zhang2026Evoroute"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}}, {"evidence_id": "E-P0156-0b34b55332", "text": "We propose CyberSleuth, an autonomous agent that processes packet-level traces and application logs to identify the targeted service, the exploited vulnerability (CVE), and attack success.", "paper_id": "P0156", "citations": ["Fumero2025Cybersleuth"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0156#method"}}, {"evidence_id": "E-P0236-1ca9e70d75", "text": "These third-party MCP services provider are potentially malicious and have the economic incentives to exploit vulnerabilities and sabotage user-agent interactions.", "paper_id": "P0236", "citations": ["Fang2025Should"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0236#summary_bullets[4]"}}, {"evidence_id": "E-P0159-39281e5083", "text": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "paper_id": "P0159", "citations": ["Zhang2025Datascibench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0159#key_results[1]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0201-1d5f67b08e", "text": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "paper_id": "P0201", "citations": ["Ghose2025Orfs"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0201#key_results[0]"}}, {"evidence_id": "E-P0092-ebb97f6129", "text": "With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents.", "paper_id": "P0092", "citations": ["Yin2024Mmau"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}}, {"evidence_id": "E-P0047-a06d9a2787", "text": "By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.", "paper_id": "P0047", "citations": ["Zhuang2025Hephaestus"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[1]"}}, {"evidence_id": "E-P0173-66900cdb54", "text": "Current agent benchmarks often mix agentic reasoning with challenging math reasoning, expert-level knowledge, and other advanced capabilities.", "paper_id": "P0173", "citations": ["Zhu2025Agent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0173#key_results[1]"}}, {"evidence_id": "E-P0230-15e523063d", "text": "Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content.", "paper_id": "P0230", "citations": ["Cui2025Toward"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0230#key_results[1]"}}, {"evidence_id": "E-P0023-83f8d55860", "text": "By integrating the tool registration process into the reasoning procedure, EcoAct reduces computational costs by over 50% in multiple steps reasoning tasks while maintaining performance, as demonstrated through extensive experiments.", "paper_id": "P0023", "citations": ["Zhang2024Ecoact"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}}, {"evidence_id": "E-P0045-8a32e8598f", "text": "While proprietary closed-source models such as GPT-4 demonstrate strong reasoning abilities, smaller open-source models struggle to perform complex tool use effectively.", "paper_id": "P0045", "citations": ["Min2025Goat"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0045#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Agent loop and action spaces'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent", "Song2026Envscaler"]}], "claim_candidates": [{"claim": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "evidence_field": "evidence_snippet", "citations": ["Song2025Agent"]}, {"claim": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "evidence_field": "evidence_snippet", "citations": ["Liu2025Mcpagentbench"]}, {"claim": "Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates.", "evidence_field": "evidence_snippet", "citations": ["Tomaevi2025Towards"]}, {"claim": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "evidence_field": "evidence_snippet", "citations": ["Feng2025Group"]}, {"claim": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "evidence_field": "evidence_snippet", "citations": ["Chowa2025From"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0103`, `P0291`, `P0295`", "B_papers": "Tool-use and function calling: `P0295`, `P0012`, `P0030`", "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 3}, {"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 3}, {"paper_id": "P0191", "evidence_id": "E-P0191-312a342670", "excerpt": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method", "score": 3}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Chowa2025From", "Gasmi2025Bridging", "Liu2025Mcpagentbench"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Code agents / software tasks", "A_papers": "Agent frameworks / architectures: `P0103`, `P0291`, `P0295`", "B_papers": "Code agents / software tasks: `P0295`, `P0040`, `P0119`", "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 3}, {"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Code agents / software tasks along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Chowa2025From", "Gasmi2025Bridging"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Tool-use and function calling", "B_label": "Code agents / software tasks", "A_papers": "Tool-use and function calling: `P0295`, `P0012`, `P0030`", "B_papers": "Code agents / software tasks: `P0295`, `P0040`, `P0119`", "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 3}, {"paper_id": "P0191", "evidence_id": "E-P0191-312a342670", "excerpt": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method", "score": 3}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}], "write_prompt": "Contrast Tool-use and function calling vs Code agents / software tasks along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Chowa2025From", "Liu2025Mcpagentbench", "Gasmi2025Bridging"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0103`, `P0291`, `P0295`", "B_papers": "Tool-use and function calling: `P0295`, `P0012`, `P0030`", "A_highlights": [{"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]", "score": 2}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-44cc8fbce0", "excerpt": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "citations": ["Song2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 0}, {"paper_id": "P0201", "evidence_id": "E-P0201-1d5f67b08e", "excerpt": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "pointer": "papers/paper_notes.jsonl:paper_id=P0201#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2026Evoroute", "Chowa2025From", "Song2025Agent", "Ghose2025Orfs"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Code agents / software tasks", "A_papers": "Agent frameworks / architectures: `P0103`, `P0291`, `P0295`", "B_papers": "Code agents / software tasks: `P0295`, `P0040`, `P0119`", "A_highlights": [{"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]", "score": 2}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Code agents / software tasks along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2026Evoroute", "Chowa2025From", "Gasmi2025Bridging"]}, {"axis": "compute and latency constraints", "A_label": "Tool-use and function calling", "B_label": "Code agents / software tasks", "A_papers": "Tool-use and function calling: `P0295`, `P0012`, `P0030`", "B_papers": "Code agents / software tasks: `P0295`, `P0040`, `P0119`", "A_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-44cc8fbce0", "excerpt": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "citations": ["Song2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 0}, {"paper_id": "P0201", "evidence_id": "E-P0201-1d5f67b08e", "excerpt": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "pointer": "papers/paper_notes.jsonl:paper_id=P0201#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 0}], "write_prompt": "Contrast Tool-use and function calling vs Code agents / software tasks along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Song2025Agent", "Ghose2025Orfs", "Gasmi2025Bridging"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0103`, `P0291`, `P0295`", "B_papers": "Tool-use and function calling: `P0295`, `P0012`, `P0030`", "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0191", "evidence_id": "E-P0191-312a342670", "excerpt": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method", "score": 2}, {"paper_id": "P0121", "evidence_id": "E-P0121-44cc8fbce0", "excerpt": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "citations": ["Song2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Gasmi2025Bridging", "Chowa2025From", "Liu2025Mcpagentbench", "Song2025Agent"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Code agents / software tasks", "A_papers": "Agent frameworks / architectures: `P0103`, `P0291`, `P0295`", "B_papers": "Code agents / software tasks: `P0295`, `P0040`, `P0119`", "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Code agents / software tasks along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Gasmi2025Bridging", "Chowa2025From"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Tool-use and function calling", "B_label": "Code agents / software tasks", "A_papers": "Tool-use and function calling: `P0295`, `P0012`, `P0030`", "B_papers": "Code agents / software tasks: `P0295`, `P0040`, `P0119`", "A_highlights": [{"paper_id": "P0191", "evidence_id": "E-P0191-312a342670", "excerpt": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method", "score": 2}, {"paper_id": "P0121", "evidence_id": "E-P0121-44cc8fbce0", "excerpt": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "citations": ["Song2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}], "write_prompt": "Contrast Tool-use and function calling vs Code agents / software tasks along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Liu2025Mcpagentbench", "Song2025Agent", "Gasmi2025Bridging"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0103`, `P0291`, `P0295`", "B_papers": "Tool-use and function calling: `P0295`, `P0012`, `P0030`", "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0191", "evidence_id": "E-P0191-312a342670", "excerpt": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method", "score": 2}, {"paper_id": "P0121", "evidence_id": "E-P0121-44cc8fbce0", "excerpt": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "citations": ["Song2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Gasmi2025Bridging", "Chowa2025From", "Liu2025Mcpagentbench", "Song2025Agent"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, GAIA, EvoRoute, BrowseComp, LLM-simulated, SFT, RUC-NLPIR, EnvScaler, SkelBuilder, ScenGenerator.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent", "Song2026Envscaler", "Soliman2026Intagent"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent"]}], "failures_limitations": [{"bullet": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"]}, {"bullet": "With the spread of generative AI in recent years, attacks known as Whaling have become a serious threat.", "citations": ["Miyamoto2026Agent"]}, {"bullet": "We design agents that (i) build vulnerability profiles for each target from publicly available information on faculty members, (ii) identify potential risk scenarios relevant to Whaling defense based on those profiles, (iii) construct defense profiles corresponding to the vulnerabilities and anticipated risks, and (iv) analyze Whaling emails using the defense profiles.", "citations": ["Miyamoto2026Agent"]}, {"bullet": "Furthermore, we conduct a preliminary risk-assessment experiment.", "citations": ["Miyamoto2026Agent"]}, {"bullet": "Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency.", "citations": ["Yang2025Survey"]}, {"bullet": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "citations": ["Gasmi2025Bridging"]}, {"bullet": "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "citations": ["Gasmi2025Bridging"]}, {"bullet": "We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service).", "citations": ["Gasmi2025Bridging"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-02-07T19:55:33", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "evidence_ids": ["E-P0014-b6b7af5a81", "E-P0162-192e78b614", "E-P0022-f5a0e32a25", "E-P0168-37f9ea924c", "E-P0229-468a77ff1d", "E-P0263-0ea7b39672", "E-P0013-4e80a740e1", "E-P0044-74547b154c", "E-P0110-dab37b0fe2", "E-P0194-35271418ac", "E-P0189-419e1464da", "E-P0066-ee5ba721d6", "E-P0250-a8aa0f8a7d", "E-P0273-9640816b42", "E-P0012-e0bcca0d9e", "E-P0108-3e2edc05cd", "E-P0297-ed4427964c", "E-P0001-ca4a00b5cf", "E-P0201-1d5f67b08e", "E-P0246-3575a7c673", "E-P0297-7bed142d5c", "E-P0079-7cd49652d3", "E-P0055-9f71b81590", "E-P0187-9c914b3dcf"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0014-b6b7af5a81", "text": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "paper_id": "P0014", "citations": ["Zhou2026Beyond"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}}, {"evidence_id": "E-P0162-192e78b614", "text": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "paper_id": "P0162", "citations": ["Dong2025Etom"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method"}}, {"evidence_id": "E-P0022-f5a0e32a25", "text": "To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances.", "paper_id": "P0022", "citations": ["Yang2025Toolmind"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0022#limitations[1]"}}, {"evidence_id": "E-P0168-37f9ea924c", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling.", "paper_id": "P0168", "citations": ["Mohammadi2025Evaluation"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}}, {"evidence_id": "E-P0229-468a77ff1d", "text": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "paper_id": "P0229", "citations": ["Liu2025Toolscope"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0229#key_results[0]"}}, {"evidence_id": "E-P0263-0ea7b39672", "text": "This attack shows a nearly 80% success rate in an end-to-end evaluation.", "paper_id": "P0263", "citations": ["Fu2024Imprompter"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0263#key_results[0]"}}, {"evidence_id": "E-P0013-4e80a740e1", "text": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "paper_id": "P0013", "citations": ["Nandi2025Bench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}}, {"evidence_id": "E-P0044-74547b154c", "text": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "paper_id": "P0044", "citations": ["Ferrag2025From"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}}, {"evidence_id": "E-P0110-dab37b0fe2", "text": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents.", "paper_id": "P0110", "citations": ["Doshi2026Towards"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0]"}}, {"evidence_id": "E-P0194-35271418ac", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "paper_id": "P0194", "citations": ["Lumer2025Memtool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]"}}, {"evidence_id": "E-P0189-419e1464da", "text": "MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations.", "paper_id": "P0189", "citations": ["Gao2025Radar"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0189#key_results[0]"}}, {"evidence_id": "E-P0066-ee5ba721d6", "text": "Evaluation assumes Korean queries and Korean tool/parameter specifications; it covers single-chain, multi-chain, missing-parameters, and missing-functions scenarios, and is conducted via an LLM-as-a-Judge protocol averaged over five runs under a unified I/O interface.", "paper_id": "P0066", "citations": ["Jeon2025Based"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0066#key_results[0]"}}, {"evidence_id": "E-P0250-a8aa0f8a7d", "text": "Next, on three benchmarks, we quantitatively compare the effectiveness of code-use (which only has access to the target codebase) to tool-use (which has privileged access to all tool names and descriptions).", "paper_id": "P0250", "citations": ["Gupta2024Codenav"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0250#key_results[0]"}}, {"evidence_id": "E-P0273-9640816b42", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "paper_id": "P0273", "citations": ["Shen2024Small"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]"}}, {"evidence_id": "E-P0012-e0bcca0d9e", "text": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "paper_id": "P0012", "citations": ["Chowa2025From"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}}, {"evidence_id": "E-P0108-3e2edc05cd", "text": "However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings.", "paper_id": "P0108", "citations": ["Li2026Toolprmbench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0108#key_results[0]"}}, {"evidence_id": "E-P0297-ed4427964c", "text": "Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process.", "paper_id": "P0297", "citations": ["Hao2026From"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0297#key_results[1]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Tool interfaces and orchestration'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench", "Doshi2026Towards"]}], "claim_candidates": [{"claim": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "evidence_field": "evidence_snippet", "citations": ["Zhou2026Beyond"]}, {"claim": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "evidence_field": "evidence_snippet", "citations": ["Dong2025Etom"]}, {"claim": "To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances.", "evidence_field": "evidence_snippet", "citations": ["Yang2025Toolmind"]}, {"claim": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation", "evidence_field": "evidence_snippet", "citations": ["Mohammadi2025Evaluation"]}, {"claim": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "evidence_field": "evidence_snippet", "citations": ["Liu2025Toolscope"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0014`, `P0108`, `P0110`", "B_papers": "Tool-use and function calling: `P0014`, `P0108`, `P0110`", "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-4e80a740e1", "excerpt": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "citations": ["Nandi2025Bench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]", "score": 4}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 3}], "B_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 3}, {"paper_id": "P0022", "evidence_id": "E-P0022-f5a0e32a25", "excerpt": "To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances.", "citations": ["Yang2025Toolmind"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#limitations[1]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Nandi2025Bench", "Chowa2025From", "Yang2025Toolmind"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0014`, `P0108`, `P0110`", "B_papers": "Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`", "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-4e80a740e1", "excerpt": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "citations": ["Nandi2025Bench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]", "score": 4}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 3}], "B_highlights": [{"paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method", "score": 4}, {"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 3}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Nandi2025Bench", "Chowa2025From", "Dong2025Etom", "Mohammadi2025Evaluation"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Tool-use and function calling", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Tool-use and function calling: `P0014`, `P0108`, `P0110`", "B_papers": "Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`", "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]", "score": 3}, {"paper_id": "P0022", "evidence_id": "E-P0022-f5a0e32a25", "excerpt": "To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances.", "citations": ["Yang2025Toolmind"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#limitations[1]", "score": 2}], "B_highlights": [{"paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method", "score": 4}, {"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 3}], "write_prompt": "Contrast Tool-use and function calling vs Evaluation / benchmark-focused works along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Chowa2025From", "Yang2025Toolmind", "Dong2025Etom", "Mohammadi2025Evaluation"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0014`, `P0108`, `P0110`", "B_papers": "Tool-use and function calling: `P0014`, `P0108`, `P0110`", "A_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]", "score": 1}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]", "score": 1}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhou2026Beyond", "Ferrag2025From"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0014`, `P0108`, `P0110`", "B_papers": "Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`", "A_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]", "score": 1}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 0}, {"paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhou2026Beyond", "Ferrag2025From", "Mohammadi2025Evaluation", "Dong2025Etom"]}, {"axis": "compute and latency constraints", "A_label": "Tool-use and function calling", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Tool-use and function calling: `P0014`, `P0108`, `P0110`", "B_papers": "Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`", "A_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]", "score": 1}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 0}, {"paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method", "score": 0}], "write_prompt": "Contrast Tool-use and function calling vs Evaluation / benchmark-focused works along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhou2026Beyond", "Ferrag2025From", "Mohammadi2025Evaluation", "Dong2025Etom"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0014`, `P0108`, `P0110`", "B_papers": "Tool-use and function calling: `P0014`, `P0108`, `P0110`", "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-4e80a740e1", "excerpt": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "citations": ["Nandi2025Bench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]", "score": 3}, {"paper_id": "P0110", "evidence_id": "E-P0110-dab37b0fe2", "excerpt": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents.", "citations": ["Doshi2026Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0]", "score": 2}], "B_highlights": [{"paper_id": "P0110", "evidence_id": "E-P0110-dab37b0fe2", "excerpt": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents.", "citations": ["Doshi2026Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0]", "score": 2}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Nandi2025Bench", "Doshi2026Towards", "Ferrag2025From"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0014`, `P0108`, `P0110`", "B_papers": "Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`", "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-4e80a740e1", "excerpt": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "citations": ["Nandi2025Bench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]", "score": 3}, {"paper_id": "P0110", "evidence_id": "E-P0110-dab37b0fe2", "excerpt": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents.", "citations": ["Doshi2026Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0]", "score": 2}], "B_highlights": [{"paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method", "score": 3}, {"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Nandi2025Bench", "Doshi2026Towards", "Dong2025Etom", "Mohammadi2025Evaluation"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Tool-use and function calling", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Tool-use and function calling: `P0014`, `P0108`, `P0110`", "B_papers": "Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`", "A_highlights": [{"paper_id": "P0110", "evidence_id": "E-P0110-dab37b0fe2", "excerpt": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents.", "citations": ["Doshi2026Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0]", "score": 2}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method", "score": 3}, {"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 1}], "write_prompt": "Contrast Tool-use and function calling vs Evaluation / benchmark-focused works along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Doshi2026Towards", "Ferrag2025From", "Dong2025Etom", "Mohammadi2025Evaluation"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0014`, `P0108`, `P0110`", "B_papers": "Tool-use and function calling: `P0014`, `P0108`, `P0110`", "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-4e80a740e1", "excerpt": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "citations": ["Nandi2025Bench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]", "score": 3}, {"paper_id": "P0110", "evidence_id": "E-P0110-dab37b0fe2", "excerpt": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents.", "citations": ["Doshi2026Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0]", "score": 2}], "B_highlights": [{"paper_id": "P0110", "evidence_id": "E-P0110-dab37b0fe2", "excerpt": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents.", "citations": ["Doshi2026Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0]", "score": 2}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Nandi2025Bench", "Doshi2026Towards", "Ferrag2025From"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: RAG, MCP, MCTS, LLMs, BFCL, GPU, ToolBench, PRMs, PRM, ToolPRMBench.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench", "Doshi2026Towards", "Hao2026From"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench"]}], "failures_limitations": [{"bullet": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents.", "citations": ["Zhou2026Beyond"]}, {"bullet": "We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task.", "citations": ["Zhou2026Beyond"]}, {"bullet": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"]}, {"bullet": "These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.", "citations": ["Zhou2026Beyond"]}, {"bullet": "Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces.", "citations": ["Hao2026From"]}, {"bullet": "However, the rapid growth of plugins, connectors, and inter-agent protocols has outpaced security practices, leading to brittle integrations that rely on ad-hoc authentication, inconsistent schemas, and weak validation.", "citations": ["Ferrag2025From"]}, {"bullet": "This survey introduces a unified end-to-end threat model for LLM-agent ecosystems, covering host-to-tool and agent-to-agent communications.", "citations": ["Ferrag2025From"]}, {"bullet": "We systematically categorize more than thirty attack techniques spanning input manipulation, model compromise, system and privacy attacks, and protocol-level vulnerabilities.", "citations": ["Ferrag2025From"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-02-07T19:55:33", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "evidence_ids": ["E-P0243-38a26e4777", "E-P0190-84445d1a19", "E-P0046-741891e300", "E-P0215-0b753b9422", "E-P0233-771620f84f", "E-P0089-d36844b954", "E-P0019-8517628bd0", "E-P0086-a9c15ec0c8", "E-P0001-ca4a00b5cf", "E-P0060-5377f4f9b7", "E-P0180-55a2731a38", "E-P0254-2a7ea60588", "E-P0274-c0a98eb625", "E-P0199-9406c14ad1", "E-P0018-2bad4bca21", "E-P0145-c90a434db5", "E-P0148-b35b53de13", "E-P0170-e4ac5005b3", "E-P0211-4bcafdb221", "E-P0131-8e39e9502f", "E-P0205-1ad09376fe", "E-P0019-c17bcfb7d4", "E-P0089-dc268ffd9e", "E-P0131-02eb45e937"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0243-38a26e4777", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "paper_id": "P0243", "citations": ["Shang2024Agentsquare"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]"}}, {"evidence_id": "E-P0190-84445d1a19", "text": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "paper_id": "P0190", "citations": ["Luo2025Universe"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0190#method"}}, {"evidence_id": "E-P0046-741891e300", "text": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "paper_id": "P0046", "citations": ["Nakano2025Guided"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]"}}, {"evidence_id": "E-P0215-0b753b9422", "text": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "paper_id": "P0215", "citations": ["Zhou2025Siraj"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0215#key_results[0]"}}, {"evidence_id": "E-P0233-771620f84f", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "paper_id": "P0233", "citations": ["Hu2025Training"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0233#key_results[0]"}}, {"evidence_id": "E-P0089-d36844b954", "text": "We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset.", "paper_id": "P0089", "citations": ["Radha2024Iteration"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}}, {"evidence_id": "E-P0019-8517628bd0", "text": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "paper_id": "P0019", "citations": ["Zhou2025Reasoning"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]"}}, {"evidence_id": "E-P0086-a9c15ec0c8", "text": "LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.", "paper_id": "P0086", "citations": ["Gao2024Efficient"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[1]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0060-5377f4f9b7", "text": "To measure the performance of task-oriented agents comprehensively, we propose a two-level evaluation framework: (1) turn level and (2) end-to-end.", "paper_id": "P0060", "citations": ["Rawat2025Multi"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}}, {"evidence_id": "E-P0180-55a2731a38", "text": "We rigorously evaluate LARC on a carefully curated set of 48 constrained retrosynthesis planning tasks across 3 constraint types.", "paper_id": "P0180", "citations": ["Baker2025Larc"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0180#key_results[1]"}}, {"evidence_id": "E-P0254-2a7ea60588", "text": "Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate.", "paper_id": "P0254", "citations": ["Shi2024Ehragent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0254#key_results[0]"}}, {"evidence_id": "E-P0274-c0a98eb625", "text": "We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4).", "paper_id": "P0274", "citations": ["Ji2024Testing"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0274#key_results[0]"}}, {"evidence_id": "E-P0199-9406c14ad1", "text": "Through comprehensive evaluation in both agent-versus-agent simulations and studies with human players, we demonstrate MultiMind's superior performance in gameplay.", "paper_id": "P0199", "citations": ["Zhang2025Multimind"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[0]"}}, {"evidence_id": "E-P0018-2bad4bca21", "text": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation).", "paper_id": "P0018", "citations": ["Zeng2025Rejump"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]"}}, {"evidence_id": "E-P0145-c90a434db5", "text": "We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios.", "paper_id": "P0145", "citations": ["Qin2025Compass"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0145#key_results[1]"}}, {"evidence_id": "E-P0148-b35b53de13", "text": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "paper_id": "P0148", "citations": ["Silva2025Agents"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]"}}, {"evidence_id": "E-P0170-e4ac5005b3", "text": "Our best performing strategy generates executable API calls 88% of the time.", "paper_id": "P0170", "citations": ["Mudur2025Feabench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0170#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Planning and reasoning loops'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning", "Nakano2025Guided"]}], "claim_candidates": [{"claim": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "evidence_field": "evidence_snippet", "citations": ["Shang2024Agentsquare"]}, {"claim": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "evidence_field": "evidence_snippet", "citations": ["Luo2025Universe"]}, {"claim": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "evidence_field": "evidence_snippet", "citations": ["Nakano2025Guided"]}, {"claim": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "evidence_field": "evidence_snippet", "citations": ["Zhou2025Siraj"]}, {"claim": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "evidence_field": "evidence_snippet", "citations": ["Hu2025Training"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "A_papers": "Planning / reasoning loops: `P0018`, `P0019`, `P0046`", "B_papers": "Agent frameworks / architectures: `P0019`, `P0060`, `P0064`", "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]", "score": 0}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]", "score": 0}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Silva2025Agents", "Zhou2025Reasoning"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Tool-use and function calling", "A_papers": "Planning / reasoning loops: `P0018`, `P0019`, `P0046`", "B_papers": "Tool-use and function calling: `P0130`, `P0131`, `P0145`", "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]", "score": 0}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0190", "evidence_id": "E-P0190-84445d1a19", "excerpt": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "citations": ["Luo2025Universe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0190#method", "score": 2}, {"paper_id": "P0145", "evidence_id": "E-P0145-c90a434db5", "excerpt": "We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios.", "citations": ["Qin2025Compass"], "pointer": "papers/paper_notes.jsonl:paper_id=P0145#key_results[1]", "score": 1}], "write_prompt": "Contrast Planning / reasoning loops vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Luo2025Universe", "Qin2025Compass"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0019`, `P0060`, `P0064`", "B_papers": "Tool-use and function calling: `P0130`, `P0131`, `P0145`", "A_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]", "score": 0}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]", "score": 0}], "B_highlights": [{"paper_id": "P0190", "evidence_id": "E-P0190-84445d1a19", "excerpt": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "citations": ["Luo2025Universe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0190#method", "score": 2}, {"paper_id": "P0145", "evidence_id": "E-P0145-c90a434db5", "excerpt": "We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios.", "citations": ["Qin2025Compass"], "pointer": "papers/paper_notes.jsonl:paper_id=P0145#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Silva2025Agents", "Zhou2025Reasoning", "Luo2025Universe", "Qin2025Compass"]}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "A_papers": "Planning / reasoning loops: `P0018`, `P0019`, `P0046`", "B_papers": "Agent frameworks / architectures: `P0019`, `P0060`, `P0064`", "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]", "score": 0}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]", "score": 0}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Silva2025Agents", "Zhou2025Reasoning"]}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Tool-use and function calling", "A_papers": "Planning / reasoning loops: `P0018`, `P0019`, `P0046`", "B_papers": "Tool-use and function calling: `P0130`, `P0131`, `P0145`", "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]", "score": 0}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0086", "evidence_id": "E-P0086-a9c15ec0c8", "excerpt": "LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.", "citations": ["Gao2024Efficient"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[1]", "score": 3}, {"paper_id": "P0145", "evidence_id": "E-P0145-c90a434db5", "excerpt": "We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios.", "citations": ["Qin2025Compass"], "pointer": "papers/paper_notes.jsonl:paper_id=P0145#key_results[1]", "score": 1}], "write_prompt": "Contrast Planning / reasoning loops vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Gao2024Efficient", "Qin2025Compass"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0019`, `P0060`, `P0064`", "B_papers": "Tool-use and function calling: `P0130`, `P0131`, `P0145`", "A_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]", "score": 0}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]", "score": 0}], "B_highlights": [{"paper_id": "P0086", "evidence_id": "E-P0086-a9c15ec0c8", "excerpt": "LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.", "citations": ["Gao2024Efficient"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[1]", "score": 3}, {"paper_id": "P0145", "evidence_id": "E-P0145-c90a434db5", "excerpt": "We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios.", "citations": ["Qin2025Compass"], "pointer": "papers/paper_notes.jsonl:paper_id=P0145#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Silva2025Agents", "Zhou2025Reasoning", "Gao2024Efficient", "Qin2025Compass"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "A_papers": "Planning / reasoning loops: `P0018`, `P0019`, `P0046`", "B_papers": "Agent frameworks / architectures: `P0019`, `P0060`, `P0064`", "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]", "score": 0}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]", "score": 0}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Silva2025Agents", "Zhou2025Reasoning"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Planning / reasoning loops", "B_label": "Tool-use and function calling", "A_papers": "Planning / reasoning loops: `P0018`, `P0019`, `P0046`", "B_papers": "Tool-use and function calling: `P0130`, `P0131`, `P0145`", "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]", "score": 0}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0190", "evidence_id": "E-P0190-84445d1a19", "excerpt": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "citations": ["Luo2025Universe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0190#method", "score": 1}, {"paper_id": "P0086", "evidence_id": "E-P0086-a9c15ec0c8", "excerpt": "LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.", "citations": ["Gao2024Efficient"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[1]", "score": 1}], "write_prompt": "Contrast Planning / reasoning loops vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Luo2025Universe", "Gao2024Efficient"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0019`, `P0060`, `P0064`", "B_papers": "Tool-use and function calling: `P0130`, `P0131`, `P0145`", "A_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]", "score": 0}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]", "score": 0}], "B_highlights": [{"paper_id": "P0190", "evidence_id": "E-P0190-84445d1a19", "excerpt": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "citations": ["Luo2025Universe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0190#method", "score": 1}, {"paper_id": "P0086", "evidence_id": "E-P0086-a9c15ec0c8", "excerpt": "LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.", "citations": ["Gao2024Efficient"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Silva2025Agents", "Zhou2025Reasoning", "Luo2025Universe", "Gao2024Efficient"]}, {"axis": "tool selection / routing policy", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "A_papers": "Planning / reasoning loops: `P0018`, `P0019`, `P0046`", "B_papers": "Agent frameworks / architectures: `P0019`, `P0060`, `P0064`", "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]", "score": 0}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]", "score": 0}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]", "score": 0}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Silva2025Agents", "Zhou2025Reasoning"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LRMs, LLMs, UW-Madison-Lee-Lab, CoTs, ReJump, CoT-prompted, ReJump-guided, RSP, GSI, RSV.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning", "Nakano2025Guided", "Rawat2025Multi"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning"]}], "failures_limitations": [{"bullet": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"]}, {"bullet": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"]}, {"bullet": "This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures.", "citations": ["Nakano2025Guided"]}, {"bullet": "To address this limitation, we fine-tune relatively small models such as Llama 3.1 (8B & 70B) using the proposed Pre-Act approach.", "citations": ["Rawat2025Multi"]}, {"bullet": "Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making.", "citations": ["Hatalis2025Review"]}, {"bullet": "Their dependence on rule-based control and narrow AI limits adaptability in dynamic and uncertain missions.", "citations": ["Koubaa2025Agentic"]}, {"bullet": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"]}, {"bullet": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "citations": ["Luo2025Universe"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-02-07T19:55:33", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "evidence_ids": ["E-P0061-68db58914f", "E-P0162-192e78b614", "E-P0243-38a26e4777", "E-P0128-904ba35500", "E-P0072-0301cf089d", "E-P0197-e294aeefb5", "E-P0057-f36b515991", "E-P0102-98d4089bc9", "E-P0238-46914a4804", "E-P0054-897bcc2f50", "E-P0234-4af0cf3c02", "E-P0001-ca4a00b5cf", "E-P0007-dc2266b72d", "E-P0158-69c0aa3079", "E-P0287-9abcf1bf8a", "E-P0011-f0ea009256", "E-P0091-52fea1d199", "E-P0033-1b6fe3407a", "E-P0290-f0f0faaada", "E-P0045-8a32e8598f", "E-P0045-bb45089f75", "E-P0072-af89bfa234", "E-P0175-d568c53e1d", "E-P0227-53536132a8"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0061-68db58914f", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "paper_id": "P0061", "citations": ["Shi2025Progent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0061#key_results[0]"}}, {"evidence_id": "E-P0162-192e78b614", "text": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "paper_id": "P0162", "citations": ["Dong2025Etom"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method"}}, {"evidence_id": "E-P0243-38a26e4777", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "paper_id": "P0243", "citations": ["Shang2024Agentsquare"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]"}}, {"evidence_id": "E-P0128-904ba35500", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "paper_id": "P0128", "citations": ["Li2025Agentswift"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0128#key_results[0]"}}, {"evidence_id": "E-P0072-0301cf089d", "text": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools that keep pace with this new degree of autonomy.", "paper_id": "P0072", "citations": ["Maragheh2025Future"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}}, {"evidence_id": "E-P0197-e294aeefb5", "text": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "paper_id": "P0197", "citations": ["Abbineni2025Muallm"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0197#key_results[1]"}}, {"evidence_id": "E-P0057-f36b515991", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "paper_id": "P0057", "citations": ["Tawosi2025Meta"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]"}}, {"evidence_id": "E-P0102-98d4089bc9", "text": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience.", "paper_id": "P0102", "citations": ["Peng2026Enhancing"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0102#summary_bullets[0]"}}, {"evidence_id": "E-P0238-46914a4804", "text": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "paper_id": "P0238", "citations": ["Zhu2025Where"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0238#summary_bullets[1]"}}, {"evidence_id": "E-P0054-897bcc2f50", "text": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs.", "paper_id": "P0054", "citations": ["Zhang2025Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}}, {"evidence_id": "E-P0234-4af0cf3c02", "text": "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "paper_id": "P0234", "citations": ["Huang2025Retrieval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0234#key_results[1]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0007-dc2266b72d", "text": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of LMPRs and workflow designs across different agent paradigms and frameworks.", "paper_id": "P0007", "citations": ["Li2024Review"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0007#key_results[0]"}}, {"evidence_id": "E-P0158-69c0aa3079", "text": "Experiments show that DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval and QRData compared to baseline agents.", "paper_id": "P0158", "citations": ["Wang2025Dsmentor"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[0]"}}, {"evidence_id": "E-P0287-9abcf1bf8a", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "paper_id": "P0287", "citations": ["Verma2026Active"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0287#key_results[1]"}}, {"evidence_id": "E-P0011-f0ea009256", "text": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "paper_id": "P0011", "citations": ["Du2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}}, {"evidence_id": "E-P0091-52fea1d199", "text": "We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness.", "paper_id": "P0091", "citations": ["Zhang2024Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0091#key_results[1]"}}, {"evidence_id": "E-P0033-1b6fe3407a", "text": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "paper_id": "P0033", "citations": ["Yao2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Memory and retrieval (RAG)'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Peng2026Enhancing", "Verma2026Active", "Yu2026Agentic"]}], "claim_candidates": [{"claim": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "evidence_field": "evidence_snippet", "citations": ["Shi2025Progent"]}, {"claim": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "evidence_field": "evidence_snippet", "citations": ["Dong2025Etom"]}, {"claim": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "evidence_field": "evidence_snippet", "citations": ["Shang2024Agentsquare"]}, {"claim": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "evidence_field": "evidence_snippet", "citations": ["Li2025Agentswift"]}, {"claim": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools that keep pace with this new degree of autonomy.", "evidence_field": "evidence_snippet", "citations": ["Maragheh2025Future"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0102`, `P0287`, `P0290`", "B_papers": "Memory / retrieval augmentation: `P0287`, `P0290`, `P0054`", "A_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}, {"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 1}, {"paper_id": "P0197", "evidence_id": "E-P0197-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0197#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Du2025Survey", "Zhang2025Large", "Abbineni2025Muallm"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0102`, `P0287`, `P0290`", "B_papers": "Planning / reasoning loops: `P0041`, `P0128`, `P0205`", "A_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}, {"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]", "score": 2}, {"paper_id": "P0128", "evidence_id": "E-P0128-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0128#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Du2025Survey", "Zhang2025Large", "Shang2024Agentsquare", "Li2025Agentswift"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Memory / retrieval augmentation", "B_label": "Planning / reasoning loops", "A_papers": "Memory / retrieval augmentation: `P0287`, `P0290`, `P0054`", "B_papers": "Planning / reasoning loops: `P0041`, `P0128`, `P0205`", "A_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 1}, {"paper_id": "P0197", "evidence_id": "E-P0197-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0197#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]", "score": 2}, {"paper_id": "P0128", "evidence_id": "E-P0128-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0128#key_results[0]", "score": 2}], "write_prompt": "Contrast Memory / retrieval augmentation vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zhang2025Large", "Abbineni2025Muallm", "Shang2024Agentsquare", "Li2025Agentswift"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0102`, `P0287`, `P0290`", "B_papers": "Memory / retrieval augmentation: `P0287`, `P0290`, `P0054`", "A_highlights": [{"paper_id": "P0033", "evidence_id": "E-P0033-1b6fe3407a", "excerpt": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "citations": ["Yao2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 2}, {"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 0}, {"paper_id": "P0057", "evidence_id": "E-P0057-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Yao2025Survey", "Zhang2025Large", "Tawosi2025Meta"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0102`, `P0287`, `P0290`", "B_papers": "Planning / reasoning loops: `P0041`, `P0128`, `P0205`", "A_highlights": [{"paper_id": "P0033", "evidence_id": "E-P0033-1b6fe3407a", "excerpt": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "citations": ["Yao2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 2}, {"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0007", "evidence_id": "E-P0007-dc2266b72d", "excerpt": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0007#key_results[0]", "score": 0}, {"paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Yao2025Survey", "Zhang2025Large", "Li2024Review", "Shang2024Agentsquare"]}, {"axis": "compute and latency constraints", "A_label": "Memory / retrieval augmentation", "B_label": "Planning / reasoning loops", "A_papers": "Memory / retrieval augmentation: `P0287`, `P0290`, `P0054`", "B_papers": "Planning / reasoning loops: `P0041`, `P0128`, `P0205`", "A_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 0}, {"paper_id": "P0057", "evidence_id": "E-P0057-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0007", "evidence_id": "E-P0007-dc2266b72d", "excerpt": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0007#key_results[0]", "score": 0}, {"paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]", "score": 0}], "write_prompt": "Contrast Memory / retrieval augmentation vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2025Large", "Tawosi2025Meta", "Li2024Review", "Shang2024Agentsquare"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0102`, `P0287`, `P0290`", "B_papers": "Memory / retrieval augmentation: `P0287`, `P0290`, `P0054`", "A_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 1}, {"paper_id": "P0287", "evidence_id": "E-P0287-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0287#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 1}, {"paper_id": "P0057", "evidence_id": "E-P0057-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Zhang2025Large", "Verma2026Active", "Tawosi2025Meta"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0102`, `P0287`, `P0290`", "B_papers": "Planning / reasoning loops: `P0041`, `P0128`, `P0205`", "A_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 1}, {"paper_id": "P0287", "evidence_id": "E-P0287-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0287#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]", "score": 1}, {"paper_id": "P0128", "evidence_id": "E-P0128-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0128#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Zhang2025Large", "Verma2026Active", "Shang2024Agentsquare", "Li2025Agentswift"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Memory / retrieval augmentation", "B_label": "Planning / reasoning loops", "A_papers": "Memory / retrieval augmentation: `P0287`, `P0290`, `P0054`", "B_papers": "Planning / reasoning loops: `P0041`, `P0128`, `P0205`", "A_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 1}, {"paper_id": "P0057", "evidence_id": "E-P0057-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]", "score": 1}, {"paper_id": "P0128", "evidence_id": "E-P0128-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0128#key_results[0]", "score": 1}], "write_prompt": "Contrast Memory / retrieval augmentation vs Planning / reasoning loops along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Zhang2025Large", "Tawosi2025Meta", "Shang2024Agentsquare", "Li2025Agentswift"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0102`, `P0287`, `P0290`", "B_papers": "Memory / retrieval augmentation: `P0287`, `P0290`, `P0054`", "A_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 1}, {"paper_id": "P0287", "evidence_id": "E-P0287-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0287#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]", "score": 1}, {"paper_id": "P0057", "evidence_id": "E-P0057-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Zhang2025Large", "Verma2026Active", "Tawosi2025Meta"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: HITL, LLMs, MITRE, ATT, IPDRR-based, LLM-RL, CyberOps-Bots, ReAct, SWE-bench, LTM.", "citations": ["Peng2026Enhancing", "Verma2026Active", "Yu2026Agentic", "Du2025Survey"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Peng2026Enhancing", "Verma2026Active"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Peng2026Enhancing", "Verma2026Active"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Peng2026Enhancing", "Verma2026Active"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Peng2026Enhancing", "Verma2026Active"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Peng2026Enhancing", "Verma2026Active"]}], "failures_limitations": [{"bullet": "To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs).", "citations": ["Peng2026Enhancing"]}, {"bullet": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience.", "citations": ["Peng2026Enhancing"]}, {"bullet": "However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity.", "citations": ["Peng2026Enhancing"]}, {"bullet": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical.", "citations": ["Yu2026Agentic"]}, {"bullet": "This results in the failure to retrieve the relevant code of these fine-grained subtasks.", "citations": ["Li2025Graphcodeagent"]}, {"bullet": "To address this challenge, we propose GraphCodeAgent, a dual graph-guided LLM agent for retrieval-augmented repo-level code generation, bridging the gap between NL requirements and programming implementations.", "citations": ["Li2025Graphcodeagent"]}, {"bullet": "MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection.", "citations": ["Wu2025Meta"]}, {"bullet": "We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi-agent extensions.", "citations": ["Wu2025Meta"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-02-07T19:55:33", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "evidence_ids": ["E-P0067-2e6956a116", "E-P0100-67ea29ce26", "E-P0082-4da9e4ae32", "E-P0103-60cc0d458f", "E-P0009-30bd885b0c", "E-P0178-d099f5e3ee", "E-P0001-ca4a00b5cf", "E-P0068-c2c43d35ce", "E-P0092-ebb97f6129", "E-P0102-ec579c4773", "E-P0065-6273763a98", "E-P0141-fad03785c5", "E-P0143-fa93c04884", "E-P0011-f0ea009256", "E-P0167-ca050ed55f", "E-P0183-78bde774bf", "E-P0184-d19d8e1143", "E-P0113-a68f39bc04", "E-P0116-78b9ea1065", "E-P0169-1a05555e85", "E-P0176-af945eb2fa", "E-P0184-1dd544863c", "E-P0132-eae2720c71", "E-P0084-8b5287fcb0"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0067-2e6956a116", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "paper_id": "P0067", "citations": ["Zhou2025Self"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}}, {"evidence_id": "E-P0100-67ea29ce26", "text": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "paper_id": "P0100", "citations": ["Li2026Autonomous"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0100#method"}}, {"evidence_id": "E-P0082-4da9e4ae32", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "paper_id": "P0082", "citations": ["Du2024Anytool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0082#limitations[1]"}}, {"evidence_id": "E-P0103-60cc0d458f", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "paper_id": "P0103", "citations": ["Zhang2026Evoroute"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}}, {"evidence_id": "E-P0009-30bd885b0c", "text": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "paper_id": "P0009", "citations": ["Guo2025Comprehensive"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}}, {"evidence_id": "E-P0178-d099f5e3ee", "text": "Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.", "paper_id": "P0178", "citations": ["Yu2025Infiagent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0178#key_results[0]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0068-c2c43d35ce", "text": "In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage.", "paper_id": "P0068", "citations": ["Alizadeh2025Simple"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0068#key_results[1]"}}, {"evidence_id": "E-P0092-ebb97f6129", "text": "With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents.", "paper_id": "P0092", "citations": ["Yin2024Mmau"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}}, {"evidence_id": "E-P0102-ec579c4773", "text": "Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining.", "paper_id": "P0102", "citations": ["Peng2026Enhancing"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0102#key_results[0]"}}, {"evidence_id": "E-P0065-6273763a98", "text": "Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "paper_id": "P0065", "citations": ["Xia2025Sand"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0065#key_results[0]"}}, {"evidence_id": "E-P0141-fad03785c5", "text": "To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models.", "paper_id": "P0141", "citations": ["GendreauDistler2025Automating"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0141#key_results[0]"}}, {"evidence_id": "E-P0143-fa93c04884", "text": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis generation; and 3) self-evaluation significantly improves the novelty and relevance of proposed hypotheses.", "paper_id": "P0143", "citations": ["Yang2025Bioverge"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]"}}, {"evidence_id": "E-P0011-f0ea009256", "text": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "paper_id": "P0011", "citations": ["Du2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}}, {"evidence_id": "E-P0167-ca050ed55f", "text": "We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports.", "paper_id": "P0167", "citations": ["Samaei2025Epidemiqs"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0167#key_results[1]"}}, {"evidence_id": "E-P0183-78bde774bf", "text": "LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention.", "paper_id": "P0183", "citations": ["Yano2025Lamdagent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0183#key_results[1]"}}, {"evidence_id": "E-P0184-d19d8e1143", "text": "At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making.", "paper_id": "P0184", "citations": ["Li2025Learn"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[0]"}}, {"evidence_id": "E-P0113-a68f39bc04", "text": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "paper_id": "P0113", "citations": ["Van2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Self-improvement and adaptation'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; communication protocol / roles; aggregation (vote / debate / referee); stability / robustness.", "citations": ["Li2026Autonomous", "Peng2026Enhancing", "Zhang2026Evoroute"]}], "claim_candidates": [{"claim": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "evidence_field": "evidence_snippet", "citations": ["Zhou2025Self"]}, {"claim": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "evidence_field": "evidence_snippet", "citations": ["Li2026Autonomous"]}, {"claim": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "evidence_field": "evidence_snippet", "citations": ["Du2024Anytool"]}, {"claim": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "evidence_field": "evidence_snippet", "citations": ["Zhang2026Evoroute"]}, {"claim": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "evidence_field": "evidence_snippet", "citations": ["Guo2025Comprehensive"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0100`, `P0102`, `P0103`", "B_papers": "Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 2}, {"paper_id": "P0011", "evidence_id": "E-P0011-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 1}, {"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zhou2025Self", "Du2025Survey", "Guo2025Comprehensive", "Yang2025Bioverge"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0100`, `P0102`, `P0103`", "B_papers": "Planning / reasoning loops: `P0064`, `P0132`, `P0001`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 2}, {"paper_id": "P0011", "evidence_id": "E-P0011-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zhou2025Self", "Du2025Survey", "Yao2022React"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Evaluation / benchmark-focused works", "B_label": "Planning / reasoning loops", "A_papers": "Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`", "B_papers": "Planning / reasoning loops: `P0064`, `P0132`, `P0001`", "A_highlights": [{"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 1}, {"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 1}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Guo2025Comprehensive", "Yang2025Bioverge", "Yao2022React"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0100`, `P0102`, `P0103`", "B_papers": "Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`", "A_highlights": [{"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]", "score": 2}, {"paper_id": "P0102", "evidence_id": "E-P0102-ec579c4773", "excerpt": "Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining.", "citations": ["Peng2026Enhancing"], "pointer": "papers/paper_notes.jsonl:paper_id=P0102#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]", "score": 0}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2026Evoroute", "Peng2026Enhancing", "Yang2025Bioverge", "Guo2025Comprehensive"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0100`, `P0102`, `P0103`", "B_papers": "Planning / reasoning loops: `P0064`, `P0132`, `P0001`", "A_highlights": [{"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]", "score": 2}, {"paper_id": "P0102", "evidence_id": "E-P0102-ec579c4773", "excerpt": "Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining.", "citations": ["Peng2026Enhancing"], "pointer": "papers/paper_notes.jsonl:paper_id=P0102#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2026Evoroute", "Peng2026Enhancing", "Yao2022React"]}, {"axis": "compute and latency constraints", "A_label": "Evaluation / benchmark-focused works", "B_label": "Planning / reasoning loops", "A_papers": "Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`", "B_papers": "Planning / reasoning loops: `P0064`, `P0132`, `P0001`", "A_highlights": [{"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]", "score": 0}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 0}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Yang2025Bioverge", "Guo2025Comprehensive", "Yao2022React"]}, {"axis": "communication protocol / roles", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0100`, `P0102`, `P0103`", "B_papers": "Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 1}, {"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]", "score": 0}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"communication protocol / roles\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "communication protocol / roles", "citations": ["Zhou2025Self", "Zhang2026Evoroute", "Yang2025Bioverge", "Guo2025Comprehensive"]}, {"axis": "communication protocol / roles", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0100`, `P0102`, `P0103`", "B_papers": "Planning / reasoning loops: `P0064`, `P0132`, `P0001`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 1}, {"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"communication protocol / roles\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "communication protocol / roles", "citations": ["Zhou2025Self", "Zhang2026Evoroute", "Yao2022React"]}, {"axis": "communication protocol / roles", "A_label": "Evaluation / benchmark-focused works", "B_label": "Planning / reasoning loops", "A_papers": "Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`", "B_papers": "Planning / reasoning loops: `P0064`, `P0132`, `P0001`", "A_highlights": [{"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]", "score": 0}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]", "score": 0}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Planning / reasoning loops along \"communication protocol / roles\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "communication protocol / roles", "citations": ["Yang2025Bioverge", "Guo2025Comprehensive", "Yao2022React"]}, {"axis": "aggregation (vote / debate / referee)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0100`, `P0102`, `P0103`", "B_papers": "Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`", "A_highlights": [{"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]", "score": 0}, {"paper_id": "P0102", "evidence_id": "E-P0102-ec579c4773", "excerpt": "Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining.", "citations": ["Peng2026Enhancing"], "pointer": "papers/paper_notes.jsonl:paper_id=P0102#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]", "score": 0}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"aggregation (vote / debate / referee)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "aggregation (vote / debate / referee)", "citations": ["Zhang2026Evoroute", "Peng2026Enhancing", "Yang2025Bioverge", "Guo2025Comprehensive"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: DeepSeek-V3, HITL, LLMs, MITRE, ATT, IPDRR-based, LLM-RL, CyberOps-Bots, ReAct, GAIA.", "citations": ["Li2026Autonomous", "Peng2026Enhancing", "Zhang2026Evoroute", "Guo2025Comprehensive"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Li2026Autonomous", "Peng2026Enhancing"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Li2026Autonomous", "Peng2026Enhancing"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Li2026Autonomous", "Peng2026Enhancing"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Li2026Autonomous", "Peng2026Enhancing"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Li2026Autonomous", "Peng2026Enhancing"]}], "failures_limitations": [{"bullet": "Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.", "citations": ["Li2026Autonomous"]}, {"bullet": "To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs).", "citations": ["Peng2026Enhancing"]}, {"bullet": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience.", "citations": ["Peng2026Enhancing"]}, {"bullet": "However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity.", "citations": ["Peng2026Enhancing"]}, {"bullet": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"]}, {"bullet": "Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making.", "citations": ["Hatalis2025Review"]}, {"bullet": "The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks.", "citations": ["Zhou2025Self"]}, {"bullet": "To address these limitations, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups.", "citations": ["Yin2024Mmau"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-02-07T19:55:33", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "evidence_ids": ["E-P0177-4b027dfb27", "E-P0159-b9e7423acc", "E-P0266-c92a9d34cf", "E-P0194-35271418ac", "E-P0015-763cb193e3", "E-P0189-419e1464da", "E-P0178-d099f5e3ee", "E-P0156-0b34b55332", "E-P0017-62cd0c501b", "E-P0273-9640816b42", "E-P0072-0301cf089d", "E-P0209-c664716bd8", "E-P0239-9af3444274", "E-P0075-9af6a59afb", "E-P0144-e52cad3d44", "E-P0185-82b5e4eda3", "E-P0222-5ed988eb67", "E-P0148-baa622fa7f", "E-P0184-1dd544863c", "E-P0275-a21c19010a", "E-P0063-906d682e2a", "E-P0194-38dc800de9", "E-P0239-5ad9d205d0", "E-P0030-4c14763fd0"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0177-4b027dfb27", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "paper_id": "P0177", "citations": ["Feng2025Group"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}}, {"evidence_id": "E-P0159-b9e7423acc", "text": "We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics.", "paper_id": "P0159", "citations": ["Zhang2025Datascibench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0159#method"}}, {"evidence_id": "E-P0266-c92a9d34cf", "text": "The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs.", "paper_id": "P0266", "citations": ["Yang2024Based"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0266#limitations[1]"}}, {"evidence_id": "E-P0194-35271418ac", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "paper_id": "P0194", "citations": ["Lumer2025Memtool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]"}}, {"evidence_id": "E-P0015-763cb193e3", "text": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches.", "paper_id": "P0015", "citations": ["Lu2025Just"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]"}}, {"evidence_id": "E-P0189-419e1464da", "text": "MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations.", "paper_id": "P0189", "citations": ["Gao2025Radar"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0189#key_results[0]"}}, {"evidence_id": "E-P0178-d099f5e3ee", "text": "Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.", "paper_id": "P0178", "citations": ["Yu2025Infiagent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0178#key_results[0]"}}, {"evidence_id": "E-P0156-0b34b55332", "text": "We propose CyberSleuth, an autonomous agent that processes packet-level traces and application logs to identify the targeted service, the exploited vulnerability (CVE), and attack success.", "paper_id": "P0156", "citations": ["Fumero2025Cybersleuth"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0156#method"}}, {"evidence_id": "E-P0017-62cd0c501b", "text": "Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains.", "paper_id": "P0017", "citations": ["Luo2025Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0017#key_results[0]"}}, {"evidence_id": "E-P0273-9640816b42", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "paper_id": "P0273", "citations": ["Shen2024Small"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]"}}, {"evidence_id": "E-P0072-0301cf089d", "text": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools that keep pace with this new degree of autonomy.", "paper_id": "P0072", "citations": ["Maragheh2025Future"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}}, {"evidence_id": "E-P0209-c664716bd8", "text": "We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol.", "paper_id": "P0209", "citations": ["Chen2025Remsa"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0209#key_results[0]"}}, {"evidence_id": "E-P0239-9af3444274", "text": "Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "paper_id": "P0239", "citations": ["Shi2025Youtu"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0239#key_results[1]"}}, {"evidence_id": "E-P0075-9af6a59afb", "text": "Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.", "paper_id": "P0075", "citations": ["Ji2025Tree"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0075#key_results[0]"}}, {"evidence_id": "E-P0144-e52cad3d44", "text": "We also construct a cosmological parameter extraction evaluation dataset by collecting over 40 simulations in published papers from Arxiv and leading journals that cover diverse simulation types.", "paper_id": "P0144", "citations": ["Zhang2025Bridging"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}}, {"evidence_id": "E-P0185-82b5e4eda3", "text": "Experimental results on the JudgeBench dataset show about 15.55\\% improvement compared to raw judgments and about 8.37\\% improvement over the single-agent baseline.", "paper_id": "P0185", "citations": ["Li2025Leveraging"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0185#key_results[1]"}}, {"evidence_id": "E-P0222-5ed988eb67", "text": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency.", "paper_id": "P0222", "citations": ["Cao2025Skyrl"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0222#key_results[1]"}}, {"evidence_id": "E-P0148-baa622fa7f", "text": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "paper_id": "P0148", "citations": ["Silva2025Agents"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[1]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Multi-agent coordination'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Lu2025Just", "Luo2025Large", "Yang2025Survey"]}], "claim_candidates": [{"claim": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "evidence_field": "evidence_snippet", "citations": ["Feng2025Group"]}, {"claim": "We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics.", "evidence_field": "evidence_snippet", "citations": ["Zhang2025Datascibench"]}, {"claim": "The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs.", "evidence_field": "evidence_snippet", "citations": ["Yang2024Based"]}, {"claim": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "evidence_field": "evidence_snippet", "citations": ["Lumer2025Memtool"]}, {"claim": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches.", "evidence_field": "evidence_snippet", "citations": ["Lu2025Just"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0015`, `P0017`, `P0030`", "B_papers": "Multi-agent coordination: `P0063`, `P0072`, `P0119`", "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 2}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 2}, {"paper_id": "P0148", "evidence_id": "E-P0148-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Maragheh2025Future", "Lu2025Just", "Silva2025Agents"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0015`, `P0017`, `P0030`", "B_papers": "Tool-use and function calling: `P0030`, `P0069`, `P0189`", "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 2}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]", "score": 3}, {"paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "excerpt": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Maragheh2025Future", "Lu2025Just", "Lumer2025Memtool", "Shen2024Small"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Multi-agent coordination", "B_label": "Tool-use and function calling", "A_papers": "Multi-agent coordination: `P0063`, `P0072`, `P0119`", "B_papers": "Tool-use and function calling: `P0030`, `P0069`, `P0189`", "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 2}, {"paper_id": "P0148", "evidence_id": "E-P0148-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]", "score": 3}, {"paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "excerpt": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]", "score": 2}], "write_prompt": "Contrast Multi-agent coordination vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Maragheh2025Future", "Silva2025Agents", "Lumer2025Memtool", "Shen2024Small"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0015`, `P0017`, `P0030`", "B_papers": "Multi-agent coordination: `P0063`, `P0072`, `P0119`", "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 0}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 0}, {"paper_id": "P0148", "evidence_id": "E-P0148-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Maragheh2025Future", "Lu2025Just", "Silva2025Agents"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0015`, `P0017`, `P0030`", "B_papers": "Tool-use and function calling: `P0030`, `P0069`, `P0189`", "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 0}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]", "score": 0}, {"paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "excerpt": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Maragheh2025Future", "Lu2025Just", "Lumer2025Memtool", "Shen2024Small"]}, {"axis": "compute and latency constraints", "A_label": "Multi-agent coordination", "B_label": "Tool-use and function calling", "A_papers": "Multi-agent coordination: `P0063`, `P0072`, `P0119`", "B_papers": "Tool-use and function calling: `P0030`, `P0069`, `P0189`", "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 0}, {"paper_id": "P0148", "evidence_id": "E-P0148-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]", "score": 0}, {"paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "excerpt": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]", "score": 0}], "write_prompt": "Contrast Multi-agent coordination vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Maragheh2025Future", "Silva2025Agents", "Lumer2025Memtool", "Shen2024Small"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0015`, `P0017`, `P0030`", "B_papers": "Multi-agent coordination: `P0063`, `P0072`, `P0119`", "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 1}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 1}, {"paper_id": "P0266", "evidence_id": "E-P0266-c92a9d34cf", "excerpt": "The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs.", "citations": ["Yang2024Based"], "pointer": "papers/paper_notes.jsonl:paper_id=P0266#limitations[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Maragheh2025Future", "Lu2025Just", "Yang2024Based"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0015`, `P0017`, `P0030`", "B_papers": "Tool-use and function calling: `P0030`, `P0069`, `P0189`", "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 1}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]", "score": 2}, {"paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "excerpt": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Maragheh2025Future", "Lu2025Just", "Lumer2025Memtool", "Shen2024Small"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Multi-agent coordination", "B_label": "Tool-use and function calling", "A_papers": "Multi-agent coordination: `P0063`, `P0072`, `P0119`", "B_papers": "Tool-use and function calling: `P0030`, `P0069`, `P0189`", "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 1}, {"paper_id": "P0266", "evidence_id": "E-P0266-c92a9d34cf", "excerpt": "The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs.", "citations": ["Yang2024Based"], "pointer": "papers/paper_notes.jsonl:paper_id=P0266#limitations[1]", "score": 1}], "B_highlights": [{"paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]", "score": 2}, {"paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "excerpt": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]", "score": 1}], "write_prompt": "Contrast Multi-agent coordination vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Maragheh2025Future", "Yang2024Based", "Lumer2025Memtool", "Shen2024Small"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0015`, `P0017`, `P0030`", "B_papers": "Multi-agent coordination: `P0063`, `P0072`, `P0119`", "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 1}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]", "score": 1}, {"paper_id": "P0266", "evidence_id": "E-P0266-c92a9d34cf", "excerpt": "The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs.", "citations": ["Yang2024Based"], "pointer": "papers/paper_notes.jsonl:paper_id=P0266#limitations[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Maragheh2025Future", "Lu2025Just", "Yang2024Based"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: SFT, BFCL, SFT-based, LLMs, LLM-based, RefAgent, MCP, MCP-compliant, RecSys, GRPO.", "citations": ["Lu2025Just", "Luo2025Large", "Yang2025Survey", "Oueslati2025Refagent"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Lu2025Just", "Luo2025Large"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Lu2025Just", "Luo2025Large"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Lu2025Just", "Luo2025Large"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Lu2025Just", "Luo2025Large"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Lu2025Just", "Luo2025Large"]}], "failures_limitations": [{"bullet": "Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency.", "citations": ["Yang2025Survey"]}, {"bullet": "The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "citations": ["Sarkar2025Survey"]}, {"bullet": "We then surface five cross-cutting challenge families: protocol complexity, scalability, hallucination and error propagation, emergent misalignment (including covert collusion), and brand compliance.", "citations": ["Maragheh2025Future"]}, {"bullet": "To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step.", "citations": ["Ji2025Tree"]}, {"bullet": "As cosmological simulations and their associated software become increasingly complex, physicists face the challenge of searching through vast amounts of literature and user manuals to extract simulation parameters from dense academic papers, each using different models and formats.", "citations": ["Zhang2025Bridging"]}, {"bullet": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"]}, {"bullet": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"]}, {"bullet": "The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs.", "citations": ["Yang2024Based"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-02-07T19:55:33", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "evidence_ids": ["E-P0208-753416ce70", "E-P0147-578706f7a5", "E-P0061-68db58914f", "E-P0168-37f9ea924c", "E-P0067-2e6956a116", "E-P0062-52fcef25b6", "E-P0068-537b51e910", "E-P0071-5607dc887c", "E-P0027-79f88927fa", "E-P0009-30bd885b0c", "E-P0074-4fc221fdea", "E-P0300-8b56718f74", "E-P0141-fad03785c5", "E-P0153-9bd58ce21b", "E-P0200-39cf43509f", "E-P0080-17ec1d2e4e", "E-P0146-4ac9a2931e", "E-P0288-6c12fbfdd5", "E-P0077-5603d51445", "E-P0116-78b9ea1065", "E-P0116-d70ef2c75f", "E-P0146-ceb4754147", "E-P0168-e973488a75", "E-P0188-07f9bd6a8b"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0208-753416ce70", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "paper_id": "P0208", "citations": ["Fu2025Eval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0208#key_results[1]"}}, {"evidence_id": "E-P0147-578706f7a5", "text": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are crucial for achieving desired performance levels, requiring domain expertise.", "paper_id": "P0147", "citations": ["Kwon2025Agentnet"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]"}}, {"evidence_id": "E-P0061-68db58914f", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "paper_id": "P0061", "citations": ["Shi2025Progent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0061#key_results[0]"}}, {"evidence_id": "E-P0168-37f9ea924c", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling.", "paper_id": "P0168", "citations": ["Mohammadi2025Evaluation"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}}, {"evidence_id": "E-P0067-2e6956a116", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "paper_id": "P0067", "citations": ["Zhou2025Self"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}}, {"evidence_id": "E-P0062-52fcef25b6", "text": "Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.", "paper_id": "P0062", "citations": ["Zhong2025Rtbas"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}}, {"evidence_id": "E-P0068-537b51e910", "text": "In 16 user tasks from AgentDojo, LLMs show a 15-50 percentage point drop in utility under attack, with average attack success rates (ASR) around 20 percent; some defenses reduce ASR to zero.", "paper_id": "P0068", "citations": ["Alizadeh2025Simple"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0068#key_results[0]"}}, {"evidence_id": "E-P0071-5607dc887c", "text": "Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses.", "paper_id": "P0071", "citations": ["Ji2025Taxonomy"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0071#key_results[1]"}}, {"evidence_id": "E-P0027-79f88927fa", "text": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "paper_id": "P0027", "citations": ["Kim2026Beyond"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]"}}, {"evidence_id": "E-P0009-30bd885b0c", "text": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "paper_id": "P0009", "citations": ["Guo2025Comprehensive"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}}, {"evidence_id": "E-P0074-4fc221fdea", "text": "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "paper_id": "P0074", "citations": ["Chen2025Towards"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0074#key_results[0]"}}, {"evidence_id": "E-P0300-8b56718f74", "text": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants.", "paper_id": "P0300", "citations": ["Liu2026Agents"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0300#key_results[0]"}}, {"evidence_id": "E-P0141-fad03785c5", "text": "To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models.", "paper_id": "P0141", "citations": ["GendreauDistler2025Automating"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0141#key_results[0]"}}, {"evidence_id": "E-P0153-9bd58ce21b", "text": "When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model.", "paper_id": "P0153", "citations": ["Tao2025Code"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0153#key_results[0]"}}, {"evidence_id": "E-P0200-39cf43509f", "text": "To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains.", "paper_id": "P0200", "citations": ["Zheng2025Newtonbench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0200#key_results[0]"}}, {"evidence_id": "E-P0080-17ec1d2e4e", "text": "We proposed the two-stage framework: from ``core ability'' to ``agent'', clearly explaining how LLMs can be applied based on their specific capabilities, along with the evaluation methods in each stage.", "paper_id": "P0080", "citations": ["Peng2024Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}}, {"evidence_id": "E-P0146-4ac9a2931e", "text": "Existing agentic benchmarks often reduce evaluation to a binary judgment of the final state, overlooking critical aspects such as safety, efficiency, and intermediate correctness.", "paper_id": "P0146", "citations": ["Michelakis2025Core"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0146#key_results[0]"}}, {"evidence_id": "E-P0288-6c12fbfdd5", "text": "We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices.", "paper_id": "P0288", "citations": ["V2026Agentic"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0288#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Benchmarks and evaluation protocols'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Kim2026Beyond", "Doshi2026Towards", "V2026Agentic"]}], "claim_candidates": [{"claim": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "evidence_field": "evidence_snippet", "citations": ["Fu2025Eval"]}, {"claim": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are crucial for achieving desired performance levels, requiring domain expertise.", "evidence_field": "evidence_snippet", "citations": ["Kwon2025Agentnet"]}, {"claim": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "evidence_field": "evidence_snippet", "citations": ["Shi2025Progent"]}, {"claim": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation", "evidence_field": "evidence_snippet", "citations": ["Mohammadi2025Evaluation"]}, {"claim": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "evidence_field": "evidence_snippet", "citations": ["Zhou2025Self"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0027`, `P0110`, `P0288`", "B_papers": "Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 2}, {"paper_id": "P0062", "evidence_id": "E-P0062-52fcef25b6", "excerpt": "Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy", "citations": ["Zhong2025Rtbas"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 3}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zhou2025Self", "Zhong2025Rtbas", "Mohammadi2025Evaluation", "Guo2025Comprehensive"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0027`, `P0110`, `P0288`", "B_papers": "Tool-use and function calling: `P0027`, `P0110`, `P0147`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 2}, {"paper_id": "P0062", "evidence_id": "E-P0062-52fcef25b6", "excerpt": "Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy", "citations": ["Zhong2025Rtbas"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "excerpt": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]", "score": 2}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zhou2025Self", "Zhong2025Rtbas", "Kwon2025Agentnet", "Kim2026Beyond"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Evaluation / benchmark-focused works", "B_label": "Tool-use and function calling", "A_papers": "Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`", "B_papers": "Tool-use and function calling: `P0027`, `P0110`, `P0147`", "A_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 3}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "excerpt": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]", "score": 2}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 1}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Mohammadi2025Evaluation", "Guo2025Comprehensive", "Kwon2025Agentnet", "Kim2026Beyond"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0027`, `P0110`, `P0288`", "B_papers": "Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 2}, {"paper_id": "P0061", "evidence_id": "E-P0061-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0061#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 0}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhou2025Self", "Shi2025Progent", "Mohammadi2025Evaluation", "Guo2025Comprehensive"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0027`, `P0110`, `P0288`", "B_papers": "Tool-use and function calling: `P0027`, `P0110`, `P0147`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 2}, {"paper_id": "P0061", "evidence_id": "E-P0061-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0061#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "excerpt": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]", "score": 0}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhou2025Self", "Shi2025Progent", "Kwon2025Agentnet", "Kim2026Beyond"]}, {"axis": "compute and latency constraints", "A_label": "Evaluation / benchmark-focused works", "B_label": "Tool-use and function calling", "A_papers": "Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`", "B_papers": "Tool-use and function calling: `P0027`, `P0110`, `P0147`", "A_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 0}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "excerpt": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]", "score": 0}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 0}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Mohammadi2025Evaluation", "Guo2025Comprehensive", "Kwon2025Agentnet", "Kim2026Beyond"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0027`, `P0110`, `P0288`", "B_papers": "Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 1}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 1}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Zhou2025Self", "Kim2026Beyond", "Mohammadi2025Evaluation"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0027`, `P0110`, `P0288`", "B_papers": "Tool-use and function calling: `P0027`, `P0110`, `P0147`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 1}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "excerpt": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]", "score": 2}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Zhou2025Self", "Kim2026Beyond", "Kwon2025Agentnet"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Evaluation / benchmark-focused works", "B_label": "Tool-use and function calling", "A_papers": "Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`", "B_papers": "Tool-use and function calling: `P0027`, `P0110`, `P0147`", "A_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 1}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "excerpt": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]", "score": 2}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 1}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Mohammadi2025Evaluation", "Kim2026Beyond", "Kwon2025Agentnet"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0027`, `P0110`, `P0288`", "B_papers": "Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`", "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]", "score": 1}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]", "score": 1}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Zhou2025Self", "Kim2026Beyond", "Mohammadi2025Evaluation"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: API, LLMs, WildAGTEval, APIs, IFC, STPA, MCP, LLM-based, LLM-powered, LLM-driven.", "citations": ["Kim2026Beyond", "Doshi2026Towards", "V2026Agentic", "Liu2026Agents"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Kim2026Beyond", "Doshi2026Towards"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Kim2026Beyond", "Doshi2026Towards"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Kim2026Beyond", "Doshi2026Towards"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Kim2026Beyond", "Doshi2026Towards"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Kim2026Beyond", "Doshi2026Towards"]}], "failures_limitations": [{"bullet": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability.", "citations": ["Liu2026Agents"]}, {"bullet": "Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption.", "citations": ["Shi2025Progent"]}, {"bullet": "LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments.", "citations": ["Shi2025Progent"]}, {"bullet": "Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones.", "citations": ["Shi2025Progent"]}, {"bullet": "The framework operates deterministically at runtime, providing provable security guarantees.", "citations": ["Shi2025Progent"]}, {"bullet": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"]}, {"bullet": "Additionally, we show that LLMs can automatically generate effective policies, highlighting their potential for automating the process of writing Progent's security policies.", "citations": ["Shi2025Progent"]}, {"bullet": "Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.", "citations": ["Zhong2025Rtbas"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-02-07T19:55:34", "section_id": "6", "section_title": "Evaluation & Risks"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "evidence_ids": ["E-P0186-d6095e10e9", "E-P0190-3c65d38a2a", "E-P0208-753416ce70", "E-P0212-e0345118bc", "E-P0096-4fca49d200", "E-P0152-7edb91824f", "E-P0109-c6a59efa61", "E-P0040-8e34a29629", "E-P0136-a0b404d928", "E-P0101-a4ae2708e4", "E-P0108-3e2edc05cd", "E-P0270-dda84e50a5", "E-P0163-84b88bc47a", "E-P0241-e26328e18c", "E-P0113-a68f39bc04", "E-P0091-52fea1d199", "E-P0236-3cd6217549", "E-P0122-20fed7f11c", "E-P0160-a8b0d15cbe", "E-P0280-1c25e10ffc", "E-P0019-e38b4bdff3", "E-P0137-88081e54fd", "E-P0208-8b3195ff10", "E-P0186-65e66ae9c1"], "evidence_level_summary": {"fulltext": 0, "abstract": 28, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0186-d6095e10e9", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP).", "paper_id": "P0186", "citations": ["Zhang2025Security"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]"}}, {"evidence_id": "E-P0190-3c65d38a2a", "text": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "paper_id": "P0190", "citations": ["Luo2025Universe"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0190#limitations[1]"}}, {"evidence_id": "E-P0208-753416ce70", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "paper_id": "P0208", "citations": ["Fu2025Eval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0208#key_results[1]"}}, {"evidence_id": "E-P0212-e0345118bc", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents.", "paper_id": "P0212", "citations": ["Kale2025Reliable"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0212#key_results[0]"}}, {"evidence_id": "E-P0096-4fca49d200", "text": "We then systematically curate a high-quality dataset for function calling, which we use to fine-tune two small language models, TinyAgent-1.1B and 7B.", "paper_id": "P0096", "citations": ["Erdogan2024Tinyagent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}}, {"evidence_id": "E-P0152-7edb91824f", "text": "Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs.", "paper_id": "P0152", "citations": ["Bonagiri2025Check"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0152#key_results[0]"}}, {"evidence_id": "E-P0109-c6a59efa61", "text": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "paper_id": "P0109", "citations": ["Mou2026Toolsafe"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}}, {"evidence_id": "E-P0040-8e34a29629", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "paper_id": "P0040", "citations": ["Gasmi2025Bridging"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}}, {"evidence_id": "E-P0136-a0b404d928", "text": "Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution.", "paper_id": "P0136", "citations": ["Mo2025Attractive"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]"}}, {"evidence_id": "E-P0101-a4ae2708e4", "text": "Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior.", "paper_id": "P0101", "citations": ["Balaji2026Beyond"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[1]"}}, {"evidence_id": "E-P0108-3e2edc05cd", "text": "However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings.", "paper_id": "P0108", "citations": ["Li2026Toolprmbench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0108#key_results[0]"}}, {"evidence_id": "E-P0270-dda84e50a5", "text": "MVVM introduces two key innovations: (1) a two-way sandboxing framework leveraging hardware enclaves and accelerator extensions that protects both the agent from malicious hosts and the host from compromised agents; (2) an efficient cross platform migration mechanism using WebAssembly and WASI's platform-agnostic design, enabling seamless movement across ARM phones, RISC-V MCUs, x86 servers, and heterogeneous accelerators; and three astonishing use cases: (1) privacy-aware daemon that automatically determines whether to execute locally or remotely based on data sensitivity and resource availability; (2) multi-tier replication with intelligent quality degradation that maintains service availability despite network failures or resource constraints; (3) a comprehensive execution framework combining speculative execution for 10x latency reduction with parallel validation that ensures output safety without compromising responsiveness.", "paper_id": "P0270", "citations": ["Yang2024Mvvm"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0270#key_results[1]"}}, {"evidence_id": "E-P0163-84b88bc47a", "text": "Extensive evaluation using realistic 5G scenarios demonstrates that the edge framework achieves zero network outages under high-stress conditions, compared to 8.4% for traditional fixed-power networks and 3.3% for large language model (LLM) agent-based approaches, while maintaining near real-time responsiveness and consistent QoS.", "paper_id": "P0163", "citations": ["Salama2025Edge"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0163#key_results[0]"}}, {"evidence_id": "E-P0241-e26328e18c", "text": "Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%.", "paper_id": "P0241", "citations": ["Zhang2024Agent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0241#key_results[0]"}}, {"evidence_id": "E-P0113-a68f39bc04", "text": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "paper_id": "P0113", "citations": ["Van2025Survey"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}}, {"evidence_id": "E-P0091-52fea1d199", "text": "We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness.", "paper_id": "P0091", "citations": ["Zhang2024Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0091#key_results[1]"}}, {"evidence_id": "E-P0236-3cd6217549", "text": "(2) We then conduct a series of pilot experiments to demonstrate the safety risks in MCP-powered agent systems is a real threat and its defense is not trivial.", "paper_id": "P0236", "citations": ["Fang2025Should"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0236#key_results[1]"}}, {"evidence_id": "E-P0122-20fed7f11c", "text": "Through extensive evaluations on public and self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we demonstrate that our safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks.", "paper_id": "P0122", "citations": ["Sha2025Agent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Safety, security, and governance'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench", "Mou2026Toolsafe"]}], "claim_candidates": [{"claim": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP", "evidence_field": "evidence_snippet", "citations": ["Zhang2025Security"]}, {"claim": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "evidence_field": "evidence_snippet", "citations": ["Luo2025Universe"]}, {"claim": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "evidence_field": "evidence_snippet", "citations": ["Fu2025Eval"]}, {"claim": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents.", "evidence_field": "evidence_snippet", "citations": ["Kale2025Reliable"]}, {"claim": "We then systematically curate a high-quality dataset for function calling, which we use to fine-tune two small language models, TinyAgent-1.1B and 7B.", "evidence_field": "evidence_snippet", "citations": ["Erdogan2024Tinyagent"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0101`, `P0108`, `P0109`", "B_papers": "Safety / security / guardrails: `P0109`, `P0029`, `P0040`", "A_highlights": [{"paper_id": "P0101", "evidence_id": "E-P0101-a4ae2708e4", "excerpt": "Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior.", "citations": ["Balaji2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[1]", "score": 2}, {"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Balaji2026Beyond", "Gasmi2025Bridging", "Mou2026Toolsafe"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0101`, `P0108`, `P0109`", "B_papers": "Tool-use and function calling: `P0108`, `P0109`, `P0044`", "A_highlights": [{"paper_id": "P0101", "evidence_id": "E-P0101-a4ae2708e4", "excerpt": "Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior.", "citations": ["Balaji2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[1]", "score": 2}, {"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]", "score": 2}, {"paper_id": "P0113", "evidence_id": "E-P0113-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Balaji2026Beyond", "Gasmi2025Bridging", "Zhang2025Security", "Van2025Survey"]}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Safety / security / guardrails", "B_label": "Tool-use and function calling", "A_papers": "Safety / security / guardrails: `P0109`, `P0029`, `P0040`", "B_papers": "Tool-use and function calling: `P0108`, `P0109`, `P0044`", "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]", "score": 2}, {"paper_id": "P0113", "evidence_id": "E-P0113-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]", "score": 2}], "write_prompt": "Contrast Safety / security / guardrails vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Gasmi2025Bridging", "Mou2026Toolsafe", "Zhang2025Security", "Van2025Survey"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0101`, `P0108`, `P0109`", "B_papers": "Safety / security / guardrails: `P0109`, `P0029`, `P0040`", "A_highlights": [{"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 0}, {"paper_id": "P0101", "evidence_id": "E-P0101-a4ae2708e4", "excerpt": "Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior.", "citations": ["Balaji2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 0}, {"paper_id": "P0122", "evidence_id": "E-P0122-20fed7f11c", "excerpt": "Through extensive evaluations on public and self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we demonstrate that our safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks.", "citations": ["Sha2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Mou2026Toolsafe", "Balaji2026Beyond", "Sha2025Agent"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0101`, `P0108`, `P0109`", "B_papers": "Tool-use and function calling: `P0108`, `P0109`, `P0044`", "A_highlights": [{"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 0}, {"paper_id": "P0101", "evidence_id": "E-P0101-a4ae2708e4", "excerpt": "Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior.", "citations": ["Balaji2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[1]", "score": 0}], "B_highlights": [{"paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]", "score": 1}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Mou2026Toolsafe", "Balaji2026Beyond", "Zhang2025Security"]}, {"axis": "compute and latency constraints", "A_label": "Safety / security / guardrails", "B_label": "Tool-use and function calling", "A_papers": "Safety / security / guardrails: `P0109`, `P0029`, `P0040`", "B_papers": "Tool-use and function calling: `P0108`, `P0109`, `P0044`", "A_highlights": [{"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 0}, {"paper_id": "P0122", "evidence_id": "E-P0122-20fed7f11c", "excerpt": "Through extensive evaluations on public and self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we demonstrate that our safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks.", "citations": ["Sha2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[0]", "score": 0}], "B_highlights": [{"paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]", "score": 1}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 0}], "write_prompt": "Contrast Safety / security / guardrails vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Mou2026Toolsafe", "Sha2025Agent", "Zhang2025Security"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0101`, `P0108`, `P0109`", "B_papers": "Safety / security / guardrails: `P0109`, `P0029`, `P0040`", "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Gasmi2025Bridging", "Mou2026Toolsafe"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0101`, `P0108`, `P0109`", "B_papers": "Tool-use and function calling: `P0108`, `P0109`, `P0044`", "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]", "score": 2}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Gasmi2025Bridging", "Mou2026Toolsafe", "Zhang2025Security"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Safety / security / guardrails", "B_label": "Tool-use and function calling", "A_papers": "Safety / security / guardrails: `P0109`, `P0029`, `P0040`", "B_papers": "Tool-use and function calling: `P0108`, `P0109`, `P0044`", "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]", "score": 2}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 1}], "write_prompt": "Contrast Safety / security / guardrails vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Gasmi2025Bridging", "Mou2026Toolsafe", "Zhang2025Security"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0101`, `P0108`, `P0109`", "B_papers": "Safety / security / guardrails: `P0109`, `P0029`, `P0040`", "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]", "score": 2}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"tool selection / routing policy\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Gasmi2025Bridging", "Mou2026Toolsafe"]}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: IVR, LLMs, SPA, DPA, GPT-4o-mini, GPT-4o, AI-driven, IVR-era, JourneyBench, PRMs.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench", "Mou2026Toolsafe", "Zhou2025Reasoning"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench"]}], "failures_limitations": [{"bullet": "While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge.", "citations": ["Balaji2026Beyond"]}, {"bullet": "Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.", "citations": ["Balaji2026Beyond"]}, {"bullet": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks.", "citations": ["Mou2026Toolsafe"]}, {"bullet": "Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored.", "citations": ["Mou2026Toolsafe"]}, {"bullet": "The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history.", "citations": ["Mou2026Toolsafe"]}, {"bullet": "It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback.", "citations": ["Mou2026Toolsafe"]}, {"bullet": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"]}, {"bullet": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-02-07T19:55:34", "section_id": "6", "section_title": "Evaluation & Risks"}
