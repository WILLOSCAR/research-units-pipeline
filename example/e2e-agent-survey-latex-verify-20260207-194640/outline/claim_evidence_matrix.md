# Claim–Evidence matrix

This artifact is bullets-only and is meant to make evidence explicit before writing.

Generated as a projection of `outline/evidence_drafts.jsonl` (evidence packs).

## 3.1 Agent loop and action spaces

- RQ: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?
- Claim: We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and rese
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0121` [@Song2025Agent] — We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0121#key_results[0])
  - Evidence: `P0191` [@Liu2025Mcpagentbench] — To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0191#method)
  - Evidence: `P0232` [@Tomaevi2025Towards] — Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0232#limitations[1])
  - Evidence: `P0177` [@Feng2025Group] — We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
  - Evidence: `P0012` [@Chowa2025From] — Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - Evidence: `P0040` [@Gasmi2025Bridging] — Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 3.2 Tool interfaces and orchestration

- RQ: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?
- Claim: Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0014` [@Zhou2026Beyond] — Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - Evidence: `P0162` [@Dong2025Etom] — We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0162#method)
  - Evidence: `P0022` [@Yang2025Toolmind] — To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0022#limitations[1])
  - Evidence: `P0168` [@Mohammadi2025Evaluation] — This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - Evidence: `P0229` [@Liu2025Toolscope] — Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0229#key_results[0])
  - Evidence: `P0263` [@Fu2024Imprompter] — This attack shows a nearly 80% success rate in an end-to-end evaluation. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0263#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 4.1 Planning and reasoning loops

- RQ: Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?
- Claim: Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average perf
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0243` [@Shang2024Agentsquare] — Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0243#key_results[0])
  - Evidence: `P0190` [@Luo2025Universe] — To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0190#method)
  - Evidence: `P0046` [@Nakano2025Guided] — To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0046#key_results[1])
  - Evidence: `P0215` [@Zhou2025Siraj] — Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0215#key_results[0])
  - Evidence: `P0233` [@Hu2025Training] — Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0233#key_results[0])
  - Evidence: `P0089` [@Radha2024Iteration] — We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 4.2 Memory and retrieval (RAG)

- RQ: Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?
- Claim: Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0061` [@Shi2025Progent] — Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0061#key_results[0])
  - Evidence: `P0162` [@Dong2025Etom] — We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0162#method)
  - Evidence: `P0243` [@Shang2024Agentsquare] — Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0243#key_results[0])
  - Evidence: `P0128` [@Li2025Agentswift] — Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0128#key_results[0])
  - Evidence: `P0072` [@Maragheh2025Future] — The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools that keep pace with this new degree of autonomy. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - Evidence: `P0197` [@Abbineni2025Muallm] — To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0197#key_results[1])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 5.1 Self-improvement and adaptation

- RQ: Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?
- Claim: Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-genera
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; communication protocol / roles; aggregation (vote / debate / referee); stability / robustness
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0067` [@Zhou2025Self] — Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - Evidence: `P0100` [@Li2026Autonomous] — We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0100#method)
  - Evidence: `P0082` [@Du2024Anytool] — We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0082#limitations[1])
  - Evidence: `P0103` [@Zhang2026Evoroute] — Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0103#key_results[0])
  - Evidence: `P0009` [@Guo2025Comprehensive] — Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
  - Evidence: `P0178` [@Yu2025Infiagent] — Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0178#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 5.2 Multi-agent coordination

- RQ: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?
- Claim: We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0177` [@Feng2025Group] — We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
  - Evidence: `P0159` [@Zhang2025Datascibench] — We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0159#method)
  - Evidence: `P0266` [@Yang2024Based] — The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0266#limitations[1])
  - Evidence: `P0194` [@Lumer2025Memtool] — Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0194#key_results[0])
  - Evidence: `P0015` [@Lu2025Just] — Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0015#key_results[0])
  - Evidence: `P0189` [@Gao2025Radar] — MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0189#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 6.1 Benchmarks and evaluation protocols

- RQ: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?
- Claim: RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0208` [@Fu2025Eval] — RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0208#key_results[1])
  - Evidence: `P0147` [@Kwon2025Agentnet] — Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are crucial for achieving desired performance levels, requiring domain expertise. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0147#limitations[1])
  - Evidence: `P0061` [@Shi2025Progent] — Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0061#key_results[0])
  - Evidence: `P0168` [@Mohammadi2025Evaluation] — This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - Evidence: `P0067` [@Zhou2025Self] — Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - Evidence: `P0062` [@Zhong2025Rtbas] — Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 6.2 Safety, security, and governance

- RQ: Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?
- Claim: MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error
  - Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability
  - Evidence levels: fulltext=0, abstract=28, title=0.
  - Evidence: `P0186` [@Zhang2025Security] — MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0186#key_results[0])
  - Evidence: `P0190` [@Luo2025Universe] — Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0190#limitations[1])
  - Evidence: `P0208` [@Fu2025Eval] — RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0208#key_results[1])
  - Evidence: `P0212` [@Kale2025Reliable] — To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0212#key_results[0])
  - Evidence: `P0096` [@Erdogan2024Tinyagent] — We then systematically curate a high-quality dataset for function calling, which we use to fine-tune two small language models, TinyAgent-1.1B and 7B. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
  - Evidence: `P0152` [@Bonagiri2025Check] — Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0152#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.
