{"sub_id": "3.1", "title": "Agent loop and action spaces", "section_id": "3", "section_title": "Foundations & Interfaces", "rq": "Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Agent loop and action spaces, differences in evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "tension_statement": "A central tension in Agent loop and action spaces is the trade-off between evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints and what can be evaluated reliably under realistic constraints.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["A practical decision is", "One design choice is", "For system builders, the crux is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Stepping back,", "Viewed across reported protocols,", "Across reported settings,", "A consistent theme is that", "Collectively,", "In summary,", "The evidence suggests that", "Across these studies,"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "These results hinge on"}, {"avoid_stem": "Three limitations", "prefer_stem": "A key limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Across these studies,", "For practitioners,", "For system builders,", "For builders,", "A caveat is", "Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"], "Across these studies,": ["Collectively,", "Stepping back,", "Across reported protocols,", "Viewed across settings,", "One way to read this evidence is that"], "For practitioners,": ["In deployed settings,", "Operationally,", "From a systems perspective,", "Under typical budget constraints,", "In practice,"], "For system builders,": ["From a system-design perspective,", "In system design,", "When building tool-using agents,", "A practical design implication is that", "In practice,"], "For builders,": ["From a system-design perspective,", "In system design,", "When building agents,", "A practical design implication is that", "In practice,"], "A caveat is": ["Interpretation depends on", "Generalization is unclear when", "These results hinge on", "Evidence is thin when", "A key boundary is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "tension-first", "opener_hint": "Start with the subsectionâ€™s central tension/trade-off; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["benchmarks/metrics", "compute"], "contrast_hook": "evaluation", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Tool-use and function calling", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Tool-use and function calling vs Agent frameworks / architectures)", "use_clusters": ["Tool-use and function calling"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Tool-use and function calling", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Tool-use and function calling"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Tool-use and function calling", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Tool-use and function calling"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Tool-use and function calling", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Tool-use and function calling)", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Code agents / software tasks"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Code agents / software tasks"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Code agents / software tasks"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent latex survey.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["evaluation", "tool interfaces"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Song2025Agent", "Liu2025Mcpagentbench", "Tomaevi2025Towards", "Feng2025Group", "Chowa2025From", "Gasmi2025Bridging", "Zhang2026Evoroute", "Fumero2025Cybersleuth", "Fang2025Should", "Zhang2025Datascibench", "Yao2022React", "Ghose2025Orfs", "Yin2024Mmau", "Zhuang2025Hephaestus", "Zhu2025Agent", "Cui2025Toward", "Zhang2024Ecoact", "Min2025Goat", "Yang2025Survey", "Yang2025Proagent", "Miyamoto2026Agent", "Song2026Envscaler"], "allowed_bibkeys_mapped": ["Kim2025Bridging", "Song2025Agent", "Zhang2024Ecoact", "Kulkarni2025Agent", "Fumero2025Cybersleuth", "Feng2025Group", "Zhang2026Evoroute", "Yin2024Mmau", "Song2026Envscaler", "Soliman2026Intagent", "Chowa2025From", "Yang2025Survey", "Zhang2025Datascibench", "Ghose2025Orfs", "Tomaevi2025Towards", "Min2025Goat", "Miyamoto2026Agent", "Gasmi2025Bridging", "Zhuang2025Hephaestus", "Tawosi2025Almas", "Zhu2025Agent", "Liu2025Mcpagentbench", "Yang2025Proagent", "Cui2025Toward", "Fang2025Should", "Li2025What", "Chin2024Human", "Yao2022React"], "allowed_bibkeys_chapter": ["Cheng2025Your", "Chin2024Human", "Chowa2025From", "Cui2025Toward", "Dong2025Etom", "Doshi2026Towards", "Fang2025Should", "Feng2025Group", "Ferrag2025From", "Fu2024Imprompter", "Fumero2025Cybersleuth", "Gao2025Radar", "Gasmi2025Bridging", "Ghose2025Orfs", "Gupta2024Codenav", "Hao2026From", "Jeon2025Based", "Jia2025Autotool", "Kim2025Bridging", "Kulkarni2025Agent", "Kumar2024Swissnyf", "Li2025What", "Li2026Toolprmbench", "Liu2025Mcpagentbench", "Liu2025Toolscope", "Lumer2025Memtool", "Michelakis2025Core", "Min2025Goat", "Miyamoto2026Agent", "Mohammadi2025Evaluation", "Nandi2025Bench", "Shen2024Small", "Soliman2026Intagent", "Song2025Agent", "Song2026Envscaler", "Tawosi2025Almas", "Tomaevi2025Towards", "Wang2025Bench", "Wu2024Avatar", "Yang2025Proagent", "Yang2025Survey", "Yang2025Toolmind", "Yao2022React", "Yin2024Mmau", "Yin2025Magnet", "Zhang2024Ecoact", "Zhang2025Datascibench", "Zhang2026Evoroute", "Zhou2026Beyond", "Zhu2024Menti", "Zhu2025Agent", "Zhuang2025Hephaestus"], "allowed_bibkeys_global": ["Hu2023Avis", "Yao2022React"], "evidence_ids": ["E-P0121-44cc8fbce0", "E-P0191-312a342670", "E-P0232-1232593f0e", "E-P0177-4b027dfb27", "E-P0012-e0bcca0d9e", "E-P0040-8e34a29629", "E-P0103-60cc0d458f", "E-P0156-0b34b55332", "E-P0236-1ca9e70d75", "E-P0159-39281e5083", "E-P0001-ca4a00b5cf", "E-P0201-1d5f67b08e", "E-P0092-ebb97f6129", "E-P0047-a06d9a2787", "E-P0173-66900cdb54", "E-P0230-15e523063d", "E-P0023-83f8d55860", "E-P0045-8a32e8598f", "E-P0030-4c14763fd0", "E-P0206-aa504d163c", "E-P0230-171b93237a", "E-P0291-3728b2e26e", "E-P0295-f1a25e82c1", "E-P0206-50bf74b151"], "anchor_facts": [{"hook_type": "quant", "text": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}, {"hook_type": "quant", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}, {"hook_type": "quant", "text": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "citations": ["Song2025Agent"], "paper_id": "P0121", "evidence_id": "E-P0121-44cc8fbce0", "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"hook_type": "quant", "text": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "paper_id": "P0201", "evidence_id": "E-P0201-1d5f67b08e", "pointer": "papers/paper_notes.jsonl:paper_id=P0201#key_results[0]"}, {"hook_type": "eval", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "paper_id": "P0191", "evidence_id": "E-P0191-312a342670", "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method"}, {"hook_type": "quant", "text": "Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates.", "citations": ["Tomaevi2025Towards"], "paper_id": "P0232", "evidence_id": "E-P0232-1232593f0e", "pointer": "papers/paper_notes.jsonl:paper_id=P0232#limitations[1]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0177", "evidence_id": "E-P0177-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}, {"hook_type": "quant", "text": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "paper_id": "P0159", "evidence_id": "E-P0159-39281e5083", "pointer": "papers/paper_notes.jsonl:paper_id=P0159#key_results[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Chowa2025From", "Gasmi2025Bridging", "Liu2025Mcpagentbench"], "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}, {"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}], "B_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}, {"paper_id": "P0191", "evidence_id": "E-P0191-312a342670", "excerpt": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Code agents / software tasks", "citations": ["Chowa2025From", "Gasmi2025Bridging"], "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}, {"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Code agents / software tasks along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Tool-use and function calling", "B_label": "Code agents / software tasks", "citations": ["Chowa2025From", "Liu2025Mcpagentbench", "Gasmi2025Bridging"], "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}, {"paper_id": "P0191", "evidence_id": "E-P0191-312a342670", "excerpt": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method"}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}], "write_prompt": "Contrast Tool-use and function calling vs Code agents / software tasks along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Zhang2026Evoroute", "Chowa2025From", "Song2025Agent", "Ghose2025Orfs"], "A_highlights": [{"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}], "B_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-44cc8fbce0", "excerpt": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "citations": ["Song2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"paper_id": "P0201", "evidence_id": "E-P0201-1d5f67b08e", "excerpt": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "pointer": "papers/paper_notes.jsonl:paper_id=P0201#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Code agents / software tasks", "citations": ["Zhang2026Evoroute", "Chowa2025From", "Gasmi2025Bridging"], "A_highlights": [{"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Code agents / software tasks along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Tool-use and function calling", "B_label": "Code agents / software tasks", "citations": ["Song2025Agent", "Ghose2025Orfs", "Gasmi2025Bridging"], "A_highlights": [{"paper_id": "P0121", "evidence_id": "E-P0121-44cc8fbce0", "excerpt": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "citations": ["Song2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"paper_id": "P0201", "evidence_id": "E-P0201-1d5f67b08e", "excerpt": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "pointer": "papers/paper_notes.jsonl:paper_id=P0201#key_results[0]"}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}], "write_prompt": "Contrast Tool-use and function calling vs Code agents / software tasks along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Gasmi2025Bridging", "Chowa2025From", "Liu2025Mcpagentbench", "Song2025Agent"], "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}], "B_highlights": [{"paper_id": "P0191", "evidence_id": "E-P0191-312a342670", "excerpt": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method"}, {"paper_id": "P0121", "evidence_id": "E-P0121-44cc8fbce0", "excerpt": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "citations": ["Song2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, GAIA, EvoRoute, BrowseComp, LLM-simulated, SFT, RUC-NLPIR, EnvScaler, SkelBuilder, ScenGenerator.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent", "Song2026Envscaler", "Soliman2026Intagent"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Zhang2026Evoroute", "Miyamoto2026Agent"]}], "limitation_hooks": [{"excerpt": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"], "pointer": ""}, {"excerpt": "With the spread of generative AI in recent years, attacks known as Whaling have become a serious threat.", "citations": ["Miyamoto2026Agent"], "pointer": ""}, {"excerpt": "We design agents that (i) build vulnerability profiles for each target from publicly available information on faculty members, (ii) identify potential risk scenarios relevant to Whaling defense based on those profiles, (iii) construct defense profiles corresponding to the vulnerabilities and anticipated risks, and", "citations": ["Miyamoto2026Agent"], "pointer": ""}, {"excerpt": "Furthermore, we conduct a preliminary risk-assessment experiment.", "citations": ["Miyamoto2026Agent"], "pointer": ""}, {"excerpt": "Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency.", "citations": ["Yang2025Survey"], "pointer": ""}, {"excerpt": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "citations": ["Gasmi2025Bridging"], "pointer": ""}, {"excerpt": "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "citations": ["Gasmi2025Bridging"], "pointer": ""}, {"excerpt": "We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service).", "citations": ["Gasmi2025Bridging"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-02-07T20:01:31"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "section_id": "3", "section_title": "Foundations & Interfaces", "rq": "Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Tool interfaces and orchestration methods emphasize evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "tension_statement": "In Tool interfaces and orchestration, a practical tension is expressivity versus control: richer interfaces expand capability but make behavior harder to constrain and verify.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["A practical decision is", "One design choice is", "For system builders, the crux is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Stepping back,", "Viewed across reported protocols,", "Across reported settings,", "A consistent theme is that", "Collectively,", "In summary,", "The evidence suggests that", "Across these studies,"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "These results hinge on"}, {"avoid_stem": "Three limitations", "prefer_stem": "A key limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Across these studies,", "For practitioners,", "For system builders,", "For builders,", "A caveat is", "Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"], "Across these studies,": ["Collectively,", "Stepping back,", "Across reported protocols,", "Viewed across settings,", "One way to read this evidence is that"], "For practitioners,": ["In deployed settings,", "Operationally,", "From a systems perspective,", "Under typical budget constraints,", "In practice,"], "For system builders,": ["From a system-design perspective,", "In system design,", "When building tool-using agents,", "A practical design implication is that", "In practice,"], "For builders,": ["From a system-design perspective,", "In system design,", "When building agents,", "A practical design implication is that", "In practice,"], "A caveat is": ["Interpretation depends on", "Generalization is unclear when", "These results hinge on", "Evidence is thin when", "A key boundary is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "decision-first", "opener_hint": "Start with a builder/research decision; state what it depends on; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability", "benchmarks/metrics"], "contrast_hook": "tool interfaces", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Tool-use and function calling", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Tool-use and function calling vs Agent frameworks / architectures)", "use_clusters": ["Tool-use and function calling"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Tool-use and function calling", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Tool-use and function calling"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Tool-use and function calling", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Tool-use and function calling"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Tool-use and function calling", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Tool-use and function calling)", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Evaluation / benchmark-focused works"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Evaluation / benchmark-focused works"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Evaluation / benchmark-focused works"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent latex survey.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["evaluation", "tool interfaces"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhou2026Beyond", "Dong2025Etom", "Yang2025Toolmind", "Mohammadi2025Evaluation", "Liu2025Toolscope", "Fu2024Imprompter", "Nandi2025Bench", "Ferrag2025From", "Doshi2026Towards", "Lumer2025Memtool", "Gao2025Radar", "Jeon2025Based", "Gupta2024Codenav", "Shen2024Small", "Chowa2025From", "Li2026Toolprmbench", "Hao2026From", "Yao2022React", "Ghose2025Orfs", "Wu2024Avatar", "Cheng2025Your", "Yin2025Magnet", "Wang2025Bench"], "allowed_bibkeys_mapped": ["Dong2025Etom", "Doshi2026Towards", "Gao2025Radar", "Yang2025Toolmind", "Wang2025Bench", "Li2026Toolprmbench", "Jia2025Autotool", "Cheng2025Your", "Shen2024Small", "Zhou2026Beyond", "Hao2026From", "Ferrag2025From", "Yin2025Magnet", "Jeon2025Based", "Kumar2024Swissnyf", "Wu2024Avatar", "Nandi2025Bench", "Michelakis2025Core", "Lumer2025Memtool", "Liu2025Toolscope", "Zhu2024Menti", "Gupta2024Codenav", "Fu2024Imprompter", "Chowa2025From", "Ghose2025Orfs", "Cui2025Toward", "Mohammadi2025Evaluation", "Yao2022React"], "allowed_bibkeys_chapter": ["Cheng2025Your", "Chin2024Human", "Chowa2025From", "Cui2025Toward", "Dong2025Etom", "Doshi2026Towards", "Fang2025Should", "Feng2025Group", "Ferrag2025From", "Fu2024Imprompter", "Fumero2025Cybersleuth", "Gao2025Radar", "Gasmi2025Bridging", "Ghose2025Orfs", "Gupta2024Codenav", "Hao2026From", "Jeon2025Based", "Jia2025Autotool", "Kim2025Bridging", "Kulkarni2025Agent", "Kumar2024Swissnyf", "Li2025What", "Li2026Toolprmbench", "Liu2025Mcpagentbench", "Liu2025Toolscope", "Lumer2025Memtool", "Michelakis2025Core", "Min2025Goat", "Miyamoto2026Agent", "Mohammadi2025Evaluation", "Nandi2025Bench", "Shen2024Small", "Soliman2026Intagent", "Song2025Agent", "Song2026Envscaler", "Tawosi2025Almas", "Tomaevi2025Towards", "Wang2025Bench", "Wu2024Avatar", "Yang2025Proagent", "Yang2025Survey", "Yang2025Toolmind", "Yao2022React", "Yin2024Mmau", "Yin2025Magnet", "Zhang2024Ecoact", "Zhang2025Datascibench", "Zhang2026Evoroute", "Zhou2026Beyond", "Zhu2024Menti", "Zhu2025Agent", "Zhuang2025Hephaestus"], "allowed_bibkeys_global": ["Hu2023Avis", "Yao2022React"], "evidence_ids": ["E-P0014-b6b7af5a81", "E-P0162-192e78b614", "E-P0022-f5a0e32a25", "E-P0168-37f9ea924c", "E-P0229-468a77ff1d", "E-P0263-0ea7b39672", "E-P0013-4e80a740e1", "E-P0044-74547b154c", "E-P0110-dab37b0fe2", "E-P0194-35271418ac", "E-P0189-419e1464da", "E-P0066-ee5ba721d6", "E-P0250-a8aa0f8a7d", "E-P0273-9640816b42", "E-P0012-e0bcca0d9e", "E-P0108-3e2edc05cd", "E-P0297-ed4427964c", "E-P0001-ca4a00b5cf", "E-P0201-1d5f67b08e", "E-P0246-3575a7c673", "E-P0297-7bed142d5c", "E-P0079-7cd49652d3", "E-P0055-9f71b81590", "E-P0187-9c914b3dcf"], "anchor_facts": [{"hook_type": "quant", "text": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "citations": ["Nandi2025Bench"], "paper_id": "P0013", "evidence_id": "E-P0013-4e80a740e1", "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}, {"hook_type": "quant", "text": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}, {"hook_type": "quant", "text": "To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances.", "citations": ["Yang2025Toolmind"], "paper_id": "P0022", "evidence_id": "E-P0022-f5a0e32a25", "pointer": "papers/paper_notes.jsonl:paper_id=P0022#limitations[1]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"hook_type": "quant", "text": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "paper_id": "P0014", "evidence_id": "E-P0014-b6b7af5a81", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "eval", "text": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"hook_type": "quant", "text": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "citations": ["Liu2025Toolscope"], "paper_id": "P0229", "evidence_id": "E-P0229-468a77ff1d", "pointer": "papers/paper_notes.jsonl:paper_id=P0229#key_results[0]"}, {"hook_type": "quant", "text": "This attack shows a nearly 80% success rate in an end-to-end evaluation.", "citations": ["Fu2024Imprompter"], "paper_id": "P0263", "evidence_id": "E-P0263-0ea7b39672", "pointer": "papers/paper_notes.jsonl:paper_id=P0263#key_results[0]"}, {"hook_type": "eval", "text": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Nandi2025Bench", "Chowa2025From", "Yang2025Toolmind"], "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-4e80a740e1", "excerpt": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "citations": ["Nandi2025Bench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}], "B_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}, {"paper_id": "P0022", "evidence_id": "E-P0022-f5a0e32a25", "excerpt": "To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances.", "citations": ["Yang2025Toolmind"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#limitations[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Nandi2025Bench", "Chowa2025From", "Dong2025Etom", "Mohammadi2025Evaluation"], "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-4e80a740e1", "excerpt": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "citations": ["Nandi2025Bench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}, {"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}], "B_highlights": [{"paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method"}, {"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Tool-use and function calling", "B_label": "Evaluation / benchmark-focused works", "citations": ["Chowa2025From", "Yang2025Toolmind", "Dong2025Etom", "Mohammadi2025Evaluation"], "A_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "excerpt": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}, {"paper_id": "P0022", "evidence_id": "E-P0022-f5a0e32a25", "excerpt": "To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances.", "citations": ["Yang2025Toolmind"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#limitations[1]"}], "B_highlights": [{"paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method"}, {"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}], "write_prompt": "Contrast Tool-use and function calling vs Evaluation / benchmark-focused works along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Zhou2026Beyond", "Ferrag2025From"], "A_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}], "B_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Zhou2026Beyond", "Ferrag2025From", "Mohammadi2025Evaluation", "Dong2025Etom"], "A_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Tool-use and function calling", "B_label": "Evaluation / benchmark-focused works", "citations": ["Zhou2026Beyond", "Ferrag2025From", "Mohammadi2025Evaluation", "Dong2025Etom"], "A_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method"}], "write_prompt": "Contrast Tool-use and function calling vs Evaluation / benchmark-focused works along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Nandi2025Bench", "Doshi2026Towards", "Ferrag2025From"], "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-4e80a740e1", "excerpt": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "citations": ["Nandi2025Bench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}, {"paper_id": "P0110", "evidence_id": "E-P0110-dab37b0fe2", "excerpt": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents.", "citations": ["Doshi2026Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0]"}], "B_highlights": [{"paper_id": "P0110", "evidence_id": "E-P0110-dab37b0fe2", "excerpt": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents.", "citations": ["Doshi2026Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0]"}, {"paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "excerpt": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: RAG, MCP, MCTS, LLMs, BFCL, GPU, ToolBench, PRMs, PRM, ToolPRMBench.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench", "Doshi2026Towards", "Hao2026From"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench"]}], "limitation_hooks": [{"excerpt": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents.", "citations": ["Zhou2026Beyond"], "pointer": ""}, {"excerpt": "We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task.", "citations": ["Zhou2026Beyond"], "pointer": ""}, {"excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": ""}, {"excerpt": "These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.", "citations": ["Zhou2026Beyond"], "pointer": ""}, {"excerpt": "Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces.", "citations": ["Hao2026From"], "pointer": ""}, {"excerpt": "However, the rapid growth of plugins, connectors, and inter-agent protocols has outpaced security practices, leading to brittle integrations that rely on ad-hoc authentication, inconsistent schemas, and weak validation.", "citations": ["Ferrag2025From"], "pointer": ""}, {"excerpt": "This survey introduces a unified end-to-end threat model for LLM-agent ecosystems, covering host-to-tool and agent-to-agent communications.", "citations": ["Ferrag2025From"], "pointer": ""}, {"excerpt": "We systematically categorize more than thirty attack techniques spanning input manipulation, model compromise, system and privacy attacks, and protocol-level vulnerabilities.", "citations": ["Ferrag2025From"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-02-07T20:01:31"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "rq": "Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Planning and reasoning loops, differences in evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "tension_statement": "In Planning and reasoning loops, a recurring tension is deliberation depth versus cost: more planning can improve reliability but increases latency and budget sensitivity.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["A practical decision is", "One design choice is", "For system builders, the crux is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Stepping back,", "Viewed across reported protocols,", "Across reported settings,", "A consistent theme is that", "Collectively,", "In summary,", "The evidence suggests that", "Across these studies,"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "These results hinge on"}, {"avoid_stem": "Three limitations", "prefer_stem": "A key limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Across these studies,", "For practitioners,", "For system builders,", "For builders,", "A caveat is", "Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"], "Across these studies,": ["Collectively,", "Stepping back,", "Across reported protocols,", "Viewed across settings,", "One way to read this evidence is that"], "For practitioners,": ["In deployed settings,", "Operationally,", "From a systems perspective,", "Under typical budget constraints,", "In practice,"], "For system builders,": ["From a system-design perspective,", "In system design,", "When building tool-using agents,", "A practical design implication is that", "In practice,"], "For builders,": ["From a system-design perspective,", "In system design,", "When building agents,", "A practical design implication is that", "In practice,"], "A caveat is": ["Interpretation depends on", "Generalization is unclear when", "These results hinge on", "Evidence is thin when", "A key boundary is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "protocol-first", "opener_hint": "Start with comparability constraints (protocol/budget/tool access); state what they make meaningful; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["planner/executor", "search", "deliberation", "action grounding", "benchmarks/metrics", "compute"], "contrast_hook": "planning/control loop", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Planning / reasoning loops"], "rq": "Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Planning / reasoning loops", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Planning / reasoning loops)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Planning / reasoning loops", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Planning / reasoning loops"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Agent frameworks / architectures", "contrast with Planning / reasoning loops", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Agent frameworks / architectures vs Planning / reasoning loops)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Planning / reasoning loops vs Agent frameworks / architectures", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Planning / reasoning loops vs Agent frameworks / architectures)", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Tool-use and function calling"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Tool-use and function calling"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Tool-use and function calling"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent latex survey.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["planning/control loop", "memory/retrieval"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Shang2024Agentsquare", "Luo2025Universe", "Nakano2025Guided", "Zhou2025Siraj", "Hu2025Training", "Radha2024Iteration", "Zhou2025Reasoning", "Gao2024Efficient", "Yao2022React", "Rawat2025Multi", "Baker2025Larc", "Shi2024Ehragent", "Ji2024Testing", "Zhang2025Multimind", "Zeng2025Rejump", "Qin2025Compass", "Silva2025Agents", "Mudur2025Feabench", "Choi2025Reactree", "Koubaa2025Agentic", "Hong2025Planning"], "allowed_bibkeys_mapped": ["Rawat2025Multi", "Hu2025Training", "Hong2025Planning", "Silva2025Agents", "Zhou2025Reasoning", "Baker2025Larc", "Shi2024Ehragent", "Zeng2025Rejump", "Wu2025Agentic", "Zhang2025Multimind", "Zhao2024Lightva", "Ji2024Testing", "Hatalis2025Review", "Qin2025Compass", "Mudur2025Feabench", "Choi2025Reactree", "Gao2024Efficient", "Radha2024Iteration", "Huang2024Understanding", "Li2024Personal", "Shang2024Agentsquare", "Nakano2025Guided", "Koubaa2025Agentic", "Zhao2025Autonomous", "Li2025Encouraging", "Luo2025Universe", "Zhou2025Siraj", "Yao2022React"], "allowed_bibkeys_chapter": ["Abbineni2025Muallm", "Baker2025Larc", "Choi2025Reactree", "Dong2025Etom", "Du2025Survey", "Gao2024Efficient", "Hatalis2025Review", "Hong2025Planning", "Hu2023Avis", "Hu2025Training", "Huang2024Understanding", "Huang2025Retrieval", "Ji2024Testing", "Kim2025Bridging", "Koubaa2025Agentic", "Li2024Personal", "Li2024Review", "Li2025Agentswift", "Li2025Encouraging", "Li2025Graphcodeagent", "Luo2025Universe", "Maragheh2025Future", "Min2025Goat", "Mudur2025Feabench", "Nakano2025Guided", "Peng2026Enhancing", "Qin2025Compass", "Radha2024Iteration", "Rawat2025Multi", "Shang2024Agentsquare", "Shi2024Ehragent", "Shi2025Progent", "Silva2025Agents", "Tawosi2025Meta", "Verma2026Active", "Wang2025Dsmentor", "Wu2025Agentic", "Wu2025Meta", "Yao2022React", "Yao2025Survey", "Ye2025Task", "Ye2025Taska", "Yu2026Agentic", "Zeng2025Rejump", "Zhang2024Large", "Zhang2025Large", "Zhang2025Multimind", "Zhao2024Lightva", "Zhao2025Autonomous", "Zhou2025Reasoning", "Zhou2025Siraj", "Zhu2025Where"], "allowed_bibkeys_global": ["Hu2023Avis", "Yao2022React"], "evidence_ids": ["E-P0243-38a26e4777", "E-P0190-84445d1a19", "E-P0046-741891e300", "E-P0215-0b753b9422", "E-P0233-771620f84f", "E-P0089-d36844b954", "E-P0019-8517628bd0", "E-P0086-a9c15ec0c8", "E-P0001-ca4a00b5cf", "E-P0060-5377f4f9b7", "E-P0180-55a2731a38", "E-P0254-2a7ea60588", "E-P0274-c0a98eb625", "E-P0199-9406c14ad1", "E-P0018-2bad4bca21", "E-P0145-c90a434db5", "E-P0148-b35b53de13", "E-P0170-e4ac5005b3", "E-P0211-4bcafdb221", "E-P0131-8e39e9502f", "E-P0205-1ad09376fe", "E-P0019-c17bcfb7d4", "E-P0089-dc268ffd9e", "E-P0131-02eb45e937"], "anchor_facts": [{"hook_type": "quant", "text": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]"}, {"hook_type": "quant", "text": "LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.", "citations": ["Gao2024Efficient"], "paper_id": "P0086", "evidence_id": "E-P0086-a9c15ec0c8", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[1]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]"}, {"hook_type": "eval", "text": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "citations": ["Luo2025Universe"], "paper_id": "P0190", "evidence_id": "E-P0190-84445d1a19", "pointer": "papers/paper_notes.jsonl:paper_id=P0190#method"}, {"hook_type": "quant", "text": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "paper_id": "P0215", "evidence_id": "E-P0215-0b753b9422", "pointer": "papers/paper_notes.jsonl:paper_id=P0215#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["Hu2025Training"], "paper_id": "P0233", "evidence_id": "E-P0233-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0233#key_results[0]"}, {"hook_type": "quant", "text": "We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset.", "citations": ["Radha2024Iteration"], "paper_id": "P0089", "evidence_id": "E-P0089-d36844b954", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "To measure the performance of task-oriented agents comprehensively, we propose a two-level evaluation framework: (1) turn level and (2) end-to-end.", "citations": ["Rawat2025Multi"], "paper_id": "P0060", "evidence_id": "E-P0060-5377f4f9b7", "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}, {"hook_type": "quant", "text": "We rigorously evaluate LARC on a carefully curated set of 48 constrained retrosynthesis planning tasks across 3 constraint types.", "citations": ["Baker2025Larc"], "paper_id": "P0180", "evidence_id": "E-P0180-55a2731a38", "pointer": "papers/paper_notes.jsonl:paper_id=P0180#key_results[1]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Silva2025Agents", "Zhou2025Reasoning"], "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]"}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]"}], "B_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]"}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Planning / reasoning loops", "B_label": "Tool-use and function calling", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Luo2025Universe", "Qin2025Compass"], "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]"}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]"}], "B_highlights": [{"paper_id": "P0190", "evidence_id": "E-P0190-84445d1a19", "excerpt": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "citations": ["Luo2025Universe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0190#method"}, {"paper_id": "P0145", "evidence_id": "E-P0145-c90a434db5", "excerpt": "We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios.", "citations": ["Qin2025Compass"], "pointer": "papers/paper_notes.jsonl:paper_id=P0145#key_results[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Silva2025Agents", "Zhou2025Reasoning", "Luo2025Universe", "Qin2025Compass"], "A_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]"}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]"}], "B_highlights": [{"paper_id": "P0190", "evidence_id": "E-P0190-84445d1a19", "excerpt": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "citations": ["Luo2025Universe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0190#method"}, {"paper_id": "P0145", "evidence_id": "E-P0145-c90a434db5", "excerpt": "We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios.", "citations": ["Qin2025Compass"], "pointer": "papers/paper_notes.jsonl:paper_id=P0145#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Silva2025Agents", "Zhou2025Reasoning"], "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]"}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]"}], "B_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]"}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Planning / reasoning loops", "B_label": "Tool-use and function calling", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Gao2024Efficient", "Qin2025Compass"], "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]"}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]"}], "B_highlights": [{"paper_id": "P0086", "evidence_id": "E-P0086-a9c15ec0c8", "excerpt": "LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.", "citations": ["Gao2024Efficient"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[1]"}, {"paper_id": "P0145", "evidence_id": "E-P0145-c90a434db5", "excerpt": "We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios.", "citations": ["Qin2025Compass"], "pointer": "papers/paper_notes.jsonl:paper_id=P0145#key_results[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Silva2025Agents", "Zhou2025Reasoning", "Gao2024Efficient", "Qin2025Compass"], "A_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]"}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]"}], "B_highlights": [{"paper_id": "P0086", "evidence_id": "E-P0086-a9c15ec0c8", "excerpt": "LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.", "citations": ["Gao2024Efficient"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[1]"}, {"paper_id": "P0145", "evidence_id": "E-P0145-c90a434db5", "excerpt": "We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel-planning scenarios.", "citations": ["Qin2025Compass"], "pointer": "papers/paper_notes.jsonl:paper_id=P0145#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "citations": ["Zeng2025Rejump", "Nakano2025Guided", "Silva2025Agents", "Zhou2025Reasoning"], "A_highlights": [{"paper_id": "P0018", "evidence_id": "E-P0018-2bad4bca21", "excerpt": "Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying", "citations": ["Zeng2025Rejump"], "pointer": "papers/paper_notes.jsonl:paper_id=P0018#key_results[0]"}, {"paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]"}], "B_highlights": [{"paper_id": "P0148", "evidence_id": "E-P0148-b35b53de13", "excerpt": "We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[0]"}, {"paper_id": "P0019", "evidence_id": "E-P0019-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#summary_bullets[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LRMs, LLMs, UW-Madison-Lee-Lab, CoTs, ReJump, CoT-prompted, ReJump-guided, RSP, GSI, RSV.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning", "Nakano2025Guided", "Rawat2025Multi"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Zeng2025Rejump", "Zhou2025Reasoning"]}], "limitation_hooks": [{"excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures.", "citations": ["Nakano2025Guided"], "pointer": ""}, {"excerpt": "To address this limitation, we fine-tune relatively small models such as Llama 3.1 (8B & 70B) using the proposed Pre-Act approach.", "citations": ["Rawat2025Multi"], "pointer": ""}, {"excerpt": "Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making.", "citations": ["Hatalis2025Review"], "pointer": ""}, {"excerpt": "Their dependence on rule-based control and narrow AI limits adaptability in dynamic and uncertain missions.", "citations": ["Koubaa2025Agentic"], "pointer": ""}, {"excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": ""}, {"excerpt": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "citations": ["Luo2025Universe"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-02-07T20:01:31"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "rq": "Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Memory and retrieval (RAG), differences in evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "tension_statement": "In Memory and retrieval (RAG), the core tension is persistence versus freshness: retaining more context helps long-horizon tasks but raises staleness, contamination, and verification challenges.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["A practical decision is", "One design choice is", "For system builders, the crux is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Stepping back,", "Viewed across reported protocols,", "Across reported settings,", "A consistent theme is that", "Collectively,", "In summary,", "The evidence suggests that", "Across these studies,"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "These results hinge on"}, {"avoid_stem": "Three limitations", "prefer_stem": "A key limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Across these studies,", "For practitioners,", "For system builders,", "For builders,", "A caveat is", "Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"], "Across these studies,": ["Collectively,", "Stepping back,", "Across reported protocols,", "Viewed across settings,", "One way to read this evidence is that"], "For practitioners,": ["In deployed settings,", "Operationally,", "From a systems perspective,", "Under typical budget constraints,", "In practice,"], "For system builders,": ["From a system-design perspective,", "In system design,", "When building tool-using agents,", "A practical design implication is that", "In practice,"], "For builders,": ["From a system-design perspective,", "In system design,", "When building agents,", "A practical design implication is that", "In practice,"], "A caveat is": ["Interpretation depends on", "Generalization is unclear when", "These results hinge on", "Evidence is thin when", "A key boundary is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "lens-first", "opener_hint": "Start by naming the lens (interface/protocol/threat model); state what it reveals; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["retrieval", "index", "write policy", "long-term memory", "benchmarks/metrics", "compute"], "contrast_hook": "memory/retrieval", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Memory / retrieval augmentation", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Memory / retrieval augmentation vs Agent frameworks / architectures)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Memory / retrieval augmentation", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Memory / retrieval augmentation", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Memory / retrieval augmentation", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Memory / retrieval augmentation)", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent latex survey.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["planning/control loop", "memory/retrieval"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Shi2025Progent", "Dong2025Etom", "Shang2024Agentsquare", "Li2025Agentswift", "Maragheh2025Future", "Abbineni2025Muallm", "Tawosi2025Meta", "Peng2026Enhancing", "Zhu2025Where", "Zhang2025Large", "Huang2025Retrieval", "Yao2022React", "Li2024Review", "Wang2025Dsmentor", "Verma2026Active", "Du2025Survey", "Zhang2024Large", "Yao2025Survey", "Yu2026Agentic", "Min2025Goat", "Li2025Graphcodeagent", "Ye2025Taska"], "allowed_bibkeys_mapped": ["Yu2026Agentic", "Huang2025Retrieval", "Peng2026Enhancing", "Wu2025Meta", "Kim2025Bridging", "Li2024Review", "Tawosi2025Meta", "Hu2023Avis", "Verma2026Active", "Yao2025Survey", "Min2025Goat", "Zhang2025Large", "Shi2025Progent", "Li2025Agentswift", "Li2025Graphcodeagent", "Abbineni2025Muallm", "Hong2025Planning", "Ye2025Task", "Ye2025Taska", "Zhu2025Where", "Li2024Personal", "Zhang2024Large", "Shang2024Agentsquare", "Du2025Survey", "Maragheh2025Future", "Wang2025Dsmentor", "Dong2025Etom", "Yao2022React"], "allowed_bibkeys_chapter": ["Abbineni2025Muallm", "Baker2025Larc", "Choi2025Reactree", "Dong2025Etom", "Du2025Survey", "Gao2024Efficient", "Hatalis2025Review", "Hong2025Planning", "Hu2023Avis", "Hu2025Training", "Huang2024Understanding", "Huang2025Retrieval", "Ji2024Testing", "Kim2025Bridging", "Koubaa2025Agentic", "Li2024Personal", "Li2024Review", "Li2025Agentswift", "Li2025Encouraging", "Li2025Graphcodeagent", "Luo2025Universe", "Maragheh2025Future", "Min2025Goat", "Mudur2025Feabench", "Nakano2025Guided", "Peng2026Enhancing", "Qin2025Compass", "Radha2024Iteration", "Rawat2025Multi", "Shang2024Agentsquare", "Shi2024Ehragent", "Shi2025Progent", "Silva2025Agents", "Tawosi2025Meta", "Verma2026Active", "Wang2025Dsmentor", "Wu2025Agentic", "Wu2025Meta", "Yao2022React", "Yao2025Survey", "Ye2025Task", "Ye2025Taska", "Yu2026Agentic", "Zeng2025Rejump", "Zhang2024Large", "Zhang2025Large", "Zhang2025Multimind", "Zhao2024Lightva", "Zhao2025Autonomous", "Zhou2025Reasoning", "Zhou2025Siraj", "Zhu2025Where"], "allowed_bibkeys_global": ["Hu2023Avis", "Yao2022React"], "evidence_ids": ["E-P0061-68db58914f", "E-P0162-192e78b614", "E-P0243-38a26e4777", "E-P0128-904ba35500", "E-P0072-0301cf089d", "E-P0197-e294aeefb5", "E-P0057-f36b515991", "E-P0102-98d4089bc9", "E-P0238-46914a4804", "E-P0054-897bcc2f50", "E-P0234-4af0cf3c02", "E-P0001-ca4a00b5cf", "E-P0007-dc2266b72d", "E-P0158-69c0aa3079", "E-P0287-9abcf1bf8a", "E-P0011-f0ea009256", "E-P0091-52fea1d199", "E-P0033-1b6fe3407a", "E-P0290-f0f0faaada", "E-P0045-8a32e8598f", "E-P0045-bb45089f75", "E-P0072-af89bfa234", "E-P0175-d568c53e1d", "E-P0227-53536132a8"], "anchor_facts": [{"hook_type": "quant", "text": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "paper_id": "P0197", "evidence_id": "E-P0197-e294aeefb5", "pointer": "papers/paper_notes.jsonl:paper_id=P0197#key_results[1]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]"}, {"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0128", "evidence_id": "E-P0128-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0128#key_results[0]"}, {"hook_type": "quant", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "paper_id": "P0057", "evidence_id": "E-P0057-f36b515991", "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]"}, {"hook_type": "quant", "text": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "paper_id": "P0007", "evidence_id": "E-P0007-dc2266b72d", "pointer": "papers/paper_notes.jsonl:paper_id=P0007#key_results[0]"}, {"hook_type": "quant", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "paper_id": "P0287", "evidence_id": "E-P0287-9abcf1bf8a", "pointer": "papers/paper_notes.jsonl:paper_id=P0287#key_results[1]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0061", "evidence_id": "E-P0061-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0061#key_results[0]"}, {"hook_type": "eval", "text": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method"}, {"hook_type": "limitation", "text": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "citations": ["Zhu2025Where"], "paper_id": "P0238", "evidence_id": "E-P0238-46914a4804", "pointer": "papers/paper_notes.jsonl:paper_id=P0238#summary_bullets[1]"}, {"hook_type": "eval", "text": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and", "citations": ["Zhang2025Large"], "paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Du2025Survey", "Zhang2025Large", "Abbineni2025Muallm"], "A_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}, {"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}], "B_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"paper_id": "P0197", "evidence_id": "E-P0197-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0197#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Du2025Survey", "Zhang2025Large", "Shang2024Agentsquare", "Li2025Agentswift"], "A_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}, {"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}], "B_highlights": [{"paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]"}, {"paper_id": "P0128", "evidence_id": "E-P0128-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0128#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Memory / retrieval augmentation", "B_label": "Planning / reasoning loops", "citations": ["Zhang2025Large", "Abbineni2025Muallm", "Shang2024Agentsquare", "Li2025Agentswift"], "A_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"paper_id": "P0197", "evidence_id": "E-P0197-e294aeefb5", "excerpt": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "pointer": "papers/paper_notes.jsonl:paper_id=P0197#key_results[1]"}], "B_highlights": [{"paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]"}, {"paper_id": "P0128", "evidence_id": "E-P0128-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0128#key_results[0]"}], "write_prompt": "Contrast Memory / retrieval augmentation vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Yao2025Survey", "Zhang2025Large", "Tawosi2025Meta"], "A_highlights": [{"paper_id": "P0033", "evidence_id": "E-P0033-1b6fe3407a", "excerpt": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "citations": ["Yao2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]"}, {"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}], "B_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"paper_id": "P0057", "evidence_id": "E-P0057-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Yao2025Survey", "Zhang2025Large", "Li2024Review", "Shang2024Agentsquare"], "A_highlights": [{"paper_id": "P0033", "evidence_id": "E-P0033-1b6fe3407a", "excerpt": "To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs.", "citations": ["Yao2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]"}, {"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}], "B_highlights": [{"paper_id": "P0007", "evidence_id": "E-P0007-dc2266b72d", "excerpt": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0007#key_results[0]"}, {"paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Memory / retrieval augmentation", "B_label": "Planning / reasoning loops", "citations": ["Zhang2025Large", "Tawosi2025Meta", "Li2024Review", "Shang2024Agentsquare"], "A_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"paper_id": "P0057", "evidence_id": "E-P0057-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]"}], "B_highlights": [{"paper_id": "P0007", "evidence_id": "E-P0007-dc2266b72d", "excerpt": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "pointer": "papers/paper_notes.jsonl:paper_id=P0007#key_results[0]"}, {"paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]"}], "write_prompt": "Contrast Memory / retrieval augmentation vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Zhang2025Large", "Verma2026Active", "Tawosi2025Meta"], "A_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"paper_id": "P0287", "evidence_id": "E-P0287-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0287#key_results[1]"}], "B_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"paper_id": "P0057", "evidence_id": "E-P0057-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: HITL, LLMs, MITRE, ATT, IPDRR-based, LLM-RL, CyberOps-Bots, ReAct, SWE-bench, LTM.", "citations": ["Peng2026Enhancing", "Verma2026Active", "Yu2026Agentic", "Du2025Survey"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Peng2026Enhancing", "Verma2026Active"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Peng2026Enhancing", "Verma2026Active"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Peng2026Enhancing", "Verma2026Active"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Peng2026Enhancing", "Verma2026Active"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Peng2026Enhancing", "Verma2026Active"]}], "limitation_hooks": [{"excerpt": "To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs).", "citations": ["Peng2026Enhancing"], "pointer": ""}, {"excerpt": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience.", "citations": ["Peng2026Enhancing"], "pointer": ""}, {"excerpt": "However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity.", "citations": ["Peng2026Enhancing"], "pointer": ""}, {"excerpt": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical.", "citations": ["Yu2026Agentic"], "pointer": ""}, {"excerpt": "This results in the failure to retrieve the relevant code of these fine-grained subtasks.", "citations": ["Li2025Graphcodeagent"], "pointer": ""}, {"excerpt": "To address this challenge, we propose GraphCodeAgent, a dual graph-guided LLM agent for retrieval-augmented repo-level code generation, bridging the gap between NL requirements and programming implementations.", "citations": ["Li2025Graphcodeagent"], "pointer": ""}, {"excerpt": "MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection.", "citations": ["Wu2025Meta"], "pointer": ""}, {"excerpt": "We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi-agent extensions.", "citations": ["Wu2025Meta"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-02-07T20:01:31"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "rq": "Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Self-improvement and adaptation highlights a tension around evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints, motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Self-improvement and adaptation, the core trade-off is adaptability versus stability: systems that change themselves can improve over time but risk drifting, overfitting, or becoming harder to evaluate and control.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["A practical decision is", "One design choice is", "For system builders, the crux is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Stepping back,", "Viewed across reported protocols,", "Across reported settings,", "A consistent theme is that", "Collectively,", "In summary,", "The evidence suggests that", "Across these studies,"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "These results hinge on"}, {"avoid_stem": "Three limitations", "prefer_stem": "A key limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Across these studies,", "For practitioners,", "For system builders,", "For builders,", "A caveat is", "Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"], "Across these studies,": ["Collectively,", "Stepping back,", "Across reported protocols,", "Viewed across settings,", "One way to read this evidence is that"], "For practitioners,": ["In deployed settings,", "Operationally,", "From a systems perspective,", "Under typical budget constraints,", "In practice,"], "For system builders,": ["From a system-design perspective,", "In system design,", "When building tool-using agents,", "A practical design implication is that", "In practice,"], "For builders,": ["From a system-design perspective,", "In system design,", "When building agents,", "A practical design implication is that", "In practice,"], "A caveat is": ["Interpretation depends on", "Generalization is unclear when", "These results hinge on", "Evidence is thin when", "A key boundary is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "decision-first", "opener_hint": "Start with a builder/research decision; state what it depends on; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "communication protocol / roles", "aggregation (vote / debate / referee)", "stability / robustness"], "bridge_terms": ["preference", "reward", "feedback", "self-improvement", "benchmarks/metrics", "compute"], "contrast_hook": "learning/feedback", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, communication protocol / roles, aggregation (vote / debate / referee), stability / robustness"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Evaluation / benchmark-focused works", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Evaluation / benchmark-focused works vs Agent frameworks / architectures)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Evaluation / benchmark-focused works", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, communication protocol / roles, aggregation (vote / debate / referee), stability / robustness"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Evaluation / benchmark-focused works", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Evaluation / benchmark-focused works", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, communication protocol / roles, aggregation (vote / debate / referee), stability / robustness"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Evaluation / benchmark-focused works)", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent latex survey.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: communication protocol / roles.", "Compare approaches along: aggregation (vote / debate / referee).", "Compare approaches along: stability / robustness."], "chapter_key_contrasts": ["learning/feedback", "coordination"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhou2025Self", "Li2026Autonomous", "Du2024Anytool", "Zhang2026Evoroute", "Guo2025Comprehensive", "Yu2025Infiagent", "Yao2022React", "Alizadeh2025Simple", "Yin2024Mmau", "Peng2026Enhancing", "Xia2025Sand", "GendreauDistler2025Automating", "Yang2025Bioverge", "Du2025Survey", "Samaei2025Epidemiqs", "Yano2025Lamdagent", "Li2025Learn", "Van2025Survey", "Yao2025Agents", "Wu2025Evolver", "Chen2025Grounded", "Belle2025Agents", "Zhang2024Autocoderover"], "allowed_bibkeys_mapped": ["Zhou2025Self", "Yu2025Infiagent", "Yao2025Agents", "Chen2025Grounded", "Du2025Survey", "Xia2025Sand", "Belle2025Agents", "Du2024Anytool", "Zhang2026Evoroute", "GendreauDistler2025Automating", "Li2025Learn", "Tennant2024Moral", "Peng2026Enhancing", "Yang2025Bioverge", "Yano2025Lamdagent", "Wu2025Meta", "Zhou2024Archer", "Guo2025Comprehensive", "Alizadeh2025Simple", "Van2025Survey", "He2025Enabling", "Samaei2025Epidemiqs", "Wu2025Evolver", "Zhang2024Autocoderover", "Yin2024Mmau", "Li2026Autonomous", "Hatalis2025Review", "Yao2022React"], "allowed_bibkeys_chapter": ["Alizadeh2025Simple", "Belle2025Agents", "Cao2025Skyrl", "Chen2025Grounded", "Chen2025Remsa", "Du2024Anytool", "Du2024Text2Bim", "Du2025Survey", "Feng2025Group", "Fumero2025Cybersleuth", "Gao2025Radar", "GendreauDistler2025Automating", "Guo2025Comprehensive", "Hatalis2025Review", "He2025Enabling", "Hu2023Avis", "Ji2025Tree", "Li2025Learn", "Li2025Leveraging", "Li2026Autonomous", "Lu2025Just", "Lumer2025Memtool", "Luo2025Large", "Maragheh2025Future", "Oueslati2025Refagent", "Peng2026Enhancing", "Samaei2025Epidemiqs", "Sarkar2025Survey", "Shen2024Small", "Shi2025Youtu", "Silva2025Agents", "Tawosi2025Almas", "Tennant2024Moral", "Van2025Survey", "Wu2024Federated", "Wu2025Evolver", "Wu2025Meta", "Xia2025Sand", "Yang2024Based", "Yang2025Bioverge", "Yang2025Survey", "Yano2025Lamdagent", "Yao2022React", "Yao2025Agents", "Yim2024Evaluating", "Yin2024Mmau", "Yu2025Infiagent", "Zhang2024Autocoderover", "Zhang2025Bridging", "Zhang2025Datascibench", "Zhang2026Evoroute", "Zhou2024Archer", "Zhou2025Self"], "allowed_bibkeys_global": ["Hu2023Avis", "Yao2022React"], "evidence_ids": ["E-P0067-2e6956a116", "E-P0100-67ea29ce26", "E-P0082-4da9e4ae32", "E-P0103-60cc0d458f", "E-P0009-30bd885b0c", "E-P0178-d099f5e3ee", "E-P0001-ca4a00b5cf", "E-P0068-c2c43d35ce", "E-P0092-ebb97f6129", "E-P0102-ec579c4773", "E-P0065-6273763a98", "E-P0141-fad03785c5", "E-P0143-fa93c04884", "E-P0011-f0ea009256", "E-P0167-ca050ed55f", "E-P0183-78bde774bf", "E-P0184-d19d8e1143", "E-P0113-a68f39bc04", "E-P0116-78b9ea1065", "E-P0169-1a05555e85", "E-P0176-af945eb2fa", "E-P0184-1dd544863c", "E-P0132-eae2720c71", "E-P0084-8b5287fcb0"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"hook_type": "quant", "text": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}, {"hook_type": "quant", "text": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining.", "citations": ["Peng2026Enhancing"], "paper_id": "P0102", "evidence_id": "E-P0102-ec579c4773", "pointer": "papers/paper_notes.jsonl:paper_id=P0102#key_results[0]"}, {"hook_type": "quant", "text": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "citations": ["Li2026Autonomous"], "paper_id": "P0100", "evidence_id": "E-P0100-67ea29ce26", "pointer": "papers/paper_notes.jsonl:paper_id=P0100#method"}, {"hook_type": "eval", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "paper_id": "P0082", "evidence_id": "E-P0082-4da9e4ae32", "pointer": "papers/paper_notes.jsonl:paper_id=P0082#limitations[1]"}, {"hook_type": "quant", "text": "Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recog", "citations": ["Yu2025Infiagent"], "paper_id": "P0178", "evidence_id": "E-P0178-d099f5e3ee", "pointer": "papers/paper_notes.jsonl:paper_id=P0178#key_results[0]"}, {"hook_type": "quant", "text": "In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage.", "citations": ["Alizadeh2025Simple"], "paper_id": "P0068", "evidence_id": "E-P0068-c2c43d35ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0068#key_results[1]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Zhou2025Self", "Du2025Survey", "Guo2025Comprehensive", "Yang2025Bioverge"], "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"paper_id": "P0011", "evidence_id": "E-P0011-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}], "B_highlights": [{"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}, {"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhou2025Self", "Du2025Survey", "Yao2022React"], "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"paper_id": "P0011", "evidence_id": "E-P0011-f0ea009256", "excerpt": "Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions.", "citations": ["Du2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Evaluation / benchmark-focused works", "B_label": "Planning / reasoning loops", "citations": ["Guo2025Comprehensive", "Yang2025Bioverge", "Yao2022React"], "A_highlights": [{"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}, {"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Planning / reasoning loops along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Zhang2026Evoroute", "Peng2026Enhancing", "Yang2025Bioverge", "Guo2025Comprehensive"], "A_highlights": [{"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}, {"paper_id": "P0102", "evidence_id": "E-P0102-ec579c4773", "excerpt": "Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining.", "citations": ["Peng2026Enhancing"], "pointer": "papers/paper_notes.jsonl:paper_id=P0102#key_results[0]"}], "B_highlights": [{"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhang2026Evoroute", "Peng2026Enhancing", "Yao2022React"], "A_highlights": [{"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}, {"paper_id": "P0102", "evidence_id": "E-P0102-ec579c4773", "excerpt": "Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining.", "citations": ["Peng2026Enhancing"], "pointer": "papers/paper_notes.jsonl:paper_id=P0102#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Evaluation / benchmark-focused works", "B_label": "Planning / reasoning loops", "citations": ["Yang2025Bioverge", "Guo2025Comprehensive", "Yao2022React"], "A_highlights": [{"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Planning / reasoning loops along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "communication protocol / roles", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Zhou2025Self", "Zhang2026Evoroute", "Yang2025Bioverge", "Guo2025Comprehensive"], "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}], "B_highlights": [{"paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "excerpt": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"communication protocol / roles\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: DeepSeek-V3, HITL, LLMs, MITRE, ATT, IPDRR-based, LLM-RL, CyberOps-Bots, ReAct, GAIA.", "citations": ["Li2026Autonomous", "Peng2026Enhancing", "Zhang2026Evoroute", "Guo2025Comprehensive"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Li2026Autonomous", "Peng2026Enhancing"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Li2026Autonomous", "Peng2026Enhancing"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Li2026Autonomous", "Peng2026Enhancing"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Li2026Autonomous", "Peng2026Enhancing"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Li2026Autonomous", "Peng2026Enhancing"]}], "limitation_hooks": [{"excerpt": "Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.", "citations": ["Li2026Autonomous"], "pointer": ""}, {"excerpt": "To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs).", "citations": ["Peng2026Enhancing"], "pointer": ""}, {"excerpt": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience.", "citations": ["Peng2026Enhancing"], "pointer": ""}, {"excerpt": "However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity.", "citations": ["Peng2026Enhancing"], "pointer": ""}, {"excerpt": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"], "pointer": ""}, {"excerpt": "Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making.", "citations": ["Hatalis2025Review"], "pointer": ""}, {"excerpt": "The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks.", "citations": ["Zhou2025Self"], "pointer": ""}, {"excerpt": "To address these limitations, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups.", "citations": ["Yin2024Mmau"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-02-07T20:01:31"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "rq": "Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Multi-agent coordination highlights a tension around evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints, motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Multi-agent coordination, the central trade-off is specialization versus coordination: dividing labor can boost performance but adds communication overhead and stability risks.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["A practical decision is", "One design choice is", "For system builders, the crux is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Stepping back,", "Viewed across reported protocols,", "Across reported settings,", "A consistent theme is that", "Collectively,", "In summary,", "The evidence suggests that", "Across these studies,"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "These results hinge on"}, {"avoid_stem": "Three limitations", "prefer_stem": "A key limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Across these studies,", "For practitioners,", "For system builders,", "For builders,", "A caveat is", "Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"], "Across these studies,": ["Collectively,", "Stepping back,", "Across reported protocols,", "Viewed across settings,", "One way to read this evidence is that"], "For practitioners,": ["In deployed settings,", "Operationally,", "From a systems perspective,", "Under typical budget constraints,", "In practice,"], "For system builders,": ["From a system-design perspective,", "In system design,", "When building tool-using agents,", "A practical design implication is that", "In practice,"], "For builders,": ["From a system-design perspective,", "In system design,", "When building agents,", "A practical design implication is that", "In practice,"], "A caveat is": ["Interpretation depends on", "Generalization is unclear when", "These results hinge on", "Evidence is thin when", "A key boundary is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "contrast-first", "opener_hint": "Start with an explicit A-vs-B contrast; state why it matters; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["roles", "communication", "debate", "aggregation", "stability", "benchmarks/metrics"], "contrast_hook": "coordination", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Multi-agent coordination", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Multi-agent coordination vs Agent frameworks / architectures)", "use_clusters": ["Multi-agent coordination"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Multi-agent coordination", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Multi-agent coordination"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Multi-agent coordination", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Multi-agent coordination"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Multi-agent coordination", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Multi-agent coordination)", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Tool-use and function calling"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Tool-use and function calling"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Tool-use and function calling"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent latex survey.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: communication protocol / roles.", "Compare approaches along: aggregation (vote / debate / referee).", "Compare approaches along: stability / robustness."], "chapter_key_contrasts": ["learning/feedback", "coordination"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Feng2025Group", "Zhang2025Datascibench", "Yang2024Based", "Lumer2025Memtool", "Lu2025Just", "Gao2025Radar", "Yu2025Infiagent", "Fumero2025Cybersleuth", "Luo2025Large", "Shen2024Small", "Maragheh2025Future", "Chen2025Remsa", "Shi2025Youtu", "Ji2025Tree", "Zhang2025Bridging", "Li2025Leveraging", "Cao2025Skyrl", "Silva2025Agents", "Li2025Learn", "Du2024Text2Bim", "Oueslati2025Refagent", "Yang2025Survey"], "allowed_bibkeys_mapped": ["Li2025Leveraging", "Maragheh2025Future", "Li2025Learn", "Shen2024Small", "Zhang2025Bridging", "Cao2025Skyrl", "Yang2024Based", "Du2024Text2Bim", "Tawosi2025Almas", "Yim2024Evaluating", "Oueslati2025Refagent", "Sarkar2025Survey", "Ji2025Tree", "Wu2024Federated", "Silva2025Agents", "Yu2025Infiagent", "Gao2025Radar", "Lumer2025Memtool", "Zhou2024Archer", "Chen2025Remsa", "Shi2025Youtu", "Yang2025Survey", "Fumero2025Cybersleuth", "Zhang2025Datascibench", "Feng2025Group", "Lu2025Just", "Luo2025Large", "Hu2023Avis"], "allowed_bibkeys_chapter": ["Alizadeh2025Simple", "Belle2025Agents", "Cao2025Skyrl", "Chen2025Grounded", "Chen2025Remsa", "Du2024Anytool", "Du2024Text2Bim", "Du2025Survey", "Feng2025Group", "Fumero2025Cybersleuth", "Gao2025Radar", "GendreauDistler2025Automating", "Guo2025Comprehensive", "Hatalis2025Review", "He2025Enabling", "Hu2023Avis", "Ji2025Tree", "Li2025Learn", "Li2025Leveraging", "Li2026Autonomous", "Lu2025Just", "Lumer2025Memtool", "Luo2025Large", "Maragheh2025Future", "Oueslati2025Refagent", "Peng2026Enhancing", "Samaei2025Epidemiqs", "Sarkar2025Survey", "Shen2024Small", "Shi2025Youtu", "Silva2025Agents", "Tawosi2025Almas", "Tennant2024Moral", "Van2025Survey", "Wu2024Federated", "Wu2025Evolver", "Wu2025Meta", "Xia2025Sand", "Yang2024Based", "Yang2025Bioverge", "Yang2025Survey", "Yano2025Lamdagent", "Yao2022React", "Yao2025Agents", "Yim2024Evaluating", "Yin2024Mmau", "Yu2025Infiagent", "Zhang2024Autocoderover", "Zhang2025Bridging", "Zhang2025Datascibench", "Zhang2026Evoroute", "Zhou2024Archer", "Zhou2025Self"], "allowed_bibkeys_global": ["Hu2023Avis", "Yao2022React"], "evidence_ids": ["E-P0177-4b027dfb27", "E-P0159-b9e7423acc", "E-P0266-c92a9d34cf", "E-P0194-35271418ac", "E-P0015-763cb193e3", "E-P0189-419e1464da", "E-P0178-d099f5e3ee", "E-P0156-0b34b55332", "E-P0017-62cd0c501b", "E-P0273-9640816b42", "E-P0072-0301cf089d", "E-P0209-c664716bd8", "E-P0239-9af3444274", "E-P0075-9af6a59afb", "E-P0144-e52cad3d44", "E-P0185-82b5e4eda3", "E-P0222-5ed988eb67", "E-P0148-baa622fa7f", "E-P0184-1dd544863c", "E-P0275-a21c19010a", "E-P0063-906d682e2a", "E-P0194-38dc800de9", "E-P0239-5ad9d205d0", "E-P0030-4c14763fd0"], "anchor_facts": [{"hook_type": "quant", "text": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0177", "evidence_id": "E-P0177-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}, {"hook_type": "eval", "text": "We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics.", "citations": ["Zhang2025Datascibench"], "paper_id": "P0159", "evidence_id": "E-P0159-b9e7423acc", "pointer": "papers/paper_notes.jsonl:paper_id=P0159#method"}, {"hook_type": "quant", "text": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performanc", "citations": ["Lu2025Just"], "paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]"}, {"hook_type": "quant", "text": "MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations.", "citations": ["Gao2025Radar"], "paper_id": "P0189", "evidence_id": "E-P0189-419e1464da", "pointer": "papers/paper_notes.jsonl:paper_id=P0189#key_results[0]"}, {"hook_type": "quant", "text": "Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recog", "citations": ["Yu2025Infiagent"], "paper_id": "P0178", "evidence_id": "E-P0178-d099f5e3ee", "pointer": "papers/paper_notes.jsonl:paper_id=P0178#key_results[0]"}, {"hook_type": "eval", "text": "Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains.", "citations": ["Luo2025Large"], "paper_id": "P0017", "evidence_id": "E-P0017-62cd0c501b", "pointer": "papers/paper_notes.jsonl:paper_id=P0017#key_results[0]"}, {"hook_type": "eval", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]"}, {"hook_type": "quant", "text": "We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol.", "citations": ["Chen2025Remsa"], "paper_id": "P0209", "evidence_id": "E-P0209-c664716bd8", "pointer": "papers/paper_notes.jsonl:paper_id=P0209#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Maragheh2025Future", "Lu2025Just", "Silva2025Agents"], "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]"}], "B_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}, {"paper_id": "P0148", "evidence_id": "E-P0148-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Maragheh2025Future", "Lu2025Just", "Lumer2025Memtool", "Shen2024Small"], "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]"}], "B_highlights": [{"paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]"}, {"paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "excerpt": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Multi-agent coordination", "B_label": "Tool-use and function calling", "citations": ["Maragheh2025Future", "Silva2025Agents", "Lumer2025Memtool", "Shen2024Small"], "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}, {"paper_id": "P0148", "evidence_id": "E-P0148-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[1]"}], "B_highlights": [{"paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]"}, {"paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "excerpt": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]"}], "write_prompt": "Contrast Multi-agent coordination vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Maragheh2025Future", "Lu2025Just", "Silva2025Agents"], "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]"}], "B_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}, {"paper_id": "P0148", "evidence_id": "E-P0148-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Maragheh2025Future", "Lu2025Just", "Lumer2025Memtool", "Shen2024Small"], "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]"}], "B_highlights": [{"paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]"}, {"paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "excerpt": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Multi-agent coordination", "B_label": "Tool-use and function calling", "citations": ["Maragheh2025Future", "Silva2025Agents", "Lumer2025Memtool", "Shen2024Small"], "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}, {"paper_id": "P0148", "evidence_id": "E-P0148-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0148#key_results[1]"}], "B_highlights": [{"paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]"}, {"paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "excerpt": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]"}], "write_prompt": "Contrast Multi-agent coordination vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Maragheh2025Future", "Lu2025Just", "Yang2024Based"], "A_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}, {"paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "excerpt": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]"}], "B_highlights": [{"paper_id": "P0072", "evidence_id": "E-P0072-0301cf089d", "excerpt": "The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools", "citations": ["Maragheh2025Future"], "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[1]"}, {"paper_id": "P0266", "evidence_id": "E-P0266-c92a9d34cf", "excerpt": "The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs.", "citations": ["Yang2024Based"], "pointer": "papers/paper_notes.jsonl:paper_id=P0266#limitations[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: SFT, BFCL, SFT-based, LLMs, LLM-based, RefAgent, MCP, MCP-compliant, RecSys, GRPO.", "citations": ["Lu2025Just", "Luo2025Large", "Yang2025Survey", "Oueslati2025Refagent"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Lu2025Just", "Luo2025Large"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Lu2025Just", "Luo2025Large"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Lu2025Just", "Luo2025Large"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Lu2025Just", "Luo2025Large"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Lu2025Just", "Luo2025Large"]}], "limitation_hooks": [{"excerpt": "Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency.", "citations": ["Yang2025Survey"], "pointer": ""}, {"excerpt": "The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "citations": ["Sarkar2025Survey"], "pointer": ""}, {"excerpt": "We then surface five cross-cutting challenge families: protocol complexity, scalability, hallucination and error propagation, emergent misalignment (including covert collusion), and brand compliance.", "citations": ["Maragheh2025Future"], "pointer": ""}, {"excerpt": "To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step.", "citations": ["Ji2025Tree"], "pointer": ""}, {"excerpt": "As cosmological simulations and their associated software become increasingly complex, physicists face the challenge of searching through vast amounts of literature and user manuals to extract simulation parameters from dense academic papers, each using different models and formats.", "citations": ["Zhang2025Bridging"], "pointer": ""}, {"excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": ""}, {"excerpt": "We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation.", "citations": ["Li2025Learn"], "pointer": ""}, {"excerpt": "The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs.", "citations": ["Yang2024Based"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-02-07T20:01:31"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "section_id": "6", "section_title": "Evaluation & Risks", "rq": "Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Benchmarks and evaluation protocols methods emphasize evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "tension_statement": "In Benchmarks and evaluation protocols, a recurring tension is coverage versus comparability: broader suites capture more behaviors but make head-to-head comparison fragile when protocols and constraints differ.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["A practical decision is", "One design choice is", "For system builders, the crux is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Stepping back,", "Viewed across reported protocols,", "Across reported settings,", "A consistent theme is that", "Collectively,", "In summary,", "The evidence suggests that", "Across these studies,"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "These results hinge on"}, {"avoid_stem": "Three limitations", "prefer_stem": "A key limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Across these studies,", "For practitioners,", "For system builders,", "For builders,", "A caveat is", "Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"], "Across these studies,": ["Collectively,", "Stepping back,", "Across reported protocols,", "Viewed across settings,", "One way to read this evidence is that"], "For practitioners,": ["In deployed settings,", "Operationally,", "From a systems perspective,", "Under typical budget constraints,", "In practice,"], "For system builders,": ["From a system-design perspective,", "In system design,", "When building tool-using agents,", "A practical design implication is that", "In practice,"], "For builders,": ["From a system-design perspective,", "In system design,", "When building agents,", "A practical design implication is that", "In practice,"], "A caveat is": ["Interpretation depends on", "Generalization is unclear when", "These results hinge on", "Evidence is thin when", "A key boundary is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "decision-first", "opener_hint": "Start with a builder/research decision; state what it depends on; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability", "benchmarks"], "contrast_hook": "tool interfaces", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Evaluation / benchmark-focused works", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Evaluation / benchmark-focused works vs Agent frameworks / architectures)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Evaluation / benchmark-focused works", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Evaluation / benchmark-focused works", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Evaluation / benchmark-focused works", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Evaluation / benchmark-focused works)", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Tool-use and function calling"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Tool-use and function calling"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Tool-use and function calling"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent latex survey.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["tool interfaces", "security"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Fu2025Eval", "Kwon2025Agentnet", "Shi2025Progent", "Mohammadi2025Evaluation", "Zhou2025Self", "Zhong2025Rtbas", "Alizadeh2025Simple", "Ji2025Taxonomy", "Kim2026Beyond", "Guo2025Comprehensive", "Chen2025Towards", "Liu2026Agents", "GendreauDistler2025Automating", "Tao2025Code", "Zheng2025Newtonbench", "Peng2024Survey", "Michelakis2025Core", "V2026Agentic", "Agrawal2025Language", "Yao2025Agents", "Wang2025Flow"], "allowed_bibkeys_mapped": ["Fu2025Eval", "Guo2025Comprehensive", "Mohammadi2025Evaluation", "Kim2026Beyond", "Chen2025Towards", "Alizadeh2025Simple", "Michelakis2025Core", "Ji2025Taxonomy", "Zheng2025Newtonbench", "Peng2024Survey", "Yao2025Agents", "GendreauDistler2025Automating", "V2026Agentic", "Almeida2025Ticket", "Kwon2025Agentnet", "Tao2025Code", "Zhang2025Detective", "Wang2025Flow", "Testini2025Measuring", "Doshi2026Towards", "Shi2025Progent", "Zhou2025Self", "Liu2026Agents", "Hong2025Natural", "Zhong2025Rtbas", "Hadeliya2025When", "Agrawal2025Language", "Hu2023Avis"], "allowed_bibkeys_chapter": ["Agrawal2025Language", "Alizadeh2025Simple", "Almeida2025Ticket", "Balaji2026Beyond", "Bonagiri2025Check", "Chen2025Towards", "Doshi2026Towards", "Erdogan2024Tinyagent", "Fang2025Should", "Ferrag2025From", "Fu2025Eval", "Gasmi2025Bridging", "GendreauDistler2025Automating", "Guo2025Comprehensive", "Hadeliya2025When", "He2024Emerged", "Hong2025Natural", "Hu2023Avis", "Ji2025Taxonomy", "Kale2025Reliable", "Kim2026Beyond", "Kwon2025Agentnet", "Li2026Toolprmbench", "Liu2025Secure", "Liu2026Agents", "Luo2025Agrail", "Luo2025Universe", "Martin2025Autoodd", "Michelakis2025Core", "Mo2025Attractive", "Mohammadi2025Evaluation", "Mou2026Toolsafe", "Peng2024Survey", "Russo2025Deep", "Salama2025Edge", "Sha2025Agent", "Shi2025Progent", "Tao2025Code", "Testini2025Measuring", "V2026Agentic", "Van2025Survey", "Wang2025Comprehensive", "Wang2025Flow", "Yang2024Mvvm", "Yao2025Agents", "Zhang2024Agent", "Zhang2024Large", "Zhang2025Detective", "Zhang2025Security", "Zheng2025Newtonbench", "Zhong2025Rtbas", "Zhou2025Reasoning", "Zhou2025Self"], "allowed_bibkeys_global": ["Hu2023Avis", "Yao2022React"], "evidence_ids": ["E-P0208-753416ce70", "E-P0147-578706f7a5", "E-P0061-68db58914f", "E-P0168-37f9ea924c", "E-P0067-2e6956a116", "E-P0062-52fcef25b6", "E-P0068-537b51e910", "E-P0071-5607dc887c", "E-P0027-79f88927fa", "E-P0009-30bd885b0c", "E-P0074-4fc221fdea", "E-P0300-8b56718f74", "E-P0141-fad03785c5", "E-P0153-9bd58ce21b", "E-P0200-39cf43509f", "E-P0080-17ec1d2e4e", "E-P0146-4ac9a2931e", "E-P0288-6c12fbfdd5", "E-P0077-5603d51445", "E-P0116-78b9ea1065", "E-P0116-d70ef2c75f", "E-P0146-ceb4754147", "E-P0168-e973488a75", "E-P0188-07f9bd6a8b"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"hook_type": "quant", "text": "Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy", "citations": ["Zhong2025Rtbas"], "paper_id": "P0062", "evidence_id": "E-P0062-52fcef25b6", "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"hook_type": "quant", "text": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}, {"hook_type": "quant", "text": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]"}, {"hook_type": "quant", "text": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0061", "evidence_id": "E-P0061-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0061#key_results[0]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0208", "evidence_id": "E-P0208-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0208#key_results[1]"}, {"hook_type": "quant", "text": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are cru", "citations": ["Kwon2025Agentnet"], "paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Zhou2025Self", "Zhong2025Rtbas", "Mohammadi2025Evaluation", "Guo2025Comprehensive"], "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"paper_id": "P0062", "evidence_id": "E-P0062-52fcef25b6", "excerpt": "Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy", "citations": ["Zhong2025Rtbas"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Zhou2025Self", "Zhong2025Rtbas", "Kwon2025Agentnet", "Kim2026Beyond"], "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"paper_id": "P0062", "evidence_id": "E-P0062-52fcef25b6", "excerpt": "Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy", "citations": ["Zhong2025Rtbas"], "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}], "B_highlights": [{"paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "excerpt": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]"}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Evaluation / benchmark-focused works", "B_label": "Tool-use and function calling", "citations": ["Mohammadi2025Evaluation", "Guo2025Comprehensive", "Kwon2025Agentnet", "Kim2026Beyond"], "A_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}], "B_highlights": [{"paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "excerpt": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]"}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]"}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Zhou2025Self", "Shi2025Progent", "Mohammadi2025Evaluation", "Guo2025Comprehensive"], "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"paper_id": "P0061", "evidence_id": "E-P0061-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0061#key_results[0]"}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Zhou2025Self", "Shi2025Progent", "Kwon2025Agentnet", "Kim2026Beyond"], "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"paper_id": "P0061", "evidence_id": "E-P0061-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0061#key_results[0]"}], "B_highlights": [{"paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "excerpt": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]"}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Evaluation / benchmark-focused works", "B_label": "Tool-use and function calling", "citations": ["Mohammadi2025Evaluation", "Guo2025Comprehensive", "Kwon2025Agentnet", "Kim2026Beyond"], "A_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "excerpt": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}], "B_highlights": [{"paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "excerpt": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]"}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]"}], "write_prompt": "Contrast Evaluation / benchmark-focused works vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Zhou2025Self", "Kim2026Beyond", "Mohammadi2025Evaluation"], "A_highlights": [{"paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]"}], "B_highlights": [{"paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: API, LLMs, WildAGTEval, APIs, IFC, STPA, MCP, LLM-based, LLM-powered, LLM-driven.", "citations": ["Kim2026Beyond", "Doshi2026Towards", "V2026Agentic", "Liu2026Agents"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Kim2026Beyond", "Doshi2026Towards"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Kim2026Beyond", "Doshi2026Towards"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Kim2026Beyond", "Doshi2026Towards"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Kim2026Beyond", "Doshi2026Towards"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Kim2026Beyond", "Doshi2026Towards"]}], "limitation_hooks": [{"excerpt": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability.", "citations": ["Liu2026Agents"], "pointer": ""}, {"excerpt": "Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption.", "citations": ["Shi2025Progent"], "pointer": ""}, {"excerpt": "LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments.", "citations": ["Shi2025Progent"], "pointer": ""}, {"excerpt": "Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones.", "citations": ["Shi2025Progent"], "pointer": ""}, {"excerpt": "The framework operates deterministically at runtime, providing provable security guarantees.", "citations": ["Shi2025Progent"], "pointer": ""}, {"excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": ""}, {"excerpt": "Additionally, we show that LLMs can automatically generate effective policies, highlighting their potential for automating the process of writing Progent's security policies.", "citations": ["Shi2025Progent"], "pointer": ""}, {"excerpt": "Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.", "citations": ["Zhong2025Rtbas"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-02-07T20:01:31"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "section_id": "6", "section_title": "Evaluation & Risks", "rq": "Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Safety, security, and governance highlights a tension around evaluation protocol (datasets, metrics, human evaluation) and compute and latency constraints, motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Safety, security, and governance, a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["A practical decision is", "One design choice is", "For system builders, the crux is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"], "contrast-first": ["A useful contrast is between", "One sharp contrast is between", "A recurring split is between"], "protocol-first": ["Comparisons hinge on", "Results are only comparable when", "Evaluation claims depend on"]}, "synthesis_stems": ["Stepping back,", "Viewed across reported protocols,", "Across reported settings,", "A consistent theme is that", "Collectively,", "In summary,", "The evidence suggests that", "Across these studies,"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "provides an overview", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}, {"avoid_stem": "Two limitations", "prefer_stem": "These results hinge on"}, {"avoid_stem": "Three limitations", "prefer_stem": "A key limitation is that"}, {"avoid_stem": "In this survey", "prefer_stem": "We examine"}, {"avoid_stem": "In our survey", "prefer_stem": "We examine"}, {"avoid_stem": "Our survey", "prefer_stem": "This work"}, {"avoid_stem": "This survey", "prefer_stem": "This work"}], "discourse_stem_watchlist": ["Across these studies,", "For practitioners,", "For system builders,", "For builders,", "A caveat is", "Additionally,", "Moreover,", "Furthermore,", "This suggests", "Taken together,", "The key point is that", "Overall,", "In summary,", "In practice,", "More broadly,", "Importantly,", "Notably,", "Crucially,", "In general,", "At a high level,", "In other words,", "Therefore,", "In addition,", "As a result,"], "discourse_stem_rewrites": {"Additionally,": ["More importantly,", "At the same time,", "A second consideration is"], "This suggests": ["This pattern indicates", "These results point to", "A plausible explanation is"], "Taken together,": ["Across these studies,", "Collectively,", "The evidence suggests"], "The key point is that": ["A practical implication is that", "A useful way to read these results is that", "One takeaway is that"], "Overall,": ["In practice,", "More broadly,", "A practical implication is that", "One way to read this evidence is that"], "In summary,": ["Across these studies,", "Collectively,", "A consistent theme is that", "The evidence suggests that"], "Importantly,": ["More importantly,", "A key constraint is that", "A practical implication is that"], "Notably,": ["In particular,", "A concrete example is that", "One implication is that"], "Crucially,": ["More importantly,", "A central constraint is that", "A key takeaway is that"], "At a high level,": ["Concretely,", "Under this protocol,", "In practice,"], "In other words,": ["Put differently,", "Equivalently,", "More concretely,"], "Therefore,": ["As a result,", "This in turn means that", "A practical implication is that"], "In addition,": ["At the same time,", "A second consideration is", "More importantly,"], "As a result,": ["This in turn means that", "Consequently,", "A practical implication is that"], "Across these studies,": ["Collectively,", "Stepping back,", "Across reported protocols,", "Viewed across settings,", "One way to read this evidence is that"], "For practitioners,": ["In deployed settings,", "Operationally,", "From a systems perspective,", "Under typical budget constraints,", "In practice,"], "For system builders,": ["From a system-design perspective,", "In system design,", "When building tool-using agents,", "A practical design implication is that", "In practice,"], "For builders,": ["From a system-design perspective,", "In system design,", "When building agents,", "A practical design implication is that", "In practice,"], "A caveat is": ["Interpretation depends on", "Generalization is unclear when", "These results hinge on", "Evidence is thin when", "A key boundary is that"]}, "role_cards": {"section_author": {"mission": "Write one subsection as an argument (not a topic list), using in-scope citations as evidence.", "do": ["State a concrete tension/trade-off early and commit to a thesis.", "Make at least two explicit A-vs-B contrasts (mechanism -> outcome) instead of per-paper summaries.", "When citing numbers, add minimal context (task + metric + constraint) in the same paragraph.", "End with a limitation that changes interpretation (protocol mismatch, unclear threat model, missing ablations)."], "avoid": ["Outline narration (This subsection..., In this subsection...).", "Slide navigation (Next, we move..., We now turn...).", "Meta survey advice (survey comparisons should...).", "Cite dumps (a trailing [@a; @b; @c] without a claim)."]}, "evidence_steward": {"mission": "Keep claims auditable: citations support the sentence that needs them, and scope stays local.", "do": ["Embed citations inside claim sentences; name concrete nouns (system/benchmark/protocol) near each cite.", "Prefer subsection-scoped citations; use chapter/global only when truly cross-cutting.", "Downgrade overconfident claims when evidence is thin; convert unknowns into verification targets (once, not spam)."], "avoid": ["Out-of-scope citations to make a paragraph sound stronger.", "Repeating evidence-policy disclaimers in every subsection.", "Ambiguous model naming (e.g., GPT-5) unless the cited work uses it."]}, "style_harmonizer": {"mission": "Make the prose read like a paper: calm, specific, and varied in rhythm without sounding templated.", "do": ["Vary opener cadence across sections (tension-first / decision-first / lens-first) without reusing the same stem.", "Prefer argument bridges over navigation; keep signposting light and content-bearing.", "Reduce slash-enumerations (A/B/C axis labels) by rewriting into natural prose."], "avoid": ["Count-based openers used as a slot (e.g., Two limitations..., Three takeaways...).", "Repeated discourse stems (Additionally, This suggests, Taken together) across many paragraphs.", "Pipeline words (workspace/unit/quality gate/evidence pack) in reader-facing text.", "PPT speaker-note tone (Now we..., The remainder of...).", "Internal shorthand that reads like planning notes (e.g., protocol/metric/constraint tokens)."]}}, "version": "0.4"}, "opener_mode": "lens-first", "opener_hint": "Start by naming the lens (interface/protocol/threat model); state what it reveals; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets, metrics, human evaluation)", "compute and latency constraints", "tool interface contract (schemas / protocols)", "tool selection / routing policy", "sandboxing / permissions / observability"], "bridge_terms": ["threat model", "prompt/tool injection", "monitoring", "guardrails", "benchmarks/metrics", "compute"], "contrast_hook": "security", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Safety / security / guardrails", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Safety / security / guardrails vs Agent frameworks / architectures)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Safety / security / guardrails", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Safety / security / guardrails", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Safety / security / guardrails", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, tool interface contract (schemas / protocols), tool selection / routing policy, sandboxing / permissions / observability"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Safety / security / guardrails)", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent latex survey.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "chapter_key_contrasts": ["tool interfaces", "security"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhang2025Security", "Luo2025Universe", "Fu2025Eval", "Kale2025Reliable", "Erdogan2024Tinyagent", "Bonagiri2025Check", "Mou2026Toolsafe", "Gasmi2025Bridging", "Mo2025Attractive", "Balaji2026Beyond", "Li2026Toolprmbench", "Yang2024Mvvm", "Salama2025Edge", "Zhang2024Agent", "Van2025Survey", "Zhang2024Large", "Fang2025Should", "Sha2025Agent", "Russo2025Deep", "Hu2023Avis", "Zhou2025Reasoning", "Martin2025Autoodd"], "allowed_bibkeys_mapped": ["Zhang2025Security", "Zhang2024Agent", "Luo2025Agrail", "Bonagiri2025Check", "Gasmi2025Bridging", "Mou2026Toolsafe", "Sha2025Agent", "Fang2025Should", "Kale2025Reliable", "Hadeliya2025When", "Fu2025Eval", "Wang2025Comprehensive", "Ferrag2025From", "Salama2025Edge", "Yang2024Mvvm", "Mo2025Attractive", "Martin2025Autoodd", "Russo2025Deep", "Liu2025Secure", "He2024Emerged", "Erdogan2024Tinyagent", "Li2026Toolprmbench", "Zhou2025Reasoning", "Van2025Survey", "Luo2025Universe", "Zhang2024Large", "Balaji2026Beyond", "Hu2023Avis"], "allowed_bibkeys_chapter": ["Agrawal2025Language", "Alizadeh2025Simple", "Almeida2025Ticket", "Balaji2026Beyond", "Bonagiri2025Check", "Chen2025Towards", "Doshi2026Towards", "Erdogan2024Tinyagent", "Fang2025Should", "Ferrag2025From", "Fu2025Eval", "Gasmi2025Bridging", "GendreauDistler2025Automating", "Guo2025Comprehensive", "Hadeliya2025When", "He2024Emerged", "Hong2025Natural", "Hu2023Avis", "Ji2025Taxonomy", "Kale2025Reliable", "Kim2026Beyond", "Kwon2025Agentnet", "Li2026Toolprmbench", "Liu2025Secure", "Liu2026Agents", "Luo2025Agrail", "Luo2025Universe", "Martin2025Autoodd", "Michelakis2025Core", "Mo2025Attractive", "Mohammadi2025Evaluation", "Mou2026Toolsafe", "Peng2024Survey", "Russo2025Deep", "Salama2025Edge", "Sha2025Agent", "Shi2025Progent", "Tao2025Code", "Testini2025Measuring", "V2026Agentic", "Van2025Survey", "Wang2025Comprehensive", "Wang2025Flow", "Yang2024Mvvm", "Yao2025Agents", "Zhang2024Agent", "Zhang2024Large", "Zhang2025Detective", "Zhang2025Security", "Zheng2025Newtonbench", "Zhong2025Rtbas", "Zhou2025Reasoning", "Zhou2025Self"], "allowed_bibkeys_global": ["Hu2023Avis", "Yao2022React"], "evidence_ids": ["E-P0186-d6095e10e9", "E-P0190-3c65d38a2a", "E-P0208-753416ce70", "E-P0212-e0345118bc", "E-P0096-4fca49d200", "E-P0152-7edb91824f", "E-P0109-c6a59efa61", "E-P0040-8e34a29629", "E-P0136-a0b404d928", "E-P0101-a4ae2708e4", "E-P0108-3e2edc05cd", "E-P0270-dda84e50a5", "E-P0163-84b88bc47a", "E-P0241-e26328e18c", "E-P0113-a68f39bc04", "E-P0091-52fea1d199", "E-P0236-3cd6217549", "E-P0122-20fed7f11c", "E-P0160-a8b0d15cbe", "E-P0280-1c25e10ffc", "E-P0019-e38b4bdff3", "E-P0137-88081e54fd", "E-P0208-8b3195ff10", "E-P0186-65e66ae9c1"], "anchor_facts": [{"hook_type": "quant", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"hook_type": "quant", "text": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]"}, {"hook_type": "quant", "text": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "citations": ["Luo2025Universe"], "paper_id": "P0190", "evidence_id": "E-P0190-3c65d38a2a", "pointer": "papers/paper_notes.jsonl:paper_id=P0190#limitations[1]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0208", "evidence_id": "E-P0208-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0208#key_results[1]"}, {"hook_type": "quant", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Ar", "citations": ["Kale2025Reliable"], "paper_id": "P0212", "evidence_id": "E-P0212-e0345118bc", "pointer": "papers/paper_notes.jsonl:paper_id=P0212#key_results[0]"}, {"hook_type": "quant", "text": "We then systematically curate a high-quality dataset for function calling, which we use to fine-tune two small language models, TinyAgent-1.1B and 7B.", "citations": ["Erdogan2024Tinyagent"], "paper_id": "P0096", "evidence_id": "E-P0096-4fca49d200", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}, {"hook_type": "quant", "text": "Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs.", "citations": ["Bonagiri2025Check"], "paper_id": "P0152", "evidence_id": "E-P0152-7edb91824f", "pointer": "papers/paper_notes.jsonl:paper_id=P0152#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution.", "citations": ["Mo2025Attractive"], "paper_id": "P0136", "evidence_id": "E-P0136-a0b404d928", "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Balaji2026Beyond", "Gasmi2025Bridging", "Mou2026Toolsafe"], "A_highlights": [{"paper_id": "P0101", "evidence_id": "E-P0101-a4ae2708e4", "excerpt": "Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior.", "citations": ["Balaji2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[1]"}, {"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Balaji2026Beyond", "Gasmi2025Bridging", "Zhang2025Security", "Van2025Survey"], "A_highlights": [{"paper_id": "P0101", "evidence_id": "E-P0101-a4ae2708e4", "excerpt": "Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior.", "citations": ["Balaji2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[1]"}, {"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}], "B_highlights": [{"paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]"}, {"paper_id": "P0113", "evidence_id": "E-P0113-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Safety / security / guardrails", "B_label": "Tool-use and function calling", "citations": ["Gasmi2025Bridging", "Mou2026Toolsafe", "Zhang2025Security", "Van2025Survey"], "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}], "B_highlights": [{"paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]"}, {"paper_id": "P0113", "evidence_id": "E-P0113-a68f39bc04", "excerpt": "This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field.", "citations": ["Van2025Survey"], "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}], "write_prompt": "Contrast Safety / security / guardrails vs Tool-use and function calling along \"evaluation protocol (datasets, metrics, human evaluation)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Mou2026Toolsafe", "Balaji2026Beyond", "Sha2025Agent"], "A_highlights": [{"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}, {"paper_id": "P0101", "evidence_id": "E-P0101-a4ae2708e4", "excerpt": "Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior.", "citations": ["Balaji2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[1]"}], "B_highlights": [{"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}, {"paper_id": "P0122", "evidence_id": "E-P0122-20fed7f11c", "excerpt": "Through extensive evaluations on public and self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we demonstrate that our safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks.", "citations": ["Sha2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Mou2026Toolsafe", "Balaji2026Beyond", "Zhang2025Security"], "A_highlights": [{"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}, {"paper_id": "P0101", "evidence_id": "E-P0101-a4ae2708e4", "excerpt": "Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior.", "citations": ["Balaji2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[1]"}], "B_highlights": [{"paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]"}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Safety / security / guardrails", "B_label": "Tool-use and function calling", "citations": ["Mou2026Toolsafe", "Sha2025Agent", "Zhang2025Security"], "A_highlights": [{"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}, {"paper_id": "P0122", "evidence_id": "E-P0122-20fed7f11c", "excerpt": "Through extensive evaluations on public and self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we demonstrate that our safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks.", "citations": ["Sha2025Agent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[0]"}], "B_highlights": [{"paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]"}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}], "write_prompt": "Contrast Safety / security / guardrails vs Tool-use and function calling along \"compute and latency constraints\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Gasmi2025Bridging", "Mou2026Toolsafe"], "A_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}], "B_highlights": [{"paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "excerpt": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along \"tool interface contract (schemas / protocols)\". Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: IVR, LLMs, SPA, DPA, GPT-4o-mini, GPT-4o, AI-driven, IVR-era, JourneyBench, PRMs.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench", "Mou2026Toolsafe", "Zhou2025Reasoning"]}, {"bullet": "When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench"]}, {"bullet": "If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes.", "citations": ["Balaji2026Beyond", "Li2026Toolprmbench"]}], "limitation_hooks": [{"excerpt": "While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge.", "citations": ["Balaji2026Beyond"], "pointer": ""}, {"excerpt": "Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.", "citations": ["Balaji2026Beyond"], "pointer": ""}, {"excerpt": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks.", "citations": ["Mou2026Toolsafe"], "pointer": ""}, {"excerpt": "Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored.", "citations": ["Mou2026Toolsafe"], "pointer": ""}, {"excerpt": "The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history.", "citations": ["Mou2026Toolsafe"], "pointer": ""}, {"excerpt": "It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback.", "citations": ["Mou2026Toolsafe"], "pointer": ""}, {"excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"], "pointer": ""}], "must_use": {"min_anchor_facts": 4, "min_comparison_cards": 4, "min_limitation_hooks": 2, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "Methodology note:", "Methodology note (evidence policy):", "Methodology note (evidence-policy):", "Method note:", "This subsection surveys", "This subsection argues", "In this subsection", "This section surveys", "This section reviews", "This section discusses", "This section covers", "This section presents", "This section introduces", "provides an overview", "This survey provides an overview", "In this section, we provide an overview", "This section provides an overview", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "The key point is that", "Three limitations", "Two limitations", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:", "protocol token", "protocol tokens", "constraint token", "constraint tokens", "metric token", "metric tokens", "evaluation token", "evaluation tokens", "In this survey", "In our survey", "Our survey", "This survey"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 10, "kept": 10, "dropped_no_cites": 0}, "comparisons": {"raw": 10, "considered": 7, "kept": 7, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-02-07T20:01:31"}
