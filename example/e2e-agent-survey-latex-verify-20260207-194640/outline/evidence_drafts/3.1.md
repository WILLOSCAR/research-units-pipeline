# Evidence draft: 3.1 Agent loop and action spaces

## Evidence snippets (with provenance)
- (E-P0121-44cc8fbce0) We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. Song2025Agent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0121#key_results[0])
- (E-P0191-312a342670) To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0191#method)
- (E-P0232-1232593f0e) Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates. Tomaevi2025Towards (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0232#limitations[1])
- (E-P0177-4b027dfb27) We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
- (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
- (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
- (E-P0103-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0103#key_results[0])
- (E-P0156-0b34b55332) We propose CyberSleuth, an autonomous agent that processes packet-level traces and application logs to identify the targeted service, the exploited vulnerability (CVE), and attack success. Fumero2025Cybersleuth (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0156#method)
- (E-P0236-1ca9e70d75) These third-party MCP services provider are potentially malicious and have the economic incentives to exploit vulnerabilities and sabotage user-agent interactions. Fang2025Should (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0236#summary_bullets[4])
- (E-P0159-39281e5083) Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. Zhang2025Datascibench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0159#key_results[1])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0201-1d5f67b08e) Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Ghose2025Orfs (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0201#key_results[0])
- (E-P0092-ebb97f6129) With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. Yin2024Mmau (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
- (E-P0047-a06d9a2787) By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments. Zhuang2025Hephaestus (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0047#key_results[1])
- (E-P0173-66900cdb54) Current agent benchmarks often mix agentic reasoning with challenging math reasoning, expert-level knowledge, and other advanced capabilities. Zhu2025Agent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0173#key_results[1])
- (E-P0230-15e523063d) Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content. Cui2025Toward (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0230#key_results[1])
- (E-P0023-83f8d55860) By integrating the tool registration process into the reasoning procedure, EcoAct reduces computational costs by over 50% in multiple steps reasoning tasks while maintaining performance, as demonstrated through extensive experiments. Zhang2024Ecoact (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
- (E-P0045-8a32e8598f) While proprietary closed-source models such as GPT-4 demonstrate strong reasoning abilities, smaller open-source models struggle to perform complex tool use effectively. Min2025Goat (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0045#key_results[0])

## Definitions / setup

- Setup: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Agent loop and action spaces'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Zhang2026Evoroute Miyamoto2026Agent Song2026Envscaler

## Claim candidates

- We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. Song2025Agent
- To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench
- Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates. Tomaevi2025Towards
- We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group
- Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0103`, `P0291`, `P0295`; B: Tool-use and function calling: `P0295`, `P0012`, `P0030`. Chowa2025From Gasmi2025Bridging Liu2025Mcpagentbench
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - A highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
  - B highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - B highlight: (E-P0191-312a342670) To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench (pointer: papers/paper_notes.jsonl:paper_id=P0191#method)
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0103`, `P0291`, `P0295`; B: Code agents / software tasks: `P0295`, `P0040`, `P0119`. Chowa2025From Gasmi2025Bridging
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - A highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
  - B highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Tool-use and function calling: `P0295`, `P0012`, `P0030`; B: Code agents / software tasks: `P0295`, `P0040`, `P0119`. Chowa2025From Liu2025Mcpagentbench Gasmi2025Bridging
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - A highlight: (E-P0191-312a342670) To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench (pointer: papers/paper_notes.jsonl:paper_id=P0191#method)
  - B highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0103`, `P0291`, `P0295`; B: Tool-use and function calling: `P0295`, `P0012`, `P0030`. Zhang2026Evoroute Chowa2025From Song2025Agent Ghose2025Orfs
  - A highlight: (E-P0103-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0103#key_results[0])
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - B highlight: (E-P0121-44cc8fbce0) We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. Song2025Agent (pointer: papers/paper_notes.jsonl:paper_id=P0121#key_results[0])
  - B highlight: (E-P0201-1d5f67b08e) Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Ghose2025Orfs (pointer: papers/paper_notes.jsonl:paper_id=P0201#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0103`, `P0291`, `P0295`; B: Code agents / software tasks: `P0295`, `P0040`, `P0119`. Zhang2026Evoroute Chowa2025From Gasmi2025Bridging
  - A highlight: (E-P0103-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0103#key_results[0])
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - B highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
- Axis: compute and latency constraints; A: Tool-use and function calling: `P0295`, `P0012`, `P0030`; B: Code agents / software tasks: `P0295`, `P0040`, `P0119`. Song2025Agent Ghose2025Orfs Gasmi2025Bridging
  - A highlight: (E-P0121-44cc8fbce0) We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. Song2025Agent (pointer: papers/paper_notes.jsonl:paper_id=P0121#key_results[0])
  - A highlight: (E-P0201-1d5f67b08e) Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Ghose2025Orfs (pointer: papers/paper_notes.jsonl:paper_id=P0201#key_results[0])
  - B highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0103`, `P0291`, `P0295`; B: Tool-use and function calling: `P0295`, `P0012`, `P0030`. Gasmi2025Bridging Chowa2025From Liu2025Mcpagentbench Song2025Agent
  - A highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - B highlight: (E-P0191-312a342670) To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench (pointer: papers/paper_notes.jsonl:paper_id=P0191#method)
  - B highlight: (E-P0121-44cc8fbce0) We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. Song2025Agent (pointer: papers/paper_notes.jsonl:paper_id=P0121#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0103`, `P0291`, `P0295`; B: Code agents / software tasks: `P0295`, `P0040`, `P0119`. Gasmi2025Bridging Chowa2025From
  - A highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - B highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Tool-use and function calling: `P0295`, `P0012`, `P0030`; B: Code agents / software tasks: `P0295`, `P0040`, `P0119`. Liu2025Mcpagentbench Song2025Agent Gasmi2025Bridging
  - A highlight: (E-P0191-312a342670) To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench (pointer: papers/paper_notes.jsonl:paper_id=P0191#method)
  - A highlight: (E-P0121-44cc8fbce0) We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. Song2025Agent (pointer: papers/paper_notes.jsonl:paper_id=P0121#key_results[0])
  - B highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0103`, `P0291`, `P0295`; B: Tool-use and function calling: `P0295`, `P0012`, `P0030`. Gasmi2025Bridging Chowa2025From Liu2025Mcpagentbench Song2025Agent
  - A highlight: (E-P0040-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - B highlight: (E-P0191-312a342670) To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench (pointer: papers/paper_notes.jsonl:paper_id=P0191#method)
  - B highlight: (E-P0121-44cc8fbce0) We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. Song2025Agent (pointer: papers/paper_notes.jsonl:paper_id=P0121#key_results[0])

## Evaluation protocol

- Evaluation mentions include: LLMs, GAIA, EvoRoute, BrowseComp, LLM-simulated, SFT, RUC-NLPIR, EnvScaler, SkelBuilder, ScenGenerator. Zhang2026Evoroute Miyamoto2026Agent Song2026Envscaler Soliman2026Intagent
- When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated. Zhang2026Evoroute Miyamoto2026Agent
- Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority. Zhang2026Evoroute Miyamoto2026Agent
- Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing. Zhang2026Evoroute Miyamoto2026Agent
- If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative. Zhang2026Evoroute Miyamoto2026Agent
- If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes. Zhang2026Evoroute Miyamoto2026Agent

## Failures / limitations

- We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. Zhang2026Evoroute
- With the spread of generative AI in recent years, attacks known as Whaling have become a serious threat. Miyamoto2026Agent
- We design agents that (i) build vulnerability profiles for each target from publicly available information on faculty members, (ii) identify potential risk scenarios relevant to Whaling defense based on those profiles, (iii) construct defense profiles corresponding to the vulnerabilities and anticipated risks, and (iv) analyze Whaling emails using the defense profiles. Miyamoto2026Agent
- Furthermore, we conduct a preliminary risk-assessment experiment. Miyamoto2026Agent
- Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Yang2025Survey
- Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. Gasmi2025Bridging
- This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. Gasmi2025Bridging
- We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Gasmi2025Bridging

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
