# Evidence draft: 6.1 Benchmarks and evaluation protocols

## Evidence snippets (with provenance)
- (E-P0208-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0208#key_results[1])
- (E-P0147-578706f7a5) Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are crucial for achieving desired performance levels, requiring domain expertise. Kwon2025Agentnet (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0147#limitations[1])
- (E-P0061-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0061#key_results[0])
- (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. Mohammadi2025Evaluation (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
- (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
- (E-P0062-52fcef25b6) Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks. Zhong2025Rtbas (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
- (E-P0068-537b51e910) In 16 user tasks from AgentDojo, LLMs show a 15-50 percentage point drop in utility under attack, with average attack success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Alizadeh2025Simple (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0068#key_results[0])
- (E-P0071-5607dc887c) Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Ji2025Taxonomy (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0071#key_results[1])
- (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
- (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
- (E-P0074-4fc221fdea) This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. Chen2025Towards (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0074#key_results[0])
- (E-P0300-8b56718f74) Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants. Liu2026Agents (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0300#key_results[0])
- (E-P0141-fad03785c5) To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. GendreauDistler2025Automating (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0141#key_results[0])
- (E-P0153-9bd58ce21b) When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. Tao2025Code (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0153#key_results[0])
- (E-P0200-39cf43509f) To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Zheng2025Newtonbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0200#key_results[0])
- (E-P0080-17ec1d2e4e) We proposed the two-stage framework: from ``core ability'' to ``agent'', clearly explaining how LLMs can be applied based on their specific capabilities, along with the evaluation methods in each stage. Peng2024Survey (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0080#key_results[0])
- (E-P0146-4ac9a2931e) Existing agentic benchmarks often reduce evaluation to a binary judgment of the final state, overlooking critical aspects such as safety, efficiency, and intermediate correctness. Michelakis2025Core (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0146#key_results[0])
- (E-P0288-6c12fbfdd5) We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. V2026Agentic (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0288#key_results[0])

## Definitions / setup

- Setup: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Benchmarks and evaluation protocols'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Kim2026Beyond Doshi2026Towards V2026Agentic

## Claim candidates

- RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval
- Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are crucial for achieving desired performance levels, requiring domain expertise. Kwon2025Agentnet
- Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent
- This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation Mohammadi2025Evaluation
- Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0027`, `P0110`, `P0288`; B: Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`. Zhou2025Self Zhong2025Rtbas Mohammadi2025Evaluation Guo2025Comprehensive
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0062-52fcef25b6) Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy Zhong2025Rtbas (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
  - B highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - B highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0027`, `P0110`, `P0288`; B: Tool-use and function calling: `P0027`, `P0110`, `P0147`. Zhou2025Self Zhong2025Rtbas Kwon2025Agentnet Kim2026Beyond
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0062-52fcef25b6) Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy Zhong2025Rtbas (pointer: papers/paper_notes.jsonl:paper_id=P0062#key_results[0])
  - B highlight: (E-P0147-578706f7a5) Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are Kwon2025Agentnet (pointer: papers/paper_notes.jsonl:paper_id=P0147#limitations[1])
  - B highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`; B: Tool-use and function calling: `P0027`, `P0110`, `P0147`. Mohammadi2025Evaluation Guo2025Comprehensive Kwon2025Agentnet Kim2026Beyond
  - A highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - A highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
  - B highlight: (E-P0147-578706f7a5) Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are Kwon2025Agentnet (pointer: papers/paper_notes.jsonl:paper_id=P0147#limitations[1])
  - B highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0027`, `P0110`, `P0288`; B: Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`. Zhou2025Self Shi2025Progent Mohammadi2025Evaluation Guo2025Comprehensive
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0061-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0061#key_results[0])
  - B highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - B highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0027`, `P0110`, `P0288`; B: Tool-use and function calling: `P0027`, `P0110`, `P0147`. Zhou2025Self Shi2025Progent Kwon2025Agentnet Kim2026Beyond
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0061-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0061#key_results[0])
  - B highlight: (E-P0147-578706f7a5) Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are Kwon2025Agentnet (pointer: papers/paper_notes.jsonl:paper_id=P0147#limitations[1])
  - B highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
- Axis: compute and latency constraints; A: Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`; B: Tool-use and function calling: `P0027`, `P0110`, `P0147`. Mohammadi2025Evaluation Guo2025Comprehensive Kwon2025Agentnet Kim2026Beyond
  - A highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - A highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
  - B highlight: (E-P0147-578706f7a5) Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are Kwon2025Agentnet (pointer: papers/paper_notes.jsonl:paper_id=P0147#limitations[1])
  - B highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0027`, `P0110`, `P0288`; B: Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`. Zhou2025Self Kim2026Beyond Mohammadi2025Evaluation
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
  - B highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - B highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0027`, `P0110`, `P0288`; B: Tool-use and function calling: `P0027`, `P0110`, `P0147`. Zhou2025Self Kim2026Beyond Kwon2025Agentnet
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
  - B highlight: (E-P0147-578706f7a5) Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are Kwon2025Agentnet (pointer: papers/paper_notes.jsonl:paper_id=P0147#limitations[1])
  - B highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`; B: Tool-use and function calling: `P0027`, `P0110`, `P0147`. Mohammadi2025Evaluation Kim2026Beyond Kwon2025Agentnet
  - A highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - A highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
  - B highlight: (E-P0147-578706f7a5) Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are Kwon2025Agentnet (pointer: papers/paper_notes.jsonl:paper_id=P0147#limitations[1])
  - B highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0027`, `P0110`, `P0288`; B: Evaluation / benchmark-focused works: `P0027`, `P0288`, `P0009`. Zhou2025Self Kim2026Beyond Mohammadi2025Evaluation
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])
  - B highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - B highlight: (E-P0027-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0027#key_results[0])

## Evaluation protocol

- Evaluation mentions include: API, LLMs, WildAGTEval, APIs, IFC, STPA, MCP, LLM-based, LLM-powered, LLM-driven. Kim2026Beyond Doshi2026Towards V2026Agentic Liu2026Agents
- When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated. Kim2026Beyond Doshi2026Towards
- Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority. Kim2026Beyond Doshi2026Towards
- Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing. Kim2026Beyond Doshi2026Towards
- If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative. Kim2026Beyond Doshi2026Towards
- If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes. Kim2026Beyond Doshi2026Towards

## Failures / limitations

- Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. Liu2026Agents
- Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Shi2025Progent
- LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments. Shi2025Progent
- Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones. Shi2025Progent
- The framework operates deterministically at runtime, providing provable security guarantees. Shi2025Progent
- Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent
- Additionally, we show that LLMs can automatically generate effective policies, highlighting their potential for automating the process of writing Progent's security policies. Shi2025Progent
- Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks. Zhong2025Rtbas

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
