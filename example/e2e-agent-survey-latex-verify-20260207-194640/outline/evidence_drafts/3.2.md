# Evidence draft: 3.2 Tool interfaces and orchestration

## Evidence snippets (with provenance)
- (E-P0014-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
- (E-P0162-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0162#method)
- (E-P0022-f5a0e32a25) To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Yang2025Toolmind (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0022#limitations[1])
- (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. Mohammadi2025Evaluation (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
- (E-P0229-468a77ff1d) Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. Liu2025Toolscope (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0229#key_results[0])
- (E-P0263-0ea7b39672) This attack shows a nearly 80% success rate in an end-to-end evaluation. Fu2024Imprompter (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0263#key_results[0])
- (E-P0013-4e80a740e1) Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases. Nandi2025Bench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
- (E-P0044-74547b154c) Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems. Ferrag2025From (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0044#key_results[0])
- (E-P0110-dab37b0fe2) Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. Doshi2026Towards (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0])
- (E-P0194-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0194#key_results[0])
- (E-P0189-419e1464da) MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations. Gao2025Radar (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0189#key_results[0])
- (E-P0066-ee5ba721d6) Evaluation assumes Korean queries and Korean tool/parameter specifications; it covers single-chain, multi-chain, missing-parameters, and missing-functions scenarios, and is conducted via an LLM-as-a-Judge protocol averaged over five runs under a unified I/O interface. Jeon2025Based (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0066#key_results[0])
- (E-P0250-a8aa0f8a7d) Next, on three benchmarks, we quantitatively compare the effectiveness of code-use (which only has access to the target codebase) to tool-use (which has privileged access to all tool names and descriptions). Gupta2024Codenav (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0250#key_results[0])
- (E-P0273-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0273#key_results[1])
- (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
- (E-P0108-3e2edc05cd) However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. Li2026Toolprmbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0108#key_results[0])
- (E-P0297-ed4427964c) Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Hao2026From (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0297#key_results[1])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])

## Definitions / setup

- Setup: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Tool interfaces and orchestration'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Zhou2026Beyond Li2026Toolprmbench Doshi2026Towards

## Claim candidates

- Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond
- We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom
- To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Yang2025Toolmind
- This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation Mohammadi2025Evaluation
- Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. Liu2025Toolscope

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0014`, `P0108`, `P0110`; B: Tool-use and function calling: `P0014`, `P0108`, `P0110`. Nandi2025Bench Chowa2025From Yang2025Toolmind
  - A highlight: (E-P0013-4e80a740e1) Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases. Nandi2025Bench (pointer: papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - B highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - B highlight: (E-P0022-f5a0e32a25) To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Yang2025Toolmind (pointer: papers/paper_notes.jsonl:paper_id=P0022#limitations[1])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0014`, `P0108`, `P0110`; B: Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`. Nandi2025Bench Chowa2025From Dong2025Etom Mohammadi2025Evaluation
  - A highlight: (E-P0013-4e80a740e1) Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases. Nandi2025Bench (pointer: papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - B highlight: (E-P0162-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (pointer: papers/paper_notes.jsonl:paper_id=P0162#method)
  - B highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Tool-use and function calling: `P0014`, `P0108`, `P0110`; B: Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`. Chowa2025From Yang2025Toolmind Dong2025Etom Mohammadi2025Evaluation
  - A highlight: (E-P0012-e0bcca0d9e) Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. Chowa2025From (pointer: papers/paper_notes.jsonl:paper_id=P0012#key_results[0])
  - A highlight: (E-P0022-f5a0e32a25) To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Yang2025Toolmind (pointer: papers/paper_notes.jsonl:paper_id=P0022#limitations[1])
  - B highlight: (E-P0162-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (pointer: papers/paper_notes.jsonl:paper_id=P0162#method)
  - B highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0014`, `P0108`, `P0110`; B: Tool-use and function calling: `P0014`, `P0108`, `P0110`. Zhou2026Beyond Ferrag2025From
  - A highlight: (E-P0014-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - A highlight: (E-P0044-74547b154c) Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems. Ferrag2025From (pointer: papers/paper_notes.jsonl:paper_id=P0044#key_results[0])
  - B highlight: (E-P0014-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - B highlight: (E-P0044-74547b154c) Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems. Ferrag2025From (pointer: papers/paper_notes.jsonl:paper_id=P0044#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0014`, `P0108`, `P0110`; B: Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`. Zhou2026Beyond Ferrag2025From Mohammadi2025Evaluation Dong2025Etom
  - A highlight: (E-P0014-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - A highlight: (E-P0044-74547b154c) Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems. Ferrag2025From (pointer: papers/paper_notes.jsonl:paper_id=P0044#key_results[0])
  - B highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - B highlight: (E-P0162-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (pointer: papers/paper_notes.jsonl:paper_id=P0162#method)
- Axis: compute and latency constraints; A: Tool-use and function calling: `P0014`, `P0108`, `P0110`; B: Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`. Zhou2026Beyond Ferrag2025From Mohammadi2025Evaluation Dong2025Etom
  - A highlight: (E-P0014-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - A highlight: (E-P0044-74547b154c) Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems. Ferrag2025From (pointer: papers/paper_notes.jsonl:paper_id=P0044#key_results[0])
  - B highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
  - B highlight: (E-P0162-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (pointer: papers/paper_notes.jsonl:paper_id=P0162#method)
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0014`, `P0108`, `P0110`; B: Tool-use and function calling: `P0014`, `P0108`, `P0110`. Nandi2025Bench Doshi2026Towards Ferrag2025From
  - A highlight: (E-P0013-4e80a740e1) Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases. Nandi2025Bench (pointer: papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - A highlight: (E-P0110-dab37b0fe2) Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. Doshi2026Towards (pointer: papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0])
  - B highlight: (E-P0110-dab37b0fe2) Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. Doshi2026Towards (pointer: papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0])
  - B highlight: (E-P0044-74547b154c) Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems. Ferrag2025From (pointer: papers/paper_notes.jsonl:paper_id=P0044#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0014`, `P0108`, `P0110`; B: Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`. Nandi2025Bench Doshi2026Towards Dong2025Etom Mohammadi2025Evaluation
  - A highlight: (E-P0013-4e80a740e1) Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases. Nandi2025Bench (pointer: papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - A highlight: (E-P0110-dab37b0fe2) Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. Doshi2026Towards (pointer: papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0])
  - B highlight: (E-P0162-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (pointer: papers/paper_notes.jsonl:paper_id=P0162#method)
  - B highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Tool-use and function calling: `P0014`, `P0108`, `P0110`; B: Evaluation / benchmark-focused works: `P0146`, `P0162`, `P0168`. Doshi2026Towards Ferrag2025From Dong2025Etom Mohammadi2025Evaluation
  - A highlight: (E-P0110-dab37b0fe2) Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. Doshi2026Towards (pointer: papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0])
  - A highlight: (E-P0044-74547b154c) Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems. Ferrag2025From (pointer: papers/paper_notes.jsonl:paper_id=P0044#key_results[0])
  - B highlight: (E-P0162-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (pointer: papers/paper_notes.jsonl:paper_id=P0162#method)
  - B highlight: (E-P0168-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0168#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0014`, `P0108`, `P0110`; B: Tool-use and function calling: `P0014`, `P0108`, `P0110`. Nandi2025Bench Doshi2026Towards Ferrag2025From
  - A highlight: (E-P0013-4e80a740e1) Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases. Nandi2025Bench (pointer: papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - A highlight: (E-P0110-dab37b0fe2) Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. Doshi2026Towards (pointer: papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0])
  - B highlight: (E-P0110-dab37b0fe2) Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. Doshi2026Towards (pointer: papers/paper_notes.jsonl:paper_id=P0110#summary_bullets[0])
  - B highlight: (E-P0044-74547b154c) Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems. Ferrag2025From (pointer: papers/paper_notes.jsonl:paper_id=P0044#key_results[0])

## Evaluation protocol

- Evaluation mentions include: RAG, MCP, MCTS, LLMs, BFCL, GPU, ToolBench, PRMs, PRM, ToolPRMBench. Zhou2026Beyond Li2026Toolprmbench Doshi2026Towards Hao2026From
- When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated. Zhou2026Beyond Li2026Toolprmbench
- Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority. Zhou2026Beyond Li2026Toolprmbench
- Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing. Zhou2026Beyond Li2026Toolprmbench
- If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative. Zhou2026Beyond Li2026Toolprmbench
- If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes. Zhou2026Beyond Li2026Toolprmbench

## Failures / limitations

- The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Zhou2026Beyond
- We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Zhou2026Beyond
- Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond
- These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process. Zhou2026Beyond
- Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Hao2026From
- However, the rapid growth of plugins, connectors, and inter-agent protocols has outpaced security practices, leading to brittle integrations that rely on ad-hoc authentication, inconsistent schemas, and weak validation. Ferrag2025From
- This survey introduces a unified end-to-end threat model for LLM-agent ecosystems, covering host-to-tool and agent-to-agent communications. Ferrag2025From
- We systematically categorize more than thirty attack techniques spanning input manipulation, model compromise, system and privacy attacks, and protocol-level vulnerabilities. Ferrag2025From

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
