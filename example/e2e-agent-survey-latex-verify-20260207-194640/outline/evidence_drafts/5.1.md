# Evidence draft: 5.1 Self-improvement and adaptation

## Evidence snippets (with provenance)
- (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
- (E-P0100-67ea29ce26) We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Li2026Autonomous (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0100#method)
- (E-P0082-4da9e4ae32) We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0082#limitations[1])
- (E-P0103-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0103#key_results[0])
- (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
- (E-P0178-d099f5e3ee) Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences. Yu2025Infiagent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0178#key_results[0])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0068-c2c43d35ce) In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage. Alizadeh2025Simple (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0068#key_results[1])
- (E-P0092-ebb97f6129) With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. Yin2024Mmau (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
- (E-P0102-ec579c4773) Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. Peng2026Enhancing (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0102#key_results[0])
- (E-P0065-6273763a98) Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches. Xia2025Sand (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0065#key_results[0])
- (E-P0141-fad03785c5) To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. GendreauDistler2025Automating (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0141#key_results[0])
- (E-P0143-fa93c04884) Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis generation; and 3) self-evaluation significantly improves the novelty and relevance of proposed hypotheses. Yang2025Bioverge (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0143#key_results[0])
- (E-P0011-f0ea009256) Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Du2025Survey (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
- (E-P0167-ca050ed55f) We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports. Samaei2025Epidemiqs (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0167#key_results[1])
- (E-P0183-78bde774bf) LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention. Yano2025Lamdagent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0183#key_results[1])
- (E-P0184-d19d8e1143) At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. Li2025Learn (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0184#key_results[0])
- (E-P0113-a68f39bc04) This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. Van2025Survey (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0113#key_results[0])

## Definitions / setup

- Setup: Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Self-improvement and adaptation'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; communication protocol / roles; aggregation (vote / debate / referee); stability / robustness. Li2026Autonomous Peng2026Enhancing Zhang2026Evoroute

## Claim candidates

- Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self
- We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Li2026Autonomous
- We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool
- Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute
- Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0100`, `P0102`, `P0103`; B: Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`. Zhou2025Self Du2025Survey Guo2025Comprehensive Yang2025Bioverge
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0011-f0ea009256) Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Du2025Survey (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
  - B highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
  - B highlight: (E-P0143-fa93c04884) Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis Yang2025Bioverge (pointer: papers/paper_notes.jsonl:paper_id=P0143#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0100`, `P0102`, `P0103`; B: Planning / reasoning loops: `P0064`, `P0132`, `P0001`. Zhou2025Self Du2025Survey Yao2022React
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0011-f0ea009256) Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Du2025Survey (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`; B: Planning / reasoning loops: `P0064`, `P0132`, `P0001`. Guo2025Comprehensive Yang2025Bioverge Yao2022React
  - A highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
  - A highlight: (E-P0143-fa93c04884) Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis Yang2025Bioverge (pointer: papers/paper_notes.jsonl:paper_id=P0143#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0100`, `P0102`, `P0103`; B: Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`. Zhang2026Evoroute Peng2026Enhancing Yang2025Bioverge Guo2025Comprehensive
  - A highlight: (E-P0103-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0103#key_results[0])
  - A highlight: (E-P0102-ec579c4773) Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. Peng2026Enhancing (pointer: papers/paper_notes.jsonl:paper_id=P0102#key_results[0])
  - B highlight: (E-P0143-fa93c04884) Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis Yang2025Bioverge (pointer: papers/paper_notes.jsonl:paper_id=P0143#key_results[0])
  - B highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0100`, `P0102`, `P0103`; B: Planning / reasoning loops: `P0064`, `P0132`, `P0001`. Zhang2026Evoroute Peng2026Enhancing Yao2022React
  - A highlight: (E-P0103-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0103#key_results[0])
  - A highlight: (E-P0102-ec579c4773) Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. Peng2026Enhancing (pointer: papers/paper_notes.jsonl:paper_id=P0102#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: compute and latency constraints; A: Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`; B: Planning / reasoning loops: `P0064`, `P0132`, `P0001`. Yang2025Bioverge Guo2025Comprehensive Yao2022React
  - A highlight: (E-P0143-fa93c04884) Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis Yang2025Bioverge (pointer: papers/paper_notes.jsonl:paper_id=P0143#key_results[0])
  - A highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: communication protocol / roles; A: Agent frameworks / architectures: `P0100`, `P0102`, `P0103`; B: Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`. Zhou2025Self Zhang2026Evoroute Yang2025Bioverge Guo2025Comprehensive
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0103-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0103#key_results[0])
  - B highlight: (E-P0143-fa93c04884) Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis Yang2025Bioverge (pointer: papers/paper_notes.jsonl:paper_id=P0143#key_results[0])
  - B highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
- Axis: communication protocol / roles; A: Agent frameworks / architectures: `P0100`, `P0102`, `P0103`; B: Planning / reasoning loops: `P0064`, `P0132`, `P0001`. Zhou2025Self Zhang2026Evoroute Yao2022React
  - A highlight: (E-P0067-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
  - A highlight: (E-P0103-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0103#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: communication protocol / roles; A: Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`; B: Planning / reasoning loops: `P0064`, `P0132`, `P0001`. Yang2025Bioverge Guo2025Comprehensive Yao2022React
  - A highlight: (E-P0143-fa93c04884) Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis Yang2025Bioverge (pointer: papers/paper_notes.jsonl:paper_id=P0143#key_results[0])
  - A highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: aggregation (vote / debate / referee); A: Agent frameworks / architectures: `P0100`, `P0102`, `P0103`; B: Evaluation / benchmark-focused works: `P0009`, `P0143`, `P0092`. Zhang2026Evoroute Peng2026Enhancing Yang2025Bioverge Guo2025Comprehensive
  - A highlight: (E-P0103-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0103#key_results[0])
  - A highlight: (E-P0102-ec579c4773) Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. Peng2026Enhancing (pointer: papers/paper_notes.jsonl:paper_id=P0102#key_results[0])
  - B highlight: (E-P0143-fa93c04884) Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis Yang2025Bioverge (pointer: papers/paper_notes.jsonl:paper_id=P0143#key_results[0])
  - B highlight: (E-P0009-30bd885b0c) Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. Guo2025Comprehensive (pointer: papers/paper_notes.jsonl:paper_id=P0009#key_results[1])

## Evaluation protocol

- Evaluation mentions include: DeepSeek-V3, HITL, LLMs, MITRE, ATT, IPDRR-based, LLM-RL, CyberOps-Bots, ReAct, GAIA. Li2026Autonomous Peng2026Enhancing Zhang2026Evoroute Guo2025Comprehensive
- When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated. Li2026Autonomous Peng2026Enhancing
- Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority. Li2026Autonomous Peng2026Enhancing
- Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing. Li2026Autonomous Peng2026Enhancing
- If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative. Li2026Autonomous Peng2026Enhancing
- If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes. Li2026Autonomous Peng2026Enhancing

## Failures / limitations

- Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures. Li2026Autonomous
- To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Peng2026Enhancing
- While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Peng2026Enhancing
- However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Peng2026Enhancing
- We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. Zhang2026Evoroute
- Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. Hatalis2025Review
- The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. Zhou2025Self
- To address these limitations, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups. Yin2024Mmau

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
