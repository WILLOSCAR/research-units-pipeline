# Evidence draft: 5.2 Multi-agent coordination

## Evidence snippets (with provenance)
- (E-P0177-4b027dfb27) We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0177#key_results[0])
- (E-P0159-b9e7423acc) We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. Zhang2025Datascibench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0159#method)
- (E-P0266-c92a9d34cf) The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. Yang2024Based (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0266#limitations[1])
- (E-P0194-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0194#key_results[0])
- (E-P0015-763cb193e3) Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. Lu2025Just (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0015#key_results[0])
- (E-P0189-419e1464da) MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations. Gao2025Radar (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0189#key_results[0])
- (E-P0178-d099f5e3ee) Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences. Yu2025Infiagent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0178#key_results[0])
- (E-P0156-0b34b55332) We propose CyberSleuth, an autonomous agent that processes packet-level traces and application logs to identify the targeted service, the exploited vulnerability (CVE), and attack success. Fumero2025Cybersleuth (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0156#method)
- (E-P0017-62cd0c501b) Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. Luo2025Large (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0017#key_results[0])
- (E-P0273-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0273#key_results[1])
- (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools that keep pace with this new degree of autonomy. Maragheh2025Future (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
- (E-P0209-c664716bd8) We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. Chen2025Remsa (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0209#key_results[0])
- (E-P0239-9af3444274) Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks. Shi2025Youtu (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0239#key_results[1])
- (E-P0075-9af6a59afb) Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method. Ji2025Tree (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0075#key_results[0])
- (E-P0144-e52cad3d44) We also construct a cosmological parameter extraction evaluation dataset by collecting over 40 simulations in published papers from Arxiv and leading journals that cover diverse simulation types. Zhang2025Bridging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0144#key_results[0])
- (E-P0185-82b5e4eda3) Experimental results on the JudgeBench dataset show about 15.55\% improvement compared to raw judgments and about 8.37\% improvement over the single-agent baseline. Li2025Leveraging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0185#key_results[1])
- (E-P0222-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Cao2025Skyrl (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0222#key_results[1])
- (E-P0148-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0148#key_results[1])

## Definitions / setup

- Setup: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Multi-agent coordination'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Lu2025Just Luo2025Large Yang2025Survey

## Claim candidates

- We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group
- We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. Zhang2025Datascibench
- The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. Yang2024Based
- Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool
- Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. Lu2025Just

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0015`, `P0017`, `P0030`; B: Multi-agent coordination: `P0063`, `P0072`, `P0119`. Maragheh2025Future Lu2025Just Silva2025Agents
  - A highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - A highlight: (E-P0015-763cb193e3) Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the Lu2025Just (pointer: papers/paper_notes.jsonl:paper_id=P0015#key_results[0])
  - B highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - B highlight: (E-P0148-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0148#key_results[1])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0015`, `P0017`, `P0030`; B: Tool-use and function calling: `P0030`, `P0069`, `P0189`. Maragheh2025Future Lu2025Just Lumer2025Memtool Shen2024Small
  - A highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - A highlight: (E-P0015-763cb193e3) Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the Lu2025Just (pointer: papers/paper_notes.jsonl:paper_id=P0015#key_results[0])
  - B highlight: (E-P0194-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0194#key_results[0])
  - B highlight: (E-P0273-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (pointer: papers/paper_notes.jsonl:paper_id=P0273#key_results[1])
- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Multi-agent coordination: `P0063`, `P0072`, `P0119`; B: Tool-use and function calling: `P0030`, `P0069`, `P0189`. Maragheh2025Future Silva2025Agents Lumer2025Memtool Shen2024Small
  - A highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - A highlight: (E-P0148-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0148#key_results[1])
  - B highlight: (E-P0194-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0194#key_results[0])
  - B highlight: (E-P0273-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (pointer: papers/paper_notes.jsonl:paper_id=P0273#key_results[1])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0015`, `P0017`, `P0030`; B: Multi-agent coordination: `P0063`, `P0072`, `P0119`. Maragheh2025Future Lu2025Just Silva2025Agents
  - A highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - A highlight: (E-P0015-763cb193e3) Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the Lu2025Just (pointer: papers/paper_notes.jsonl:paper_id=P0015#key_results[0])
  - B highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - B highlight: (E-P0148-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0148#key_results[1])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0015`, `P0017`, `P0030`; B: Tool-use and function calling: `P0030`, `P0069`, `P0189`. Maragheh2025Future Lu2025Just Lumer2025Memtool Shen2024Small
  - A highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - A highlight: (E-P0015-763cb193e3) Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the Lu2025Just (pointer: papers/paper_notes.jsonl:paper_id=P0015#key_results[0])
  - B highlight: (E-P0194-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0194#key_results[0])
  - B highlight: (E-P0273-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (pointer: papers/paper_notes.jsonl:paper_id=P0273#key_results[1])
- Axis: compute and latency constraints; A: Multi-agent coordination: `P0063`, `P0072`, `P0119`; B: Tool-use and function calling: `P0030`, `P0069`, `P0189`. Maragheh2025Future Silva2025Agents Lumer2025Memtool Shen2024Small
  - A highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - A highlight: (E-P0148-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0148#key_results[1])
  - B highlight: (E-P0194-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0194#key_results[0])
  - B highlight: (E-P0273-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (pointer: papers/paper_notes.jsonl:paper_id=P0273#key_results[1])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0015`, `P0017`, `P0030`; B: Multi-agent coordination: `P0063`, `P0072`, `P0119`. Maragheh2025Future Lu2025Just Yang2024Based
  - A highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - A highlight: (E-P0015-763cb193e3) Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the Lu2025Just (pointer: papers/paper_notes.jsonl:paper_id=P0015#key_results[0])
  - B highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - B highlight: (E-P0266-c92a9d34cf) The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. Yang2024Based (pointer: papers/paper_notes.jsonl:paper_id=P0266#limitations[1])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0015`, `P0017`, `P0030`; B: Tool-use and function calling: `P0030`, `P0069`, `P0189`. Maragheh2025Future Lu2025Just Lumer2025Memtool Shen2024Small
  - A highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - A highlight: (E-P0015-763cb193e3) Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the Lu2025Just (pointer: papers/paper_notes.jsonl:paper_id=P0015#key_results[0])
  - B highlight: (E-P0194-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0194#key_results[0])
  - B highlight: (E-P0273-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (pointer: papers/paper_notes.jsonl:paper_id=P0273#key_results[1])
- Axis: tool interface contract (schemas / protocols); A: Multi-agent coordination: `P0063`, `P0072`, `P0119`; B: Tool-use and function calling: `P0030`, `P0069`, `P0189`. Maragheh2025Future Yang2024Based Lumer2025Memtool Shen2024Small
  - A highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - A highlight: (E-P0266-c92a9d34cf) The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. Yang2024Based (pointer: papers/paper_notes.jsonl:paper_id=P0266#limitations[1])
  - B highlight: (E-P0194-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0194#key_results[0])
  - B highlight: (E-P0273-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (pointer: papers/paper_notes.jsonl:paper_id=P0273#key_results[1])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0015`, `P0017`, `P0030`; B: Multi-agent coordination: `P0063`, `P0072`, `P0119`. Maragheh2025Future Lu2025Just Yang2024Based
  - A highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - A highlight: (E-P0015-763cb193e3) Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the Lu2025Just (pointer: papers/paper_notes.jsonl:paper_id=P0015#key_results[0])
  - B highlight: (E-P0072-0301cf089d) The result is both a blueprint and an agenda: a blueprint that shows how memory-augmented, tool-using LLM agents can be composed into robust recommendation pipelines, and an agenda inviting the RecSys community to develop benchmarks, theoretical guarantees, and governance tools Maragheh2025Future (pointer: papers/paper_notes.jsonl:paper_id=P0072#key_results[1])
  - B highlight: (E-P0266-c92a9d34cf) The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. Yang2024Based (pointer: papers/paper_notes.jsonl:paper_id=P0266#limitations[1])

## Evaluation protocol

- Evaluation mentions include: SFT, BFCL, SFT-based, LLMs, LLM-based, RefAgent, MCP, MCP-compliant, RecSys, GRPO. Lu2025Just Luo2025Large Yang2025Survey Oueslati2025Refagent
- When comparing results, anchor the paragraph with: task type + metric + constraint (budget, tool access, horizon, or threat model) when stated. Lu2025Just Luo2025Large
- Prefer head-to-head comparisons only when benchmark/metric are shared; otherwise frame differences as protocol-driven rather than method superiority. Lu2025Just Luo2025Large
- Avoid underspecified model/baseline naming; if abstracts omit details, state that the baseline is reported but underspecified instead of guessing. Lu2025Just Luo2025Large
- If a claim relies on a single reported number, pair it with a limitation/caveat from the same evidence so the draft remains conservative. Lu2025Just Luo2025Large
- If budgets or environments differ across papers, treat cross-paper numeric comparison as fragile and prefer qualitative contrasts aligned to the subsection axes. Lu2025Just Luo2025Large

## Failures / limitations

- Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Yang2025Survey
- The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems. Sarkar2025Survey
- We then surface five cross-cutting challenge families: protocol complexity, scalability, hallucination and error propagation, emergent misalignment (including covert collusion), and brand compliance. Maragheh2025Future
- To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. Ji2025Tree
- As cosmological simulations and their associated software become increasingly complex, physicists face the challenge of searching through vast amounts of literature and user manuals to extract simulation parameters from dense academic papers, each using different models and formats. Zhang2025Bridging
- This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents
- We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Li2025Learn
- The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. Yang2024Based

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
