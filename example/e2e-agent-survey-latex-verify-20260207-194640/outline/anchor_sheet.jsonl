{"sub_id": "3.1", "title": "Agent loop and action spaces", "anchors": [{"hook_type": "quant", "text": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}, {"hook_type": "quant", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}, {"hook_type": "quant", "text": "We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning.", "citations": ["Song2025Agent"], "paper_id": "P0121", "evidence_id": "E-P0121-44cc8fbce0", "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"hook_type": "quant", "text": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "paper_id": "P0201", "evidence_id": "E-P0201-1d5f67b08e", "pointer": "papers/paper_notes.jsonl:paper_id=P0201#key_results[0]"}, {"hook_type": "eval", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "paper_id": "P0191", "evidence_id": "E-P0191-312a342670", "pointer": "papers/paper_notes.jsonl:paper_id=P0191#method"}, {"hook_type": "quant", "text": "Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates.", "citations": ["Tomaevi2025Towards"], "paper_id": "P0232", "evidence_id": "E-P0232-1232593f0e", "pointer": "papers/paper_notes.jsonl:paper_id=P0232#limitations[1]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0177", "evidence_id": "E-P0177-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}, {"hook_type": "quant", "text": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "citations": ["Zhang2025Datascibench"], "paper_id": "P0159", "evidence_id": "E-P0159-39281e5083", "pointer": "papers/paper_notes.jsonl:paper_id=P0159#key_results[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents.", "citations": ["Yin2024Mmau"], "paper_id": "P0092", "evidence_id": "E-P0092-ebb97f6129", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"hook_type": "eval", "text": "Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content.", "citations": ["Cui2025Toward"], "paper_id": "P0230", "evidence_id": "E-P0230-15e523063d", "pointer": "papers/paper_notes.jsonl:paper_id=P0230#key_results[1]"}], "generated_at": "2026-02-07T19:55:34", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "anchors": [{"hook_type": "quant", "text": "Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases.", "citations": ["Nandi2025Bench"], "paper_id": "P0013", "evidence_id": "E-P0013-4e80a740e1", "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}, {"hook_type": "quant", "text": "Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks.", "citations": ["Chowa2025From"], "paper_id": "P0012", "evidence_id": "E-P0012-e0bcca0d9e", "pointer": "papers/paper_notes.jsonl:paper_id=P0012#key_results[0]"}, {"hook_type": "quant", "text": "To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances.", "citations": ["Yang2025Toolmind"], "paper_id": "P0022", "evidence_id": "E-P0022-f5a0e32a25", "pointer": "papers/paper_notes.jsonl:paper_id=P0022#limitations[1]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"hook_type": "quant", "text": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "paper_id": "P0014", "evidence_id": "E-P0014-b6b7af5a81", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "eval", "text": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"hook_type": "quant", "text": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "citations": ["Liu2025Toolscope"], "paper_id": "P0229", "evidence_id": "E-P0229-468a77ff1d", "pointer": "papers/paper_notes.jsonl:paper_id=P0229#key_results[0]"}, {"hook_type": "quant", "text": "This attack shows a nearly 80% success rate in an end-to-end evaluation.", "citations": ["Fu2024Imprompter"], "paper_id": "P0263", "evidence_id": "E-P0263-0ea7b39672", "pointer": "papers/paper_notes.jsonl:paper_id=P0263#key_results[0]"}, {"hook_type": "eval", "text": "Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.", "citations": ["Ferrag2025From"], "paper_id": "P0044", "evidence_id": "E-P0044-74547b154c", "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]"}, {"hook_type": "quant", "text": "MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations.", "citations": ["Gao2025Radar"], "paper_id": "P0189", "evidence_id": "E-P0189-419e1464da", "pointer": "papers/paper_notes.jsonl:paper_id=P0189#key_results[0]"}], "generated_at": "2026-02-07T19:55:34", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "anchors": [{"hook_type": "quant", "text": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "paper_id": "P0046", "evidence_id": "E-P0046-741891e300", "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[1]"}, {"hook_type": "quant", "text": "LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.", "citations": ["Gao2024Efficient"], "paper_id": "P0086", "evidence_id": "E-P0086-a9c15ec0c8", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#key_results[1]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]"}, {"hook_type": "eval", "text": "To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers.", "citations": ["Luo2025Universe"], "paper_id": "P0190", "evidence_id": "E-P0190-84445d1a19", "pointer": "papers/paper_notes.jsonl:paper_id=P0190#method"}, {"hook_type": "quant", "text": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "paper_id": "P0215", "evidence_id": "E-P0215-0b753b9422", "pointer": "papers/paper_notes.jsonl:paper_id=P0215#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["Hu2025Training"], "paper_id": "P0233", "evidence_id": "E-P0233-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0233#key_results[0]"}, {"hook_type": "quant", "text": "We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset.", "citations": ["Radha2024Iteration"], "paper_id": "P0089", "evidence_id": "E-P0089-d36844b954", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "To measure the performance of task-oriented agents comprehensively, we propose a two-level evaluation framework: (1) turn level and (2) end-to-end.", "citations": ["Rawat2025Multi"], "paper_id": "P0060", "evidence_id": "E-P0060-5377f4f9b7", "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}, {"hook_type": "quant", "text": "We rigorously evaluate LARC on a carefully curated set of 48 constrained retrosynthesis planning tasks across 3 constraint types.", "citations": ["Baker2025Larc"], "paper_id": "P0180", "evidence_id": "E-P0180-55a2731a38", "pointer": "papers/paper_notes.jsonl:paper_id=P0180#key_results[1]"}, {"hook_type": "quant", "text": "Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate.", "citations": ["Shi2024Ehragent"], "paper_id": "P0254", "evidence_id": "E-P0254-2a7ea60588", "pointer": "papers/paper_notes.jsonl:paper_id=P0254#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4).", "citations": ["Ji2024Testing"], "paper_id": "P0274", "evidence_id": "E-P0274-c0a98eb625", "pointer": "papers/paper_notes.jsonl:paper_id=P0274#key_results[0]"}], "generated_at": "2026-02-07T19:55:34", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "anchors": [{"hook_type": "quant", "text": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["Abbineni2025Muallm"], "paper_id": "P0197", "evidence_id": "E-P0197-e294aeefb5", "pointer": "papers/paper_notes.jsonl:paper_id=P0197#key_results[1]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0243", "evidence_id": "E-P0243-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0243#key_results[0]"}, {"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0128", "evidence_id": "E-P0128-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0128#key_results[0]"}, {"hook_type": "quant", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "paper_id": "P0057", "evidence_id": "E-P0057-f36b515991", "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[1]"}, {"hook_type": "quant", "text": "Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of", "citations": ["Li2024Review"], "paper_id": "P0007", "evidence_id": "E-P0007-dc2266b72d", "pointer": "papers/paper_notes.jsonl:paper_id=P0007#key_results[0]"}, {"hook_type": "quant", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "paper_id": "P0287", "evidence_id": "E-P0287-9abcf1bf8a", "pointer": "papers/paper_notes.jsonl:paper_id=P0287#key_results[1]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0061", "evidence_id": "E-P0061-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0061#key_results[0]"}, {"hook_type": "eval", "text": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "paper_id": "P0162", "evidence_id": "E-P0162-192e78b614", "pointer": "papers/paper_notes.jsonl:paper_id=P0162#method"}, {"hook_type": "limitation", "text": "Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure.", "citations": ["Zhu2025Where"], "paper_id": "P0238", "evidence_id": "E-P0238-46914a4804", "pointer": "papers/paper_notes.jsonl:paper_id=P0238#summary_bullets[1]"}, {"hook_type": "eval", "text": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and", "citations": ["Zhang2025Large"], "paper_id": "P0054", "evidence_id": "E-P0054-897bcc2f50", "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"hook_type": "quant", "text": "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "citations": ["Huang2025Retrieval"], "paper_id": "P0234", "evidence_id": "E-P0234-4af0cf3c02", "pointer": "papers/paper_notes.jsonl:paper_id=P0234#key_results[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "generated_at": "2026-02-07T19:55:34", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "anchors": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"hook_type": "quant", "text": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}, {"hook_type": "quant", "text": "Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis", "citations": ["Yang2025Bioverge"], "paper_id": "P0143", "evidence_id": "E-P0143-fa93c04884", "pointer": "papers/paper_notes.jsonl:paper_id=P0143#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0103", "evidence_id": "E-P0103-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0103#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining.", "citations": ["Peng2026Enhancing"], "paper_id": "P0102", "evidence_id": "E-P0102-ec579c4773", "pointer": "papers/paper_notes.jsonl:paper_id=P0102#key_results[0]"}, {"hook_type": "quant", "text": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "citations": ["Li2026Autonomous"], "paper_id": "P0100", "evidence_id": "E-P0100-67ea29ce26", "pointer": "papers/paper_notes.jsonl:paper_id=P0100#method"}, {"hook_type": "eval", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "paper_id": "P0082", "evidence_id": "E-P0082-4da9e4ae32", "pointer": "papers/paper_notes.jsonl:paper_id=P0082#limitations[1]"}, {"hook_type": "quant", "text": "Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recog", "citations": ["Yu2025Infiagent"], "paper_id": "P0178", "evidence_id": "E-P0178-d099f5e3ee", "pointer": "papers/paper_notes.jsonl:paper_id=P0178#key_results[0]"}, {"hook_type": "quant", "text": "In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage.", "citations": ["Alizadeh2025Simple"], "paper_id": "P0068", "evidence_id": "E-P0068-c2c43d35ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0068#key_results[1]"}, {"hook_type": "quant", "text": "With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents.", "citations": ["Yin2024Mmau"], "paper_id": "P0092", "evidence_id": "E-P0092-ebb97f6129", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "citations": ["Xia2025Sand"], "paper_id": "P0065", "evidence_id": "E-P0065-6273763a98", "pointer": "papers/paper_notes.jsonl:paper_id=P0065#key_results[0]"}], "generated_at": "2026-02-07T19:55:34", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "anchors": [{"hook_type": "quant", "text": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the", "citations": ["Lu2025Just"], "paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0194", "evidence_id": "E-P0194-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0194#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0177", "evidence_id": "E-P0177-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0177#key_results[0]"}, {"hook_type": "eval", "text": "We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics.", "citations": ["Zhang2025Datascibench"], "paper_id": "P0159", "evidence_id": "E-P0159-b9e7423acc", "pointer": "papers/paper_notes.jsonl:paper_id=P0159#method"}, {"hook_type": "quant", "text": "Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performanc", "citations": ["Lu2025Just"], "paper_id": "P0015", "evidence_id": "E-P0015-763cb193e3", "pointer": "papers/paper_notes.jsonl:paper_id=P0015#key_results[0]"}, {"hook_type": "quant", "text": "MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations.", "citations": ["Gao2025Radar"], "paper_id": "P0189", "evidence_id": "E-P0189-419e1464da", "pointer": "papers/paper_notes.jsonl:paper_id=P0189#key_results[0]"}, {"hook_type": "quant", "text": "Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recog", "citations": ["Yu2025Infiagent"], "paper_id": "P0178", "evidence_id": "E-P0178-d099f5e3ee", "pointer": "papers/paper_notes.jsonl:paper_id=P0178#key_results[0]"}, {"hook_type": "eval", "text": "Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains.", "citations": ["Luo2025Large"], "paper_id": "P0017", "evidence_id": "E-P0017-62cd0c501b", "pointer": "papers/paper_notes.jsonl:paper_id=P0017#key_results[0]"}, {"hook_type": "eval", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "paper_id": "P0273", "evidence_id": "E-P0273-9640816b42", "pointer": "papers/paper_notes.jsonl:paper_id=P0273#key_results[1]"}, {"hook_type": "quant", "text": "We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol.", "citations": ["Chen2025Remsa"], "paper_id": "P0209", "evidence_id": "E-P0209-c664716bd8", "pointer": "papers/paper_notes.jsonl:paper_id=P0209#key_results[0]"}, {"hook_type": "quant", "text": "Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "citations": ["Shi2025Youtu"], "paper_id": "P0239", "evidence_id": "E-P0239-9af3444274", "pointer": "papers/paper_notes.jsonl:paper_id=P0239#key_results[1]"}, {"hook_type": "quant", "text": "Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.", "citations": ["Ji2025Tree"], "paper_id": "P0075", "evidence_id": "E-P0075-9af6a59afb", "pointer": "papers/paper_notes.jsonl:paper_id=P0075#key_results[0]"}], "generated_at": "2026-02-07T19:55:34", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "anchors": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0067", "evidence_id": "E-P0067-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}, {"hook_type": "quant", "text": "Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy", "citations": ["Zhong2025Rtbas"], "paper_id": "P0062", "evidence_id": "E-P0062-52fcef25b6", "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"hook_type": "quant", "text": "Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria.", "citations": ["Guo2025Comprehensive"], "paper_id": "P0009", "evidence_id": "E-P0009-30bd885b0c", "pointer": "papers/paper_notes.jsonl:paper_id=P0009#key_results[1]"}, {"hook_type": "quant", "text": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are", "citations": ["Kwon2025Agentnet"], "paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]"}, {"hook_type": "quant", "text": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "paper_id": "P0027", "evidence_id": "E-P0027-79f88927fa", "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0061", "evidence_id": "E-P0061-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0061#key_results[0]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0208", "evidence_id": "E-P0208-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0208#key_results[1]"}, {"hook_type": "quant", "text": "Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are cru", "citations": ["Kwon2025Agentnet"], "paper_id": "P0147", "evidence_id": "E-P0147-578706f7a5", "pointer": "papers/paper_notes.jsonl:paper_id=P0147#limitations[1]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0168", "evidence_id": "E-P0168-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0168#key_results[0]"}, {"hook_type": "quant", "text": "Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy lea", "citations": ["Zhong2025Rtbas"], "paper_id": "P0062", "evidence_id": "E-P0062-52fcef25b6", "pointer": "papers/paper_notes.jsonl:paper_id=P0062#key_results[0]"}, {"hook_type": "quant", "text": "In 16 user tasks from AgentDojo, LLMs show a 15-50 percentage point drop in utility under attack, with average attack success rates (ASR) around 20 percent; some defenses reduce ASR to zero.", "citations": ["Alizadeh2025Simple"], "paper_id": "P0068", "evidence_id": "E-P0068-537b51e910", "pointer": "papers/paper_notes.jsonl:paper_id=P0068#key_results[0]"}], "generated_at": "2026-02-07T19:55:34", "section_id": "6", "section_title": "Evaluation & Risks"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "anchors": [{"hook_type": "quant", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "paper_id": "P0040", "evidence_id": "E-P0040-8e34a29629", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"hook_type": "quant", "text": "Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "citations": ["Mou2026Toolsafe"], "paper_id": "P0109", "evidence_id": "E-P0109-c6a59efa61", "pointer": "papers/paper_notes.jsonl:paper_id=P0109#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0186", "evidence_id": "E-P0186-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]"}, {"hook_type": "quant", "text": "Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations.", "citations": ["Luo2025Universe"], "paper_id": "P0190", "evidence_id": "E-P0190-3c65d38a2a", "pointer": "papers/paper_notes.jsonl:paper_id=P0190#limitations[1]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0208", "evidence_id": "E-P0208-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0208#key_results[1]"}, {"hook_type": "quant", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Ar", "citations": ["Kale2025Reliable"], "paper_id": "P0212", "evidence_id": "E-P0212-e0345118bc", "pointer": "papers/paper_notes.jsonl:paper_id=P0212#key_results[0]"}, {"hook_type": "quant", "text": "We then systematically curate a high-quality dataset for function calling, which we use to fine-tune two small language models, TinyAgent-1.1B and 7B.", "citations": ["Erdogan2024Tinyagent"], "paper_id": "P0096", "evidence_id": "E-P0096-4fca49d200", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}, {"hook_type": "quant", "text": "Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs.", "citations": ["Bonagiri2025Check"], "paper_id": "P0152", "evidence_id": "E-P0152-7edb91824f", "pointer": "papers/paper_notes.jsonl:paper_id=P0152#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution.", "citations": ["Mo2025Attractive"], "paper_id": "P0136", "evidence_id": "E-P0136-a0b404d928", "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]"}, {"hook_type": "eval", "text": "However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings.", "citations": ["Li2026Toolprmbench"], "paper_id": "P0108", "evidence_id": "E-P0108-3e2edc05cd", "pointer": "papers/paper_notes.jsonl:paper_id=P0108#key_results[0]"}, {"hook_type": "quant", "text": "MVVM introduces two key innovations: (1) a two-way sandboxing framework leveraging hardware enclaves and accelerator extensions that protects both the agent from malicious hosts and the host from compromised agents; (2) an efficient cross platform migration mechanism using WebAss", "citations": ["Yang2024Mvvm"], "paper_id": "P0270", "evidence_id": "E-P0270-dda84e50a5", "pointer": "papers/paper_notes.jsonl:paper_id=P0270#key_results[1]"}], "generated_at": "2026-02-07T19:55:34", "section_id": "6", "section_title": "Evaluation & Risks"}
