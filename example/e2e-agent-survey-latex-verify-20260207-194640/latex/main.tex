\documentclass[a4paper,11pt]{article}

\usepackage{fontspec}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{array}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\usepackage[numbers]{natbib}
\usepackage{url}

\title{agent latex survey}
\author{}
\date{\today}

\setlist[itemize]{noitemsep,topsep=0.25em,leftmargin=*}
\setcounter{tocdepth}{2}

\begin{document}
\maketitle

\tableofcontents
\newpage

\begin{abstract}


Tool-using large language model (LLM) agents are increasingly deployed as closed-loop systems that plan, act through external tools, and adapt to feedback in dynamic environments. Their reported gains are often difficult to interpret in isolation because results conflate the agent loop, the tool/interface contract, budget assumptions (steps, cost, latency), and threat models for tool misuse. This survey organizes the design space around (i) foundations and interfaces, (ii) core components for long-horizon behavior (planning and memory), and (iii) learning, coordination, evaluation, and risks. We synthesize representative patterns such as reasoning-action prompting, tool-call induction, reflection, and deliberate search, and connect them to benchmark suites and protocol choices that determine which comparisons are meaningful. Across these lenses, we highlight recurring failure modes - prompt injection, tool abuse, and evaluation leakage - and outline a protocol-aware checklist for reporting and auditing agent results. \citep{Yao2022React,Schick2023Toolformer,Shinn2023Reflexion,Yao2023Tree,Mohammadi2025Evaluation,Fu2025Eval}

\end{abstract}

\section{Introduction}


Tool-using LLM agents turn language models from passive generators into controllers of action: they decide what to do next, invoke external tools, read observations, and iterate until a task terminates. This closed-loop view explains both the recent leap in practical capability and the persistent confusion in interpreting results: improvements can come from better prompting, better tools, better interface contracts, or simply different budgets and stopping rules. Canonical agent patterns such as reasoning-action prompting, tool-call induction, reflection, and deliberate search already illustrate this entanglement, even when evaluated on the same interactive tasks. \citep{Yao2022React,Schick2023Toolformer,Shinn2023Reflexion,Yao2023Tree}

An operational definition used throughout this paper is: an agent is a system that (i) maintains state, (ii) selects actions in an action space that includes tool/API calls or environment operations, and (iii) updates state from observations to continue acting under an explicit protocol. This definition separates agent behavior from one-shot tool augmentation (e.g., a single retrieval call) and focuses attention on what can fail in practice: action representation, observation fidelity, and recovery strategies when tools are unreliable. Work on embodied or long-horizon settings further highlights that agents accumulate skills and abstractions over time, making the interface between the model, memory, and tools a first-class design choice rather than an implementation detail. \citep{Wang2023Voyager,Du2024Anytool,Song2025Agent}

The first organizing axis is the interface contract: what tools exist, how they are described, what arguments are legal, what the tool returns, and what permissions and logging surround execution. Two systems can share the same base model but behave very differently if one has a stable schema with sandboxed execution while the other relies on underspecified natural-language tool descriptions. Recent benchmark and security analyses show that interface design is inseparable from reliability and risk, including prompt injection pathways and tool-side manipulation that do not appear in text-only evaluations. \citep{Liu2025Mcpagentbench,Gasmi2025Bridging,Mou2026Toolsafe,Zhang2025Security}

The second axis is the agent loop itself: how decisions are formed and revised as new evidence arrives. Planning and reasoning loops control when an agent commits to an action versus exploring alternatives, and they interact strongly with budget constraints (latency, cost, step limits) that are often underreported. A practical implication is that protocol-aware comparisons should treat planning as an algorithmic component with an evaluation footprint, not merely as a prompt style, because changes in search breadth or tool-call frequency can dominate measured outcomes. \citep{Yao2023Tree,Shang2024Agentsquare,Gao2024Efficient,Nakano2025Guided}

Memory and retrieval provide the third axis: what information an agent can condition on, how it is selected, and how it is verified. Retrieval-augmented agents can appear strong on benchmarks where fresh evidence is available, yet fragile when retrieval introduces spurious citations, stale context, or adversarial content. A recurring theme across agent systems is that memory modules are not interchangeable; their failure modes depend on indexing choices, update policies, and how retrieval results are integrated into the loop (as constraints, as suggestions, or as authoritative facts). \citep{Shi2025Progent,Abbineni2025Muallm,Li2025Agentswift,Dong2025Etom}

Learning and adaptation mechanisms add another layer of coupling between interface and behavior. Reflection and self-improvement loops can change not only the final answer but also tool usage patterns (which tools are called, how often, and with what arguments), complicating attribution when improvements are measured as task success alone. Multi-agent variants further amplify these effects: role specialization and communication protocols introduce new degrees of freedom, while aggregation rules can hide individual agent failures behind ensemble outcomes. \citep{Shinn2023Reflexion,Zhou2025Self,Feng2025Group,Lu2025Just,Lumer2025Memtool}

Evaluation is therefore not just a downstream reporting step; it defines what claims are meaningful. Benchmark suites for tool-use agents and agentic systems differ in task families, scoring rules, interaction formats, and threat models, and comparisons are often brittle unless protocol fields (tool access, budgets, termination, and safety constraints) are normalized. Recent evaluations emphasize this protocol gap by cataloging benchmark landscapes and by introducing suites that stress realistic tool definitions, industrial workflows, and adversarial settings. \citep{Mohammadi2025Evaluation,Chowa2025From,Nandi2025Bench,Yin2024Mmau,Fu2025Eval}

Security and governance constraints are increasingly central because the action space itself creates new attack surfaces: prompt injection, data exfiltration through tools, and deceptive tool outputs can break the implicit assumptions behind standard success metrics. Defenses often trade off robustness and utility, and the threat model (what the adversary controls, what the agent can call, what is logged) changes the interpretation of any reported improvement. A key motivation for a protocol-aware survey is to connect these risk considerations to the same interface and loop choices that drive performance. \citep{Gasmi2025Bridging,Zhong2025Rtbas,Alizadeh2025Simple,Kale2025Reliable,Mo2025Attractive}

Methodology. We retrieved a candidate pool of 1,800 papers and curated a 300-paper core set for synthesis; unless otherwise noted, evidence is drawn from abstracts and metadata rather than full-text extraction. This evidence level is sufficient to map the design space and protocols, but it can underrepresent implementation-specific details; accordingly, the survey emphasizes protocol fields (interfaces, budgets, threat models) that are typically stated explicitly, and it flags where deeper verification is most likely to change interpretation. \citep{Plaat2025Agentic,Li2024Review,Guo2025Comprehensive,Yang2025Survey}

The remainder of the paper follows the above lenses. Section 3 frames foundations and interfaces by separating the agent loop from the action space and the tool contract. Section 4 focuses on core components for long-horizon behavior, with planning and memory treated as protocol-sensitive modules. Section 5 covers learning, adaptation, and multi-agent coordination as mechanisms that alter both behavior and tool usage. Section 6 surveys evaluation and risks, emphasizing benchmark design and security threat models. Appendix tables summarize representative loop patterns and evaluation settings to make protocol assumptions explicit. \citep{Yao2022React,Schick2023Toolformer,Mohammadi2025Evaluation,Zhang2025Security}

\section{Related Work}


Related work for LLM agents spans three overlapping categories: (i) surveys and taxonomies that map the broader agentic landscape, (ii) system and method papers that define agent loops, tool interfaces, planning, and memory, and (iii) evaluation and security work that establishes benchmarks, protocols, and threat models. A recurring limitation across these streams is that comparisons are often presented without fully normalizing protocol fields such as tool access, budgets, and safety constraints, which makes head-to-head interpretation fragile even when papers appear to address similar tasks. \citep{Mohammadi2025Evaluation,Chowa2025From,Fu2025Eval}

Surveys of agentic LLMs provide useful overviews of common architectures and application domains, including tool use, planning, feedback learning, and coordination. They typically emphasize the breadth of agent behaviors and the rapid growth of systems, but they vary in how explicitly they represent protocol assumptions and how tightly they connect evaluation results to interface contracts. The present work builds on these mappings and focuses on making protocol fields and interface assumptions explicit as the primary lens for synthesis. \citep{Plaat2025Agentic,Li2024Review,Guo2025Comprehensive,Yang2025Survey,Mohammadi2025Evaluation}

Foundational method papers define several reusable agent loop templates that continue to anchor system design. Reasoning-action prompting formalizes iterative decision making with tool calls, while tool-call induction and reflection-based loops treat tool usage and revision as learnable behaviors rather than fixed prompting conventions. Deliberate search approaches frame planning as exploration in a space of intermediate reasoning states, which interacts strongly with budgets and stopping criteria in interactive tasks. These lines of work motivate treating the loop design and interface contract as coupled objects when comparing agent performance. \citep{Yao2022React,Schick2023Toolformer,Shinn2023Reflexion,Yao2023Tree}

Tool interface and orchestration papers highlight that the action space is not merely a set of tools, but also a contract about schemas, permissions, and observability. Benchmarks built around realistic tool definitions and execution environments stress that tool-use evaluation must model failure recovery, argument correctness, and the interaction format between the agent and tools. Dataset-driven approaches further suggest that tool usage patterns can be improved via scale, but only when tools are standardized and the execution harness is stable enough to provide meaningful feedback signals. \citep{Liu2025Mcpagentbench,Nandi2025Bench,Yang2025Toolmind,Song2025Agent,Du2024Anytool}

Planning and reasoning for long-horizon agents is often studied through variants of guided decision making, search, and protocol-aware optimization. A common theme is that measured gains depend on how exploration is budgeted and how tool calls are scheduled relative to internal deliberation; as a result, work that reports improvements in efficiency, cost, or latency should be read jointly with the protocol constraints under which those improvements are achieved. This motivates distinguishing algorithmic changes in the planner from changes in the tool contract and execution environment. \citep{Yao2023Tree,Shang2024Agentsquare,Gao2024Efficient,Nakano2025Guided,Zhang2026Evoroute}

Memory and retrieval augmentations are frequently presented as interchangeable modules, but evaluation work suggests that retrieval policy and integration strategy materially affect both correctness and robustness. Systems that emphasize agent memory for tool use and browsing must confront citation and grounding issues, as well as the susceptibility of retrieval channels to injection and stale context. In practice, memory design also changes the agent loop: it determines what evidence can be used for planning and when the agent should re-query or revise. \citep{Shi2025Progent,Abbineni2025Muallm,Li2025Agentswift,Dong2025Etom,Maragheh2025Future}

Learning-based adaptation and self-improvement papers extend agents beyond static prompting by introducing feedback-driven revision, preference shaping, and evaluation-guided tuning. These mechanisms make it easier to report improvements on benchmark suites, but they also increase the risk of overfitting to narrow protocols or exploiting metric artifacts. As a consequence, adaptation results are best interpreted together with the evaluation setting, including which tools are accessible, how many steps are allowed, and what failure behaviors are penalized. \citep{Shinn2023Reflexion,Zhou2025Self,Li2026Autonomous,Zhang2026Evoroute}

Multi-agent coordination introduces additional protocol degrees of freedom: role definitions, communication formats, verification strategies, and aggregation rules. Related work covers both cooperative setups (task decomposition and planning) and adversarial or debate-style variants (verification and robustness), but comparisons are often inconsistent because messaging bandwidth, agent count, and stopping rules are not standardized. Empirical results therefore require careful reading of the interaction protocol and the evaluation harness rather than relying on high-level architectural labels alone. \citep{Feng2025Group,Lu2025Just,Lumer2025Memtool,Yang2024Based}

Evaluation-focused papers and benchmark suites increasingly treat tool-using agents as a distinct class from static LLMs. They catalog task families and protocols, propose new suites with industrial workflows or realistic tool schemas, and emphasize reproducibility concerns such as leakage, hidden prompts, and uncontrolled environment variability. These contributions are complementary to architecture-focused surveys: they provide the protocol vocabulary (tasks, metrics, budgets, threat models) needed to make cross-paper synthesis interpretable. \citep{Mohammadi2025Evaluation,Chowa2025From,Yin2024Mmau,Liu2025Mcpagentbench,Nandi2025Bench}

Security and governance work demonstrates that tool access expands the threat surface, and that standard success metrics can be misleading under attack. Analyses of prompt injection and tool abuse quantify attack success rates and privacy leakage, while guardrail and monitoring frameworks explore how to trade off robustness and benign utility under explicit adversarial models. These lines of work motivate incorporating threat models and safety constraints into the same protocol description used for performance evaluation, rather than treating security as a separate post-hoc concern. \citep{Zhang2025Security,Gasmi2025Bridging,Zhong2025Rtbas,Alizadeh2025Simple,Kale2025Reliable,Mou2026Toolsafe,Mo2025Attractive}

Recent work also treats agents as engineered systems, emphasizing execution sandboxes, tool-call datasets, and measurement harnesses that enable reproducible comparisons across models and interfaces. Examples include small-model function-calling pipelines and infrastructure-oriented evaluations, as well as system studies that surface how protocol choices (statefulness, time windows, interaction format) constrain external validity. Complementary system papers further stress that environment design and tracing affect reliability, from scaling interactive environments to specialized agent platforms and domain deployments. \citep{Erdogan2024Tinyagent,Zhang2025Datascibench,Kwon2025Agentnet,Cui2025Toward,Tomaevi2025Towards,Song2026Envscaler,Soliman2026Intagent,Kim2025Bridging,Fumero2025Cybersleuth,Ghose2025Orfs,Fang2025Should,Bonagiri2025Check,Wang2023Voyager}

Taken together, the above literature suggests that a survey of LLM agents benefits from a protocol-aware synthesis: interface contracts and budgets determine the meaning of reported numbers, and risk models determine whether gains persist outside benign settings. The organization adopted here reflects this perspective by grouping work around (i) interfaces and loop design, (ii) core components for planning and memory, (iii) adaptation and coordination mechanisms, and (iv) evaluation and risks, so that comparisons are made within compatible protocols and limitations remain visible rather than being smoothed away. \citep{Yao2022React,Zhang2026Evoroute,Mohammadi2025Evaluation,Fu2025Eval,Zhang2025Security}

\section{Foundations \& Interfaces}


Interfaces define what an agent can do, while the loop defines how reliably it does it. This chapter frames tool-using agents as closed-loop systems whose measured performance depends on action representations, tool schemas, and the execution boundary between the model and the environment. A practical consequence is that many apparent method differences collapse to interface and protocol differences once tool access, budgets, and observability are made explicit. \citep{Yao2022React} \citep{Liu2025Mcpagentbench,Chowa2025From}

The first subsection isolates the agent loop and action space: state representation, action granularity, recovery behavior, and how observations are fed back into decision making. The second subsection focuses on the tool contract itself - orchestration, permissions, sandboxing, and routing - because these choices determine both correctness and the dominant risk surface under deployment. Read together, the two subsections provide the "execution semantics" needed to interpret later comparisons in planning, memory, and adaptation. \citep{Gasmi2025Bridging,Mou2026Toolsafe}

\subsection{Agent loop and action spaces}


Agent loops are where "capability" becomes "behavior": they determine how a model turns an intent into a sequence of actions under a protocol. The central tension is that richer loops and action spaces can increase task success, but they also make results harder to verify because evaluation depends on hidden choices such as budgets, stopping rules, and what counts as a valid action or observation. A useful synthesis therefore treats the loop as an executable contract - not just a prompt style - and asks which parts of the contract are held fixed when numbers are reported. \citep{Yao2022React} \citep{Chowa2025From,Yin2024Mmau}

At a minimum, a tool-using agent loop instantiates a state -> decide -> act -> observe cycle, where "act" includes structured tool calls and "observe" includes tool outputs plus any environment feedback. The action space can be constrained (few tools with stable schemas) or expansive (many tools, open-ended arguments, dynamic tool discovery), and the choice affects both reliability and failure recovery. In practice, the same high-level loop label hides substantial variation in what is logged, what is remembered, and how errors are handled, which makes protocol-aware comparisons essential. Recent systems provide concrete instantiations of these loop and action-space choices under varied protocols. \citep{Kim2025Bridging} \citep{Liu2025Mcpagentbench,Zhang2024Ecoact} \citep{Zhuang2025Hephaestus,Zhu2025Agent,Yang2025Proagent,Miyamoto2026Agent,Kulkarni2025Agent,Tawosi2025Almas}

A first contrast is between loops that expand internal deliberation and loops that constrain external actions. More deliberation can reduce obvious mistakes, whereas tighter action representations (schemas, validators, or restricted tool sets) can reduce the probability of catastrophic tool misuse by narrowing what the agent can do. The literature increasingly mixes these approaches, but synthesis is clearest when the evaluation protocol states which constraints are enforced by the interface and which are learned or prompted behaviors. \citep{Zhang2024Ecoact} \citep{Liu2025Mcpagentbench,Cui2025Toward}

Benchmark landscapes already reflect this protocol dependence. One survey of agent evaluation reports 68 publicly available datasets spanning diverse tasks, which highlights how quickly evaluation has broadened without converging on a standard protocol vocabulary. At the same time, aggregate suites aim to compare agents across task families, but these comparisons remain brittle unless budgets and interaction formats are aligned. The practical takeaway is that "agent loop" claims should be read together with the benchmark and the scoring rule, not in isolation. \citep{Chowa2025From} \citep{Yin2024Mmau,Cui2025Toward}

Concrete numbers illustrate why loop semantics matter. On two interactive decision-making benchmarks (ALFWorld and WebShop), ReAct-style reasoning-action prompting is reported to outperform imitation and reinforcement learning baselines by 34\% and 10\% absolute success rate, respectively, while using only one or two in-context examples. These gains are meaningful, but they are also protocol-specific: the action interface and termination criteria define what "success" means and how much exploration is allowed. \citep{Yao2022React,Yin2024Mmau}

Loop design is also inseparable from security because the action space creates an attack surface. For example, an analysis comparing function calling and MCP-style tool interfaces reports higher overall attack success rates for function calling (73.5\% vs 62.59\% for MCP), with different exposure patterns depending on whether the vulnerability is system-centric or model-centric. Such results imply that the same planner can appear robust or fragile depending on interface constraints and monitoring; in contrast, analyses that ignore the interface contract tend to attribute these differences to the planner or model alone. Loop comparisons should therefore include a threat model and the corresponding policy/sandbox setting. \citep{Gasmi2025Bridging} \citep{Liu2025Mcpagentbench,Fang2025Should}

Efficiency and cost claims further amplify protocol sensitivity. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ report that EvoRoute, when integrated into off-the-shelf agentic systems, can sustain or improve task performance while reducing execution cost by up to 80\% and latency by over 70\%. These improvements are best interpreted as properties of the full loop (routing policy + tool schedule + termination) under the benchmark's interaction format, rather than as model-only gains. \citep{Zhang2026Evoroute,Chowa2025From}

Training data and supervision choices can shift loop behavior even when the interface is fixed. One large-scale tool-agent dataset reports an average performance gain of about 20\% over corresponding base models after supervised fine-tuning, improving results across coding, browsing, and tool-use benchmarks without domain-specific tuning. This suggests that the "decide" step can be made more reliable via scale, but it also raises comparability questions: the resulting loop may call tools more or less often, changing cost and failure modes even when success rates improve. \citep{Song2025Agent,Zhang2025Datascibench}

Several limitations recur across loop evaluations and matter for how conclusions should transfer. Stateless agent designs can inflate apparent robustness by ignoring long-term drift, while short evaluation windows and single-run studies limit variance estimates and external validity. In addition, domain-specific agent deployments can exhibit strong gains on narrow protocols but unclear generality when tool access or environment dynamics change. These are not reasons to dismiss results; they are reasons to surface protocol fields explicitly so that later work can reproduce and stress-test the loop semantics. \citep{Tomaevi2025Towards} \citep{Ghose2025Orfs,Fumero2025Cybersleuth}

Taken together, the most stable way to compare agent loops is to treat the action space and protocol as the object of study: what actions are admissible, how observations are represented, what budgets apply, and what threats are modeled. This framing naturally motivates the next subsection on tool interfaces and orchestration, because the interface contract defines the "physics" of the action space that every loop - regardless of planner sophistication - must obey. \citep{Liu2025Mcpagentbench} \citep{Zhang2026Evoroute,Yang2025Survey}

If loop design determines which actions an agent can attempt, the tool/interface contract determines how those actions are encoded, constrained, and audited during execution.

\subsection{Tool interfaces and orchestration}


Tool interfaces are the boundary between a model's intent and executable actions, and orchestration is the policy that decides how to cross that boundary repeatedly. The practical trade-off is expressivity versus control: richer interfaces expand what an agent can attempt, but they also widen the space of failure modes and make behavior harder to constrain and verify. As a result, comparisons between tool-using agents are most interpretable when they treat the interface contract (schemas, permissions, observability) as part of the method rather than as an incidental engineering choice. \citep{Mohammadi2025Evaluation} \citep{Yao2022React,Chowa2025From}

Interface design starts with how actions are represented. Function-calling schemas and API specifications enable structured arguments and post-hoc validation, while natural-language tool descriptions favor flexibility but shift error detection into prompting or downstream checks. Orchestration then layers routing and control flow on top of these representations: selecting tools, handling retries, and deciding when to terminate under step and latency budgets. Even when two agents share the same base model, these interface and orchestration choices can dominate reliability. Recent systems explore these design choices across diverse tool contracts and budget regimes. \citep{Dong2025Etom} \citep{Liu2025Toolscope,Cui2025Toward} \citep{Fu2024Imprompter,Jeon2025Based,Gupta2024Codenav,Hao2026From,Wu2024Avatar,Yin2025Magnet}

A key contrast is between schema-first interfaces that make invalid actions unrepresentable and prompt-first interfaces that rely on the model to stay within an implicit contract. Schema-first designs can reduce argument errors and improve auditability, whereas prompt-first designs often require additional guardrails to prevent tool misuse and to make failures observable. In practice, hybrid approaches are common, but synthesis benefits from naming which parts of the contract are enforced by tooling (validators, sandboxes) and which are left to the model's behavior. \citep{Li2026Toolprmbench} \citep{Shen2024Small,Doshi2026Towards}

Benchmarks increasingly encode this distinction by making the tool contract explicit. SOP-Bench, for example, operationalizes tool-use evaluation as end-to-end workflows: it contains over 1,800 tasks across 10 industrial domains, with APIs, tool interfaces, and human-validated test cases. This style of benchmark shifts attention from single-call correctness to orchestration reliability under realistic interface constraints. \citep{Nandi2025Bench,Mohammadi2025Evaluation}

Data resources further emphasize that tool interfaces are learnable behaviors when a stable contract exists. ToolMind introduces a large-scale tool-agent dataset with 160k synthetic instances generated over many tool types, aiming to improve tool selection and execution through supervised training. Such datasets can raise benchmark performance, but they also increase the importance of protocol reporting: tool availability, schema versions, and execution environment details can change what the model is actually learning to do. \citep{Yang2025Toolmind,Cui2025Toward}

Orchestration methods can be viewed as policies over a tool graph: they decide which tool to call next, how to sequence tools, and how to recover from partial failures. Automation-oriented systems focus on discovering tools, composing them, and learning routing heuristics; in contrast, evaluation-oriented systems focus on measuring tool correctness, failure recovery, and budget sensitivity under fixed protocols. This distinction matters because an orchestration improvement that reduces calls under one harness can look like a planning improvement even when it is primarily a routing or caching change. \citep{Jia2025Autotool} \citep{Cheng2025Your,Gao2025Radar}

The evaluation literature reinforces that interface details must be represented in the protocol, not inferred from narrative descriptions. Surveys and core benchmark studies propose taxonomies for agent evaluation, but they also expose how heterogeneous current reporting is, especially for tool access assumptions and budget constraints. A protocol-aware perspective therefore treats tool interface fields - schemas, permissions, sandboxing, observability - as the minimal metadata required to make cross-paper comparisons meaningful. \citep{Mohammadi2025Evaluation} \citep{Michelakis2025Core,Chowa2025From,Cui2025Toward}

Tool interfaces also interact with coordination and state: orchestration is not only about single-agent routing but also about managing shared resources, memory, and multi-agent communication under constraints. For instance, memory-augmented tooling and coordination frameworks often change the effective action space by adding "retrieve" or "delegate" as first-class operations, which can improve task completion but can also obscure where errors originate (model reasoning, tool outputs, or orchestration policy). \citep{Lumer2025Memtool,Liu2025Toolscope}

Several limitations recur in interface and orchestration research. Expressive tool contracts can amplify prompt injection and tool-side manipulation if permissions and monitoring are weak, while overly restrictive contracts can reduce capability by preventing the agent from taking corrective actions. Moreover, benchmarks may underrepresent long-tail tool failures (timeouts, partial outputs, schema drift), so reported gains can be sensitive to the specific harness design. These caveats motivate evaluation protocols that stress tool realism and that report interface fields as first-class variables. \citep{Zhou2026Beyond} \citep{Ferrag2025From,Doshi2026Towards,Li2026Toolprmbench}

In practice, tool interfaces and orchestration define the operational semantics that later components - planners, memory modules, and adaptation mechanisms - must work within. Once the interface contract is fixed, planning and memory choices become easier to compare because the action space and observation format are stable; when the contract shifts, many "algorithmic" claims reduce to protocol differences. This motivates the next chapter, which treats planning and memory as components whose behavior is only interpretable under explicit interface and budget assumptions. \citep{Yao2022React,Mohammadi2025Evaluation}

\section{Core Components (Planning + Memory)}


Once interfaces define the action space, the next bottleneck is how agents choose actions over time under uncertainty and budget constraints. This chapter treats planning and memory as protocol-sensitive components: reported gains often depend as much on search breadth, tool-call scheduling, and termination rules as on the underlying model. Efficient variants therefore need to be interpreted jointly with the evaluation protocol (task format, step limits, latency/cost budgets) used to measure them. \citep{Yao2023Tree} \citep{Zhang2026Evoroute,Mohammadi2025Evaluation}

The planning subsection emphasizes control loops and deliberation: when agents branch, how they score alternatives, and how plans are revised after tool feedback. The memory subsection focuses on what evidence the agent can condition on - retrieval policy, write/update rules, and how retrieved context is verified - because memory modules can shift both correctness and robustness without changing the nominal planner. Together, these components explain why "stronger reasoning" claims frequently hinge on grounded state and comparable protocols rather than prompt wording alone. \citep{Shi2025Progent} \citep{Abbineni2025Muallm,Li2025Agentswift}

\subsection{Planning and reasoning loops}


Planning and reasoning loops determine how an agent turns observations into a sequence of actions under uncertainty. The recurring tension is deliberation depth versus cost: deeper planning can improve reliability, but it increases latency, step budgets, and sensitivity to protocol choices such as termination and tool access. As a result, planning papers are best compared as control-loop designs evaluated under explicit budgets, rather than as purely "better reasoning" claims detached from the execution harness. \citep{Hong2025Planning} \citep{Zhou2025Reasoning,Hatalis2025Review}

At a high level, planning loops instantiate a control architecture (planner, executor, critic, verifier) and decide when to branch, when to commit, and how to revise after tool feedback. Some approaches emphasize lightweight iteration with frequent tool calls, while others allocate more budget to internal deliberation before acting. These design choices change not only task success but also the distribution of failures (wrong tool choice, brittle step ordering, or compounding errors), which motivates protocol-aware reporting of step limits, retry rules, and observation formats. Recent planning systems instantiate different trade-offs between deliberation, tool-call scheduling, and revision under constraint. \citep{Silva2025Agents} \citep{Baker2025Larc,Radha2024Iteration} \citep{Hu2025Training,Rawat2025Multi,Shi2024Ehragent,Zhang2025Multimind,Koubaa2025Agentic,Zhao2025Autonomous}

A useful contrast is between reactive loops that act early and revise often, and planning-heavy loops that explore alternatives before committing. Reactive loops can be efficient when tools provide strong signals, whereas planning-heavy loops can be more robust when early actions are costly or irreversible. However, the two families can look identical in aggregate success metrics if their budgets differ; comparisons therefore need to normalize for latency/cost and to report how much of the budget is spent on deliberation versus tool execution. \citep{Yao2022React} \citep{Choi2025Reactree,Gao2024Efficient}

Concrete evaluations illustrate how domain constraints shape planning design. One penetration-testing agent is evaluated using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and is applied to navigate 10 target websites, making the interaction protocol (what is observable, what actions are permitted) central to interpreting its reported effectiveness. In such settings, planning quality is inseparable from the environment interface and from how failures are detected and recovered. \citep{Nakano2025Guided,Zhou2025Reasoning}

Efficiency-oriented results further highlight budget sensitivity. In one study, agents trained with the proposed method show more efficient tool use, with inference speed reported to be about 1.4x faster than baseline tool-augmented LLMs on the evaluated tasks. These gains are valuable for deployment, but they should be read as properties of the full planning loop under a specific harness, because changes in tool-call scheduling or caching can dominate latency without changing the underlying model. \citep{Gao2024Efficient,Hong2025Planning}

Benchmark suites that span multiple environments help stress-test whether planning behaviors transfer. AgentSquare reports experiments across six benchmarks covering web, embodied, tool-use, and game scenarios, which makes protocol alignment (task format, scoring, budgets) a prerequisite for synthesis. Multi-benchmark reporting is an improvement over single-task demos, yet head-to-head comparison remains fragile when different papers implicitly use different termination rules or tool access assumptions. \citep{Shang2024Agentsquare,Huang2024Understanding}

Realistic evaluation targets also motivate planning loops that can handle hard, tool-rich tasks. MCP-Universe is introduced as a benchmark designed to evaluate LLMs in realistic and difficult settings, shifting attention toward robust planning under complex tool interfaces rather than narrow prompt-chasing. Such benchmarks make it easier to identify which planning design choices are stable under protocol variation, because they force explicit specification of tools, observations, and failure recovery. \citep{Luo2025Universe,Zhou2025Siraj}

Testing and measurement methodology matters because planning loops can overfit to interaction quirks. Work on agent testing and diagnostic benchmarks emphasizes that a single aggregate score can hide failure modes such as brittle tool sequencing, hidden leakage, or reliance on privileged hints. Protocol-aware evaluation therefore benefits from structured test generation, ablations that isolate planning components, and reporting that separates model improvements from orchestration artifacts. \citep{Ji2024Testing} \citep{Qin2025Compass,Mudur2025Feabench}

Several limitations recur across planning papers. Planning-heavy loops can degrade when observations are noisy or adversarial, while reactive loops can fail catastrophically when early actions have high downside. Moreover, planning comparisons are often under-identified: if budgets, tool availability, and retry policies are not matched, it is unclear whether reported gains reflect better control logic or simply more compute. These caveats motivate explicit protocol fields and controlled comparisons, especially when claims target general agent capability rather than a single benchmark. \citep{Huang2024Understanding} \citep{Zeng2025Rejump,Zhao2024Lightva}

In sum, planning and reasoning loops are best synthesized as control policies operating under a protocol: they trade off reliability, cost, and robustness depending on how much evidence is available and how expensive actions are. This perspective naturally connects to memory and retrieval, because planning quality depends on what state and evidence are available for decision making; the next subsection therefore treats memory as a component that changes what planning can reliably condition on. \citep{Li2024Personal} \citep{Wu2025Agentic,Shang2024Agentsquare}

Planning quality depends on what state and evidence are available at decision time, so memory and retrieval become the mechanism that makes long-horizon deliberation reliable under budget.

\subsection{Memory and retrieval (RAG)}


Memory and retrieval determine what evidence an agent can condition on and how long-horizon state is represented across steps. The core tension is persistence versus freshness: retaining more context can improve performance on multi-step tasks, but it also raises staleness, contamination, and verification challenges. Because retrieval is itself an action in the loop, memory design must be compared under explicit protocols - what can be retrieved, how often, and how retrieved content is validated - rather than being treated as a plug-in component. \citep{Shi2025Progent} \citep{Huang2025Retrieval,Yao2025Survey}

In tool-using agents, memory spans several layers: short-term scratchpads, episodic traces of prior actions, and long-term stores indexed by text, structure, or learned embeddings. Retrieval-augmented generation (RAG) is a common mechanism, but its behavior depends on indexing, query formation, and integration strategy (constraints vs suggestions vs authoritative facts). These choices shape both correctness and robustness, especially when tools can write to or read from the same memory substrate. Recent memory designs illustrate a wide range of retrieval policies and memory substrates, which is why validation and write governance matter. \citep{Verma2026Active} \citep{Peng2026Enhancing,Hu2023Avis} \citep{Zhu2025Where,Wang2025Dsmentor,Zhang2024Large,Li2025Graphcodeagent,Wu2025Meta,Li2025Encouraging}

A useful contrast is between retrieval-as-context and retrieval-as-control. Retrieval-as-context treats retrieved content as additional conditioning information, whereas retrieval-as-control uses retrieved items to constrain action selection (e.g., which tool to call or which plan branch to pursue). However, the two can be indistinguishable in aggregate scores if verification is weak; protocol-aware evaluation should therefore report how retrieval is validated (citation checks, self-consistency, or external verification) and how memory writes are governed. \citep{Yu2026Agentic} \citep{Zhang2025Large,Kim2025Bridging}

Concrete benchmark design makes these distinctions measurable. MuaLLM, for example, introduces two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100, focused on multistep reasoning. This formulation clarifies what the memory module is expected to provide (relevant evidence and grounded citations) and what counts as a failure mode (hallucinated support, stale context, or brittle query behavior). \citep{Abbineni2025Muallm,Shi2025Progent}

Compression-oriented memory systems highlight a different trade-off: retaining more context often requires summarization or distillation. Meta-RAG reports condensing codebases by an average of 79.8\%, which can make long contexts tractable for agents operating over large repositories. Yet compression changes the evidence surface: in contrast to raw retrieval, summaries can introduce abstraction errors that are hard to detect without explicit verification steps, so evaluation protocols should separate retrieval quality from summarization fidelity. \citep{Tawosi2025Meta,Huang2025Retrieval}

Multi-benchmark evaluations also stress that memory benefits are domain-dependent. AgentSwift is evaluated across seven benchmarks spanning embodied, math, web, tool, and game domains, and reports average gains across this diverse suite. Such breadth improves external validity, but synthesis still requires protocol alignment: different benchmarks expose different observation noise and different opportunities for retrieval to help or harm. \citep{Li2025Agentswift,Shang2024Agentsquare}

Retrieval policy is increasingly treated as an object of study rather than a default setting. Survey and analysis work emphasizes that retrieval frequency, query selection, and ranking criteria can materially change agent behavior under the same planner. As a result, comparisons between memory-augmented agents should report retrieval budgets and the interface between retrieval and tool calls (e.g., whether retrieval results are treated as tool outputs with their own trust and permission model). \citep{Li2024Review} \citep{Yao2025Survey,Dong2025Etom}

Memory also interacts with planning: planning quality depends on what state is reliably available at decision time, and memory determines which evidence is accessible under a budget. Systems that combine planning loops with retrieval can appear stronger simply because they re-query more often, whereas systems that write structured traces can reduce rework at the cost of accumulating stale assumptions. Protocol-aware evaluation therefore benefits from tracking memory write policies and from measuring how often plans are revised due to retrieved evidence. \citep{Hong2025Planning} \citep{Ye2025Task,Ye2025Taska}

Several limitations and risks recur in memory-based agents. Retrieval channels can carry adversarial content or prompt injections, and long-lived memories can encode false premises that persist across tasks. In addition, some benchmarks under-specify what counts as permissible evidence, making it unclear whether retrieval gains reflect better grounding or leakage through the environment. These issues motivate explicit threat models and verification steps for memory, especially in settings where memory can influence tool invocation. \citep{Maragheh2025Future} \citep{Kim2025Bridging,Hu2023Avis}

More broadly, memory and retrieval are most useful when treated as protocolized components: they expose a budgeted evidence interface to the planner and must be evaluated for both correctness and robustness. This framing connects naturally to learning and adaptation, because feedback loops often operate on the same memory traces and can amplify both improvements and failures over time. \citep{Peng2026Enhancing} \citep{Du2025Survey,Min2025Goat}

\section{Learning, Adaptation \& Coordination}


Adaptation mechanisms change the agent over time, while coordination mechanisms change the effective agent by distributing cognition across roles. This chapter organizes learning and multi-agent systems around the feedback signals and interaction protocols they assume: what counts as success, how errors are detected, how revisions are applied, and how multiple agents reconcile disagreements. These assumptions directly shape both benchmark outcomes and failure modes such as reward hacking, overfitting to narrow protocols, and brittle aggregation. \citep{Shinn2023Reflexion} \citep{Zhou2025Self,Mohammadi2025Evaluation}

The self-improvement subsection examines critique and revision loops and their stability under repeated interaction, focusing on what is learned (prompts, policies, tool usage) and under which evaluation constraints. The multi-agent subsection covers role specialization and communication/aggregation rules, highlighting that coordination gains are inseparable from protocol choices such as message bandwidth, agent count, and stopping conditions. Read together, they motivate reporting feedback and coordination protocols as first-class metadata when claiming generality. \citep{Feng2025Group} \citep{Lu2025Just,Lumer2025Memtool}

\subsection{Self-improvement and adaptation}


Self-improvement makes agents dynamic systems: they change their behavior based on feedback, experience, or evaluation signals. A core trade-off is adaptability versus stability - systems that revise themselves can improve over time, but they also risk drifting, overfitting to narrow protocols, or becoming harder to evaluate and control. A protocol-aware synthesis therefore asks not only whether performance increases, but also what is being updated (prompts, policies, tool usage) and under which constraints the update is valid. \citep{Zhou2025Self} \citep{Du2025Survey,Van2025Survey}

Adaptation mechanisms span several families. Reflection and critique loops revise intermediate reasoning or tool plans after failures; preference- or reward-driven methods shape behavior via feedback signals; and evaluation-driven tuning uses benchmark performance as an optimization target. These approaches differ in how tightly they bind to an evaluation harness: some treat the harness as a learning environment, whereas others treat it as a diagnostic that informs manual or automated revisions. Recent systems operationalize adaptation via different feedback signals and update targets, making constraints part of the claim. \citep{Chen2025Grounded} \citep{Belle2025Agents,Hatalis2025Review} \citep{Yu2025Infiagent,GendreauDistler2025Automating,Yang2025Bioverge,Samaei2025Epidemiqs,Yano2025Lamdagent,Zhang2024Autocoderover}

A useful contrast is between updates that are explicitly constrained by a protocol and updates that implicitly absorb protocol quirks. Constrained updates can improve reliability under matched tool access and budgets, whereas unconstrained updates may pick up brittle shortcuts that do not transfer when tools, prompts, or termination rules change. In practice, many adaptation results should be interpreted as conditional on the training/evaluation setting, and reporting should make those conditions explicit to avoid overclaiming. \citep{Yao2025Agents} \citep{Li2025Learn,Yin2024Mmau}

Concrete evaluations show that adaptation can deliver large gains on tool-use benchmarks. On two multi-turn tool-use agent benchmarks (M3ToolEval and TauBench), the Self-Challenging framework reports over a two-fold improvement, illustrating that feedback and revision policies can be as important as base-model capability for interactive success. These results also highlight why protocol fields matter: the same revision policy can trade off cost, latency, and robustness depending on how tool calls and retries are counted. \citep{Zhou2025Self,Guo2025Comprehensive}

Benchmark coverage has expanded rapidly, and this growth changes what "improvement" means. A comprehensive survey connects 50+ benchmarks to solution strategies, underscoring that adaptation can be measured across heterogeneous task families with different scoring rules and interaction formats. This heterogeneity motivates a conservative stance: gains on a single suite should be treated as evidence about that suite's protocol, not as a blanket statement about agent capability. \citep{Guo2025Comprehensive} \citep{Yin2024Mmau,Du2025Survey}

Foundational agent-loop results also motivate adaptation as a response to long-horizon brittleness. ReAct-style agents show large success-rate gains on interactive tasks (e.g., ALFWorld and WebShop), but they still exhibit failure modes such as premature commitment and error compounding. Adaptation mechanisms can be viewed as attempts to correct these failure modes online, yet the magnitude of benefit depends on whether the evaluation protocol rewards cautious verification or merely short-horizon completion. \citep{Yao2022React,Zhou2025Self}

Data and tooling can act as implicit adaptation channels by changing what the agent learns to do with tools. Tool-use training resources and structured supervision can improve tool selection and execution, but they can also shift behavior toward the idiosyncrasies of a particular tool contract or benchmark harness. In contrast to pure prompting, training-based adaptation should therefore report the tool definitions, access constraints, and the distribution of tasks used for supervision. \citep{Du2024Anytool} \citep{Li2025Learn,Peng2026Enhancing}

Several method lines explore how to make adaptation more stable and controllable. Some approaches emphasize bounded revision policies and explicit failure triggers, while others focus on iterative improvement over longer horizons or on domain-specific adaptation where evaluation is more structured. These designs often trade off sample efficiency and reliability: aggressive updates can improve quickly but may destabilize behavior, whereas conservative updates may be slower but easier to audit. \citep{Zhou2024Archer} \citep{Wu2025Evolver,Xia2025Sand,He2025Enabling}

Limitations and risks are central for self-improving agents. Adaptation can exploit metric artifacts, reduce interpretability of the decision policy, or create reward-hacking behaviors when feedback signals are misaligned. In addition, ethical and governance constraints can matter in practice: when agents are optimized for performance, it becomes unclear how to bound behavior in sensitive settings without explicit constraints and monitoring. These caveats argue for reporting stability diagnostics and for stress-testing adaptation under shifts in tools, prompts, and threat models. \citep{Tennant2024Moral} \citep{Du2025Survey,Alizadeh2025Simple}

A practical implication is that self-improvement should be evaluated as a controlled process: what feedback is used, what is updated, and how improvements trade off robustness, cost, and safety under the stated protocol. This framing also sets up multi-agent coordination, where adaptation can be distributed across roles and where stability depends on communication and aggregation rules as much as on individual agent learning. \citep{Li2026Autonomous} \citep{Van2025Survey,Zhang2026Evoroute}

Once behavior can be updated from feedback, the same stability question reappears at the team level, where coordination protocols determine how roles share evidence, verify claims, and avoid correlated drift.

\subsection{Multi-agent coordination}


Multi-agent coordination treats an "agent" as a team: capability comes from specialization and verification across roles rather than from a single loop. A central trade-off is specialization versus coordination - dividing labor can boost performance, but it adds communication overhead, introduces new failure modes, and makes evaluation protocol choices (agent count, message budget, stopping rules) decisive. A protocol-aware synthesis therefore focuses on interaction contracts: what roles exist, how information is exchanged, and how disagreements are resolved. \citep{Yang2024Based} \citep{Sarkar2025Survey,Yim2024Evaluating}

Coordination patterns span several families. Some systems use role specialization for decomposition (planner, executor, reviewer), others use debate or referee mechanisms for verification, and others use aggregation (vote, rank, consensus) to stabilize noisy decisions. These patterns differ in what they assume about observability and trust: coordination can amplify errors if agents share the same flawed evidence, whereas diversity can help when independent perspectives are available. Recent multi-agent systems instantiate these coordination patterns across roles, message budgets, and aggregation rules. \citep{Cao2025Skyrl} \citep{Oueslati2025Refagent,Zhang2025Bridging} \citep{Luo2025Large,Chen2025Remsa,Shi2025Youtu,Ji2025Tree,Du2024Text2Bim,Yu2025Infiagent}

A useful contrast is between centralized orchestration and decentralized coordination. Centralized orchestration can impose consistent tool access and enforce budgets, whereas decentralized setups may be more flexible but can suffer from runaway communication and unstable convergence. However, the two are often compared under different protocols; careful evaluation therefore needs to report message budgets, aggregation rules, and whether agents share memory or tool outputs. \citep{Shen2024Small} \citep{Wu2024Federated,Yim2024Evaluating}

Concrete benchmarks show how sensitive coordination is to evaluation setup. Using only 400 problem instances from the Berkeley Function-Calling Leaderboard (BFCL), one method reports competitive in-distribution performance while improving generalization, highlighting that coordination benefits can be measured even under relatively small test sets. At the same time, small test sets can under-sample rare coordination failures, so reported gains should be interpreted with attention to variance and protocol details. \citep{Lu2025Just,Yim2024Evaluating}

Memory-sharing and tool mediation are common coordination mechanisms. MemTool evaluates different coordination modes across 13+ LLMs on the ScaleMCP benchmark, with experiments over 100 consecutive user interactions; this style of evaluation makes coordination overhead observable and ties performance to an explicit interaction protocol. Such results underscore that coordination is not free: policies that improve success can increase cost or latency, which must be part of the claim when coordination is justified as an engineering strategy. \citep{Lumer2025Memtool,Gao2025Radar}

Coordination is also used to improve planning and tool-integrated reasoning on hard agent benchmarks. GiGPO is evaluated on ALFWorld and WebShop as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct as base models. This illustrates a common pattern: multi-agent designs often trade off model size against interaction structure, using coordination to compensate for weaker individual agents under a fixed tool interface. \citep{Feng2025Group,Lu2025Just}

Evaluation methodology matters because multi-agent systems can "win" by exploiting protocol artifacts (e.g., hidden hints in shared memory, or aggregation rules that mask individual failures). Work on generating ground truth and validating evaluation metrics emphasizes that agent scoring must be tied to observable behaviors, not only to end outcomes, especially when multiple agents contribute. Protocol-aware evaluation can therefore benefit from logging communication traces and attributing errors to specific roles or messages. \citep{Zhang2025Datascibench,Yim2024Evaluating}

Several coordination designs emphasize robustness through verification, but robustness depends on the threat model. Coordination can reduce single-agent hallucinations, yet it can also create correlated failures when agents share the same adversarial context or when a single compromised tool output is broadcast to the team. In contrast to single-agent settings, coordination protocols must specify trust boundaries: what is verified, who can call tools, and how tool outputs are sanitized before being shared. \citep{Zhang2025Bridging} \citep{Shen2024Small,Maragheh2025Future}

Limitations and risks are prominent. Coordination increases surface area for instability (non-convergence, oscillation, or premature consensus), and it can degrade under tight latency or message budgets. Moreover, coordination success can be brittle to agent count and role definitions, making it unclear whether reported gains reflect general mechanisms or narrow protocol tuning. These caveats motivate reporting coordination protocol fields as explicitly as tool schemas. \citep{Sarkar2025Survey} \citep{Maragheh2025Future,Li2025Leveraging}

One way to read the evidence is that multi-agent coordination should be synthesized as a family of protocols, not as a single architectural choice: the same roles can behave very differently under different budgets, memory-sharing policies, and aggregation rules. This perspective connects directly to evaluation and risk considerations, where the choice of benchmarks, logging, and threat assumptions determines which coordination claims are reproducible and which are artifacts of a particular harness. \citep{Yim2024Evaluating} \citep{Sarkar2025Survey,Yang2025Survey}

\section{Evaluation \& Risks}


Evaluation determines which claims about agents are meaningful, and risk models determine which gains survive outside benign settings. This chapter connects benchmark design, protocol fields, and threat models so that reported numbers can be interpreted as statements about closed-loop systems rather than isolated model snapshots. In practice, differences in tool access, budgets, hidden prompts, and environment variability can dominate conclusions unless they are controlled or at least explicitly reported. \citep{Mohammadi2025Evaluation} \citep{Chowa2025From,Fu2025Eval}

The benchmarks subsection surveys task suites and protocol conventions for tool-use and long-horizon evaluation, emphasizing how metrics interact with interaction formats and budgets. The risks subsection focuses on security and governance: prompt injection, tool abuse, and monitoring/guardrail strategies that trade off robustness with task utility under explicit adversarial assumptions. Together, they provide a protocol-aware lens for interpreting both performance improvements and safety claims in the rest of the paper. \citep{Zhang2025Security} \citep{Gasmi2025Bridging,Mou2026Toolsafe}

\subsection{Benchmarks and evaluation protocols}


Benchmarks and evaluation protocols decide which claims about agents are meaningful. A core tension is coverage versus comparability: broader suites capture more behaviors, but they also make head-to-head comparison fragile when protocols and constraints differ. A protocol-aware view therefore treats evaluation as a specification problem - tasks, metrics, budgets, tool access, and threat assumptions must be explicit - rather than as a post-hoc score attached to a system description. \citep{Mohammadi2025Evaluation} \citep{Michelakis2025Core,Ji2025Taxonomy}

Evaluation protocols for tool-using agents include more moving parts than static LLM evaluation. Beyond task and metric choice, protocols define interaction formats (observations, action schemas), budget constraints (steps, latency, cost), termination rules, and what constitutes permissible evidence. Human evaluation and safety constraints can further change the interpretation of the same success rate. As a result, two papers can report similar scores while evaluating different objects, motivating explicit protocol fields in benchmark metadata and in paper reporting. Recent benchmark work illustrates how these protocol fields are being made explicit for comparability. \citep{Mohammadi2025Evaluation} \citep{Testini2025Measuring,Wang2025Flow} \citep{Chen2025Towards,Liu2026Agents,Tao2025Code,V2026Agentic,Almeida2025Ticket,Zhang2025Detective}

A useful contrast is between benchmark suites designed for breadth and benchmarks designed for diagnostic comparability. Broad suites aim to cover diverse environments and task families, whereas diagnostic benchmarks isolate specific capabilities such as tool invocation correctness, grounded retrieval, or long-horizon planning under a fixed interface. In practice, both are necessary: breadth supports external validity, but diagnostic structure is needed to attribute gains to specific components rather than to protocol quirks. \citep{Peng2024Survey} \citep{Zheng2025Newtonbench,Kwon2025Agentnet}

Surveys and taxonomies of agent evaluation increasingly emphasize this protocol gap. They catalog benchmark dimensions and propose organizing frameworks, but they also highlight that many evaluation reports omit key fields such as tool access, budget assumptions, or whether hidden prompts and environment variability are controlled. A direct implication is that evaluation results should be treated as conditional evidence unless protocol metadata is sufficient for replication. \citep{Mohammadi2025Evaluation} \citep{Ji2025Taxonomy,Kim2026Beyond}

The benchmark landscape is also expanding rapidly. One comprehensive survey connects 50+ benchmarks to corresponding solution strategies, which helps readers map what is being tested and which methods are commonly applied. At the same time, this growth makes it easier to cherry-pick results; protocol-aware synthesis therefore benefits from tracking which benchmarks share the same interaction format and constraints, and which do not. \citep{Guo2025Comprehensive,Peng2024Survey}

Tool-use agent benchmarks illustrate how interaction constraints shape measured capability. On multi-turn tool-use benchmarks such as M3ToolEval and TauBench, self-improvement frameworks report large gains, suggesting that revision policies and tool-selection behaviors are central. However, these benchmarks also depend on how tool calls are counted, what constitutes a valid argument, and how partial failures are handled; without those protocol details, it is unclear whether gains reflect better decision making or looser harness assumptions. \citep{Zhou2025Self,Michelakis2025Core}

Security-oriented benchmarks make the protocol and threat model explicit by construction. On the AgentDojo prompt injection benchmark, RTBAS reports preventing all targeted attacks with only a 2\% loss of task utility under attack, while other studies quantify that LLM utility can drop by 15-50 percentage points under attack with average attack success rates around 20\% in similar settings. These results show why evaluation protocols must include adversary models and defense policies; otherwise, robustness claims are not comparable. \citep{Zhong2025Rtbas} \citep{Alizadeh2025Simple,Hadeliya2025When}

Measurement methodology is therefore part of the benchmark contribution. Work on core evaluation design and on measuring agent behavior emphasizes logging and attribution: what the agent saw, what it did, and how errors map to protocol fields. This is especially important for tool-using agents because end outcomes can hide brittle behaviors such as over-reliance on privileged tool outputs, hidden leakage, or unstable retries. Benchmark suites that publish traces and explicit schemas make it easier to diagnose such issues. \citep{Michelakis2025Core} \citep{Testini2025Measuring,Kwon2025Agentnet,Wang2025Flow}

Limitations remain. Benchmarks can leak information through environment artifacts, prompt templates, or tool descriptions, and many suites underrepresent long-tail failures such as timeouts and partial tool outputs. In addition, comparability can break when different papers use different tool schemas or evaluation scripts under the same benchmark name. These caveats motivate reproducible releases (schemas, scripts, and traces) and caution against overinterpreting small score differences. \citep{Fu2025Eval} \citep{Hu2023Avis,Doshi2026Towards}

Taken together, benchmarks and protocols should be read as defining the object of evaluation: a closed-loop system under constraints. Protocol-aware synthesis therefore emphasizes which fields are comparable across papers and which require normalization before claims can be aggregated. This perspective also sets up the next subsection on risks, where threat models, monitoring, and governance constraints become part of the evaluation protocol rather than external concerns. \citep{Mohammadi2025Evaluation} \citep{Fu2025Eval,Agrawal2025Language}

Protocol metadata makes comparisons meaningful, but deployment decisions additionally hinge on threat models and governance controls that define what robustness means for tool-using systems.

\subsection{Safety, security, and governance}


Safety and governance become first-order concerns once agents can act through tools: capability increases utility, but it also widens the attack surface and raises containment requirements. The key tension is therefore capability versus safety - stronger actions can complete harder tasks, yet they can also enable prompt injection, tool abuse, and data exfiltration when interfaces and monitoring are weak. A protocol-aware synthesis treats security as part of the evaluation contract: threat models, permissions, and logging determine what robustness claims mean. \citep{Zhang2025Security} \citep{Fu2025Eval,Wang2025Comprehensive}

Threat models for tool-using agents include prompt injection (in user inputs, tool descriptions, or retrieved content), malicious tool outputs, and policy bypass via argument manipulation. Governance mechanisms span guardrails, monitors, sandboxing, and auditing. These controls are tightly coupled to the tool interface: if tool schemas are underspecified or permissions are broad, agents can be coerced into actions that look like "task completion" but violate safety objectives. Recent governance and safety work spans a range of threat models and control surfaces, so robustness claims should be read as contract-dependent. \citep{Fang2025Should} \citep{Li2026Toolprmbench,Hu2023Avis} \citep{Zhang2024Agent,Sha2025Agent,Russo2025Deep,Luo2025Agrail,He2024Emerged,Hong2025Natural}

Empirical security analyses highlight how interface choice affects robustness. One comparison of function calling and MCP-style tool interfaces reports higher overall attack success rates for function calling (73.5\% vs 62.59\% for MCP), with different exposure patterns depending on whether the vulnerability is system-centric or model-centric. This implies that robustness cannot be inferred from model quality alone; in contrast, security claims must be interpreted as properties of a full system under a specific interface contract and monitoring policy. \citep{Gasmi2025Bridging,Li2026Toolprmbench}

Guardrail frameworks attempt to reduce harmful tool use while preserving benign utility. TS-Flow, for example, reports reducing harmful tool invocations of ReAct-style agents by about 65\% on average and improving benign task completion by roughly 10\% under prompt injection attacks. These results illustrate a common trade-off: stronger defenses can alter the agent loop and therefore change what success metrics measure, which motivates reporting both robustness and utility under matched protocols. \citep{Mou2026Toolsafe,Zhou2025Reasoning}

Taxonomies of attacks help make the risk surface explicit and comparable. MSB, for instance, catalogs attacks such as name collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, and mixed-attack combinations. Such taxonomies are useful because they connect failure modes to interface and governance fields (tool descriptions, schema validation, permission models) that can be controlled and audited. \citep{Zhang2025Security,Wang2025Comprehensive}

Evaluation suites increasingly incorporate these threat models into benchmarks. RAS-Eval reports 80 test cases and 3,802 attack tasks mapped to 11 CWE categories, with tools implemented in JSON, LangGraph, and MCP formats, enabling controlled comparisons across interface variants. Attack suites also quantify that tool-use scenarios can sustain very high attack success rates and privacy leakage with little impact on benign-task execution, which underscores why success metrics alone are insufficient. \citep{Fu2025Eval,Mo2025Attractive}

Monitoring and red-teaming workflows provide another layer of governance. Work on monitor red teaming emphasizes varying monitor awareness, adversarial strategies to evade detection, and datasets/environments designed to stress safety mechanisms under realistic constraints. Such approaches treat robustness as an interaction between the agent and the monitor policy, so evaluation protocols should report both sides of the system to avoid attributing outcomes solely to the agent model. \citep{Kale2025Reliable} \citep{Martin2025Autoodd,Liu2025Secure}

Containment and sandboxing mechanisms aim to limit blast radius even when an agent is compromised. Two-way sandboxing designs and secure execution frameworks propose isolating both the agent from the host and the host from the agent, which becomes important when agents can call arbitrary tools or execute code. These system-level controls interact with orchestration: restrictive sandboxes can prevent harmful actions; however, they can also break benign workflows unless interfaces and policies are designed jointly. \citep{Yang2024Mvvm} \citep{Salama2025Edge,Erdogan2024Tinyagent}

Several limitations remain for safety evaluations. Threat models are often incomplete, and defenses can be brittle to distribution shifts in tool descriptions, retrieval content, or user behavior. In addition, system studies report that even strong models can exhibit significant performance limitations on realistic tasks, which complicates the interpretation of robustness claims when failures may stem from capability gaps rather than adversarial pressure alone. These caveats motivate combined reporting of capability, robustness, and protocol fields. \citep{Luo2025Universe} \citep{Bonagiri2025Check,Hadeliya2025When}

In sum, safety, security, and governance for tool-using agents is best framed as protocol design: interfaces, permissions, monitoring, and auditing jointly define what the agent is allowed to do and how failures are detected. This framing connects back to the benchmark discussion: without threat models and governance fields, published numbers are not comparable and do not support deployment decisions. A practical next step is to standardize reporting of tool contracts, sandbox settings, and attack suites as part of benchmark metadata and system papers. \citep{Wang2025Comprehensive} \citep{Fu2025Eval,Balaji2026Beyond}

\section{Discussion}


Across agent systems, protocol assumptions - tool access, budgets, termination, and threat models - are the dominant hidden variables that decide whether two reported results are comparable. A practical implication is that architectural labels (planner, memory, multi-agent) are insufficient without an explicit interface contract: two planners behave differently when tool schemas are underspecified, tool outputs are noisy, or execution is not sandboxed. This suggests that future benchmarks should publish protocol fields as first-class metadata and treat interface variability as a controlled factor rather than an uncontrolled nuisance. \citep{Mohammadi2025Evaluation,Liu2025Mcpagentbench,Fu2025Eval}

Another cross-cutting observation is that efficiency claims (lower cost, fewer steps, reduced latency) are increasingly used as a proxy for general progress, yet they are especially sensitive to protocol details. Systems that optimize routing or execution schedules can deliver large gains under a fixed harness, but the same techniques may shift failure modes when tool availability, environment variability, or monitoring changes. A useful next step is to standardize reporting of budgets and to separate model-level improvements from orchestration-level improvements so that progress can be attributed. \citep{Zhang2026Evoroute,Chowa2025From,Gasmi2025Bridging}

Security results underline why evaluation and deployment cannot be separated for tool-using agents. Prompt injection and tool abuse demonstrate that benign-task performance can coexist with severe failures in confidentiality or integrity, and defenses often trade off robustness with task utility. Rather than treating safety as an add-on, protocol-aware evaluation can incorporate threat models and monitoring constraints into benchmark design, making robustness claims interpretable and reproducible. \citep{Zhang2025Security,Zhong2025Rtbas,Mou2026Toolsafe,Kale2025Reliable}

Finally, evidence quality remains uneven across sub-areas: some results are supported by broad benchmark coverage, while others rely on narrow task suites or single-run case studies. A conservative synthesis therefore prioritizes comparisons that are stable under protocol variation and highlights where stronger verification is needed (e.g., full-text protocol details, tool definitions, and ablations that isolate interface effects). \citep{Chowa2025From,Yin2024Mmau,Tomaevi2025Towards}

\section{Conclusion}


Tool-using LLM agents are best understood as closed-loop systems whose behavior depends jointly on the agent loop, the interface contract, and protocol constraints such as budgets and threat models. This perspective clarifies why results can be difficult to compare across papers: changing tool schemas, permissions, or termination rules can shift measured performance as much as changing the underlying model or planner. \citep{Yao2022React,Liu2025Mcpagentbench,Mohammadi2025Evaluation}

Three takeaways follow. First, interface contracts determine what evaluation claims are even interpretable, so benchmarks and papers should publish tool schemas and execution constraints alongside metrics. Second, planning, memory, and adaptation mechanisms should be compared within compatible protocols, with explicit accounting for cost and latency when those are part of the claim. Third, safety and governance constraints must be integrated into evaluation because tool access expands the attack surface and creates new failure modes that benign metrics can hide. \citep{Fu2025Eval,Zhang2026Evoroute,Zhang2025Security,Mou2026Toolsafe}

The most actionable path forward is to standardize protocol reporting and benchmark metadata so that future work can separate model-level capability from orchestration choices and can evaluate robustness under explicit threat models. \citep{Chowa2025From,Mohammadi2025Evaluation,Kale2025Reliable}

\appendix

\section{Tables}


Appendix Table A1. Agent loop patterns and interface assumptions (representative works).

\begin{center}
\begin{tabularx}{\linewidth}{YYYl}
\toprule
Pattern & What changes in the agent loop & Interface / assumptions (what must be specified) & Key refs \\
\midrule
Think-Act-Observe prompting & Interleave reasoning and tool actions; update plans from observations & Tool schema + observation format; step budget/state & \citep{Yao2022React} \\
Self-supervised tool calling & Model learns to insert tool calls during generation & Tool execution sandbox; consistent tool outputs & \citep{Schick2023Toolformer} \\
Reflection-based self-improvement & Use self-critique/feedback to revise prompts or plans after failures & Feedback signal; memory of past attempts & \citep{Shinn2023Reflexion} \\
Search/planning over thoughts & Explore candidate reasoning branches before acting & Scoring/selection rule; budgeted search & \citep{Yao2023Tree} \\
Skill/library-driven embodied agent & Acquire reusable skills/tools for long-horizon tasks & Environment API; skill representation + retrieval & \citep{Wang2023Voyager} \\
Dataset-driven tool-use SFT & Scale instruction/traces to improve tool selection and execution & Standardized tool definitions; train/eval harness & \citep{Yang2025Toolmind,Song2025Agent,Du2024Anytool} \\
Protocolized tool interface (MCP / function calling) & Standardize discovery, invocation, and tool-side metadata & Schema/versioning; permissions/sandbox; observability & \citep{Liu2025Mcpagentbench,Gasmi2025Bridging} \\
Multi-agent coordination & Split subtasks across agents; aggregate via vote/referee & Role protocol; message format; conflict resolution & \citep{Feng2025Group,Yang2024Based} \\
Cost/latency-aware routing & Optimize tool routing to reduce cost/latency without losing task success & Budget metrics + tracing; cost-aware policy & \citep{Zhang2026Evoroute} \\
Safety/guardrail layer & Detect and mitigate prompt injection and malicious tool outputs & Threat model; safe tool invocation policy & \citep{Zhang2025Security,Mou2026Toolsafe,Fu2025Eval} \\
\bottomrule
\end{tabularx}
\end{center}


Appendix Table A2. Benchmarks and evaluation settings for tool-using agents (examples).

\begin{center}
\begin{tabularx}{\linewidth}{YYYYl}
\toprule
Benchmark / setting & What it tests & Typical metric(s) & Notes / constraints & Key refs \\
\midrule
ALFWorld & Interactive, multi-step decision making & Success rate / completion & Environment interaction; step budget & \citep{Yao2022React} \\
WebShop & Web navigation + tool-augmented reasoning & Success rate / task completion & Long-horizon browsing; noisy observations & \citep{Yao2022React} \\
GAIA & General agentic problem solving & Task success & Often paired with cost/latency analysis & \citep{Zhang2026Evoroute} \\
BrowseComp+ & Web browsing competency & Task success; cost/latency & Tool calls; latency-sensitive workflows & \citep{Zhang2026Evoroute} \\
MCPAgentBench & Tool-use via MCP definitions & Pass rate / tool correctness & Realistic tool schemas/protocols & \citep{Liu2025Mcpagentbench} \\
SOP-Bench & Industrial API workflows & Task success on 1,800+ tasks & Domain diversity; human-validated cases & \citep{Nandi2025Bench} \\
BFCL (function calling) & Function-call selection + arguments & Leaderboard score / success & Focus on structured tool calls & \citep{Lu2025Just} \\
M3ToolEval, TauBench & Multi-turn tool-use agents & Benchmark score / success & Emphasizes multi-step tool use & \citep{Zhou2025Self} \\
MMAU & Broad agent capabilities suite & Aggregate score & Multi-task prompting protocol & \citep{Yin2024Mmau} \\
AgentDojo (prompt injection) & Robustness to injection attacks & ASR vs utility & Defense trade-offs are common & \citep{Zhong2025Rtbas,Alizadeh2025Simple} \\
RAS-Eval and related attack suites & Tool-use security evaluation & ASR / privacy leakage & Attack-task suites; multiple tool formats & \citep{Fu2025Eval,Mo2025Attractive} \\
Tool-safety guardrail evaluation & Harm reduction under attack & Harmful tool invocations; utility & Requires explicit threat model + benign baseline & \citep{Mou2026Toolsafe} \\
\bottomrule
\end{tabularx}
\end{center}

\bibliographystyle{plainnat}
\bibliography{../citations/ref}

\end{document}
