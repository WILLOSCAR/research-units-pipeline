{"kind": "h3", "id": "3.1", "title": "Agent loop and action spaces", "section_id": "3", "section_title": "Foundations & Interfaces", "section_role": "Define the agent loop as an executable contract and surface protocol-dependent trade-offs for action spaces.", "depends_on": ["Agent = closed-loop system (observe, decide, act via tools, receive feedback) evaluated under a protocol."], "adds": ["Loop-as-contract framing (action/observation/budget/termination/threat model are part of the method).", "Action-space vs verifiability trade-off for loop design."], "paragraphs": [{"i": 1, "moves": ["claim", "definition_setup"], "output": "Frames agent loops as executable contracts and states the core tension between richer action spaces and verifiable evaluation."}, {"i": 2, "moves": ["definition_setup", "justification"], "output": "Defines the minimal loop and shows how loop labels hide variation that breaks naive cross-paper comparisons."}, {"i": 3, "moves": ["contrast", "justification"], "output": "Contrasts internal deliberation vs externally constrained actions and ties the distinction to what the protocol actually fixes."}, {"i": 4, "moves": ["justification", "local_conclusion"], "output": "Uses benchmark diversity to argue that loop claims are interpretable only relative to budgets and interaction formats."}, {"i": 5, "moves": ["justification", "boundary_failure"], "output": "Uses a ReAct benchmark result as an anchor while marking success-rate gains as conditional on interface and termination rules."}, {"i": 6, "moves": ["justification", "boundary_failure"], "output": "Shows why security is inseparable from action space: interface constraints and monitoring shape robustness outcomes."}, {"i": 7, "moves": ["justification", "boundary_failure"], "output": "Interprets efficiency gains as properties of the full loop under a harness rather than model-only improvements."}, {"i": 8, "moves": ["justification", "boundary_failure"], "output": "Explains how training/supervision shifts loop behavior and why comparability requires controlling protocol and tool access."}, {"i": 9, "moves": ["boundary_failure"], "output": "Enumerates threats to validity (stateless designs, short windows, narrow protocols) and motivates explicit protocol fields for transfer."}, {"i": 10, "moves": ["local_conclusion", "claim"], "output": "Concludes with a protocol-aware comparison lens and hands off to tool interfaces as the next dependency."}]}
{"kind": "h3", "id": "3.2", "title": "Tool interfaces and orchestration", "section_id": "3", "section_title": "Foundations & Interfaces", "section_role": "Treat the tool/interface contract as a first-class method variable and explain orchestration as policy over that contract.", "depends_on": ["Agent loop framing (actions/observations) and the idea that protocol fields determine interpretability."], "adds": ["Interface contract variables (schemas, permissions, observability, sandboxing) as required comparison metadata.", "Orchestration-as-policy view (routing, retries, termination under budgets)."], "paragraphs": [{"i": 1, "moves": ["claim", "definition_setup"], "output": "States the expressivity-vs-control trade-off and argues that interface contracts must be treated as part of the method."}, {"i": 2, "moves": ["definition_setup", "justification"], "output": "Defines how action representations and orchestration choices dominate reliability, even with the same base model."}, {"i": 3, "moves": ["contrast", "justification"], "output": "Contrasts schema-first vs prompt-first interfaces and explains why synthesis must name what is enforced by tooling vs left to the model."}, {"i": 4, "moves": ["justification"], "output": "Uses a workflow benchmark example to show how explicit tool contracts shift evaluation from single calls to orchestration reliability."}, {"i": 5, "moves": ["justification", "boundary_failure"], "output": "Uses tool-agent datasets to argue that learnability depends on a stable contract and that reporting must include schema/tool/version details."}, {"i": 6, "moves": ["definition_setup", "contrast", "boundary_failure"], "output": "Frames orchestration as policy over a tool graph and warns that routing/caching changes can be misattributed to planning gains."}, {"i": 7, "moves": ["justification", "claim"], "output": "Argues that interface fields are minimal protocol metadata; without them, cross-paper comparisons are under-identified."}, {"i": 8, "moves": ["justification"], "output": "Links interface design to memory and coordination by showing how adding retrieve/delegate operations changes the effective action space."}, {"i": 9, "moves": ["contrast", "boundary_failure"], "output": "Summarizes recurring limitations: weak permissions amplify attacks while over-restriction harms capability; harness realism and long-tail failures matter."}, {"i": 10, "moves": ["local_conclusion", "claim"], "output": "Concludes that fixing the interface contract stabilizes downstream comparisons and motivates the planning+memory chapter."}]}
{"kind": "h3", "id": "4.1", "title": "Planning and reasoning loops", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "section_role": "Compare planning methods as control-loop policies under explicit budgets and interfaces, not as model-only reasoning gains.", "depends_on": ["Tool/interface contract is part of the method; budget fields (steps/latency/cost) must be explicit."], "adds": ["Deliberation-depth vs cost framing for planning loops.", "Reactive vs planning-heavy contrast + normalization requirement (success vs budget)."], "paragraphs": [{"i": 1, "moves": ["claim", "definition_setup"], "output": "Frames planning as control under uncertainty and states the depth-vs-cost tension that confounds naive comparisons."}, {"i": 2, "moves": ["definition_setup", "justification"], "output": "Defines planning loop architectures and argues for protocol-aware reporting of retries, step limits, and observation formats."}, {"i": 3, "moves": ["contrast", "boundary_failure"], "output": "Contrasts reactive vs planning-heavy loops and notes that budget differences can make aggregate success metrics incomparable."}, {"i": 4, "moves": ["justification"], "output": "Uses a domain-specific evaluation example to show that planning quality is inseparable from the environment interface and recovery protocol."}, {"i": 5, "moves": ["justification", "boundary_failure"], "output": "Uses an efficiency result to emphasize that harness-level scheduling or caching can dominate latency without changing the model."}, {"i": 6, "moves": ["justification", "boundary_failure"], "output": "Argues that multi-benchmark reporting improves coverage but still requires protocol alignment to support synthesis."}, {"i": 7, "moves": ["justification"], "output": "Uses a realistic benchmark example to motivate explicit tool/observation specifications for robust planning comparisons."}, {"i": 8, "moves": ["definition_setup", "justification"], "output": "Explains why testing methodology and ablations are needed to separate planning improvements from orchestration artifacts."}, {"i": 9, "moves": ["boundary_failure"], "output": "Lists recurring limitations (noise/adversarial observations, compute confounds, unmatched tool access) and motivates controlled comparisons."}, {"i": 10, "moves": ["local_conclusion"], "output": "Concludes with planning as protocolized control and transitions to memory as a prerequisite for reliable decision state."}]}
{"kind": "h3", "id": "4.2", "title": "Memory and retrieval (RAG)", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "section_role": "Make memory/retrieval comparable by treating retrieval and writes as protocolized actions with validation and budgets.", "depends_on": ["Protocol-aware comparison lens (task/metric/budget/tool access) and the view that retrieval is an action in the loop."], "adds": ["Persistence vs freshness framing for memory.", "Retrieval-as-context vs retrieval-as-control contrast + verification/write-policy requirements."], "paragraphs": [{"i": 1, "moves": ["claim", "definition_setup"], "output": "Frames memory as the evidence interface to the agent and states the persistence-vs-freshness tension that drives verification needs."}, {"i": 2, "moves": ["definition_setup", "justification"], "output": "Defines memory layers and RAG integration choices that change both correctness and robustness."}, {"i": 3, "moves": ["contrast", "boundary_failure"], "output": "Contrasts retrieval used for conditioning vs for control and argues that validation and write governance are required for comparability."}, {"i": 4, "moves": ["justification"], "output": "Uses benchmark design to specify what the memory module must provide and what counts as a failure mode."}, {"i": 5, "moves": ["contrast", "boundary_failure", "justification"], "output": "Uses compression-based memory to show that summarization changes the evidence surface and should be evaluated separately from retrieval quality."}, {"i": 6, "moves": ["justification", "boundary_failure"], "output": "Uses multi-domain evaluation to argue memory benefits are domain-dependent and require protocol alignment for synthesis."}, {"i": 7, "moves": ["claim", "justification"], "output": "Treats retrieval policy as a first-class variable and argues that budgets and trust models for retrieved content must be reported."}, {"i": 8, "moves": ["justification", "boundary_failure"], "output": "Explains planning-memory interaction and notes that re-query frequency and write policies can confound apparent planning strength."}, {"i": 9, "moves": ["boundary_failure"], "output": "Summarizes recurring risks (adversarial retrieval, false persistent premises, underspecified evidence rules) and motivates threat models plus verification."}, {"i": 10, "moves": ["local_conclusion"], "output": "Concludes with memory as a protocolized component and hands off to learning/adaptation operating on the same traces."}]}
{"kind": "h3", "id": "5.1", "title": "Self-improvement and adaptation", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "section_role": "Evaluate adaptation as a controlled update process: what changes, under what constraints, and with what stability risks.", "depends_on": ["Protocol-aware interpretation of performance gains (avoid model-only attribution)."], "adds": ["Adaptability vs stability trade-off for self-improving agents.", "Constrained vs unconstrained update distinction (transfer vs harness overfit)."], "paragraphs": [{"i": 1, "moves": ["claim", "definition_setup"], "output": "Frames self-improvement as making the agent dynamic and states the adaptability-vs-stability trade-off that drives drift concerns."}, {"i": 2, "moves": ["definition_setup"], "output": "Defines major adaptation families (reflection, reward/preference feedback, evaluation-driven tuning) and their coupling to the harness."}, {"i": 3, "moves": ["contrast", "boundary_failure"], "output": "Contrasts protocol-constrained vs unconstrained updates and marks many gains as conditional on matched tool access and budgets."}, {"i": 4, "moves": ["justification", "boundary_failure"], "output": "Uses a benchmark gain as an anchor while noting that revision policies trade off cost/latency/robustness depending on counting rules."}, {"i": 5, "moves": ["justification", "boundary_failure"], "output": "Uses benchmark heterogeneity to argue against overgeneralizing improvement and motivates conservative claims tied to protocols."}, {"i": 6, "moves": ["justification", "boundary_failure"], "output": "Uses foundational loop results to motivate adaptation as a response to brittleness while noting dependence on protocol incentives for verification."}, {"i": 7, "moves": ["definition_setup", "justification"], "output": "Explains how data/tooling act as adaptation channels and motivates reporting tool definitions, access constraints, and supervision distributions."}, {"i": 8, "moves": ["contrast", "justification"], "output": "Summarizes methods aimed at stable adaptation and explains the aggressive-vs-conservative update trade-off for auditability."}, {"i": 9, "moves": ["boundary_failure"], "output": "Lists risks (metric gaming, reduced interpretability, reward hacking) and motivates stress-testing under tool/prompt/threat-model shifts."}, {"i": 10, "moves": ["local_conclusion"], "output": "Concludes with adaptation as a controlled process and transitions to coordination where stability depends on interaction rules."}]}
{"kind": "h3", "id": "5.2", "title": "Multi-agent coordination", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "section_role": "Treat multi-agent systems as coordination protocols (roles/messages/aggregation) and make protocol fields explicit for evaluation and robustness.", "depends_on": ["Interface contract + budget framing (message budget, stopping rules, shared memory/tool outputs)."], "adds": ["Specialization vs coordination overhead framing.", "Trust-boundary requirement for robustness claims in multi-agent settings."], "paragraphs": [{"i": 1, "moves": ["claim", "definition_setup"], "output": "Frames multi-agent capability as team protocols and states the specialization-vs-coordination trade-off that makes evaluation setup decisive."}, {"i": 2, "moves": ["definition_setup", "boundary_failure"], "output": "Defines coordination families (roles, debate, aggregation) and notes correlated failure risks when evidence is shared."}, {"i": 3, "moves": ["contrast", "boundary_failure"], "output": "Contrasts centralized orchestration vs decentralized coordination and argues evaluation must report message budgets, aggregation rules, and sharing policies."}, {"i": 4, "moves": ["justification", "boundary_failure"], "output": "Uses a small-test-set benchmark example to highlight both measurable gains and the risk of under-sampling rare coordination failures."}, {"i": 5, "moves": ["justification", "boundary_failure"], "output": "Uses a coordination benchmark example to argue coordination claims must include overhead (cost/latency) under an explicit interaction protocol."}, {"i": 6, "moves": ["justification"], "output": "Uses benchmark coverage to show a common pattern: coordination trades model size against interaction structure under a fixed tool interface."}, {"i": 7, "moves": ["definition_setup", "justification"], "output": "Argues evaluation should be trace-based (logs/attribution) so multi-agent outcomes are not judged only by end success."}, {"i": 8, "moves": ["claim", "boundary_failure"], "output": "States that robustness depends on threat models and requires explicit trust boundaries for tools, messages, and shared memory."}, {"i": 9, "moves": ["boundary_failure"], "output": "Lists prominent limitations (non-convergence, oscillation, tight budgets, brittleness to agent count/roles) that constrain transfer."}, {"i": 10, "moves": ["local_conclusion"], "output": "Concludes that coordination should be synthesized as a protocol family and connects directly to evaluation and risk requirements."}]}
{"kind": "h3", "id": "6.1", "title": "Benchmarks and evaluation protocols", "section_id": "6", "section_title": "Evaluation & Risks", "section_role": "Make comparability explicit by treating benchmarks as protocol specifications and distinguishing breadth coverage from diagnostic comparability.", "depends_on": ["Prior chapters define the object of study (loop + interface contract) and the protocol-aware reading of results."], "adds": ["Benchmark/protocol metadata as the unit of comparability (task/metric/budget/tool access/threat model).", "Normalization requirement before aggregating claims across papers."], "paragraphs": [{"i": 1, "moves": ["claim", "definition_setup"], "output": "Frames evaluation as specification and states the coverage-vs-comparability tension that drives protocol-aware synthesis."}, {"i": 2, "moves": ["definition_setup", "justification"], "output": "Defines key protocol fields and explains why similar scores can evaluate different objects without explicit metadata."}, {"i": 3, "moves": ["contrast", "justification"], "output": "Contrasts breadth-focused suites with diagnostic benchmarks and argues both are needed to separate component gains from protocol quirks."}, {"i": 4, "moves": ["justification", "boundary_failure"], "output": "Uses surveys/taxonomies to show reporting omissions and treats results as conditional evidence absent replicable protocol metadata."}, {"i": 5, "moves": ["justification", "boundary_failure"], "output": "Uses benchmark expansion to highlight cherry-picking risk and motivates tracking shared interaction formats and constraints."}, {"i": 6, "moves": ["justification", "boundary_failure"], "output": "Uses tool-use benchmarks to show harness details (tool-call counting, validity rules) determine how gains should be interpreted."}, {"i": 7, "moves": ["justification", "boundary_failure"], "output": "Uses security benchmarks to show threat models and defense policies must be part of the protocol for comparability."}, {"i": 8, "moves": ["definition_setup", "justification"], "output": "Argues that measurement methodology (logging, traces, schemas) is part of the benchmark contribution for attributing failures."}, {"i": 9, "moves": ["boundary_failure"], "output": "Summarizes limitations (leakage, long-tail failures, schema drift) and cautions against over-interpreting small deltas."}, {"i": 10, "moves": ["local_conclusion"], "output": "Concludes with benchmarks as defining the object of evaluation and sets up governance/risk fields as part of the same contract."}]}
{"kind": "h3", "id": "6.2", "title": "Safety, security, and governance", "section_id": "6", "section_title": "Evaluation & Risks", "section_role": "Treat safety and governance as protocol design: robustness claims depend on threat models plus permissions, monitoring, and sandboxing.", "depends_on": ["Tool/interface contract framing and the benchmark/protocol view of comparability."], "adds": ["Threat model + governance surface (permissions/monitoring/sandboxing/auditing) as required evaluation context.", "System-property interpretation of robustness (not model-only)."], "paragraphs": [{"i": 1, "moves": ["claim", "definition_setup"], "output": "Frames safety as a first-order constraint and states capability-vs-safety tension; robustness is defined by the evaluation contract."}, {"i": 2, "moves": ["definition_setup", "justification"], "output": "Defines core threat models and governance mechanisms and ties their effectiveness to tool schemas and permissions."}, {"i": 3, "moves": ["justification", "boundary_failure"], "output": "Uses interface-level security comparisons to argue robustness outcomes are system properties under a specific contract and monitoring policy."}, {"i": 4, "moves": ["contrast", "justification"], "output": "Uses a guardrail result to show defense-utility trade-offs and motivates matched-protocol reporting for both robustness and benign success."}, {"i": 5, "moves": ["justification"], "output": "Uses attack taxonomies to make risk surfaces comparable and link failures to controllable interface/governance fields."}, {"i": 6, "moves": ["justification", "boundary_failure"], "output": "Uses threat-model-aware suites to show why end success metrics are insufficient when attack success and leakage can remain high."}, {"i": 7, "moves": ["definition_setup", "boundary_failure"], "output": "Frames monitoring/red-teaming as part of the system and argues protocols must report monitor assumptions to avoid misattribution."}, {"i": 8, "moves": ["contrast", "boundary_failure"], "output": "Explains containment/sandboxing trade-offs: restricting actions limits blast radius but can also break benign workflows if policies are mismatched."}, {"i": 9, "moves": ["boundary_failure"], "output": "Lists key limitations (incomplete threat models, distribution shifts, capability confounds) and motivates combined reporting of capability and robustness."}, {"i": 10, "moves": ["local_conclusion"], "output": "Concludes with governance as protocol design and proposes standardizing tool contracts, sandbox settings, and attack suites in benchmark metadata."}]}
