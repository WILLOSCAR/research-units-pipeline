## Discussion

Across agent systems, protocol assumptions - tool access, budgets, termination, and threat models - are the dominant hidden variables that decide whether two reported results are comparable. A practical implication is that architectural labels (planner, memory, multi-agent) are insufficient without an explicit interface contract: two planners behave differently when tool schemas are underspecified, tool outputs are noisy, or execution is not sandboxed. This suggests that future benchmarks should publish protocol fields as first-class metadata and treat interface variability as a controlled factor rather than an uncontrolled nuisance. [@Mohammadi2025Evaluation; @Liu2025Mcpagentbench; @Fu2025Eval]

Another cross-cutting observation is that efficiency claims (lower cost, fewer steps, reduced latency) are increasingly used as a proxy for general progress, yet they are especially sensitive to protocol details. Systems that optimize routing or execution schedules can deliver large gains under a fixed harness, but the same techniques may shift failure modes when tool availability, environment variability, or monitoring changes. A useful next step is to standardize reporting of budgets and to separate model-level improvements from orchestration-level improvements so that progress can be attributed. [@Zhang2026Evoroute; @Chowa2025From; @Gasmi2025Bridging]

Security results underline why evaluation and deployment cannot be separated for tool-using agents. Prompt injection and tool abuse demonstrate that benign-task performance can coexist with severe failures in confidentiality or integrity, and defenses often trade off robustness with task utility. Rather than treating safety as an add-on, protocol-aware evaluation can incorporate threat models and monitoring constraints into benchmark design, making robustness claims interpretable and reproducible. [@Zhang2025Security; @Zhong2025Rtbas; @Mou2026Toolsafe; @Kale2025Reliable]

Finally, evidence quality remains uneven across sub-areas: some results are supported by broad benchmark coverage, while others rely on narrow task suites or single-run case studies. A conservative synthesis therefore prioritizes comparisons that are stable under protocol variation and highlights where stronger verification is needed (e.g., full-text protocol details, tool definitions, and ablations that isolate interface effects). [@Chowa2025From; @Yin2024Mmau; @Tomaevi2025Towards]

