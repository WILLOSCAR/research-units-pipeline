Self-improvement makes agents dynamic systems: they change their behavior based on feedback, experience, or evaluation signals. A core trade-off is adaptability versus stability - systems that revise themselves can improve over time, but they also risk drifting, overfitting to narrow protocols, or becoming harder to evaluate and control. A protocol-aware synthesis therefore asks not only whether performance increases, but also what is being updated (prompts, policies, tool usage) and under which constraints the update is valid. [@Zhou2025Self] [@Du2025Survey; @Van2025Survey]

Adaptation mechanisms span several families. Reflection and critique loops revise intermediate reasoning or tool plans after failures; preference- or reward-driven methods shape behavior via feedback signals; and evaluation-driven tuning uses benchmark performance as an optimization target. These approaches differ in how tightly they bind to an evaluation harness: some treat the harness as a learning environment, whereas others treat it as a diagnostic that informs manual or automated revisions. [@Chen2025Grounded] [@Belle2025Agents; @Hatalis2025Review]

A useful contrast is between updates that are explicitly constrained by a protocol and updates that implicitly absorb protocol quirks. Constrained updates can improve reliability under matched tool access and budgets, whereas unconstrained updates may pick up brittle shortcuts that do not transfer when tools, prompts, or termination rules change. In practice, many adaptation results should be interpreted as conditional on the training/evaluation setting, and reporting should make those conditions explicit to avoid overclaiming. [@Yao2025Agents] [@Li2025Learn; @Yin2024Mmau]

Concrete evaluations show that adaptation can deliver large gains on tool-use benchmarks. On two multi-turn tool-use agent benchmarks (M3ToolEval and TauBench), the Self-Challenging framework reports over a two-fold improvement, illustrating that feedback and revision policies can be as important as base-model capability for interactive success. These results also highlight why protocol fields matter: the same revision policy can trade off cost, latency, and robustness depending on how tool calls and retries are counted. [@Zhou2025Self; @Guo2025Comprehensive]

Benchmark coverage has expanded rapidly, and this growth changes what "improvement" means. A comprehensive survey connects 50+ benchmarks to solution strategies, underscoring that adaptation can be measured across heterogeneous task families with different scoring rules and interaction formats. This heterogeneity motivates a conservative stance: gains on a single suite should be treated as evidence about that suite's protocol, not as a blanket statement about agent capability. [@Guo2025Comprehensive] [@Yin2024Mmau; @Du2025Survey]

Foundational agent-loop results also motivate adaptation as a response to long-horizon brittleness. ReAct-style agents show large success-rate gains on interactive tasks (e.g., ALFWorld and WebShop), but they still exhibit failure modes such as premature commitment and error compounding. Adaptation mechanisms can be viewed as attempts to correct these failure modes online, yet the magnitude of benefit depends on whether the evaluation protocol rewards cautious verification or merely short-horizon completion. [@Yao2022React; @Zhou2025Self]

Data and tooling can act as implicit adaptation channels by changing what the agent learns to do with tools. Tool-use training resources and structured supervision can improve tool selection and execution, but they can also shift behavior toward the idiosyncrasies of a particular tool contract or benchmark harness. In contrast to pure prompting, training-based adaptation should therefore report the tool definitions, access constraints, and the distribution of tasks used for supervision. [@Du2024Anytool] [@Li2025Learn; @Peng2026Enhancing]

Several method lines explore how to make adaptation more stable and controllable. Some approaches emphasize bounded revision policies and explicit failure triggers, while others focus on iterative improvement over longer horizons or on domain-specific adaptation where evaluation is more structured. These designs often trade off sample efficiency and reliability: aggressive updates can improve quickly but may destabilize behavior, whereas conservative updates may be slower but easier to audit. [@Zhou2024Archer] [@Wu2025Evolver; @Xia2025Sand; @He2025Enabling]

Limitations and risks are central for self-improving agents. Adaptation can exploit metric artifacts, reduce interpretability of the decision policy, or create reward-hacking behaviors when feedback signals are misaligned. In addition, ethical and governance constraints can matter in practice: when agents are optimized for performance, it becomes unclear how to bound behavior in sensitive settings without explicit constraints and monitoring. These caveats argue for reporting stability diagnostics and for stress-testing adaptation under shifts in tools, prompts, and threat models. [@Tennant2024Moral] [@Du2025Survey; @Alizadeh2025Simple]

A practical implication is that self-improvement should be evaluated as a controlled process: what feedback is used, what is updated, and how improvements trade off robustness, cost, and safety under the stated protocol. This framing also sets up multi-agent coordination, where adaptation can be distributed across roles and where stability depends on communication and aggregation rules as much as on individual agent learning. [@Li2026Autonomous] [@Van2025Survey; @Zhang2026Evoroute]
