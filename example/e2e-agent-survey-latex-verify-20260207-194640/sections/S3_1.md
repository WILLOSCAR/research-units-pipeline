Agent loops are where "capability" becomes "behavior": they determine how a model turns an intent into a sequence of actions under a protocol. The central tension is that richer loops and action spaces can increase task success, but they also make results harder to verify because evaluation depends on hidden choices such as budgets, stopping rules, and what counts as a valid action or observation. A useful synthesis therefore treats the loop as an executable contract - not just a prompt style - and asks which parts of the contract are held fixed when numbers are reported. [@Yao2022React] [@Chowa2025From; @Yin2024Mmau]

At a minimum, a tool-using agent loop instantiates a state -> decide -> act -> observe cycle, where "act" includes structured tool calls and "observe" includes tool outputs plus any environment feedback. The action space can be constrained (few tools with stable schemas) or expansive (many tools, open-ended arguments, dynamic tool discovery), and the choice affects both reliability and failure recovery. In practice, the same high-level loop label hides substantial variation in what is logged, what is remembered, and how errors are handled, which makes protocol-aware comparisons essential. [@Kim2025Bridging] [@Liu2025Mcpagentbench; @Zhang2024Ecoact]

A first contrast is between loops that expand internal deliberation and loops that constrain external actions. More deliberation can reduce obvious mistakes, whereas tighter action representations (schemas, validators, or restricted tool sets) can reduce the probability of catastrophic tool misuse by narrowing what the agent can do. The literature increasingly mixes these approaches, but synthesis is clearest when the evaluation protocol states which constraints are enforced by the interface and which are learned or prompted behaviors. [@Zhang2024Ecoact] [@Liu2025Mcpagentbench; @Cui2025Toward]

Benchmark landscapes already reflect this protocol dependence. One survey of agent evaluation reports 68 publicly available datasets spanning diverse tasks, which highlights how quickly evaluation has broadened without converging on a standard protocol vocabulary. At the same time, aggregate suites aim to compare agents across task families, but these comparisons remain brittle unless budgets and interaction formats are aligned. The practical takeaway is that "agent loop" claims should be read together with the benchmark and the scoring rule, not in isolation. [@Chowa2025From] [@Yin2024Mmau; @Cui2025Toward]

Concrete numbers illustrate why loop semantics matter. On two interactive decision-making benchmarks (ALFWorld and WebShop), ReAct-style reasoning-action prompting is reported to outperform imitation and reinforcement learning baselines by 34% and 10% absolute success rate, respectively, while using only one or two in-context examples. These gains are meaningful, but they are also protocol-specific: the action interface and termination criteria define what "success" means and how much exploration is allowed. [@Yao2022React; @Yin2024Mmau]

Loop design is also inseparable from security because the action space creates an attack surface. For example, an analysis comparing function calling and MCP-style tool interfaces reports higher overall attack success rates for function calling (73.5% vs 62.59% for MCP), with different exposure patterns depending on whether the vulnerability is system-centric or model-centric. Such results imply that the same planner can appear robust or fragile depending on interface constraints and monitoring; in contrast, analyses that ignore the interface contract tend to attribute these differences to the planner or model alone. Loop comparisons should therefore include a threat model and the corresponding policy/sandbox setting. [@Gasmi2025Bridging] [@Liu2025Mcpagentbench; @Fang2025Should]

Efficiency and cost claims further amplify protocol sensitivity. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ report that EvoRoute, when integrated into off-the-shelf agentic systems, can sustain or improve task performance while reducing execution cost by up to 80% and latency by over 70%. These improvements are best interpreted as properties of the full loop (routing policy + tool schedule + termination) under the benchmark's interaction format, rather than as model-only gains. [@Zhang2026Evoroute; @Chowa2025From]

Training data and supervision choices can shift loop behavior even when the interface is fixed. One large-scale tool-agent dataset reports an average performance gain of about 20% over corresponding base models after supervised fine-tuning, improving results across coding, browsing, and tool-use benchmarks without domain-specific tuning. This suggests that the "decide" step can be made more reliable via scale, but it also raises comparability questions: the resulting loop may call tools more or less often, changing cost and failure modes even when success rates improve. [@Song2025Agent; @Zhang2025Datascibench]

Several limitations recur across loop evaluations and matter for how conclusions should transfer. Stateless agent designs can inflate apparent robustness by ignoring long-term drift, while short evaluation windows and single-run studies limit variance estimates and external validity. In addition, domain-specific agent deployments can exhibit strong gains on narrow protocols but unclear generality when tool access or environment dynamics change. These are not reasons to dismiss results; they are reasons to surface protocol fields explicitly so that later work can reproduce and stress-test the loop semantics. [@Tomaevi2025Towards] [@Ghose2025Orfs; @Fumero2025Cybersleuth]

Taken together, the most stable way to compare agent loops is to treat the action space and protocol as the object of study: what actions are admissible, how observations are represented, what budgets apply, and what threats are modeled. This framing naturally motivates the next subsection on tool interfaces and orchestration, because the interface contract defines the "physics" of the action space that every loop - regardless of planner sophistication - must obey. [@Liu2025Mcpagentbench] [@Zhang2026Evoroute; @Yang2025Survey]
