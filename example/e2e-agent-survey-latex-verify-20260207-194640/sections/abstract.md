## Abstract

Tool-using large language model (LLM) agents are increasingly deployed as closed-loop systems that plan, act through external tools, and adapt to feedback in dynamic environments. Their reported gains are often difficult to interpret in isolation because results conflate the agent loop, the tool/interface contract, budget assumptions (steps, cost, latency), and threat models for tool misuse. This survey organizes the design space around (i) foundations and interfaces, (ii) core components for long-horizon behavior (planning and memory), and (iii) learning, coordination, evaluation, and risks. We synthesize representative patterns such as reasoning-action prompting, tool-call induction, reflection, and deliberate search, and connect them to benchmark suites and protocol choices that determine which comparisons are meaningful. Across these lenses, we highlight recurring failure modes - prompt injection, tool abuse, and evaluation leakage - and outline a protocol-aware checklist for reporting and auditing agent results. [@Yao2022React; @Schick2023Toolformer; @Shinn2023Reflexion; @Yao2023Tree; @Mohammadi2025Evaluation; @Fu2025Eval]

