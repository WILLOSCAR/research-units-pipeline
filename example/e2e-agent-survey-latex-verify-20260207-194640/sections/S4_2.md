Memory and retrieval determine what evidence an agent can condition on and how long-horizon state is represented across steps. The core tension is persistence versus freshness: retaining more context can improve performance on multi-step tasks, but it also raises staleness, contamination, and verification challenges. Because retrieval is itself an action in the loop, memory design must be compared under explicit protocols - what can be retrieved, how often, and how retrieved content is validated - rather than being treated as a plug-in component. [@Shi2025Progent] [@Huang2025Retrieval; @Yao2025Survey]

In tool-using agents, memory spans several layers: short-term scratchpads, episodic traces of prior actions, and long-term stores indexed by text, structure, or learned embeddings. Retrieval-augmented generation (RAG) is a common mechanism, but its behavior depends on indexing, query formation, and integration strategy (constraints vs suggestions vs authoritative facts). These choices shape both correctness and robustness, especially when tools can write to or read from the same memory substrate. [@Verma2026Active] [@Peng2026Enhancing; @Hu2023Avis]

A useful contrast is between retrieval-as-context and retrieval-as-control. Retrieval-as-context treats retrieved content as additional conditioning information, whereas retrieval-as-control uses retrieved items to constrain action selection (e.g., which tool to call or which plan branch to pursue). However, the two can be indistinguishable in aggregate scores if verification is weak; protocol-aware evaluation should therefore report how retrieval is validated (citation checks, self-consistency, or external verification) and how memory writes are governed. [@Yu2026Agentic] [@Zhang2025Large; @Kim2025Bridging]

Concrete benchmark design makes these distinctions measurable. MuaLLM, for example, introduces two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100, focused on multistep reasoning. This formulation clarifies what the memory module is expected to provide (relevant evidence and grounded citations) and what counts as a failure mode (hallucinated support, stale context, or brittle query behavior). [@Abbineni2025Muallm; @Shi2025Progent]

Compression-oriented memory systems highlight a different trade-off: retaining more context often requires summarization or distillation. Meta-RAG reports condensing codebases by an average of 79.8%, which can make long contexts tractable for agents operating over large repositories. Yet compression changes the evidence surface: in contrast to raw retrieval, summaries can introduce abstraction errors that are hard to detect without explicit verification steps, so evaluation protocols should separate retrieval quality from summarization fidelity. [@Tawosi2025Meta; @Huang2025Retrieval]

Multi-benchmark evaluations also stress that memory benefits are domain-dependent. AgentSwift is evaluated across seven benchmarks spanning embodied, math, web, tool, and game domains, and reports average gains across this diverse suite. Such breadth improves external validity, but synthesis still requires protocol alignment: different benchmarks expose different observation noise and different opportunities for retrieval to help or harm. [@Li2025Agentswift; @Shang2024Agentsquare]

Retrieval policy is increasingly treated as an object of study rather than a default setting. Survey and analysis work emphasizes that retrieval frequency, query selection, and ranking criteria can materially change agent behavior under the same planner. As a result, comparisons between memory-augmented agents should report retrieval budgets and the interface between retrieval and tool calls (e.g., whether retrieval results are treated as tool outputs with their own trust and permission model). [@Li2024Review] [@Yao2025Survey; @Dong2025Etom]

Memory also interacts with planning: planning quality depends on what state is reliably available at decision time, and memory determines which evidence is accessible under a budget. Systems that combine planning loops with retrieval can appear stronger simply because they re-query more often, whereas systems that write structured traces can reduce rework at the cost of accumulating stale assumptions. Protocol-aware evaluation therefore benefits from tracking memory write policies and from measuring how often plans are revised due to retrieved evidence. [@Hong2025Planning] [@Ye2025Task; @Ye2025Taska]

Several limitations and risks recur in memory-based agents. Retrieval channels can carry adversarial content or prompt injections, and long-lived memories can encode false premises that persist across tasks. In addition, some benchmarks under-specify what counts as permissible evidence, making it unclear whether retrieval gains reflect better grounding or leakage through the environment. These issues motivate explicit threat models and verification steps for memory, especially in settings where memory can influence tool invocation. [@Maragheh2025Future] [@Kim2025Bridging; @Hu2023Avis]

More broadly, memory and retrieval are most useful when treated as protocolized components: they expose a budgeted evidence interface to the planner and must be evaluated for both correctness and robustness. This framing connects naturally to learning and adaptation, because feedback loops often operate on the same memory traces and can amplify both improvements and failures over time. [@Peng2026Enhancing] [@Du2025Survey; @Min2025Goat]
