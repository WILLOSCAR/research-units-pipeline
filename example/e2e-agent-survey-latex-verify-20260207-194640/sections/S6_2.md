Safety and governance become first-order concerns once agents can act through tools: capability increases utility, but it also widens the attack surface and raises containment requirements. The key tension is therefore capability versus safety - stronger actions can complete harder tasks, yet they can also enable prompt injection, tool abuse, and data exfiltration when interfaces and monitoring are weak. A protocol-aware synthesis treats security as part of the evaluation contract: threat models, permissions, and logging determine what robustness claims mean. [@Zhang2025Security] [@Fu2025Eval; @Wang2025Comprehensive]

Threat models for tool-using agents include prompt injection (in user inputs, tool descriptions, or retrieved content), malicious tool outputs, and policy bypass via argument manipulation. Governance mechanisms span guardrails, monitors, sandboxing, and auditing. These controls are tightly coupled to the tool interface: if tool schemas are underspecified or permissions are broad, agents can be coerced into actions that look like "task completion" but violate safety objectives. [@Fang2025Should] [@Li2026Toolprmbench; @Hu2023Avis]

Empirical security analyses highlight how interface choice affects robustness. One comparison of function calling and MCP-style tool interfaces reports higher overall attack success rates for function calling (73.5% vs 62.59% for MCP), with different exposure patterns depending on whether the vulnerability is system-centric or model-centric. This implies that robustness cannot be inferred from model quality alone; in contrast, security claims must be interpreted as properties of a full system under a specific interface contract and monitoring policy. [@Gasmi2025Bridging; @Li2026Toolprmbench]

Guardrail frameworks attempt to reduce harmful tool use while preserving benign utility. TS-Flow, for example, reports reducing harmful tool invocations of ReAct-style agents by about 65% on average and improving benign task completion by roughly 10% under prompt injection attacks. These results illustrate a common trade-off: stronger defenses can alter the agent loop and therefore change what success metrics measure, which motivates reporting both robustness and utility under matched protocols. [@Mou2026Toolsafe; @Zhou2025Reasoning]

Taxonomies of attacks help make the risk surface explicit and comparable. MSB, for instance, catalogs attacks such as name collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, and mixed-attack combinations. Such taxonomies are useful because they connect failure modes to interface and governance fields (tool descriptions, schema validation, permission models) that can be controlled and audited. [@Zhang2025Security; @Wang2025Comprehensive]

Evaluation suites increasingly incorporate these threat models into benchmarks. RAS-Eval reports 80 test cases and 3,802 attack tasks mapped to 11 CWE categories, with tools implemented in JSON, LangGraph, and MCP formats, enabling controlled comparisons across interface variants. Attack suites also quantify that tool-use scenarios can sustain very high attack success rates and privacy leakage with little impact on benign-task execution, which underscores why success metrics alone are insufficient. [@Fu2025Eval; @Mo2025Attractive]

Monitoring and red-teaming workflows provide another layer of governance. Work on monitor red teaming emphasizes varying monitor awareness, adversarial strategies to evade detection, and datasets/environments designed to stress safety mechanisms under realistic constraints. Such approaches treat robustness as an interaction between the agent and the monitor policy, so evaluation protocols should report both sides of the system to avoid attributing outcomes solely to the agent model. [@Kale2025Reliable] [@Martin2025Autoodd; @Liu2025Secure]

Containment and sandboxing mechanisms aim to limit blast radius even when an agent is compromised. Two-way sandboxing designs and secure execution frameworks propose isolating both the agent from the host and the host from the agent, which becomes important when agents can call arbitrary tools or execute code. These system-level controls interact with orchestration: restrictive sandboxes can prevent harmful actions; however, they can also break benign workflows unless interfaces and policies are designed jointly. [@Yang2024Mvvm] [@Salama2025Edge; @Erdogan2024Tinyagent]

Several limitations remain for safety evaluations. Threat models are often incomplete, and defenses can be brittle to distribution shifts in tool descriptions, retrieval content, or user behavior. In addition, system studies report that even strong models can exhibit significant performance limitations on realistic tasks, which complicates the interpretation of robustness claims when failures may stem from capability gaps rather than adversarial pressure alone. These caveats motivate combined reporting of capability, robustness, and protocol fields. [@Luo2025Universe] [@Bonagiri2025Check; @Hadeliya2025When]

In sum, safety, security, and governance for tool-using agents is best framed as protocol design: interfaces, permissions, monitoring, and auditing jointly define what the agent is allowed to do and how failures are detected. This framing connects back to the benchmark discussion: without threat models and governance fields, published numbers are not comparable and do not support deployment decisions. A practical next step is to standardize reporting of tool contracts, sandbox settings, and attack suites as part of benchmark metadata and system papers. [@Wang2025Comprehensive] [@Fu2025Eval; @Balaji2026Beyond]
