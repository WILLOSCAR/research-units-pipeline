Tool-using LLM agents turn language models from passive generators into controllers of action: they decide what to do next, invoke external tools, read observations, and iterate until a task terminates. This closed-loop view explains both the recent leap in practical capability and the persistent confusion in interpreting results: improvements can come from better prompting, better tools, better interface contracts, or simply different budgets and stopping rules. Canonical agent patterns such as reasoning-action prompting, tool-call induction, reflection, and deliberate search already illustrate this entanglement, even when evaluated on the same interactive tasks. [@Yao2022React; @Schick2023Toolformer; @Shinn2023Reflexion; @Yao2023Tree]

An operational definition used throughout this paper is: an agent is a system that (i) maintains state, (ii) selects actions in an action space that includes tool/API calls or environment operations, and (iii) updates state from observations to continue acting under an explicit protocol. This definition separates agent behavior from one-shot tool augmentation (e.g., a single retrieval call) and focuses attention on what can fail in practice: action representation, observation fidelity, and recovery strategies when tools are unreliable. Work on embodied or long-horizon settings further highlights that agents accumulate skills and abstractions over time, making the interface between the model, memory, and tools a first-class design choice rather than an implementation detail. [@Wang2023Voyager; @Du2024Anytool; @Song2025Agent]

The first organizing axis is the interface contract: what tools exist, how they are described, what arguments are legal, what the tool returns, and what permissions and logging surround execution. Two systems can share the same base model but behave very differently if one has a stable schema with sandboxed execution while the other relies on underspecified natural-language tool descriptions. Recent benchmark and security analyses show that interface design is inseparable from reliability and risk, including prompt injection pathways and tool-side manipulation that do not appear in text-only evaluations. [@Liu2025Mcpagentbench; @Gasmi2025Bridging; @Mou2026Toolsafe; @Zhang2025Security]

The second axis is the agent loop itself: how decisions are formed and revised as new evidence arrives. Planning and reasoning loops control when an agent commits to an action versus exploring alternatives, and they interact strongly with budget constraints (latency, cost, step limits) that are often underreported. A practical implication is that protocol-aware comparisons should treat planning as an algorithmic component with an evaluation footprint, not merely as a prompt style, because changes in search breadth or tool-call frequency can dominate measured outcomes. [@Yao2023Tree; @Shang2024Agentsquare; @Gao2024Efficient; @Nakano2025Guided]

Memory and retrieval provide the third axis: what information an agent can condition on, how it is selected, and how it is verified. Retrieval-augmented agents can appear strong on benchmarks where fresh evidence is available, yet fragile when retrieval introduces spurious citations, stale context, or adversarial content. A recurring theme across agent systems is that memory modules are not interchangeable; their failure modes depend on indexing choices, update policies, and how retrieval results are integrated into the loop (as constraints, as suggestions, or as authoritative facts). [@Shi2025Progent; @Abbineni2025Muallm; @Li2025Agentswift; @Dong2025Etom]

Learning and adaptation mechanisms add another layer of coupling between interface and behavior. Reflection and self-improvement loops can change not only the final answer but also tool usage patterns (which tools are called, how often, and with what arguments), complicating attribution when improvements are measured as task success alone. Multi-agent variants further amplify these effects: role specialization and communication protocols introduce new degrees of freedom, while aggregation rules can hide individual agent failures behind ensemble outcomes. [@Shinn2023Reflexion; @Zhou2025Self; @Feng2025Group; @Lu2025Just; @Lumer2025Memtool]

Evaluation is therefore not just a downstream reporting step; it defines what claims are meaningful. Benchmark suites for tool-use agents and agentic systems differ in task families, scoring rules, interaction formats, and threat models, and comparisons are often brittle unless protocol fields (tool access, budgets, termination, and safety constraints) are normalized. Recent evaluations emphasize this protocol gap by cataloging benchmark landscapes and by introducing suites that stress realistic tool definitions, industrial workflows, and adversarial settings. [@Mohammadi2025Evaluation; @Chowa2025From; @Nandi2025Bench; @Yin2024Mmau; @Fu2025Eval]

Security and governance constraints are increasingly central because the action space itself creates new attack surfaces: prompt injection, data exfiltration through tools, and deceptive tool outputs can break the implicit assumptions behind standard success metrics. Defenses often trade off robustness and utility, and the threat model (what the adversary controls, what the agent can call, what is logged) changes the interpretation of any reported improvement. A key motivation for a protocol-aware survey is to connect these risk considerations to the same interface and loop choices that drive performance. [@Gasmi2025Bridging; @Zhong2025Rtbas; @Alizadeh2025Simple; @Kale2025Reliable; @Mo2025Attractive]

Methodology. We retrieved a candidate pool of 1,800 papers and curated a 300-paper core set for synthesis; unless otherwise noted, evidence is drawn from abstracts and metadata rather than full-text extraction. This evidence level is sufficient to map the design space and protocols, but it can underrepresent implementation-specific details; accordingly, the survey emphasizes protocol fields (interfaces, budgets, threat models) that are typically stated explicitly, and it flags where deeper verification is most likely to change interpretation. [@Plaat2025Agentic; @Li2024Review; @Guo2025Comprehensive; @Yang2025Survey]

The remainder of the paper follows the above lenses. Section 3 frames foundations and interfaces by separating the agent loop from the action space and the tool contract. Section 4 focuses on core components for long-horizon behavior, with planning and memory treated as protocol-sensitive modules. Section 5 covers learning, adaptation, and multi-agent coordination as mechanisms that alter both behavior and tool usage. Section 6 surveys evaluation and risks, emphasizing benchmark design and security threat models. Appendix tables summarize representative loop patterns and evaluation settings to make protocol assumptions explicit. [@Yao2022React; @Schick2023Toolformer; @Mohammadi2025Evaluation; @Zhang2025Security]

