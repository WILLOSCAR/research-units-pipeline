Tool interfaces are the boundary between a model's intent and executable actions, and orchestration is the policy that decides how to cross that boundary repeatedly. The practical trade-off is expressivity versus control: richer interfaces expand what an agent can attempt, but they also widen the space of failure modes and make behavior harder to constrain and verify. As a result, comparisons between tool-using agents are most interpretable when they treat the interface contract (schemas, permissions, observability) as part of the method rather than as an incidental engineering choice. [@Mohammadi2025Evaluation] [@Yao2022React; @Chowa2025From]

Interface design starts with how actions are represented. Function-calling schemas and API specifications enable structured arguments and post-hoc validation, while natural-language tool descriptions favor flexibility but shift error detection into prompting or downstream checks. Orchestration then layers routing and control flow on top of these representations: selecting tools, handling retries, and deciding when to terminate under step and latency budgets. Even when two agents share the same base model, these interface and orchestration choices can dominate reliability. [@Dong2025Etom] [@Liu2025Toolscope; @Cui2025Toward]

A key contrast is between schema-first interfaces that make invalid actions unrepresentable and prompt-first interfaces that rely on the model to stay within an implicit contract. Schema-first designs can reduce argument errors and improve auditability, whereas prompt-first designs often require additional guardrails to prevent tool misuse and to make failures observable. In practice, hybrid approaches are common, but synthesis benefits from naming which parts of the contract are enforced by tooling (validators, sandboxes) and which are left to the model's behavior. [@Li2026Toolprmbench] [@Shen2024Small; @Doshi2026Towards]

Benchmarks increasingly encode this distinction by making the tool contract explicit. SOP-Bench, for example, operationalizes tool-use evaluation as end-to-end workflows: it contains over 1,800 tasks across 10 industrial domains, with APIs, tool interfaces, and human-validated test cases. This style of benchmark shifts attention from single-call correctness to orchestration reliability under realistic interface constraints. [@Nandi2025Bench; @Mohammadi2025Evaluation]

Data resources further emphasize that tool interfaces are learnable behaviors when a stable contract exists. ToolMind introduces a large-scale tool-agent dataset with 160k synthetic instances generated over many tool types, aiming to improve tool selection and execution through supervised training. Such datasets can raise benchmark performance, but they also increase the importance of protocol reporting: tool availability, schema versions, and execution environment details can change what the model is actually learning to do. [@Yang2025Toolmind; @Cui2025Toward]

Orchestration methods can be viewed as policies over a tool graph: they decide which tool to call next, how to sequence tools, and how to recover from partial failures. Automation-oriented systems focus on discovering tools, composing them, and learning routing heuristics; in contrast, evaluation-oriented systems focus on measuring tool correctness, failure recovery, and budget sensitivity under fixed protocols. This distinction matters because an orchestration improvement that reduces calls under one harness can look like a planning improvement even when it is primarily a routing or caching change. [@Jia2025Autotool] [@Cheng2025Your; @Gao2025Radar]

The evaluation literature reinforces that interface details must be represented in the protocol, not inferred from narrative descriptions. Surveys and core benchmark studies propose taxonomies for agent evaluation, but they also expose how heterogeneous current reporting is, especially for tool access assumptions and budget constraints. A protocol-aware perspective therefore treats tool interface fields - schemas, permissions, sandboxing, observability - as the minimal metadata required to make cross-paper comparisons meaningful. [@Mohammadi2025Evaluation] [@Michelakis2025Core; @Chowa2025From; @Cui2025Toward]

Tool interfaces also interact with coordination and state: orchestration is not only about single-agent routing but also about managing shared resources, memory, and multi-agent communication under constraints. For instance, memory-augmented tooling and coordination frameworks often change the effective action space by adding "retrieve" or "delegate" as first-class operations, which can improve task completion but can also obscure where errors originate (model reasoning, tool outputs, or orchestration policy). [@Lumer2025Memtool; @Liu2025Toolscope]

Several limitations recur in interface and orchestration research. Expressive tool contracts can amplify prompt injection and tool-side manipulation if permissions and monitoring are weak, while overly restrictive contracts can reduce capability by preventing the agent from taking corrective actions. Moreover, benchmarks may underrepresent long-tail tool failures (timeouts, partial outputs, schema drift), so reported gains can be sensitive to the specific harness design. These caveats motivate evaluation protocols that stress tool realism and that report interface fields as first-class variables. [@Zhou2026Beyond] [@Ferrag2025From; @Doshi2026Towards; @Li2026Toolprmbench]

In practice, tool interfaces and orchestration define the operational semantics that later components - planners, memory modules, and adaptation mechanisms - must work within. Once the interface contract is fixed, planning and memory choices become easier to compare because the action space and observation format are stable; when the contract shifts, many "algorithmic" claims reduce to protocol differences. This motivates the next chapter, which treats planning and memory as components whose behavior is only interpretable under explicit interface and budget assumptions. [@Yao2022React; @Mohammadi2025Evaluation]
