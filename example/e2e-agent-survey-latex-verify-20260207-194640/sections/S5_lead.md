Adaptation mechanisms change the agent over time, while coordination mechanisms change the effective agent by distributing cognition across roles. This chapter organizes learning and multi-agent systems around the feedback signals and interaction protocols they assume: what counts as success, how errors are detected, how revisions are applied, and how multiple agents reconcile disagreements. These assumptions directly shape both benchmark outcomes and failure modes such as reward hacking, overfitting to narrow protocols, and brittle aggregation. [@Shinn2023Reflexion] [@Zhou2025Self; @Mohammadi2025Evaluation]

The self-improvement subsection examines critique and revision loops and their stability under repeated interaction, focusing on what is learned (prompts, policies, tool usage) and under which evaluation constraints. The multi-agent subsection covers role specialization and communication/aggregation rules, highlighting that coordination gains are inseparable from protocol choices such as message bandwidth, agent count, and stopping conditions. Read together, they motivate reporting feedback and coordination protocols as first-class metadata when claiming generality. [@Feng2025Group] [@Lu2025Just; @Lumer2025Memtool]
