Related work for LLM agents spans three overlapping categories: (i) surveys and taxonomies that map the broader agentic landscape, (ii) system and method papers that define agent loops, tool interfaces, planning, and memory, and (iii) evaluation and security work that establishes benchmarks, protocols, and threat models. A recurring limitation across these streams is that comparisons are often presented without fully normalizing protocol fields such as tool access, budgets, and safety constraints, which makes head-to-head interpretation fragile even when papers appear to address similar tasks. [@Mohammadi2025Evaluation; @Chowa2025From; @Fu2025Eval]

Surveys of agentic LLMs provide useful overviews of common architectures and application domains, including tool use, planning, feedback learning, and coordination. They typically emphasize the breadth of agent behaviors and the rapid growth of systems, but they vary in how explicitly they represent protocol assumptions and how tightly they connect evaluation results to interface contracts. The present work builds on these mappings and focuses on making protocol fields and interface assumptions explicit as the primary lens for synthesis. [@Plaat2025Agentic; @Li2024Review; @Guo2025Comprehensive; @Yang2025Survey; @Mohammadi2025Evaluation]

Foundational method papers define several reusable agent loop templates that continue to anchor system design. Reasoning-action prompting formalizes iterative decision making with tool calls, while tool-call induction and reflection-based loops treat tool usage and revision as learnable behaviors rather than fixed prompting conventions. Deliberate search approaches frame planning as exploration in a space of intermediate reasoning states, which interacts strongly with budgets and stopping criteria in interactive tasks. These lines of work motivate treating the loop design and interface contract as coupled objects when comparing agent performance. [@Yao2022React; @Schick2023Toolformer; @Shinn2023Reflexion; @Yao2023Tree]

Tool interface and orchestration papers highlight that the action space is not merely a set of tools, but also a contract about schemas, permissions, and observability. Benchmarks built around realistic tool definitions and execution environments stress that tool-use evaluation must model failure recovery, argument correctness, and the interaction format between the agent and tools. Dataset-driven approaches further suggest that tool usage patterns can be improved via scale, but only when tools are standardized and the execution harness is stable enough to provide meaningful feedback signals. [@Liu2025Mcpagentbench; @Nandi2025Bench; @Yang2025Toolmind; @Song2025Agent; @Du2024Anytool]

Planning and reasoning for long-horizon agents is often studied through variants of guided decision making, search, and protocol-aware optimization. A common theme is that measured gains depend on how exploration is budgeted and how tool calls are scheduled relative to internal deliberation; as a result, work that reports improvements in efficiency, cost, or latency should be read jointly with the protocol constraints under which those improvements are achieved. This motivates distinguishing algorithmic changes in the planner from changes in the tool contract and execution environment. [@Yao2023Tree; @Shang2024Agentsquare; @Gao2024Efficient; @Nakano2025Guided; @Zhang2026Evoroute]

Memory and retrieval augmentations are frequently presented as interchangeable modules, but evaluation work suggests that retrieval policy and integration strategy materially affect both correctness and robustness. Systems that emphasize agent memory for tool use and browsing must confront citation and grounding issues, as well as the susceptibility of retrieval channels to injection and stale context. In practice, memory design also changes the agent loop: it determines what evidence can be used for planning and when the agent should re-query or revise. [@Shi2025Progent; @Abbineni2025Muallm; @Li2025Agentswift; @Dong2025Etom; @Maragheh2025Future]

Learning-based adaptation and self-improvement papers extend agents beyond static prompting by introducing feedback-driven revision, preference shaping, and evaluation-guided tuning. These mechanisms make it easier to report improvements on benchmark suites, but they also increase the risk of overfitting to narrow protocols or exploiting metric artifacts. As a consequence, adaptation results are best interpreted together with the evaluation setting, including which tools are accessible, how many steps are allowed, and what failure behaviors are penalized. [@Shinn2023Reflexion; @Zhou2025Self; @Li2026Autonomous; @Zhang2026Evoroute]

Multi-agent coordination introduces additional protocol degrees of freedom: role definitions, communication formats, verification strategies, and aggregation rules. Related work covers both cooperative setups (task decomposition and planning) and adversarial or debate-style variants (verification and robustness), but comparisons are often inconsistent because messaging bandwidth, agent count, and stopping rules are not standardized. Empirical results therefore require careful reading of the interaction protocol and the evaluation harness rather than relying on high-level architectural labels alone. [@Feng2025Group; @Lu2025Just; @Lumer2025Memtool; @Yang2024Based]

Evaluation-focused papers and benchmark suites increasingly treat tool-using agents as a distinct class from static LLMs. They catalog task families and protocols, propose new suites with industrial workflows or realistic tool schemas, and emphasize reproducibility concerns such as leakage, hidden prompts, and uncontrolled environment variability. These contributions are complementary to architecture-focused surveys: they provide the protocol vocabulary (tasks, metrics, budgets, threat models) needed to make cross-paper synthesis interpretable. [@Mohammadi2025Evaluation; @Chowa2025From; @Yin2024Mmau; @Liu2025Mcpagentbench; @Nandi2025Bench]

Security and governance work demonstrates that tool access expands the threat surface, and that standard success metrics can be misleading under attack. Analyses of prompt injection and tool abuse quantify attack success rates and privacy leakage, while guardrail and monitoring frameworks explore how to trade off robustness and benign utility under explicit adversarial models. These lines of work motivate incorporating threat models and safety constraints into the same protocol description used for performance evaluation, rather than treating security as a separate post-hoc concern. [@Zhang2025Security; @Gasmi2025Bridging; @Zhong2025Rtbas; @Alizadeh2025Simple; @Kale2025Reliable; @Mou2026Toolsafe; @Mo2025Attractive]

Recent work also treats agents as engineered systems, emphasizing execution sandboxes, tool-call datasets, and measurement harnesses that enable reproducible comparisons across models and interfaces. Examples include small-model function-calling pipelines and infrastructure-oriented evaluations, as well as system studies that surface how protocol choices (statefulness, time windows, interaction format) constrain external validity. Complementary system papers further stress that environment design and tracing affect reliability, from scaling interactive environments to specialized agent platforms and domain deployments. [@Erdogan2024Tinyagent; @Zhang2025Datascibench; @Kwon2025Agentnet; @Cui2025Toward; @Tomaevi2025Towards; @Song2026Envscaler; @Soliman2026Intagent; @Kim2025Bridging; @Fumero2025Cybersleuth; @Ghose2025Orfs; @Fang2025Should; @Bonagiri2025Check; @Wang2023Voyager]

Taken together, the above literature suggests that a survey of LLM agents benefits from a protocol-aware synthesis: interface contracts and budgets determine the meaning of reported numbers, and risk models determine whether gains persist outside benign settings. The organization adopted here reflects this perspective by grouping work around (i) interfaces and loop design, (ii) core components for planning and memory, (iii) adaptation and coordination mechanisms, and (iv) evaluation and risks, so that comparisons are made within compatible protocols and limitations remain visible rather than being smoothed away. [@Yao2022React; @Zhang2026Evoroute; @Mohammadi2025Evaluation; @Fu2025Eval; @Zhang2025Security]
