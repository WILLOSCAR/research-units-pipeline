Multi-agent coordination treats an "agent" as a team: capability comes from specialization and verification across roles rather than from a single loop. A central trade-off is specialization versus coordination - dividing labor can boost performance, but it adds communication overhead, introduces new failure modes, and makes evaluation protocol choices (agent count, message budget, stopping rules) decisive. A protocol-aware synthesis therefore focuses on interaction contracts: what roles exist, how information is exchanged, and how disagreements are resolved. [@Yang2024Based] [@Sarkar2025Survey; @Yim2024Evaluating]

Coordination patterns span several families. Some systems use role specialization for decomposition (planner, executor, reviewer), others use debate or referee mechanisms for verification, and others use aggregation (vote, rank, consensus) to stabilize noisy decisions. These patterns differ in what they assume about observability and trust: coordination can amplify errors if agents share the same flawed evidence, whereas diversity can help when independent perspectives are available. [@Cao2025Skyrl] [@Oueslati2025Refagent; @Zhang2025Bridging]

A useful contrast is between centralized orchestration and decentralized coordination. Centralized orchestration can impose consistent tool access and enforce budgets, whereas decentralized setups may be more flexible but can suffer from runaway communication and unstable convergence. However, the two are often compared under different protocols; careful evaluation therefore needs to report message budgets, aggregation rules, and whether agents share memory or tool outputs. [@Shen2024Small] [@Wu2024Federated; @Yim2024Evaluating]

Concrete benchmarks show how sensitive coordination is to evaluation setup. Using only 400 problem instances from the Berkeley Function-Calling Leaderboard (BFCL), one method reports competitive in-distribution performance while improving generalization, highlighting that coordination benefits can be measured even under relatively small test sets. At the same time, small test sets can under-sample rare coordination failures, so reported gains should be interpreted with attention to variance and protocol details. [@Lu2025Just; @Yim2024Evaluating]

Memory-sharing and tool mediation are common coordination mechanisms. MemTool evaluates different coordination modes across 13+ LLMs on the ScaleMCP benchmark, with experiments over 100 consecutive user interactions; this style of evaluation makes coordination overhead observable and ties performance to an explicit interaction protocol. Such results underscore that coordination is not free: policies that improve success can increase cost or latency, which must be part of the claim when coordination is justified as an engineering strategy. [@Lumer2025Memtool; @Gao2025Radar]

Coordination is also used to improve planning and tool-integrated reasoning on hard agent benchmarks. GiGPO is evaluated on ALFWorld and WebShop as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct as base models. This illustrates a common pattern: multi-agent designs often trade off model size against interaction structure, using coordination to compensate for weaker individual agents under a fixed tool interface. [@Feng2025Group; @Lu2025Just]

Evaluation methodology matters because multi-agent systems can "win" by exploiting protocol artifacts (e.g., hidden hints in shared memory, or aggregation rules that mask individual failures). Work on generating ground truth and validating evaluation metrics emphasizes that agent scoring must be tied to observable behaviors, not only to end outcomes, especially when multiple agents contribute. Protocol-aware evaluation can therefore benefit from logging communication traces and attributing errors to specific roles or messages. [@Zhang2025Datascibench; @Yim2024Evaluating]

Several coordination designs emphasize robustness through verification, but robustness depends on the threat model. Coordination can reduce single-agent hallucinations, yet it can also create correlated failures when agents share the same adversarial context or when a single compromised tool output is broadcast to the team. In contrast to single-agent settings, coordination protocols must specify trust boundaries: what is verified, who can call tools, and how tool outputs are sanitized before being shared. [@Zhang2025Bridging] [@Shen2024Small; @Maragheh2025Future]

Limitations and risks are prominent. Coordination increases surface area for instability (non-convergence, oscillation, or premature consensus), and it can degrade under tight latency or message budgets. Moreover, coordination success can be brittle to agent count and role definitions, making it unclear whether reported gains reflect general mechanisms or narrow protocol tuning. These caveats motivate reporting coordination protocol fields as explicitly as tool schemas. [@Sarkar2025Survey] [@Maragheh2025Future; @Li2025Leveraging]

One way to read the evidence is that multi-agent coordination should be synthesized as a family of protocols, not as a single architectural choice: the same roles can behave very differently under different budgets, memory-sharing policies, and aggregation rules. This perspective connects directly to evaluation and risk considerations, where the choice of benchmarks, logging, and threat assumptions determines which coordination claims are reproducible and which are artifacts of a particular harness. [@Yim2024Evaluating] [@Sarkar2025Survey; @Yang2025Survey]
