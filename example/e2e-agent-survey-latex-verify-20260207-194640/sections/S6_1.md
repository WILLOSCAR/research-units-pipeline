Benchmarks and evaluation protocols decide which claims about agents are meaningful. A core tension is coverage versus comparability: broader suites capture more behaviors, but they also make head-to-head comparison fragile when protocols and constraints differ. A protocol-aware view therefore treats evaluation as a specification problem - tasks, metrics, budgets, tool access, and threat assumptions must be explicit - rather than as a post-hoc score attached to a system description. [@Mohammadi2025Evaluation] [@Michelakis2025Core; @Ji2025Taxonomy]

Evaluation protocols for tool-using agents include more moving parts than static LLM evaluation. Beyond task and metric choice, protocols define interaction formats (observations, action schemas), budget constraints (steps, latency, cost), termination rules, and what constitutes permissible evidence. Human evaluation and safety constraints can further change the interpretation of the same success rate. As a result, two papers can report similar scores while evaluating different objects, motivating explicit protocol fields in benchmark metadata and in paper reporting. [@Mohammadi2025Evaluation] [@Testini2025Measuring; @Wang2025Flow]

A useful contrast is between benchmark suites designed for breadth and benchmarks designed for diagnostic comparability. Broad suites aim to cover diverse environments and task families, whereas diagnostic benchmarks isolate specific capabilities such as tool invocation correctness, grounded retrieval, or long-horizon planning under a fixed interface. In practice, both are necessary: breadth supports external validity, but diagnostic structure is needed to attribute gains to specific components rather than to protocol quirks. [@Peng2024Survey] [@Zheng2025Newtonbench; @Kwon2025Agentnet]

Surveys and taxonomies of agent evaluation increasingly emphasize this protocol gap. They catalog benchmark dimensions and propose organizing frameworks, but they also highlight that many evaluation reports omit key fields such as tool access, budget assumptions, or whether hidden prompts and environment variability are controlled. A direct implication is that evaluation results should be treated as conditional evidence unless protocol metadata is sufficient for replication. [@Mohammadi2025Evaluation] [@Ji2025Taxonomy; @Kim2026Beyond]

The benchmark landscape is also expanding rapidly. One comprehensive survey connects 50+ benchmarks to corresponding solution strategies, which helps readers map what is being tested and which methods are commonly applied. At the same time, this growth makes it easier to cherry-pick results; protocol-aware synthesis therefore benefits from tracking which benchmarks share the same interaction format and constraints, and which do not. [@Guo2025Comprehensive; @Peng2024Survey]

Tool-use agent benchmarks illustrate how interaction constraints shape measured capability. On multi-turn tool-use benchmarks such as M3ToolEval and TauBench, self-improvement frameworks report large gains, suggesting that revision policies and tool-selection behaviors are central. However, these benchmarks also depend on how tool calls are counted, what constitutes a valid argument, and how partial failures are handled; without those protocol details, it is unclear whether gains reflect better decision making or looser harness assumptions. [@Zhou2025Self; @Michelakis2025Core]

Security-oriented benchmarks make the protocol and threat model explicit by construction. On the AgentDojo prompt injection benchmark, RTBAS reports preventing all targeted attacks with only a 2% loss of task utility under attack, while other studies quantify that LLM utility can drop by 15-50 percentage points under attack with average attack success rates around 20% in similar settings. These results show why evaluation protocols must include adversary models and defense policies; otherwise, robustness claims are not comparable. [@Zhong2025Rtbas] [@Alizadeh2025Simple; @Hadeliya2025When]

Measurement methodology is therefore part of the benchmark contribution. Work on core evaluation design and on measuring agent behavior emphasizes logging and attribution: what the agent saw, what it did, and how errors map to protocol fields. This is especially important for tool-using agents because end outcomes can hide brittle behaviors such as over-reliance on privileged tool outputs, hidden leakage, or unstable retries. Benchmark suites that publish traces and explicit schemas make it easier to diagnose such issues. [@Michelakis2025Core] [@Testini2025Measuring; @Kwon2025Agentnet; @Wang2025Flow]

Limitations remain. Benchmarks can leak information through environment artifacts, prompt templates, or tool descriptions, and many suites underrepresent long-tail failures such as timeouts and partial tool outputs. In addition, comparability can break when different papers use different tool schemas or evaluation scripts under the same benchmark name. These caveats motivate reproducible releases (schemas, scripts, and traces) and caution against overinterpreting small score differences. [@Fu2025Eval] [@Hu2023Avis; @Doshi2026Towards]

Taken together, benchmarks and protocols should be read as defining the object of evaluation: a closed-loop system under constraints. Protocol-aware synthesis therefore emphasizes which fields are comparable across papers and which require normalization before claims can be aggregated. This perspective also sets up the next subsection on risks, where threat models, monitoring, and governance constraints become part of the evaluation protocol rather than external concerns. [@Mohammadi2025Evaluation] [@Fu2025Eval; @Agrawal2025Language]
