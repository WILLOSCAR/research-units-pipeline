{"sub_id": "3.1", "title": "Agent loop and action spaces", "section_id": "3", "section_title": "Foundations & Interfaces", "rq": "Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?", "thesis": "For Agent loop and action spaces, evaluation protocol (datasets and metrics is a recurring axis of variation, and results are easiest to interpret when protocols and failure assumptions are explicit.", "tension_statement": "In Agent loop and action spaces, a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "decision-first", "opener_hint": "Start with a builder/research decision; state what it depends on; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets", "metrics", "human evaluation)", "compute and latency constraints", "and failure modes and limitations"], "bridge_terms": ["benchmarks/metrics", "compute"], "contrast_hook": "evaluation", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Planning / reasoning loops", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Planning / reasoning loops vs Agent frameworks / architectures)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Planning / reasoning loops", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Planning / reasoning loops", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Planning / reasoning loops)", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Tool-use and function calling"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Tool-use and function calling"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Tool-use and function calling"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agentic LLM systems / LLM agents survey (interfaces, planning, tool use, multi-agent, evaluation, safety).", "Compare approaches along: evaluation protocol (datasets.", "Compare approaches along: metrics.", "Compare approaches along: human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: and failure modes and limitations."], "chapter_key_contrasts": ["evaluation", "tool interfaces"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhang2026Evoroute", "Kim2025Bridging", "Shen2024Small", "Zhao2025Achieving", "Li2025Agentswift", "Shang2024Agentsquare", "Fumero2025Cybersleuth", "Feng2025Group", "Zheng2025Towards", "Yao2022React", "Xi2026Toolgym", "Ghose2025Orfs", "Sun2025Agent", "Lin2026Froav", "Song2026Envscaler"], "allowed_bibkeys_mapped": ["Zheng2025Towards", "Kim2025Bridging", "Feng2025Group", "Wu2025Meta", "Zhao2025Achieving", "Lin2026Froav", "Li2025Agentswift", "Gasmi2025Bridging", "Sun2025Agent", "Ghose2025Orfs", "Nusrat2025Automated", "Shang2024Agentsquare", "Xi2026Toolgym", "Fumero2025Cybersleuth", "Shen2024Small", "Song2026Envscaler", "Zhang2026Evoroute", "Yao2022React"], "allowed_bibkeys_chapter": ["Bulusu2024Mathviz", "Chen2025Agentguard", "Cheng2025Your", "Cui2025Toward", "Dong2025Etom", "Du2024Anytool", "Feng2025Group", "Fu2024Imprompter", "Fumero2025Cybersleuth", "Gasmi2025Bridging", "Ghose2025Orfs", "Hao2026From", "Jia2025Autotool", "Kim2025Bridging", "Li2025Agentswift", "Li2025Dissonances", "Li2026Toolprmbench", "Lin2026Froav", "Liu2025Mcpagentbench", "Lumer2025Memtool", "Mohammadi2025Evaluation", "Nusrat2025Automated", "Shang2024Agentsquare", "Shen2024Small", "Song2026Envscaler", "Sun2025Agent", "Wu2025Meta", "Xi2026Toolgym", "Xuan2026Confidence", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving", "Zheng2025Towards"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0092-60cc0d458f", "E-P0016-9d9d60644a", "E-P0027-c92ed293ba", "E-P0199-1063eee7ce", "E-P0014-904ba35500", "E-P0078-38a26e4777", "E-P0119-c8c4670812", "E-P0125-4b027dfb27", "E-P0156-e3f7ba21be", "E-P0001-ca4a00b5cf", "E-P0032-895b04aa5c", "E-P0140-1d5f67b08e", "E-P0032-8c9597d805", "E-P0027-9640816b42", "E-P0132-027093d5f5", "E-P0140-ddd045953e", "E-P0187-048b8b704f", "E-P0091-f1a25e82c1"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0078", "evidence_id": "E-P0078-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["Zhao2025Achieving"], "paper_id": "P0199", "evidence_id": "E-P0199-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[0]"}, {"hook_type": "quant", "text": "We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design.", "citations": ["Fumero2025Cybersleuth"], "paper_id": "P0119", "evidence_id": "E-P0119-c8c4670812", "pointer": "papers/paper_notes.jsonl:paper_id=P0119#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0125", "evidence_id": "E-P0125-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0125#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate.", "citations": ["Zheng2025Towards"], "paper_id": "P0156", "evidence_id": "E-P0156-e3f7ba21be", "pointer": "papers/paper_notes.jsonl:paper_id=P0156#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Li2025Agentswift", "Zhang2026Evoroute", "Shang2024Agentsquare"], "A_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}], "B_highlights": [{"paper_id": "P0078", "evidence_id": "E-P0078-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}, {"paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'evaluation protocol (datasets'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "metrics", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhang2026Evoroute", "Li2025Agentswift", "Shang2024Agentsquare"], "A_highlights": [{"paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}], "B_highlights": [{"paper_id": "P0078", "evidence_id": "E-P0078-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}, {"paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'metrics'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhang2026Evoroute", "Li2025Agentswift", "Shang2024Agentsquare"], "A_highlights": [{"paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}], "B_highlights": [{"paper_id": "P0078", "evidence_id": "E-P0078-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}, {"paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhang2026Evoroute", "Li2025Agentswift", "Yao2022React", "Shang2024Agentsquare"], "A_highlights": [{"paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"paper_id": "P0078", "evidence_id": "E-P0078-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, DeepSeek-v3, LLM-simulated, SFT, RUC-NLPIR, EnvScaler, SkelBuilder, ScenGenerator, GAIA, EvoRoute.", "citations": ["Xi2026Toolgym", "Song2026Envscaler", "Zhang2026Evoroute", "Lin2026Froav"]}], "limitation_hooks": [{"excerpt": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"], "pointer": ""}, {"excerpt": "Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge.", "citations": ["Li2025Agentswift"], "pointer": ""}, {"excerpt": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "citations": ["Gasmi2025Bridging"], "pointer": ""}, {"excerpt": "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "citations": ["Gasmi2025Bridging"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": ["Evaluation protocol bullets are token-list style; avoid copying them as sentences. Prefer subsection-specific benchmarks/metrics from `anchor_facts` and state task/metric/constraint explicitly."], "pack_stats": {"anchors": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-22T09:47:06"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "section_id": "3", "section_title": "Foundations & Interfaces", "rq": "Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Tool interfaces and orchestration, differences in evaluation protocol (datasets and metrics frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "tension_statement": "In Tool interfaces and orchestration, a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "tension-first", "opener_hint": "Start with the subsectionâ€™s central tension/trade-off; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets", "metrics", "human evaluation)", "compute and latency constraints", "and failure modes and limitations"], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability", "benchmarks/metrics"], "contrast_hook": "tool interfaces", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Tool-use and function calling", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Tool-use and function calling vs Agent frameworks / architectures)", "use_clusters": ["Tool-use and function calling"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Tool-use and function calling", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Tool-use and function calling"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Tool-use and function calling", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Tool-use and function calling"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Tool-use and function calling", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Tool-use and function calling)", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Evaluation / benchmark-focused works"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Evaluation / benchmark-focused works"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Evaluation / benchmark-focused works"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agentic LLM systems / LLM agents survey (interfaces, planning, tool use, multi-agent, evaluation, safety).", "Compare approaches along: evaluation protocol (datasets.", "Compare approaches along: metrics.", "Compare approaches along: human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: and failure modes and limitations."], "chapter_key_contrasts": ["evaluation", "tool interfaces"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Mohammadi2025Evaluation", "Dong2025Etom", "Liu2025Mcpagentbench", "Li2025Dissonances", "Du2024Anytool", "Lumer2025Memtool", "Yao2022React", "Shen2024Small", "Bulusu2024Mathviz", "Li2026Toolprmbench", "Hao2026From", "Xuan2026Confidence", "Ghose2025Orfs"], "allowed_bibkeys_mapped": ["Dong2025Etom", "Chen2025Agentguard", "Li2026Toolprmbench", "Cheng2025Your", "Jia2025Autotool", "Cui2025Toward", "Du2024Anytool", "Bulusu2024Mathviz", "Shen2024Small", "Hao2026From", "Xuan2026Confidence", "Liu2025Mcpagentbench", "Lumer2025Memtool", "Ghose2025Orfs", "Mohammadi2025Evaluation", "Li2025Dissonances", "Fu2024Imprompter", "Yao2022React"], "allowed_bibkeys_chapter": ["Bulusu2024Mathviz", "Chen2025Agentguard", "Cheng2025Your", "Cui2025Toward", "Dong2025Etom", "Du2024Anytool", "Feng2025Group", "Fu2024Imprompter", "Fumero2025Cybersleuth", "Gasmi2025Bridging", "Ghose2025Orfs", "Hao2026From", "Jia2025Autotool", "Kim2025Bridging", "Li2025Agentswift", "Li2025Dissonances", "Li2026Toolprmbench", "Lin2026Froav", "Liu2025Mcpagentbench", "Lumer2025Memtool", "Mohammadi2025Evaluation", "Nusrat2025Automated", "Shang2024Agentsquare", "Shen2024Small", "Song2026Envscaler", "Sun2025Agent", "Wu2025Meta", "Xi2026Toolgym", "Xuan2026Confidence", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving", "Zheng2025Towards"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0047-37f9ea924c", "E-P0017-192e78b614", "E-P0056-f7a14123f9", "E-P0054-fae121f81b", "E-P0080-d5c234444e", "E-P0058-35271418ac", "E-P0001-ca4a00b5cf", "E-P0017-55ce44af76", "E-P0017-6ee6d5b951", "E-P0027-9640816b42", "E-P0171-b9de76d922", "E-P0033-3e2edc05cd", "E-P0056-3a4792de2b", "E-P0056-67d5c4342c", "E-P0093-ed4427964c", "E-P0098-c2fdc5ad72", "E-P0033-9861661010", "E-P0140-1d5f67b08e"], "anchor_facts": [{"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "paper_id": "P0054", "evidence_id": "E-P0054-fae121f81b", "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "eval", "text": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "paper_id": "P0017", "evidence_id": "E-P0017-192e78b614", "pointer": "papers/paper_notes.jsonl:paper_id=P0017#method"}, {"hook_type": "eval", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "paper_id": "P0056", "evidence_id": "E-P0056-f7a14123f9", "pointer": "papers/paper_notes.jsonl:paper_id=P0056#limitations[1]"}, {"hook_type": "quant", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "paper_id": "P0080", "evidence_id": "E-P0080-d5c234444e", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Mohammadi2025Evaluation", "Lumer2025Memtool", "Dong2025Etom"], "A_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}], "B_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0017", "evidence_id": "E-P0017-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0017#method"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'evaluation protocol (datasets'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "metrics", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Mohammadi2025Evaluation", "Li2025Dissonances", "Lumer2025Memtool"], "A_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0054", "evidence_id": "E-P0054-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}], "B_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'metrics'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Mohammadi2025Evaluation", "Li2025Dissonances", "Lumer2025Memtool"], "A_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0054", "evidence_id": "E-P0054-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}], "B_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Li2025Dissonances", "Mohammadi2025Evaluation", "Lumer2025Memtool"], "A_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}], "B_highlights": [{"paper_id": "P0054", "evidence_id": "E-P0054-fae121f81b", "excerpt": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: PRMs, PRM, ToolPRMBench, API, GPT-5, HardGen, LLMs, AgentGuard, LLM-based, XTHP.", "citations": ["Li2026Toolprmbench", "Hao2026From", "Xuan2026Confidence", "Chen2025Agentguard"]}], "limitation_hooks": [{"excerpt": "Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces.", "citations": ["Hao2026From"], "pointer": ""}, {"excerpt": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge.", "citations": ["Xuan2026Confidence"], "pointer": ""}, {"excerpt": "We propose AgentGuard, a framework to autonomously discover and validate unsafe tool-use workflows, followed by generating safety constraints to confine the behaviors of agents, achieving the baseline of safety guarantee at deployment.", "citations": ["Chen2025Agentguard"], "pointer": ""}, {"excerpt": "The framework operates through four phases: identifying unsafe workflows, validating them in real-world execution, generating safety constraints, and validating constraint efficacy.", "citations": ["Chen2025Agentguard"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": ["Evaluation protocol bullets are token-list style; avoid copying them as sentences. Prefer subsection-specific benchmarks/metrics from `anchor_facts` and state task/metric/constraint explicitly."], "pack_stats": {"anchors": {"raw": 9, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-22T09:47:06"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "rq": "Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?", "thesis": "For Planning and reasoning loops, evaluation protocol (datasets and metrics is a recurring axis of variation, and results are easiest to interpret when protocols and failure assumptions are explicit.", "tension_statement": "In Planning and reasoning loops, a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "tension-first", "opener_hint": "Start with the subsectionâ€™s central tension/trade-off; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets", "metrics", "human evaluation)", "compute and latency constraints", "and failure modes and limitations"], "bridge_terms": ["planner/executor", "search", "deliberation", "action grounding", "benchmarks/metrics", "compute"], "contrast_hook": "planning/control loop", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Planning / reasoning loops", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Planning / reasoning loops vs Agent frameworks / architectures)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Planning / reasoning loops", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Planning / reasoning loops", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Planning / reasoning loops)", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Control / conditioning interfaces"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Control / conditioning interfaces"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Control / conditioning interfaces"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agentic LLM systems / LLM agents survey (interfaces, planning, tool use, multi-agent, evaluation, safety).", "Compare approaches along: evaluation protocol (datasets.", "Compare approaches along: metrics.", "Compare approaches along: human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: and failure modes and limitations."], "chapter_key_contrasts": ["planning/control loop", "memory/retrieval"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Hu2025Training", "Yin2024Safeagentbench", "Seo2025Simuhome", "Yao2022React", "Zhou2025Siraj", "Khoee2025Gatelens", "Wang2025Automated", "Ji2024Testing", "Silva2025Agents", "Zhou2025Reasoning", "Choi2025Reactree", "Kim2025Bridging", "Nusrat2025Automated"], "allowed_bibkeys_mapped": ["Hu2025Training", "Hong2025Planning", "Kim2025Bridging", "Hatalis2025Review", "Nusrat2025Automated", "Silva2025Agents", "Seo2025Simuhome", "Wang2025Automated", "Yin2024Safeagentbench", "Zhou2025Reasoning", "Kiruluta2025Novel", "Khoee2025Gatelens", "Choi2025Reactree", "Ji2024Testing", "Zhou2025Siraj", "Zhu2025Where", "Li2024Personal", "Yao2022React"], "allowed_bibkeys_chapter": ["Anokhin2024Arigraph", "Chen2025Grounded", "Choi2025Reactree", "Hatalis2025Review", "Hong2025Planning", "Hu2025Training", "Huang2025Retrieval", "Ji2024Testing", "Khoee2025Gatelens", "Kim2025Bridging", "Kiruluta2025Novel", "Li2024Personal", "Li2025Agentswift", "Lin2026Froav", "Nusrat2025Automated", "Seo2025Simuhome", "Shi2025Progent", "Silva2025Agents", "Tawosi2025Meta", "Verma2026Active", "Wang2025Automated", "Wu2025Meta", "Xu2025Agentic", "Yao2022React", "Ye2025Task", "Ye2025Taska", "Yin2024Safeagentbench", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large", "Zhang2025Security", "Zhou2025Reasoning", "Zhou2025Siraj", "Zhu2025Where"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0024-771620f84f", "E-P0087-076695cd77", "E-P0151-e2e3b7fa97", "E-P0151-469a70bb44", "E-P0001-ca4a00b5cf", "E-P0064-0b753b9422", "E-P0087-c2e8e2bb7f", "E-P0124-8afa74a630", "E-P0208-32aec6c669", "E-P0151-7b36039ae4", "E-P0176-c0a98eb625", "E-P0043-baa622fa7f", "E-P0021-e38b4bdff3", "E-P0043-b35b53de13", "E-P0124-6664c6cecd", "E-P0146-4bcafdb221", "E-P0016-04c60086db", "E-P0209-71f2629f1b"], "anchor_facts": [{"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"hook_type": "quant", "text": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["Hu2025Training"], "paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"hook_type": "eval", "text": "To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards.", "citations": ["Yin2024Safeagentbench"], "paper_id": "P0087", "evidence_id": "E-P0087-076695cd77", "pointer": "papers/paper_notes.jsonl:paper_id=P0087#method"}, {"hook_type": "quant", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "citations": ["Seo2025Simuhome"], "paper_id": "P0151", "evidence_id": "E-P0151-e2e3b7fa97", "pointer": "papers/paper_notes.jsonl:paper_id=P0151#limitations[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "SafeAgentBench includes: (1) an executable, diverse, and high-quality dataset of 750 tasks, rigorously curated to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17", "citations": ["Yin2024Safeagentbench"], "paper_id": "P0087", "evidence_id": "E-P0087-c2e8e2bb7f", "pointer": "papers/paper_notes.jsonl:paper_id=P0087#key_results[0]"}, {"hook_type": "quant", "text": "Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation.", "citations": ["Khoee2025Gatelens"], "paper_id": "P0124", "evidence_id": "E-P0124-8afa74a630", "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Hu2025Training", "Zhou2025Siraj"], "A_highlights": [{"paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "excerpt": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}], "B_highlights": [{"paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "excerpt": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'evaluation protocol (datasets'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "metrics", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Hu2025Training", "Zhou2025Siraj"], "A_highlights": [{"paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "excerpt": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}], "B_highlights": [{"paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "excerpt": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'metrics'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Hu2025Training", "Zhou2025Siraj"], "A_highlights": [{"paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "excerpt": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}], "B_highlights": [{"paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "excerpt": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Hu2025Training", "Zhou2025Siraj"], "A_highlights": [{"paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "excerpt": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}], "B_highlights": [{"paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "excerpt": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: SCL, CCAM, GPT-4o-powered, ReAct, AutoGPT, RSP, GSI, RSV, FEVER, RSP-M.", "citations": ["Kim2025Bridging", "Zhou2025Reasoning", "Hu2025Training", "Silva2025Agents"]}], "limitation_hooks": [{"excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": ""}, {"excerpt": "Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making.", "citations": ["Hatalis2025Review"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": ["Evaluation protocol bullets are token-list style; avoid copying them as sentences. Prefer subsection-specific benchmarks/metrics from `anchor_facts` and state task/metric/constraint explicitly."], "pack_stats": {"anchors": {"raw": 10, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-22T09:47:06"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "rq": "Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?", "thesis": "For Memory and retrieval (RAG), evaluation protocol (datasets and metrics is a recurring axis of variation, and results are easiest to interpret when protocols and failure assumptions are explicit.", "tension_statement": "In Memory and retrieval (RAG), a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "tension-first", "opener_hint": "Start with the subsectionâ€™s central tension/trade-off; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets", "metrics", "human evaluation)", "compute and latency constraints", "and failure modes and limitations"], "bridge_terms": ["retrieval", "index", "write policy", "long-term memory", "benchmarks/metrics", "compute"], "contrast_hook": "memory/retrieval", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Memory / retrieval augmentation", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Memory / retrieval augmentation vs Agent frameworks / architectures)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Memory / retrieval augmentation", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Memory / retrieval augmentation", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Memory / retrieval augmentation", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Memory / retrieval augmentation)", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agentic LLM systems / LLM agents survey (interfaces, planning, tool use, multi-agent, evaluation, safety).", "Compare approaches along: evaluation protocol (datasets.", "Compare approaches along: metrics.", "Compare approaches along: human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: and failure modes and limitations."], "chapter_key_contrasts": ["planning/control loop", "memory/retrieval"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Tawosi2025Meta", "Zhang2025Security", "Huang2025Retrieval", "Shi2025Progent", "Yao2022React", "Li2025Agentswift", "Verma2026Active", "Zhang2024Large", "Zhang2025Large", "Yu2026Agentic", "Lin2026Froav", "Chen2025Grounded", "Ye2025Task"], "allowed_bibkeys_mapped": ["Yu2026Agentic", "Huang2025Retrieval", "Wu2025Meta", "Ye2025Task", "Zhang2025Large", "Lin2026Froav", "Tawosi2025Meta", "Chen2025Grounded", "Shi2025Progent", "Xu2025Agentic", "Zhang2024Large", "Verma2026Active", "Zhang2025Security", "Ye2025Taska", "Anokhin2024Arigraph", "Li2025Agentswift", "Zhu2025Where", "Yao2022React"], "allowed_bibkeys_chapter": ["Anokhin2024Arigraph", "Chen2025Grounded", "Choi2025Reactree", "Hatalis2025Review", "Hong2025Planning", "Hu2025Training", "Huang2025Retrieval", "Ji2024Testing", "Khoee2025Gatelens", "Kim2025Bridging", "Kiruluta2025Novel", "Li2024Personal", "Li2025Agentswift", "Lin2026Froav", "Nusrat2025Automated", "Seo2025Simuhome", "Shi2025Progent", "Silva2025Agents", "Tawosi2025Meta", "Verma2026Active", "Wang2025Automated", "Wu2025Meta", "Xu2025Agentic", "Yao2022React", "Ye2025Task", "Ye2025Taska", "Yin2024Safeagentbench", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large", "Zhang2025Security", "Zhou2025Reasoning", "Zhou2025Siraj", "Zhu2025Where"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0019-f36b515991", "E-P0055-8bcb673a7d", "E-P0158-c9781caf3b", "E-P0055-d6095e10e9", "E-P0060-68db58914f", "E-P0158-4af0cf3c02", "E-P0001-ca4a00b5cf", "E-P0014-904ba35500", "E-P0055-7a6ec4daed", "E-P0184-9abcf1bf8a", "E-P0025-52fea1d199", "E-P0135-897bcc2f50", "E-P0185-f0f0faaada", "E-P0187-048b8b704f", "E-P0187-db4213c234", "E-P0051-af945eb2fa", "E-P0067-53536132a8", "E-P0135-79064ef6b3"], "anchor_facts": [{"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "quant", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "paper_id": "P0019", "evidence_id": "E-P0019-f36b515991", "pointer": "papers/paper_notes.jsonl:paper_id=P0019#key_results[1]"}, {"hook_type": "quant", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "paper_id": "P0184", "evidence_id": "E-P0184-9abcf1bf8a", "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}, {"hook_type": "quant", "text": "SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability.", "citations": ["Huang2025Retrieval"], "paper_id": "P0158", "evidence_id": "E-P0158-c9781caf3b", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#limitations[1]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Zhang2025Security", "Li2025Agentswift", "Tawosi2025Meta", "Verma2026Active"], "A_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}], "B_highlights": [{"paper_id": "P0019", "evidence_id": "E-P0019-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#key_results[1]"}, {"paper_id": "P0184", "evidence_id": "E-P0184-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'evaluation protocol (datasets'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "metrics", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Zhang2025Security", "Shi2025Progent", "Tawosi2025Meta", "Verma2026Active"], "A_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}], "B_highlights": [{"paper_id": "P0019", "evidence_id": "E-P0019-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#key_results[1]"}, {"paper_id": "P0184", "evidence_id": "E-P0184-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'metrics'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Zhang2025Security", "Shi2025Progent", "Tawosi2025Meta", "Verma2026Active"], "A_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}], "B_highlights": [{"paper_id": "P0019", "evidence_id": "E-P0019-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#key_results[1]"}, {"paper_id": "P0184", "evidence_id": "E-P0184-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Zhang2025Security", "Shi2025Progent", "Tawosi2025Meta", "Verma2026Active"], "A_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}], "B_highlights": [{"paper_id": "P0019", "evidence_id": "E-P0019-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0019#key_results[1]"}, {"paper_id": "P0184", "evidence_id": "E-P0184-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: SWE-bench, LTM, STM, GRPO, AgeMem, LLMs, LLM-based, FROAV, RAG, LLM-as-a-Judge.", "citations": ["Verma2026Active", "Yu2026Agentic", "Lin2026Froav", "Li2025Agentswift"]}], "limitation_hooks": [{"excerpt": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical.", "citations": ["Yu2026Agentic"], "pointer": ""}, {"excerpt": "Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge.", "citations": ["Li2025Agentswift"], "pointer": ""}, {"excerpt": "This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time.", "citations": ["Chen2025Grounded"], "pointer": ""}, {"excerpt": "While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O.", "citations": ["Zhang2025Security"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": ["Evaluation protocol bullets are token-list style; avoid copying them as sentences. Prefer subsection-specific benchmarks/metrics from `anchor_facts` and state task/metric/constraint explicitly."], "pack_stats": {"anchors": {"raw": 11, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-22T09:47:06"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "rq": "Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Self-improvement and adaptation methods emphasize evaluation protocol (datasets and metrics trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "tension_statement": "In Self-improvement and adaptation, a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "lens-first", "opener_hint": "Start by naming the lens (interface/protocol/threat model); state what it reveals; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets", "metrics", "human evaluation)", "compute and latency constraints", "and failure modes and limitations"], "bridge_terms": ["preference", "reward", "feedback", "self-improvement", "benchmarks/metrics", "compute"], "contrast_hook": "learning/feedback", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Planning / reasoning loops", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Planning / reasoning loops vs Agent frameworks / architectures)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Planning / reasoning loops", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Planning / reasoning loops"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Planning / reasoning loops", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Planning / reasoning loops)", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Tool-use and function calling"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Tool-use and function calling"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Tool-use and function calling"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agentic LLM systems / LLM agents survey (interfaces, planning, tool use, multi-agent, evaluation, safety).", "Compare approaches along: evaluation protocol (datasets.", "Compare approaches along: metrics.", "Compare approaches along: human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: and failure modes and limitations."], "chapter_key_contrasts": ["learning/feedback", "coordination"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhou2025Self", "Li2026Autonomous", "Du2024Anytool", "Zhang2026Evoroute", "Zhou2024Star", "Zheng2025Towards", "Yao2022React", "Sarukkai2025Context", "Shao2025Towards", "Nitin2025Faultline", "Xia2025Sand"], "allowed_bibkeys_mapped": ["Zhou2025Self", "Xia2025Sand", "Sarukkai2025Context", "Chen2025Grounded", "Belle2025Agents", "Du2024Anytool", "Xi2025Agentprm", "Zhou2024Archer", "Tennant2024Moral", "Van2025Survey", "Zhou2024Star", "Zheng2025Towards", "Wu2025Evolver", "Nitin2025Faultline", "Li2026Autonomous", "Shao2025Towards", "Zhang2026Evoroute", "Yao2022React"], "allowed_bibkeys_chapter": ["Belle2025Agents", "Cao2025Skyrl", "Chang2025Alas", "Chen2025Grounded", "Chuang2025Debate", "Cui2025Toward", "Du2024Anytool", "Hao2025Multi", "Li2025Continuum", "Li2025What", "Li2026Autonomous", "Lichkovski2025Agent", "Lumer2025Memtool", "Nitin2025Faultline", "Papadakis2025Atlas", "Sarkar2025Survey", "Sarukkai2025Context", "Shao2025Craken", "Shao2025Towards", "Silva2025Agents", "Sun2025Agent", "Tennant2024Moral", "Van2025Survey", "Wang2023Voyager", "Wu2025Agents", "Wu2025Evolver", "Xi2025Agentprm", "Xia2025Sand", "Yao2022React", "Yim2024Evaluating", "Zhang2026Evoroute", "Zhao2025Achieving", "Zheng2025Towards", "Zhou2024Archer", "Zhou2024Star", "Zhou2025Self"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0022-2e6956a116", "E-P0028-67ea29ce26", "E-P0080-4da9e4ae32", "E-P0092-60cc0d458f", "E-P0175-2d7736fc46", "E-P0028-9980bf7642", "E-P0156-e3f7ba21be", "E-P0001-ca4a00b5cf", "E-P0080-d5c234444e", "E-P0130-cfef96abfa", "E-P0130-e7c6cee652", "E-P0157-f65d57a126", "E-P0028-499402e2fb", "E-P0049-74188ef933", "E-P0049-91a368737e", "E-P0062-6273763a98", "E-P0022-17d0e7f9d9", "E-P0175-fe3f0d32b8"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0022", "evidence_id": "E-P0022-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "citations": ["Li2026Autonomous"], "paper_id": "P0028", "evidence_id": "E-P0028-67ea29ce26", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#method"}, {"hook_type": "eval", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "paper_id": "P0080", "evidence_id": "E-P0080-4da9e4ae32", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#limitations[1]"}, {"hook_type": "quant", "text": "Optimized datasets have achieved substantial improvements, with an average increase of 12% and notable gains in specific metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset.", "citations": ["Zhou2024Star"], "paper_id": "P0175", "evidence_id": "E-P0175-2d7736fc46", "pointer": "papers/paper_notes.jsonl:paper_id=P0175#key_results[0]"}, {"hook_type": "quant", "text": "Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential.", "citations": ["Li2026Autonomous"], "paper_id": "P0028", "evidence_id": "E-P0028-9980bf7642", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}, {"hook_type": "quant", "text": "Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate.", "citations": ["Zheng2025Towards"], "paper_id": "P0156", "evidence_id": "E-P0156-e3f7ba21be", "pointer": "papers/paper_notes.jsonl:paper_id=P0156#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhou2025Self", "Zhang2026Evoroute", "Yao2022React"], "A_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'evaluation protocol (datasets'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "metrics", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhou2025Self", "Zhang2026Evoroute", "Yao2022React"], "A_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'metrics'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhou2025Self", "Zhang2026Evoroute", "Yao2022React"], "A_highlights": [{"paper_id": "P0022", "evidence_id": "E-P0022-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhang2026Evoroute", "Zhou2025Self", "Yao2022React"], "A_highlights": [{"paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"paper_id": "P0022", "evidence_id": "E-P0022-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: DeepSeek-V3, LLMs, GAIA, EvoRoute, BrowseComp, FMs, MatSci, TauBench, ReAct, HexMachina.", "citations": ["Li2026Autonomous", "Zhang2026Evoroute", "Van2025Survey", "Zhou2025Self"]}], "limitation_hooks": [{"excerpt": "Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.", "citations": ["Li2026Autonomous"], "pointer": ""}, {"excerpt": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"], "pointer": ""}, {"excerpt": "We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion.", "citations": ["Van2025Survey"], "pointer": ""}, {"excerpt": "The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks.", "citations": ["Zhou2025Self"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": ["Evaluation protocol bullets are token-list style; avoid copying them as sentences. Prefer subsection-specific benchmarks/metrics from `anchor_facts` and state task/metric/constraint explicitly."], "pack_stats": {"anchors": {"raw": 10, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-22T09:47:06"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "rq": "Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Multi-agent coordination highlights a tension around evaluation protocol (datasets and metrics, motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Multi-agent coordination, a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "lens-first", "opener_hint": "Start by naming the lens (interface/protocol/threat model); state what it reveals; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets", "metrics", "human evaluation)", "compute and latency constraints", "and failure modes and limitations"], "bridge_terms": ["roles", "communication", "debate", "aggregation", "stability", "benchmarks/metrics"], "contrast_hook": "coordination", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Multi-agent coordination", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Multi-agent coordination vs Agent frameworks / architectures)", "use_clusters": ["Multi-agent coordination"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Multi-agent coordination", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Multi-agent coordination"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Multi-agent coordination", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Multi-agent coordination"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Multi-agent coordination", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Multi-agent coordination)", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agentic LLM systems / LLM agents survey (interfaces, planning, tool use, multi-agent, evaluation, safety).", "Compare approaches along: evaluation protocol (datasets.", "Compare approaches along: metrics.", "Compare approaches along: human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: and failure modes and limitations."], "chapter_key_contrasts": ["learning/feedback", "coordination"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhao2025Achieving", "Cao2025Skyrl", "Lichkovski2025Agent", "Shao2025Craken", "Li2025Continuum", "Lumer2025Memtool", "Silva2025Agents", "Wu2025Agents", "Sun2025Agent", "Cui2025Toward", "Chang2025Alas"], "allowed_bibkeys_mapped": ["Papadakis2025Atlas", "Li2025Continuum", "Cui2025Toward", "Wu2025Agents", "Chang2025Alas", "Chuang2025Debate", "Hao2025Multi", "Sarkar2025Survey", "Zhao2025Achieving", "Cao2025Skyrl", "Li2025What", "Yim2024Evaluating", "Silva2025Agents", "Lumer2025Memtool", "Sun2025Agent", "Shao2025Craken", "Lichkovski2025Agent", "Wang2023Voyager"], "allowed_bibkeys_chapter": ["Belle2025Agents", "Cao2025Skyrl", "Chang2025Alas", "Chen2025Grounded", "Chuang2025Debate", "Cui2025Toward", "Du2024Anytool", "Hao2025Multi", "Li2025Continuum", "Li2025What", "Li2026Autonomous", "Lichkovski2025Agent", "Lumer2025Memtool", "Nitin2025Faultline", "Papadakis2025Atlas", "Sarkar2025Survey", "Sarukkai2025Context", "Shao2025Craken", "Shao2025Towards", "Silva2025Agents", "Sun2025Agent", "Tennant2024Moral", "Van2025Survey", "Wang2023Voyager", "Wu2025Agents", "Wu2025Evolver", "Xi2025Agentprm", "Xia2025Sand", "Yao2022React", "Yim2024Evaluating", "Zhang2026Evoroute", "Zhao2025Achieving", "Zheng2025Towards", "Zhou2024Archer", "Zhou2024Star", "Zhou2025Self"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0199-1063eee7ce", "E-P0023-32b2c8cf19", "E-P0045-a256400826", "E-P0042-9d48d99db0", "E-P0218-a5937728f7", "E-P0058-35271418ac", "E-P0023-3af1ce8090", "E-P0043-baa622fa7f", "E-P0114-e31a1bbba7", "E-P0132-027093d5f5", "E-P0155-15e523063d", "E-P0043-b35b53de13", "E-P0155-171b93237a", "E-P0197-2430cc2982", "E-P0045-4e494f02c0", "E-P0045-696cef029c", "E-P0023-5ed988eb67", "E-P0132-dda37dfb29"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution.", "citations": ["Shao2025Craken"], "paper_id": "P0042", "evidence_id": "E-P0042-9d48d99db0", "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[1]"}, {"hook_type": "quant", "text": "Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning.", "citations": ["Cao2025Skyrl"], "paper_id": "P0023", "evidence_id": "E-P0023-3af1ce8090", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["Zhao2025Achieving"], "paper_id": "P0199", "evidence_id": "E-P0199-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[0]"}, {"hook_type": "eval", "text": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation.", "citations": ["Cao2025Skyrl"], "paper_id": "P0023", "evidence_id": "E-P0023-32b2c8cf19", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#method"}, {"hook_type": "quant", "text": "Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase.", "citations": ["Li2025Continuum"], "paper_id": "P0218", "evidence_id": "E-P0218-a5937728f7", "pointer": "papers/paper_notes.jsonl:paper_id=P0218#key_results[0]"}, {"hook_type": "limitation", "text": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "paper_id": "P0043", "evidence_id": "E-P0043-baa622fa7f", "pointer": "papers/paper_notes.jsonl:paper_id=P0043#key_results[1]"}, {"hook_type": "eval", "text": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "paper_id": "P0114", "evidence_id": "E-P0114-e31a1bbba7", "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Lumer2025Memtool", "Silva2025Agents", "Wu2025Agents"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0043", "evidence_id": "E-P0043-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0043#key_results[1]"}], "B_highlights": [{"paper_id": "P0043", "evidence_id": "E-P0043-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0043#key_results[1]"}, {"paper_id": "P0114", "evidence_id": "E-P0114-e31a1bbba7", "excerpt": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'evaluation protocol (datasets'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "metrics", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Lumer2025Memtool", "Shao2025Craken", "Wu2025Agents", "Silva2025Agents"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0042", "evidence_id": "E-P0042-9d48d99db0", "excerpt": "On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution.", "citations": ["Shao2025Craken"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[1]"}], "B_highlights": [{"paper_id": "P0114", "evidence_id": "E-P0114-e31a1bbba7", "excerpt": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"paper_id": "P0043", "evidence_id": "E-P0043-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0043#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'metrics'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Lumer2025Memtool", "Shao2025Craken", "Wu2025Agents", "Silva2025Agents"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0042", "evidence_id": "E-P0042-9d48d99db0", "excerpt": "On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution.", "citations": ["Shao2025Craken"], "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[1]"}], "B_highlights": [{"paper_id": "P0114", "evidence_id": "E-P0114-e31a1bbba7", "excerpt": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"paper_id": "P0043", "evidence_id": "E-P0043-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0043#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Cao2025Skyrl", "Wu2025Agents", "Silva2025Agents"], "A_highlights": [{"paper_id": "P0023", "evidence_id": "E-P0023-32b2c8cf19", "excerpt": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation.", "citations": ["Cao2025Skyrl"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#method"}, {"paper_id": "P0023", "evidence_id": "E-P0023-3af1ce8090", "excerpt": "Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning.", "citations": ["Cao2025Skyrl"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}], "B_highlights": [{"paper_id": "P0114", "evidence_id": "E-P0114-e31a1bbba7", "excerpt": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"paper_id": "P0043", "evidence_id": "E-P0043-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0043#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: MCP, LLM-based, MCP-compliant, SA-SWE-32B, AST-based, SWE-Bench, SWE, SkyRL-Agent, SkyRL-train, VeRL.", "citations": ["Sarkar2025Survey", "Cao2025Skyrl", "Shao2025Craken", "Silva2025Agents"]}], "limitation_hooks": [{"excerpt": "The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "citations": ["Sarkar2025Survey"], "pointer": ""}, {"excerpt": "While LLM agents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF) competitions, they have two key limitations: accessing latest cybersecurity expertise beyond training data, and integrating new knowledge into complex task planning.", "citations": ["Shao2025Craken"], "pointer": ""}, {"excerpt": "Knowledge-based approaches that incorporate technical understanding into the task-solving automation can tackle these limitations.", "citations": ["Shao2025Craken"], "pointer": ""}, {"excerpt": "We present CRAKEN, a knowledge-based LLM agent framework that improves cybersecurity capability through three core mechanisms: contextual decomposition of task-critical information, iterative self-reflected knowledge retrieval, and knowledge-hint injection that transforms insights into adaptive attack strategies.", "citations": ["Shao2025Craken"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": ["Evaluation protocol bullets are token-list style; avoid copying them as sentences. Prefer subsection-specific benchmarks/metrics from `anchor_facts` and state task/metric/constraint explicitly."], "pack_stats": {"anchors": {"raw": 9, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-22T09:47:06"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "section_id": "6", "section_title": "Evaluation & Risks", "rq": "Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?", "thesis": "For Benchmarks and evaluation protocols, evaluation protocol (datasets and metrics is a recurring axis of variation, and results are easiest to interpret when protocols and failure assumptions are explicit.", "tension_statement": "In Benchmarks and evaluation protocols, a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "tension-first", "opener_hint": "Start with the subsectionâ€™s central tension/trade-off; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets", "metrics", "human evaluation)", "compute and latency constraints", "and failure modes and limitations"], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability", "benchmarks"], "contrast_hook": "tool interfaces", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Evaluation / benchmark-focused works", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Evaluation / benchmark-focused works vs Agent frameworks / architectures)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Evaluation / benchmark-focused works", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Evaluation / benchmark-focused works", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Evaluation / benchmark-focused works", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Evaluation / benchmark-focused works)", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Safety / security / guardrails"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Safety / security / guardrails"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Safety / security / guardrails"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agentic LLM systems / LLM agents survey (interfaces, planning, tool use, multi-agent, evaluation, safety).", "Compare approaches along: evaluation protocol (datasets.", "Compare approaches along: metrics.", "Compare approaches along: human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: and failure modes and limitations."], "chapter_key_contrasts": ["tool interfaces", "security"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Mohammadi2025Evaluation", "Chen2025Towards", "Seo2025Simuhome", "Ma2023Large", "Liu2026Agents", "Das2025Beyond", "Wang2025Agentspec", "Fu2025Eval", "Kim2026Beyond", "Guo2025Cryptobench", "Zhan2025Sentinel", "Ji2025Taxonomy"], "allowed_bibkeys_mapped": ["Mohammadi2025Evaluation", "Guo2025Cryptobench", "Kim2026Beyond", "Fu2025Eval", "Chen2025Towards", "Zhan2025Sentinel", "Ji2025Taxonomy", "Dagan2024Plancraft", "Wang2025Agentspec", "Das2025Beyond", "Zhang2025Buildbench", "Ma2023Large", "Van2025Survey", "Seo2025Simuhome", "Liu2026Agents", "Liang2026Large", "Liu2025Secure", "Schick2023Toolformer"], "allowed_bibkeys_chapter": ["An2025Ipiguard", "Bonagiri2025Check", "Chen2025Towards", "Dagan2024Plancraft", "Das2025Beyond", "Fang2025Should", "Fu2025Eval", "Gasmi2025Bridging", "Guo2025Cryptobench", "Hadeliya2025When", "Ji2025Taxonomy", "Kamath2025Enforcing", "Kim2024When", "Kim2026Beyond", "Li2024Personal", "Liang2026Large", "Liu2025Secure", "Liu2026Agents", "Luo2025Agrail", "Ma2023Large", "Mohammadi2025Evaluation", "Schick2023Toolformer", "Seo2025Simuhome", "Sha2025Agent", "Shao2025Towards", "Shi2025Progent", "Van2025Survey", "Wang2023Survey", "Wang2025Adversarial", "Wang2025Agentspec", "Yuan2024Judge", "Zhan2025Sentinel", "Zhang2024Agent", "Zhang2025Buildbench", "Zhang2025Security"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0047-37f9ea924c", "E-P0069-7bac399c03", "E-P0151-e2e3b7fa97", "E-P0182-ad2b2ab52b", "E-P0030-8b56718f74", "E-P0069-4fc221fdea", "E-P0113-6a0e70c48e", "E-P0151-469a70bb44", "E-P0107-481ecd5602", "E-P0107-8e4049bab9", "E-P0144-753416ce70", "E-P0029-79f88927fa", "E-P0144-2895472ae1", "E-P0151-7b36039ae4", "E-P0219-031aeedf1f", "E-P0063-6a9a38705f", "E-P0068-e2d4798c18", "E-P0113-19bc1ede8a"], "anchor_facts": [{"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "quant", "text": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies", "citations": ["Liu2026Agents"], "paper_id": "P0030", "evidence_id": "E-P0030-8b56718f74", "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}, {"hook_type": "quant", "text": "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "citations": ["Chen2025Towards"], "paper_id": "P0069", "evidence_id": "E-P0069-4fc221fdea", "pointer": "papers/paper_notes.jsonl:paper_id=P0069#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "eval", "text": "Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.", "citations": ["Chen2025Towards"], "paper_id": "P0069", "evidence_id": "E-P0069-7bac399c03", "pointer": "papers/paper_notes.jsonl:paper_id=P0069#method"}, {"hook_type": "quant", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "citations": ["Seo2025Simuhome"], "paper_id": "P0151", "evidence_id": "E-P0151-e2e3b7fa97", "pointer": "papers/paper_notes.jsonl:paper_id=P0151#limitations[1]"}, {"hook_type": "quant", "text": "Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and", "citations": ["Ma2023Large"], "paper_id": "P0182", "evidence_id": "E-P0182-ad2b2ab52b", "pointer": "papers/paper_notes.jsonl:paper_id=P0182#key_results[0]"}, {"hook_type": "quant", "text": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specif", "citations": ["Liu2026Agents"], "paper_id": "P0030", "evidence_id": "E-P0030-8b56718f74", "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Mohammadi2025Evaluation", "Liu2026Agents", "Chen2025Towards"], "A_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0030", "evidence_id": "E-P0030-8b56718f74", "excerpt": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies", "citations": ["Liu2026Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}], "B_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0069", "evidence_id": "E-P0069-4fc221fdea", "excerpt": "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "citations": ["Chen2025Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0069#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'evaluation protocol (datasets'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "metrics", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Mohammadi2025Evaluation", "Liu2026Agents", "Chen2025Towards"], "A_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0030", "evidence_id": "E-P0030-8b56718f74", "excerpt": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies", "citations": ["Liu2026Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}], "B_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0069", "evidence_id": "E-P0069-4fc221fdea", "excerpt": "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "citations": ["Chen2025Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0069#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'metrics'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Mohammadi2025Evaluation", "Liu2026Agents", "Chen2025Towards"], "A_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0030", "evidence_id": "E-P0030-8b56718f74", "excerpt": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies", "citations": ["Liu2026Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}], "B_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0069", "evidence_id": "E-P0069-4fc221fdea", "excerpt": "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "citations": ["Chen2025Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0069#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Liu2026Agents", "Mohammadi2025Evaluation", "Chen2025Towards"], "A_highlights": [{"paper_id": "P0030", "evidence_id": "E-P0030-8b56718f74", "excerpt": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies", "citations": ["Liu2026Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}, {"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}], "B_highlights": [{"paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"paper_id": "P0069", "evidence_id": "E-P0069-4fc221fdea", "excerpt": "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "citations": ["Chen2025Towards"], "pointer": "papers/paper_notes.jsonl:paper_id=P0069#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: API, LLMs, WildAGTEval, FMs, MatSci, LLM-based, ALFRED, VirtualHome, EGI, IPI.", "citations": ["Kim2026Beyond", "Liu2026Agents", "Van2025Survey", "Mohammadi2025Evaluation"]}], "limitation_hooks": [{"excerpt": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability.", "citations": ["Liu2026Agents"], "pointer": ""}, {"excerpt": "We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion.", "citations": ["Van2025Survey"], "pointer": ""}, {"excerpt": "It then employs a multi-level verification pipeline where (i) at the semantic level, intuitive natural language safety requirements are formalized into TL formulas and the LLM agent's understanding of these requirements is probed for alignment with the TL formulas; (ii) at the plan level, high-level action plans and", "citations": ["Zhan2025Sentinel"], "pointer": ""}, {"excerpt": "Our experiments show that by grounding physical safety in temporal logic and applying verification methods across multiple levels, Sentinel provides a rigorous foundation for systematically evaluating LLM-based embodied agents in physical environments, exposing safety violations overlooked by previous methods and", "citations": ["Zhan2025Sentinel"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": ["Evaluation protocol bullets are token-list style; avoid copying them as sentences. Prefer subsection-specific benchmarks/metrics from `anchor_facts` and state task/metric/constraint explicitly."], "pack_stats": {"anchors": {"raw": 11, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-22T09:47:06"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "section_id": "6", "section_title": "Evaluation & Risks", "rq": "Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Safety, security, and governance methods emphasize evaluation protocol (datasets and metrics trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "tension_statement": "In Safety, security, and governance, a key tension is capability versus safety: stronger agent actions increase utility but widen the attack surface and raise containment requirements.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"forbidden_pipeline_voice": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):"], "high_risk_templates": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "lens-first", "opener_hint": "Start by naming the lens (interface/protocol/threat model); state what it reveals; end paragraph 1 with the thesis.", "axes": ["evaluation protocol (datasets", "metrics", "human evaluation)", "compute and latency constraints", "and failure modes and limitations"], "bridge_terms": ["threat model", "prompt/tool injection", "monitoring", "guardrails", "benchmarks/metrics", "compute"], "contrast_hook": "security", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes and limitations", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism and system architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "core mechanism and system architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "baseline route (Agent frameworks / architectures)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: training and data signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "implementation assumptions (interface + training)", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "evaluation anchor (task/metric/constraint) + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism and system architecture and what it optimizes for.", "focus": ["cluster: Safety / security / guardrails", "contrast with Agent frameworks / architectures", "core mechanism and system architecture"], "connector_to_prev": "contrast", "connector_phrase": "contrast route (Safety / security / guardrails vs Agent frameworks / architectures)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: training and data and interface assumptions (mirror A for comparability).", "focus": ["cluster: Safety / security / guardrails", "training and data setup", "interface contract", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "elaboration", "connector_phrase": "contrast implementation assumptions (B)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Safety / security / guardrails", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "contrast evaluation anchor + trade-offs (B)", "use_clusters": ["Safety / security / guardrails"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Safety / security / guardrails", "multiple citations in one paragraph", "axes: evaluation protocol (datasets, metrics, human evaluation), compute and latency constraints, and failure modes and limitations"], "connector_to_prev": "synthesis", "connector_phrase": "cross-paper synthesis (Agent frameworks / architectures vs Safety / security / guardrails)", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Evaluation / benchmark-focused works"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "decision guidance / criteria", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Evaluation / benchmark-focused works"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "limitations + verification targets", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Evaluation / benchmark-focused works"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agentic LLM systems / LLM agents survey (interfaces, planning, tool use, multi-agent, evaluation, safety).", "Compare approaches along: evaluation protocol (datasets.", "Compare approaches along: metrics.", "Compare approaches along: human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: and failure modes and limitations."], "chapter_key_contrasts": ["tool interfaces", "security"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhang2025Security", "Kim2024When", "Shi2025Progent", "Fu2025Eval", "Wang2025Adversarial", "Zhang2024Agent", "Yuan2024Judge", "Bonagiri2025Check", "Shao2025Towards", "Kamath2025Enforcing", "Gasmi2025Bridging"], "allowed_bibkeys_mapped": ["Gasmi2025Bridging", "Bonagiri2025Check", "Fang2025Should", "Luo2025Agrail", "Zhang2024Agent", "Zhang2025Security", "Li2024Personal", "Wang2025Adversarial", "Hadeliya2025When", "Kamath2025Enforcing", "Yuan2024Judge", "Sha2025Agent", "An2025Ipiguard", "Shi2025Progent", "Fu2025Eval", "Shao2025Towards", "Kim2024When", "Wang2023Survey"], "allowed_bibkeys_chapter": ["An2025Ipiguard", "Bonagiri2025Check", "Chen2025Towards", "Dagan2024Plancraft", "Das2025Beyond", "Fang2025Should", "Fu2025Eval", "Gasmi2025Bridging", "Guo2025Cryptobench", "Hadeliya2025When", "Ji2025Taxonomy", "Kamath2025Enforcing", "Kim2024When", "Kim2026Beyond", "Li2024Personal", "Liang2026Large", "Liu2025Secure", "Liu2026Agents", "Luo2025Agrail", "Ma2023Large", "Mohammadi2025Evaluation", "Schick2023Toolformer", "Seo2025Simuhome", "Sha2025Agent", "Shao2025Towards", "Shi2025Progent", "Van2025Survey", "Wang2023Survey", "Wang2025Adversarial", "Wang2025Agentspec", "Yuan2024Judge", "Zhan2025Sentinel", "Zhang2024Agent", "Zhang2025Buildbench", "Zhang2025Security"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0055-d6095e10e9", "E-P0055-8bcb673a7d", "E-P0181-03f4e0cdc0", "E-P0060-68db58914f", "E-P0144-753416ce70", "E-P0055-7a6ec4daed", "E-P0144-2895472ae1", "E-P0200-8512d7eecf", "E-P0200-ee6f97d5e5", "E-P0076-51b52c66a1", "E-P0076-e26328e18c", "E-P0174-c9f8ee0e0e", "E-P0013-7edb91824f", "E-P0157-f65d57a126", "E-P0181-37b9602feb", "E-P0120-2718dbc45e", "E-P0041-8a2c2e2291", "E-P0041-8e34a29629"], "anchor_facts": [{"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[1]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0144", "evidence_id": "E-P0144-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "paper_id": "P0144", "evidence_id": "E-P0144-2895472ae1", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}, {"hook_type": "quant", "text": "Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions.", "citations": ["Zhang2024Agent"], "paper_id": "P0076", "evidence_id": "E-P0076-51b52c66a1", "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[1]"}], "comparison_cards": [{"axis": "evaluation protocol (datasets", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Zhang2025Security"], "A_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0055", "evidence_id": "E-P0055-8bcb673a7d", "excerpt": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}], "B_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0055", "evidence_id": "E-P0055-8bcb673a7d", "excerpt": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'evaluation protocol (datasets'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "metrics", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Zhang2025Security", "Shi2025Progent"], "A_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}], "B_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0055", "evidence_id": "E-P0055-8bcb673a7d", "excerpt": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'metrics'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Zhang2025Security", "Shi2025Progent"], "A_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}], "B_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0055", "evidence_id": "E-P0055-8bcb673a7d", "excerpt": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Zhang2025Security", "Shi2025Progent"], "A_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}], "B_highlights": [{"paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"paper_id": "P0055", "evidence_id": "E-P0055-7a6ec4daed", "excerpt": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation mentions include: LLMs, GPT-4, ToolEmu, AI-specific, MCP, JSON, LLM-centric, MSB, MCP-specific, NRP.", "citations": ["Hadeliya2025When", "Bonagiri2025Check", "Gasmi2025Bridging", "Zhang2025Security"]}], "limitation_hooks": [{"excerpt": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "citations": ["Gasmi2025Bridging"], "pointer": ""}, {"excerpt": "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "citations": ["Gasmi2025Bridging"], "pointer": ""}, {"excerpt": "We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service).", "citations": ["Gasmi2025Bridging"], "pointer": ""}, {"excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["this run", "this run is", "pipeline", "workspace", "unit", "quality gate", "evidence pack", "evidence packs", "writer context pack", "Method note (evidence policy):", "This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Work in this area includes", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": ["Evaluation protocol bullets are token-list style; avoid copying them as sentences. Prefer subsection-specific benchmarks/metrics from `anchor_facts` and state task/metric/constraint explicitly."], "pack_stats": {"anchors": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-22T09:47:06"}
