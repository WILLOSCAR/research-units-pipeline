{"sub_id": "3.1", "title": "Agent loop and action spaces", "anchors": [{"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0078", "evidence_id": "E-P0078-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["Zhao2025Achieving"], "paper_id": "P0199", "evidence_id": "E-P0199-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[0]"}, {"hook_type": "quant", "text": "We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design.", "citations": ["Fumero2025Cybersleuth"], "paper_id": "P0119", "evidence_id": "E-P0119-c8c4670812", "pointer": "papers/paper_notes.jsonl:paper_id=P0119#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0125", "evidence_id": "E-P0125-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0125#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate.", "citations": ["Zheng2025Towards"], "paper_id": "P0156", "evidence_id": "E-P0156-e3f7ba21be", "pointer": "papers/paper_notes.jsonl:paper_id=P0156#key_results[0]"}], "generated_at": "2026-01-22T09:47:05", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "anchors": [{"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "paper_id": "P0054", "evidence_id": "E-P0054-fae121f81b", "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "eval", "text": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "paper_id": "P0017", "evidence_id": "E-P0017-192e78b614", "pointer": "papers/paper_notes.jsonl:paper_id=P0017#method"}, {"hook_type": "eval", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "paper_id": "P0056", "evidence_id": "E-P0056-f7a14123f9", "pointer": "papers/paper_notes.jsonl:paper_id=P0056#limitations[1]"}, {"hook_type": "quant", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "paper_id": "P0080", "evidence_id": "E-P0080-d5c234444e", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "eval", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "paper_id": "P0027", "evidence_id": "E-P0027-9640816b42", "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[1]"}], "generated_at": "2026-01-22T09:47:05", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "anchors": [{"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"hook_type": "quant", "text": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["Hu2025Training"], "paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"hook_type": "eval", "text": "To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards.", "citations": ["Yin2024Safeagentbench"], "paper_id": "P0087", "evidence_id": "E-P0087-076695cd77", "pointer": "papers/paper_notes.jsonl:paper_id=P0087#method"}, {"hook_type": "quant", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "citations": ["Seo2025Simuhome"], "paper_id": "P0151", "evidence_id": "E-P0151-e2e3b7fa97", "pointer": "papers/paper_notes.jsonl:paper_id=P0151#limitations[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "SafeAgentBench includes: (1) an executable, diverse, and high-quality dataset of 750 tasks, rigorously curated to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17", "citations": ["Yin2024Safeagentbench"], "paper_id": "P0087", "evidence_id": "E-P0087-c2e8e2bb7f", "pointer": "papers/paper_notes.jsonl:paper_id=P0087#key_results[0]"}, {"hook_type": "quant", "text": "Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation.", "citations": ["Khoee2025Gatelens"], "paper_id": "P0124", "evidence_id": "E-P0124-8afa74a630", "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation shows that CHECKMATE outperforms the state-of-the-art system (Claude Code) in penetration capability, improving benchmark success rates by over 20%.", "citations": ["Wang2025Automated"], "paper_id": "P0208", "evidence_id": "E-P0208-32aec6c669", "pointer": "papers/paper_notes.jsonl:paper_id=P0208#key_results[0]"}, {"hook_type": "quant", "text": "We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities.", "citations": ["Seo2025Simuhome"], "paper_id": "P0151", "evidence_id": "E-P0151-7b36039ae4", "pointer": "papers/paper_notes.jsonl:paper_id=P0151#key_results[0]"}], "generated_at": "2026-01-22T09:47:05", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "anchors": [{"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "quant", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "paper_id": "P0019", "evidence_id": "E-P0019-f36b515991", "pointer": "papers/paper_notes.jsonl:paper_id=P0019#key_results[1]"}, {"hook_type": "quant", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "paper_id": "P0184", "evidence_id": "E-P0184-9abcf1bf8a", "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}, {"hook_type": "quant", "text": "SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability.", "citations": ["Huang2025Retrieval"], "paper_id": "P0158", "evidence_id": "E-P0158-c9781caf3b", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#limitations[1]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "quant", "text": "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "citations": ["Huang2025Retrieval"], "paper_id": "P0158", "evidence_id": "E-P0158-4af0cf3c02", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[1]"}], "generated_at": "2026-01-22T09:47:05", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "anchors": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0022", "evidence_id": "E-P0022-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "citations": ["Li2026Autonomous"], "paper_id": "P0028", "evidence_id": "E-P0028-67ea29ce26", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#method"}, {"hook_type": "eval", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "paper_id": "P0080", "evidence_id": "E-P0080-4da9e4ae32", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#limitations[1]"}, {"hook_type": "quant", "text": "Optimized datasets have achieved substantial improvements, with an average increase of 12% and notable gains in specific metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset.", "citations": ["Zhou2024Star"], "paper_id": "P0175", "evidence_id": "E-P0175-2d7736fc46", "pointer": "papers/paper_notes.jsonl:paper_id=P0175#key_results[0]"}, {"hook_type": "quant", "text": "Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential.", "citations": ["Li2026Autonomous"], "paper_id": "P0028", "evidence_id": "E-P0028-9980bf7642", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}, {"hook_type": "quant", "text": "Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate.", "citations": ["Zheng2025Towards"], "paper_id": "P0156", "evidence_id": "E-P0156-e3f7ba21be", "pointer": "papers/paper_notes.jsonl:paper_id=P0156#key_results[0]"}, {"hook_type": "quant", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "paper_id": "P0080", "evidence_id": "E-P0080-d5c234444e", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}, {"hook_type": "quant", "text": "On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\\textbf{2$\\times$ cost reduction}$ at iso-accuracy.", "citations": ["Sarukkai2025Context"], "paper_id": "P0130", "evidence_id": "E-P0130-cfef96abfa", "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[1]"}], "generated_at": "2026-01-22T09:47:05", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "anchors": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution.", "citations": ["Shao2025Craken"], "paper_id": "P0042", "evidence_id": "E-P0042-9d48d99db0", "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[1]"}, {"hook_type": "quant", "text": "Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning.", "citations": ["Cao2025Skyrl"], "paper_id": "P0023", "evidence_id": "E-P0023-3af1ce8090", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["Zhao2025Achieving"], "paper_id": "P0199", "evidence_id": "E-P0199-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[0]"}, {"hook_type": "eval", "text": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation.", "citations": ["Cao2025Skyrl"], "paper_id": "P0023", "evidence_id": "E-P0023-32b2c8cf19", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#method"}, {"hook_type": "quant", "text": "Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase.", "citations": ["Li2025Continuum"], "paper_id": "P0218", "evidence_id": "E-P0218-a5937728f7", "pointer": "papers/paper_notes.jsonl:paper_id=P0218#key_results[0]"}, {"hook_type": "limitation", "text": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "paper_id": "P0043", "evidence_id": "E-P0043-baa622fa7f", "pointer": "papers/paper_notes.jsonl:paper_id=P0043#key_results[1]"}, {"hook_type": "eval", "text": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "paper_id": "P0114", "evidence_id": "E-P0114-e31a1bbba7", "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"hook_type": "eval", "text": "Evaluating these systems is challenging, as their rapid evolution outpaces traditional human evaluation.", "citations": ["Sun2025Agent"], "paper_id": "P0132", "evidence_id": "E-P0132-027093d5f5", "pointer": "papers/paper_notes.jsonl:paper_id=P0132#key_results[1]"}], "generated_at": "2026-01-22T09:47:05", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "anchors": [{"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "quant", "text": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies", "citations": ["Liu2026Agents"], "paper_id": "P0030", "evidence_id": "E-P0030-8b56718f74", "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}, {"hook_type": "quant", "text": "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "citations": ["Chen2025Towards"], "paper_id": "P0069", "evidence_id": "E-P0069-4fc221fdea", "pointer": "papers/paper_notes.jsonl:paper_id=P0069#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "eval", "text": "Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.", "citations": ["Chen2025Towards"], "paper_id": "P0069", "evidence_id": "E-P0069-7bac399c03", "pointer": "papers/paper_notes.jsonl:paper_id=P0069#method"}, {"hook_type": "quant", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "citations": ["Seo2025Simuhome"], "paper_id": "P0151", "evidence_id": "E-P0151-e2e3b7fa97", "pointer": "papers/paper_notes.jsonl:paper_id=P0151#limitations[1]"}, {"hook_type": "quant", "text": "Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and", "citations": ["Ma2023Large"], "paper_id": "P0182", "evidence_id": "E-P0182-ad2b2ab52b", "pointer": "papers/paper_notes.jsonl:paper_id=P0182#key_results[0]"}, {"hook_type": "quant", "text": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specif", "citations": ["Liu2026Agents"], "paper_id": "P0030", "evidence_id": "E-P0030-8b56718f74", "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}, {"hook_type": "quant", "text": "In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations.", "citations": ["Das2025Beyond"], "paper_id": "P0113", "evidence_id": "E-P0113-6a0e70c48e", "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identify 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.", "citations": ["Wang2025Agentspec"], "paper_id": "P0107", "evidence_id": "E-P0107-481ecd5602", "pointer": "papers/paper_notes.jsonl:paper_id=P0107#key_results[1]"}, {"hook_type": "quant", "text": "Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs).", "citations": ["Wang2025Agentspec"], "paper_id": "P0107", "evidence_id": "E-P0107-8e4049bab9", "pointer": "papers/paper_notes.jsonl:paper_id=P0107#key_results[0]"}], "generated_at": "2026-01-22T09:47:05", "section_id": "6", "section_title": "Evaluation & Risks"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "anchors": [{"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[1]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0144", "evidence_id": "E-P0144-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "paper_id": "P0144", "evidence_id": "E-P0144-2895472ae1", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}, {"hook_type": "quant", "text": "Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions.", "citations": ["Zhang2024Agent"], "paper_id": "P0076", "evidence_id": "E-P0076-51b52c66a1", "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[1]"}], "generated_at": "2026-01-22T09:47:05", "section_id": "6", "section_title": "Evaluation & Risks"}
