# Evidence draft: 4.1 Planning and reasoning loops

## Evidence snippets (with provenance)
- (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks. Hu2025Training (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
- (E-P0087-076695cd77) To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards. Yin2024Safeagentbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0087#method)
- (E-P0151-e2e3b7fa97) Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Seo2025Simuhome (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0151#limitations[1])
- (E-P0151-469a70bb44) Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Seo2025Simuhome (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0151#key_results[1])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0064#key_results[0])
- (E-P0087-c2e8e2bb7f) SafeAgentBench includes: (1) an executable, diverse, and high-quality dataset of 750 tasks, rigorously curated to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 9 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Yin2024Safeagentbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0087#key_results[0])
- (E-P0124-8afa74a630) Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation. Khoee2025Gatelens (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0124#key_results[0])
- (E-P0208-32aec6c669) Our evaluation shows that CHECKMATE outperforms the state-of-the-art system (Claude Code) in penetration capability, improving benchmark success rates by over 20%. Wang2025Automated (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0208#key_results[0])
- (E-P0151-7b36039ae4) We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Seo2025Simuhome (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0151#key_results[0])

## Definitions / setup

- Setup: Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Planning and reasoning loops'.. Axes: evaluation protocol (datasets; metrics; human evaluation); compute and latency constraints; and failure modes and limitations. Kim2025Bridging Zhou2025Reasoning Hu2025Training

## Claim candidates

- Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks. Hu2025Training
- To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards. Yin2024Safeagentbench
- Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Seo2025Simuhome
- Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Seo2025Simuhome
- On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React

## Concrete comparisons

- Axis: evaluation protocol (datasets; A: Agent frameworks / architectures: `P0016`, `P0021`, `P0024`; B: Planning / reasoning loops: `P0016`, `P0021`, `P0024`. Hu2025Training Zhou2025Siraj
  - A highlight: (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - A highlight: (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (pointer: papers/paper_notes.jsonl:paper_id=P0064#key_results[0])
  - B highlight: (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - B highlight: (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (pointer: papers/paper_notes.jsonl:paper_id=P0064#key_results[0])
- Axis: metrics; A: Agent frameworks / architectures: `P0016`, `P0021`, `P0024`; B: Planning / reasoning loops: `P0016`, `P0021`, `P0024`. Hu2025Training Zhou2025Siraj
  - A highlight: (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - A highlight: (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (pointer: papers/paper_notes.jsonl:paper_id=P0064#key_results[0])
  - B highlight: (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - B highlight: (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (pointer: papers/paper_notes.jsonl:paper_id=P0064#key_results[0])
- Axis: human evaluation); A: Agent frameworks / architectures: `P0016`, `P0021`, `P0024`; B: Planning / reasoning loops: `P0016`, `P0021`, `P0024`. Hu2025Training Zhou2025Siraj
  - A highlight: (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - A highlight: (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (pointer: papers/paper_notes.jsonl:paper_id=P0064#key_results[0])
  - B highlight: (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - B highlight: (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (pointer: papers/paper_notes.jsonl:paper_id=P0064#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0016`, `P0021`, `P0024`; B: Planning / reasoning loops: `P0016`, `P0021`, `P0024`. Hu2025Training Zhou2025Siraj
  - A highlight: (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - A highlight: (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (pointer: papers/paper_notes.jsonl:paper_id=P0064#key_results[0])
  - B highlight: (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - B highlight: (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (pointer: papers/paper_notes.jsonl:paper_id=P0064#key_results[0])
- Axis: and failure modes and limitations; A: Agent frameworks / architectures: `P0016`, `P0021`, `P0024`; B: Planning / reasoning loops: `P0016`, `P0021`, `P0024`. Hu2025Training Zhou2025Siraj
  - A highlight: (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - A highlight: (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (pointer: papers/paper_notes.jsonl:paper_id=P0064#key_results[0])
  - B highlight: (E-P0024-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - B highlight: (E-P0064-0b753b9422) Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Zhou2025Siraj (pointer: papers/paper_notes.jsonl:paper_id=P0064#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: SCL; CCAM; GPT-4o-powered; ReAct; AutoGPT; RSP; GSI; RSV; FEVER; RSP-M. Kim2025Bridging Zhou2025Reasoning Hu2025Training Silva2025Agents

## Failures / limitations

- While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. Zhou2025Reasoning
- We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically "analysis paralysis" or "cognitive haste"--without altering underlying facts or using explicit triggers. Zhou2025Reasoning
- This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents
- Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. Hatalis2025Review

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- failure modes / known limitations
- baseline choices and ablation evidence
