# Evidence draft: 6.2 Safety, security, and governance

## Evidence snippets (with provenance)
- (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
- (E-P0055-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0055#method)
- (E-P0181-03f4e0cdc0) Additionally, our findings underscore the limitations of existing safeguards in contemporary commercial LLMs, emphasizing the urgent need for robust security measures to prevent the misuse of LLM agents. Kim2024When (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0181#limitations[1])
- (E-P0060-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0060#key_results[0])
- (E-P0144-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0144#key_results[1])
- (E-P0055-7a6ec4daed) We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0055#key_results[1])
- (E-P0144-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0144#key_results[0])
- (E-P0200-8512d7eecf) Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. Wang2025Adversarial (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0200#key_results[0])
- (E-P0200-ee6f97d5e5) However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. Wang2025Adversarial (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0200#key_results[1])
- (E-P0076-51b52c66a1) Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Zhang2024Agent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0076#key_results[1])

## Definitions / setup

- Setup: Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Safety, security, and governance'.. Axes: evaluation protocol (datasets; metrics; human evaluation); compute and latency constraints; and failure modes and limitations. Hadeliya2025When Bonagiri2025Check Gasmi2025Bridging

## Claim candidates

- MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP Zhang2025Security
- We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security
- Additionally, our findings underscore the limitations of existing safeguards in contemporary commercial LLMs, emphasizing the urgent need for robust security measures to prevent the misuse of LLM agents. Kim2024When
- Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent
- RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval

## Concrete comparisons

- Axis: evaluation protocol (datasets; A: Agent frameworks / architectures: `P0012`, `P0013`, `P0041`; B: Safety / security / guardrails: `P0012`, `P0013`, `P0041`. Zhang2025Security
  - A highlight: (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
  - A highlight: (E-P0055-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#method)
  - B highlight: (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
  - B highlight: (E-P0055-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#method)
- Axis: metrics; A: Agent frameworks / architectures: `P0012`, `P0013`, `P0041`; B: Safety / security / guardrails: `P0012`, `P0013`, `P0041`. Zhang2025Security Shi2025Progent
  - A highlight: (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
  - A highlight: (E-P0060-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0060#key_results[0])
  - B highlight: (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
  - B highlight: (E-P0055-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#method)
- Axis: human evaluation); A: Agent frameworks / architectures: `P0012`, `P0013`, `P0041`; B: Safety / security / guardrails: `P0012`, `P0013`, `P0041`. Zhang2025Security Shi2025Progent
  - A highlight: (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
  - A highlight: (E-P0060-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0060#key_results[0])
  - B highlight: (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
  - B highlight: (E-P0055-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#method)
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0012`, `P0013`, `P0041`; B: Safety / security / guardrails: `P0012`, `P0013`, `P0041`. Zhang2025Security Shi2025Progent
  - A highlight: (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
  - A highlight: (E-P0060-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0060#key_results[0])
  - B highlight: (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
  - B highlight: (E-P0055-7a6ec4daed) We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[1])
- Axis: and failure modes and limitations; A: Agent frameworks / architectures: `P0012`, `P0013`, `P0041`; B: Safety / security / guardrails: `P0012`, `P0013`, `P0041`. Zhang2025Security Shi2025Progent
  - A highlight: (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
  - A highlight: (E-P0060-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0060#key_results[0])
  - B highlight: (E-P0055-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
  - B highlight: (E-P0055-7a6ec4daed) We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[1])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: LLMs; GPT-4; ToolEmu; AI-specific; MCP; JSON; LLM-centric; MSB; MCP-specific; NRP. Hadeliya2025When Bonagiri2025Check Gasmi2025Bridging Zhang2025Security

## Failures / limitations

- Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. Gasmi2025Bridging
- This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. Gasmi2025Bridging
- We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Gasmi2025Bridging
- Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- failure modes / known limitations
- baseline choices and ablation evidence
