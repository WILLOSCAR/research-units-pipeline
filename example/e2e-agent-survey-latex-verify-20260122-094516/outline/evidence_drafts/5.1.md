# Evidence draft: 5.1 Self-improvement and adaptation

## Evidence snippets (with provenance)
- (E-P0022-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
- (E-P0028-67ea29ce26) We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Li2026Autonomous (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0028#method)
- (E-P0080-4da9e4ae32) We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0080#limitations[1])
- (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
- (E-P0175-2d7736fc46) Optimized datasets have achieved substantial improvements, with an average increase of 12% and notable gains in specific metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset. Zhou2024Star (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0175#key_results[0])
- (E-P0028-9980bf7642) Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential. Li2026Autonomous (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0028#key_results[1])
- (E-P0156-e3f7ba21be) Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. Zheng2025Towards (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0156#key_results[0])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0080-d5c234444e) Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. Du2024Anytool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0080#key_results[0])
- (E-P0130-cfef96abfa) On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\textbf{2$\times$ cost reduction}$ at iso-accuracy. Sarukkai2025Context (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0130#key_results[1])

## Definitions / setup

- Setup: Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Self-improvement and adaptation'.. Axes: evaluation protocol (datasets; metrics; human evaluation); compute and latency constraints; and failure modes and limitations. Li2026Autonomous Zhang2026Evoroute Van2025Survey

## Claim candidates

- Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self
- We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Li2026Autonomous
- We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool
- Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute
- Optimized datasets have achieved substantial improvements, with an average increase of 12% and notable gains in specific metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset. Zhou2024Star

## Concrete comparisons

- Axis: evaluation protocol (datasets; A: Agent frameworks / architectures: `P0028`, `P0092`, `P0008`; B: Planning / reasoning loops: `P0036`, `P0001`. Zhou2025Self Zhang2026Evoroute Yao2022React
  - A highlight: (E-P0022-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
  - A highlight: (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: metrics; A: Agent frameworks / architectures: `P0028`, `P0092`, `P0008`; B: Planning / reasoning loops: `P0036`, `P0001`. Zhou2025Self Zhang2026Evoroute Yao2022React
  - A highlight: (E-P0022-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
  - A highlight: (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: human evaluation); A: Agent frameworks / architectures: `P0028`, `P0092`, `P0008`; B: Planning / reasoning loops: `P0036`, `P0001`. Zhou2025Self Zhang2026Evoroute Yao2022React
  - A highlight: (E-P0022-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
  - A highlight: (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0028`, `P0092`, `P0008`; B: Planning / reasoning loops: `P0036`, `P0001`. Zhang2026Evoroute Zhou2025Self Yao2022React
  - A highlight: (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
  - A highlight: (E-P0022-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- Axis: and failure modes and limitations; A: Agent frameworks / architectures: `P0028`, `P0092`, `P0008`; B: Planning / reasoning loops: `P0036`, `P0001`. Zhang2026Evoroute Zhou2025Self Yao2022React
  - A highlight: (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
  - A highlight: (E-P0022-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0022#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: DeepSeek-V3; LLMs; GAIA; EvoRoute; BrowseComp; FMs; MatSci; TauBench; ReAct; HexMachina. Li2026Autonomous Zhang2026Evoroute Van2025Survey Zhou2025Self

## Failures / limitations

- Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures. Li2026Autonomous
- We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. Zhang2026Evoroute
- We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Van2025Survey
- The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. Zhou2025Self

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- failure modes / known limitations
- baseline choices and ablation evidence
