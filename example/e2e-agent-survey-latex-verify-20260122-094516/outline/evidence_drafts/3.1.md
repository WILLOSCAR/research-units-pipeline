# Evidence draft: 3.1 Agent loop and action spaces

## Evidence snippets (with provenance)
- (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
- (E-P0016-9d9d60644a) We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Kim2025Bridging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0016#method)
- (E-P0027-c92ed293ba) While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0027#limitations[1])
- (E-P0199-1063eee7ce) However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. Zhao2025Achieving (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0199#key_results[0])
- (E-P0014-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
- (E-P0078-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0078#key_results[0])
- (E-P0119-c8c4670812) We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design. Fumero2025Cybersleuth (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0119#key_results[0])
- (E-P0125-4b027dfb27) We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0125#key_results[0])
- (E-P0156-e3f7ba21be) Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. Zheng2025Towards (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0156#key_results[0])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])

## Definitions / setup

- Setup: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Agent loop and action spaces'.. Axes: evaluation protocol (datasets; metrics; human evaluation); compute and latency constraints; and failure modes and limitations. Xi2026Toolgym Song2026Envscaler Zhang2026Evoroute

## Claim candidates

- Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute
- We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Kim2025Bridging
- While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Shen2024Small
- However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. Zhao2025Achieving
- Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift

## Concrete comparisons

- Axis: evaluation protocol (datasets; A: Agent frameworks / architectures: `P0032`, `P0091`, `P0092`; B: Planning / reasoning loops: `P0187`, `P0014`, `P0016`. Li2025Agentswift Zhang2026Evoroute Shang2024Agentsquare
  - A highlight: (E-P0014-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - A highlight: (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
  - B highlight: (E-P0078-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0078#key_results[0])
  - B highlight: (E-P0014-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
- Axis: metrics; A: Agent frameworks / architectures: `P0032`, `P0091`, `P0092`; B: Planning / reasoning loops: `P0187`, `P0014`, `P0016`. Zhang2026Evoroute Li2025Agentswift Shang2024Agentsquare
  - A highlight: (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
  - A highlight: (E-P0014-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - B highlight: (E-P0078-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0078#key_results[0])
  - B highlight: (E-P0014-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
- Axis: human evaluation); A: Agent frameworks / architectures: `P0032`, `P0091`, `P0092`; B: Planning / reasoning loops: `P0187`, `P0014`, `P0016`. Zhang2026Evoroute Li2025Agentswift Shang2024Agentsquare
  - A highlight: (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
  - A highlight: (E-P0014-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - B highlight: (E-P0078-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0078#key_results[0])
  - B highlight: (E-P0014-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0032`, `P0091`, `P0092`; B: Planning / reasoning loops: `P0187`, `P0014`, `P0016`. Zhang2026Evoroute Li2025Agentswift Yao2022React Shang2024Agentsquare
  - A highlight: (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
  - A highlight: (E-P0014-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - B highlight: (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (pointer: papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
  - B highlight: (E-P0078-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0078#key_results[0])
- Axis: and failure modes and limitations; A: Agent frameworks / architectures: `P0032`, `P0091`, `P0092`; B: Planning / reasoning loops: `P0187`, `P0014`, `P0016`. Zhang2026Evoroute Li2025Agentswift Shang2024Agentsquare
  - A highlight: (E-P0092-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0092#key_results[0])
  - A highlight: (E-P0014-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - B highlight: (E-P0078-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0078#key_results[0])
  - B highlight: (E-P0014-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: LLMs; DeepSeek-v3; LLM-simulated; SFT; RUC-NLPIR; EnvScaler; SkelBuilder; ScenGenerator; GAIA; EvoRoute. Xi2026Toolgym Song2026Envscaler Zhang2026Evoroute Lin2026Froav

## Failures / limitations

- We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. Zhang2026Evoroute
- Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge. Li2025Agentswift
- Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. Gasmi2025Bridging
- This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. Gasmi2025Bridging

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- failure modes / known limitations
- baseline choices and ablation evidence
