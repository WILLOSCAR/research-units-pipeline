# Evidence draft: 6.1 Benchmarks and evaluation protocols

## Evidence snippets (with provenance)
- (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. Mohammadi2025Evaluation (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
- (E-P0069-7bac399c03) Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods. Chen2025Towards (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0069#method)
- (E-P0151-e2e3b7fa97) Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Seo2025Simuhome (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0151#limitations[1])
- (E-P0182-ad2b2ab52b) Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and the impact of Chain of Summarization.Experiment results demonstrate that: 1. Ma2023Large (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0182#key_results[0])
- (E-P0030-8b56718f74) Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants. Liu2026Agents (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0030#key_results[0])
- (E-P0069-4fc221fdea) This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. Chen2025Towards (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0069#key_results[0])
- (E-P0113-6a0e70c48e) In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations. Das2025Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0113#key_results[0])
- (E-P0151-469a70bb44) Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Seo2025Simuhome (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0151#key_results[1])
- (E-P0107-481ecd5602) Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identify 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios. Wang2025Agentspec (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0107#key_results[1])
- (E-P0107-8e4049bab9) Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Wang2025Agentspec (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0107#key_results[0])

## Definitions / setup

- Setup: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Benchmarks and evaluation protocols'.. Axes: evaluation protocol (datasets; metrics; human evaluation); compute and latency constraints; and failure modes and limitations. Kim2026Beyond Liu2026Agents Van2025Survey

## Claim candidates

- This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation Mohammadi2025Evaluation
- Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods. Chen2025Towards
- Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Seo2025Simuhome
- Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and the impact of Chain of Summarization.Experiment results demonstrate that: 1. Ma2023Large
- Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for Liu2026Agents

## Concrete comparisons

- Axis: evaluation protocol (datasets; A: Agent frameworks / architectures: `P0029`, `P0030`, `P0008`; B: Evaluation / benchmark-focused works: `P0029`, `P0047`, `P0063`. Mohammadi2025Evaluation Liu2026Agents Chen2025Towards
  - A highlight: (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - A highlight: (E-P0030-8b56718f74) Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies Liu2026Agents (pointer: papers/paper_notes.jsonl:paper_id=P0030#key_results[0])
  - B highlight: (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0069-4fc221fdea) This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. Chen2025Towards (pointer: papers/paper_notes.jsonl:paper_id=P0069#key_results[0])
- Axis: metrics; A: Agent frameworks / architectures: `P0029`, `P0030`, `P0008`; B: Evaluation / benchmark-focused works: `P0029`, `P0047`, `P0063`. Mohammadi2025Evaluation Liu2026Agents Chen2025Towards
  - A highlight: (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - A highlight: (E-P0030-8b56718f74) Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies Liu2026Agents (pointer: papers/paper_notes.jsonl:paper_id=P0030#key_results[0])
  - B highlight: (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0069-4fc221fdea) This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. Chen2025Towards (pointer: papers/paper_notes.jsonl:paper_id=P0069#key_results[0])
- Axis: human evaluation); A: Agent frameworks / architectures: `P0029`, `P0030`, `P0008`; B: Evaluation / benchmark-focused works: `P0029`, `P0047`, `P0063`. Mohammadi2025Evaluation Liu2026Agents Chen2025Towards
  - A highlight: (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - A highlight: (E-P0030-8b56718f74) Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies Liu2026Agents (pointer: papers/paper_notes.jsonl:paper_id=P0030#key_results[0])
  - B highlight: (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0069-4fc221fdea) This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. Chen2025Towards (pointer: papers/paper_notes.jsonl:paper_id=P0069#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0029`, `P0030`, `P0008`; B: Evaluation / benchmark-focused works: `P0029`, `P0047`, `P0063`. Liu2026Agents Mohammadi2025Evaluation Chen2025Towards
  - A highlight: (E-P0030-8b56718f74) Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies Liu2026Agents (pointer: papers/paper_notes.jsonl:paper_id=P0030#key_results[0])
  - A highlight: (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0069-4fc221fdea) This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. Chen2025Towards (pointer: papers/paper_notes.jsonl:paper_id=P0069#key_results[0])
- Axis: and failure modes and limitations; A: Agent frameworks / architectures: `P0029`, `P0030`, `P0008`; B: Evaluation / benchmark-focused works: `P0029`, `P0047`, `P0063`. Liu2026Agents Mohammadi2025Evaluation Chen2025Towards
  - A highlight: (E-P0030-8b56718f74) Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies Liu2026Agents (pointer: papers/paper_notes.jsonl:paper_id=P0030#key_results[0])
  - A highlight: (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0047-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0047#key_results[0])
  - B highlight: (E-P0069-4fc221fdea) This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. Chen2025Towards (pointer: papers/paper_notes.jsonl:paper_id=P0069#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: API; LLMs; WildAGTEval; FMs; MatSci; LLM-based; ALFRED; VirtualHome; EGI; IPI. Kim2026Beyond Liu2026Agents Van2025Survey Mohammadi2025Evaluation

## Failures / limitations

- Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. Liu2026Agents
- We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Van2025Survey
- It then employs a multi-level verification pipeline where (i) at the semantic level, intuitive natural language safety requirements are formalized into TL formulas and the LLM agent's understanding of these requirements is probed for alignment with the TL formulas; (ii) at the plan level, high-level action plans and subgoals generated by the LLM agent are verified against the TL formulas to detect unsafe plans before execution; and (iii) at the trajectory level, multiple execution trajectories are merged into a computation tree and efficiently verified against physically-detailed TL specifications for a final safety check. Zhan2025Sentinel
- Our experiments show that by grounding physical safety in temporal logic and applying verification methods across multiple levels, Sentinel provides a rigorous foundation for systematically evaluating LLM-based embodied agents in physical environments, exposing safety violations overlooked by previous methods and offering insights into their failure modes. Zhan2025Sentinel

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- failure modes / known limitations
- baseline choices and ablation evidence
