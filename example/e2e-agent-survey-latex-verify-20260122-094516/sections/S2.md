Recent surveys and overviews of agentic LLM systems have mapped out architectures, applications, and emerging design patterns, often emphasizing the agent loop as a systems object rather than a prompting trick [@Plaat2025Agentic; @Van2025Survey; @Tang2025Agent; @Luo2025Large; @Sarkar2025Survey]. We build on this line of work but shift the organizing principle: rather than grouping papers primarily by application domains, we position the literature through interface contracts and protocol-aware evaluation, because these assumptions frequently determine whether results are comparable across settings.

On the mechanism side, a large body of work extends reasoning beyond single-pass prompting by coupling deliberation to action. ReAct makes the loop explicit by interleaving reasoning traces with tool-grounded actions [@Yao2022React], while search-style deliberation explores multiple candidates or trajectories before committing to an action sequence [@Yao2023Tree]. Reflection and critique loops further demonstrate that apparent “capability” can be amplified or destabilized by how a system stores state and revises decisions over time [@Shinn2023Reflexion]. These approaches motivate the need for protocol-aware comparisons: changing the deliberation and verification policy can change both success and cost, even when the base model is held fixed.

A complementary line of work focuses on tool use and interface learning. Training-time methods that expose explicit tool calls broaden the action space available to an agent, but they also make interface design and schema discipline central to correctness [@Schick2023Toolformer]. As evaluations move toward real-world tool ecosystems, papers increasingly report failures that are interface failures (routing mistakes, schema mismatches, tool-side instability) rather than purely reasoning errors, reinforcing the view that the interface contract is part of the research contribution [@Kim2026Beyond; @Liu2025Mcpagentbench; @Xi2026Toolgym].

Evaluation work makes the same point from the measurement side. Cost-sensitive benchmarks show that success rates must be interpreted together with budgets and search policies, because higher success can be achieved by spending more steps or tool calls [@Liu2025Costbench; @Mudur2025Feabench]. Benchmarks that stress realistic API complexity further highlight that tool noise, permissions, and logging assumptions can dominate outcomes, making “agent progress” inseparable from benchmark artifacts and environment versioning [@Kim2026Beyond; @Xi2026Toolgym; @Yin2024Safeagentbench]. Methodology papers therefore increasingly recommend protocol templates and reporting checklists that make comparisons debuggable and reproducible [@Mohammadi2025Evaluation].

Finally, security and governance work argues that agentic tool use introduces qualitatively new attack surfaces, including prompt/tool injection, tool misuse, and data exfiltration. Security benchmarks and monitoring-oriented evaluations operationalize these risks as end-to-end system properties rather than model-only attributes [@Zhang2025Security; @Lee2025Bench; @Kutasov2025Shade; @Li2025Stac; @Das2025Beyond]. This reinforces our decision to integrate threat models into the same evaluation lens used for performance, rather than treating safety as an afterthought.

In summary, prior work provides strong foundations on agent architectures and applications, but cross-paper synthesis remains difficult because protocols, cost models, and threat models vary widely and are often implicit. Our survey addresses this gap by centering interface contracts and protocol-aware evaluation as the connective tissue that makes comparisons interpretable across planning, memory, adaptation, and risk.
