Adaptation asks a different question than static agent design: instead of choosing a fixed loop once, how can the system improve its behavior as it collects feedback, observes failures, or revises its own outputs? This shift makes evaluation protocol central, because “better” can mean higher task success, lower cost, greater robustness, or safer behavior, and these objectives can conflict under realistic constraints. The key point is that adaptation mechanisms shape the frontier between success, cost, and robustness, so improvement claims are only comparable when they are tied to explicit evaluation settings and reporting conventions [@Zhou2025Self; @Du2024Anytool].

A common approach is self-generated training or self-challenging loops, where the agent creates its own difficult cases and learns from them. On multi-turn tool-use benchmarks such as M3ToolEval and TauBench, the Self-Challenging framework reports more than a two-fold improvement for Llama-3.1-8B-Instruct using only self-generated training data [@Zhou2025Self]. This result highlights a concrete trade-off: self-generated feedback can be scalable and cheap, whereas it can also drift toward idiosyncratic errors unless the evaluation protocol checks generalization beyond the agent’s own generated distribution.

A contrasting approach adapts by changing data and supervision rather than the loop policy online. Zhou et al. report that optimized datasets can yield an average improvement of 12% with larger gains on specific metrics (e.g., a 40% improvement in Fermi) as measured by benchmarks such as MT-bench, Vicuna Bench, and the WizardLM testset [@Zhou2024Star]. Compared with online self-revision, dataset-centric adaptation can stabilize training signals and make improvements more reproducible, but it can also make it harder to attribute gains to “agentic” behavior unless the evaluation includes interactive tasks and tool-use protocols.

Efficiency-oriented adaptation targets the success--cost frontier directly. EvoRoute reports that, when integrated into existing agentic systems, routing choices can sustain or enhance performance on benchmarks such as GAIA and BrowseComp+ while reducing execution cost by up to 80% and latency by over 70% [@Zhang2026Evoroute]. This illustrates a practical evaluation anchor: for adaptive systems, improvements should be reported together with budgets and latency, because a change in adaptation policy can shift where an agent sits on the frontier even if raw success looks similar.

However, adaptation is especially prone to protocol artifacts. Anytool revisits a prior evaluation protocol and identifies a limitation that leads to an artificially high pass rate, showing how self-improvement claims can be inflated when metrics are too forgiving or when the harness fails to stress the right failure modes [@Du2024Anytool]. Related analyses of “fault lines” in agent evaluation emphasize that feedback loops can overfit to loopholes in reward definitions or test harnesses, so robustness claims should be read as conditional on a clearly stated protocol [@Nitin2025Faultline; @Zheng2025Towards].

Two limitations are worth keeping explicit. First, self-improvement loops can trade off stability for performance: more aggressive self-revision can amplify errors or reward hacking when feedback signals are misaligned, which makes careful reporting and ablations essential [@Nitin2025Faultline; @Zhou2025Self]. Second, adaptation results often rely on domain-specific benchmarks (from interactive web tasks to scientific simulation), so transfer to new environments can be limited unless evaluation anchors are diversified and compared against strong non-adaptive baselines such as ReAct-style controllers [@Li2026Autonomous; @Yao2022React].

A final perspective comes from domain-specific “autonomous scientist” style agents, where adaptation is framed as iterative experimentation and correction rather than as benchmark gaming. Li et al. report that an LLM agent can autonomously perform tensor network simulations of quantum many-body systems with approximately 90% success across representative tasks, illustrating a regime where self-correction and tool-driven iteration are central to progress [@Li2026Autonomous]. At the same time, such results are often tightly coupled to the toolchain and task distribution, so transfer to other interactive settings can be limited unless evaluation protocols stress failures and distribution shift in addition to average success [@Zhou2025Self; @Nitin2025Faultline]. This reinforces the broader lesson: adaptation is powerful, but it is only interpretable when tied to explicit evaluation settings and when improvements are decomposed into cost, robustness, and failure-mode changes rather than reported as a single headline gain [@Zhang2026Evoroute; @Du2024Anytool].

For survey-level comparisons, it is also useful to separate “within-episode” adaptation (revision during a single task) from “across-episode” adaptation (learning from prior tasks). These regimes can have different failure modes: within-episode revision can amplify a bad assumption, while across-episode learning can overfit to benchmark quirks if feedback is not diverse. Evaluations that report learning curves or improvement trajectories over time make these dynamics visible, whereas one-shot scores can be misleading when adaptation continues during testing [@Zhou2025Self; @Yao2022React].

For interpretation, the most important contrast is between adaptation that changes the policy online and adaptation that changes the data or supervision offline. Online revision can respond to task-specific feedback but may compound a wrong assumption, whereas offline optimization can be more stable but may blur whether gains come from better agents or better training distributions. For synthesis, it is therefore useful to treat “self-improvement” as a protocol-bound claim: improvements must be tied to when learning happens, what feedback is available, and how evaluation prevents leakage from training into test.
