A central tension is that the tasks we want agents to solve are open-ended and interactive, whereas the comparisons we want to make require controlled, repeatable protocols. When benchmarks vary tool access, budgets, or logging assumptions, they can change not only measured performance but also which failure modes are even observable. As a result, the most useful evaluations are those that specify protocols as carefully as they specify tasks [@Mohammadi2025Evaluation; @Kim2026Beyond].

Recent survey work on agent evaluation has begun to formalize what “protocol-aware” evaluation should mean. Mohammadi et al. propose a two-dimensional taxonomy that organizes evaluation by objectives (e.g., behavior, capability, reliability, safety) and by evaluation setups, providing a vocabulary for comparing papers that would otherwise look incomparable due to different harnesses and assumptions [@Mohammadi2025Evaluation]. This taxonomy is valuable because it reframes benchmarks as design choices: a suite that measures step-by-step correctness under strict budgets answers a different question than one that measures end-task completion under permissive retries.

Realism-oriented benchmarks further highlight that API complexity and tool ecosystems are part of the measurement. Beyond Perfect APIs evaluates agents under real-world API complexity and demonstrates that assumptions about tool noise, schema strictness, and permissions can dominate outcomes [@Kim2026Beyond]. Complementary work proposes more explicit “agent specifications” and reporting conventions that treat environment artifacts and contracts as part of the evaluated object, aiming to reduce ambiguity when results are reproduced or extended [@Wang2025Agentspec; @Seo2025Simuhome].

A parallel thread focuses on evaluation design guidelines within specific domains. Chen et al. propose an evidence-based guideline for LLM-based robotic process automation evaluation by systematically reviewing 1,676 papers, arguing for more consistent evaluation design choices and reporting across studies [@Chen2025Towards]. Domain surveys such as LLM agents in law similarly emphasize that evaluation methodology and task selection must match the deployed setting, because “agent success” depends on the interaction protocol as much as on model capability [@Liu2026Agents].

Security and privacy evaluations underscore that protocol definition is inseparable from threat modeling. Fu et al. introduce RAS-Eval, which evaluates agent vulnerabilities across diverse scenarios and highlights how attack tasks and tool formats influence measured robustness [@Fu2025Eval]. Related work on contextual privacy auditing similarly shows that what is considered “safe” depends on adversarial assumptions and logging policies, making threat models a first-class part of benchmark design [@Das2025Beyond; @Fu2025Eval].

Two limitations recur across current benchmarks. First, many suites still leave important protocol degrees of freedom unspecified (budget caps, retry policies, tool catalog versions), making cross-paper comparisons limited even when benchmark names match [@Mohammadi2025Evaluation; @Wang2025Agentspec]. Second, benchmark coverage can be narrow relative to the space of real failures, motivating meta-evaluation efforts (e.g., sentinel-style monitoring benchmarks and protocol taxonomies) that treat coverage and reporting as part of the evaluation target [@Zhan2025Sentinel; @Ji2025Taxonomy].

One concrete path forward is to treat benchmarks as bundles of artifacts: tool catalogs, permission models, logging policies, budget definitions, and scoring scripts. When these are shipped and versioned, later work can replay protocol choices and make controlled changes, which turns “evaluation drift” into an explicit experimental variable rather than an accidental confound [@Wang2025Agentspec; @Kim2026Beyond]. This perspective aligns with broader evaluation-design guidance in applied domains, where consistency and traceability are prerequisites for cumulative progress [@Chen2025Towards; @Mohammadi2025Evaluation]. Without artifact-level specificity, benchmark results can remain limited to one-off demonstrations and resist the kind of synthesis that surveys aim to provide.

From this standpoint, a benchmark score is only the final line of a protocol definition. The underlying artifacts---tool catalogs, permission policies, budget caps, retry rules, and logging schemas---determine what behavior is even measurable and which failures are visible to the evaluator. Protocol templates and checklists help because they force these degrees of freedom into the paper, making later replication and ablation possible [@Mohammadi2025Evaluation; @Wang2025Agentspec]. Realism-oriented suites that vary API complexity make the same point empirically: once tool noise and schema constraints are introduced, small protocol changes can dominate outcomes [@Kim2026Beyond]. In applied domains, evaluation-design guidelines similarly stress that “what to measure” must be paired with “how to measure it” in a way that can be re-run, otherwise evidence remains limited to narrative claims [@Chen2025Towards; @Liu2026Agents].

In other words, evaluation choices create a spectrum from realism to control. Benchmarks that tightly fix tools, budgets, and logging make causal attribution easier, whereas more open-world setups better approximate deployment but make it harder to isolate what changed. Good survey synthesis depends on recognizing where each benchmark sits on this spectrum and avoiding head-to-head comparisons across incompatible protocols.
