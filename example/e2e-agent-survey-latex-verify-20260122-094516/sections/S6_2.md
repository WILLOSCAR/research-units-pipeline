Safety and security failures in agentic systems are rarely model-only. Because agents act through tools and environments, the relevant threat model includes the entire loop: tool descriptions, tool outputs, memory stores, and the policies that decide when to execute actions. This shifts governance from “prompt hygiene” to system design questions such as permissioning, observability, and end-to-end monitoring under explicit adversarial assumptions. The key point is that governance must be evaluated at the loop level, because protocol and interface choices determine both the attack surface and the meaning of robustness claims [@Zhang2025Security; @Fu2025Eval].

A concrete step toward loop-level threat modeling is to name attack classes that are specific to tool use. MSB (an MCP security benchmark) contributes a taxonomy of 12 attacks, including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, and retrieval injection [@Zhang2025Security]. The same work evaluates nine popular agents across 10 domains and 400+ tools, producing 2,000 attack instances, illustrating that “tool access” is not a benign assumption but a primary driver of security outcomes [@Zhang2025Security].

Benchmarking work also quantifies how attacks change end-task behavior. RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 CWE categories, implemented across multiple tool formats (JSON, LangGraph, MCP), and reports that attacks reduce agent task completion rates by 36.78% on average with an 85.65% attack success rate in academic settings [@Fu2025Eval]. These numbers are evaluation anchors: robustness claims should be interpreted together with the attack surface and tool format, because defenses that work under one interface may fail under another.

Defense-oriented systems emphasize that robustness is an end-to-end property. Progent reports reducing attack success rates to 0% across agent use cases while preserving agent utility and speed, suggesting that effective mitigations must intervene across planning, invocation, and response-handling stages rather than treating the model as the only control point [@Shi2025Progent; @Zhang2025Security]. Related lines of work explore enforcement and checking mechanisms around tool calls and execution, aiming to make security constraints explicit and auditable even when the model’s internal reasoning is not [@Kamath2025Enforcing; @Bonagiri2025Check].

Safety is broader than adversarial security: even non-adversarial environments can contain hazards, specification gaps, or privacy risks. Benchmarks such as Agent-SafetyBench emphasize diverse unsafe interaction patterns and failure modes, encouraging evaluation beyond task success to include harm and policy compliance under interactive protocols [@Zhang2024Agent; @Yuan2024Judge]. Adversarial reinforcement learning approaches similarly frame “agent safety” as a training problem under explicit threat models, reinforcing the idea that deployment governance must combine evaluation, monitoring, and learning signals [@Wang2025Adversarial; @Kim2024When].

Two limitations remain. First, many papers still leave threat models implicit, making it unclear which inputs are adversarial and which actions are permitted; without explicit assumptions about permissions and logging, claims about safety can be limited to narrow settings [@Zhang2025Security; @Fu2025Eval]. Second, robustness results often depend on tool and environment artifacts that are evolving in real deployments, so governance work needs to treat tool catalogs and their contracts as versioned objects and to bridge evaluation with operational monitoring, not as a one-off benchmark score [@Gasmi2025Bridging; @Kamath2025Enforcing].

Governance mechanisms can therefore be viewed as “operational protocols” that complement benchmarks. Permissioning and audit logging constrain what actions are executable and make post-hoc analysis possible, while automated checking and enforcement aim to prevent unsafe tool chains before they execute [@Kamath2025Enforcing; @Bonagiri2025Check]. Bridging evaluation with deployment monitoring is especially important for agents, because the tool ecosystem and its failure modes evolve over time; a benchmark score without ongoing monitoring can give a false sense of security once the environment shifts [@Gasmi2025Bridging; @Yuan2024Judge]. In this sense, governance is not an optional appendix but a continuation of the evaluation protocol under real constraints, and current evidence remains limited whenever papers do not specify permissions, logging, and response-handling policies alongside model and benchmark details [@Kim2024When; @Zhang2025Security].

There is also an inherent security--utility trade-off: stronger defenses can restrict tool access or add verification steps that increase latency and cost. The most actionable papers make this trade-off measurable by pairing attack success metrics with task completion and runtime, rather than claiming “secure” behavior in the abstract [@Shi2025Progent; @Fu2025Eval]. When defenses are learned (e.g., via adversarial training), the evaluation must also clarify the adversary model and whether robustness generalizes beyond the attack distribution used during training; otherwise, safety evidence remains limited to the tested regime [@Wang2025Adversarial; @Zhang2025Security].

A final contrast is between defenses that harden the interface and defenses that harden the policy. Interface hardening focuses on sanitization, permissions, and execution guards, whereas policy hardening aims to train or steer the agent to avoid unsafe actions in the first place. In practice, the strongest evidence comes from systems that combine both and report how safety interventions change not only attack success but also task completion, cost, and monitoring load.
