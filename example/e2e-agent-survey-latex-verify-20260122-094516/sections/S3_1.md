The first design decision in an agent system is what counts as an action and what counts as an observation. That choice fixes the shape of the control loop---what the model is allowed to do, what feedback it can condition on, and which failures it can even detect. In practice, results are easiest to interpret when the action space is explicit and the evaluation protocol makes loop assumptions visible, a perspective reflected in early interactive agents such as ReAct and in open-world testing environments that treat tool use as part of the task rather than an implementation detail [@Yao2022React; @Xi2026Toolgym].

A useful way to view “agent loop design” is as a bundle of interface commitments: state representation, action representation, and recovery policy. Some systems keep state largely implicit (the chat history) and let the model emit free-form action descriptions that are parsed into tool calls, while others constrain the action space with typed commands, schemas, or environment APIs. These constraints trade expressivity for debuggability: a narrower action language can reduce parsing and tool-selection ambiguity, whereas a more open action space can cover diverse tasks at the cost of higher variance and harder-to-localize failures [@Shen2024Small; @Kim2025Bridging; @Ghose2025Orfs].

This tension shows up in how agents couple reasoning to action. ReAct makes the coupling explicit and reports large gains on interactive decision-making benchmarks such as ALFWorld and WebShop (absolute success rate improvements of 34% and 10% over imitation and reinforcement learning baselines) under a fixed prompting setup [@Yao2022React]. In contrast, environment-first work emphasizes that the same loop abstraction can behave very differently once tool access, noise, and logging are varied, motivating “agent testing” as a first-class object rather than an afterthought [@Xi2026Toolgym; @Song2026Envscaler]. When loop assumptions shift---for example, whether tools fail loudly or silently, or whether observations are lossy summaries---comparisons can drift even when the model and prompts are unchanged.

A second thread treats loop design as something that can be searched or discovered rather than handcrafted. AgentSquare reports that automatically constructed agents can outperform human designs by an average of 17.2% across a set of interactive environments, suggesting that seemingly small loop choices (when to verify, how to retry, what to store as state) can dominate outcomes once held to a fixed benchmark [@Shang2024Agentsquare]. Similarly, AgentSwift evaluates discovered agents across seven benchmarks spanning embodied, math, web, tool, and game settings and reports an 8.34% average performance gain over both automated baselines and manual agents, reinforcing the point that “agent loop” is an optimization surface, not a single canonical architecture [@Li2025Agentswift; @Sun2025Agent].

Efficiency-oriented variants highlight that loop design is inseparable from cost models. EvoRoute reports that, on agentic benchmarks such as GAIA and BrowseComp+, routing choices integrated into off-the-shelf systems can sustain or improve task success while reducing execution cost by up to 80% and latency by over 70% [@Zhang2026Evoroute]. These numbers are less a claim about any one model than about the loop as a budgeted system: changing when the agent re-plans, re-queries tools, or short-circuits verification directly changes both measured performance and the resources consumed.

Two limitations recur across this literature. First, action spaces are often under-described: papers may not fully specify what the agent is permitted to do (and what is disallowed), which makes failure analysis and reproducibility limited even when benchmark names are shared [@Xi2026Toolgym; @Fumero2025Cybersleuth]. Second, evaluation protocols can hide “failure assumptions” (e.g., whether tool errors are adversarial, stochastic, or deterministic), so synthesis should treat protocol descriptions as evidence, not as boilerplate; where those assumptions are unclear, the safest takeaway is about the direction of trade-offs rather than any absolute ranking [@Song2026Envscaler; @Kim2025Bridging].

Domain differences also matter for how action spaces are engineered. In geometry problem solving, for example, recent work notes that performance can still be dominated by expert systems that rely on large-scale data synthesis and search, highlighting a regime where a specialized action language and solver loop can outweigh generic tool-calling heuristics [@Zhao2025Achieving]. At the other extreme, open-world tool environments stress breadth: they reward agents that can adapt to changing APIs and noisy observations, even when individual actions are simple [@Xi2026Toolgym; @Song2026Envscaler]. This gap suggests that “agent loop” comparisons are often limited by mismatched action languages---when papers evaluate under different state and action abstractions, synthesis is clearest when conclusions are phrased as conditional trade-offs (what breaks under which interface assumptions) rather than as universal claims about which loop is best [@Kim2025Bridging; @Zhao2025Achieving].

Across these threads, the dominant comparison is between widening the action space and tightening the contract. Whereas open-ended action languages maximize coverage and let agents improvise in novel environments, constrained action APIs make failures easier to localize and make protocols easier to reproduce. A useful reporting norm is therefore to treat the action space itself as an evaluation artifact: specify the allowed actions, the observation channels, and the recovery semantics, so readers can tell whether an improvement comes from better decision-making or simply from a more forgiving interface.
