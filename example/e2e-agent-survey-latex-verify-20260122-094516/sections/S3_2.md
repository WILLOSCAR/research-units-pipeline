Tool use widens what an agent can accomplish, but it also creates a new bottleneck: correctness becomes a property of the interface contract as much as of the model. A minor mismatch in schemas, permissions, or error semantics can turn a capable planner into a brittle system, and reported gains can be hard to compare when tool access and orchestration policies differ across evaluations. Recent evaluation work therefore treats interface contracts and protocol definitions as part of the measurable object, not as incidental implementation detail. The key point is that interface contracts constrain which comparisons are meaningful, so orchestration results must be read through the protocol that fixes tool access and error behavior [@Mohammadi2025Evaluation; @Liu2025Mcpagentbench].

At a high level, tool interfaces answer three questions: how actions are represented (free-form text versus structured calls), how tools are selected (routing and ranking), and how tool outputs are validated and incorporated into state. Some systems emphasize expressive tool descriptions and rely on the model to map language to actions, whereas others tighten the contract with typed parameters, constrained formats, or confidence/verification policies. The trade-off is familiar: looser contracts increase coverage but raise ambiguity and silent failure risk; tighter contracts reduce ambiguity but can introduce brittleness when tools evolve or when the environment deviates from assumptions [@Du2024Anytool; @Xuan2026Confidence; @Li2026Toolprmbench].

Benchmark design has started to reflect this reality by elevating orchestration as a multi-hop systems problem. ETOM introduces a five-level benchmark that evaluates end-to-end tool orchestration within a hierarchical tool ecosystem, explicitly stressing multi-step coordination rather than single tool calls [@Dong2025Etom]. MCPAgentBench similarly anchors evaluation in real-world tool definitions, aiming to make tool-use results more reproducible by standardizing the interface artifacts that agents must operate over [@Liu2025Mcpagentbench]. These efforts illustrate an important evaluation anchor: unless the tool catalog and its semantics are fixed, it is unclear whether an improvement comes from better decision-making or from a cleaner interface.

Security further raises the stakes because tool interfaces double as an input channel. An analysis of 66 real-world tools from LangChain and LlamaIndex repositories reports that 75% are vulnerable to XTHP-style attacks, suggesting that many tool ecosystems are adversarial by default unless contracts and sanitization are explicitly designed [@Li2025Dissonances]. In this regime, the relevant comparison axis is not “does the agent call the tool”, but “under what threat model is the call safe”, including whether prompts can be injected through tool descriptions, tool outputs, or chained tool behaviors [@Hao2026From; @Li2025Dissonances].

Tool interfaces also interact with memory and long-horizon execution. MemTool, for example, evaluates multiple “tool modes” across 13+ LLMs on a ScaleMCP benchmark over 100 consecutive user interactions, tracking both task completion and tool removal ratios as a proxy for short-term memory efficiency [@Lumer2025Memtool]. This kind of protocol makes explicit that orchestration is not a single decision but a policy over time: when to call tools, what to retain as state, and when to discard or summarize intermediate artifacts all change both cost and failure modes.

Two limitations shape how to read this literature. First, interface assumptions are often implicit, which makes cross-paper synthesis limited when tool catalogs, permissions, or logging policies are not fully specified [@Mohammadi2025Evaluation; @Liu2025Mcpagentbench]. Second, many benchmarks still under-exercise the adversarial and distribution-shift regimes that real deployments face, so strong performance claims should be interpreted as protocol-scoped unless they are backed by threat-model-aware evaluation and hardening analyses [@Li2025Dissonances; @Dong2025Etom; @Hao2026From].

A practical implication is that “tool use” should be evaluated as a trajectory problem, not as a one-shot classification of which tool to call. Multi-hop orchestration benchmarks make this explicit by grading chains of calls and intermediate state updates under a fixed protocol, which helps separate routing quality from tool curation [@Dong2025Etom; @Liu2025Mcpagentbench]. Complementary benchmarks target more fine-grained interface skills (e.g., parameter reasoning and schema compliance), reflecting that orchestration failures often come from small contract violations rather than from missing high-level plans [@Li2026Toolprmbench; @Du2024Anytool]. Even with these advances, comparisons remain limited when papers change tool catalogs or hide tool behaviors behind proprietary endpoints, so the most reusable results are those that ship the interface artifacts and specify evaluation constraints in a way that later work can replay [@Mohammadi2025Evaluation; @Liu2025Mcpagentbench].

A simple but high-leverage reporting practice is to publish the interface artifacts alongside results---schemas, tool descriptions, and a brief “tool card” describing expected failures and permissions. Without these artifacts, it is difficult to tell whether an orchestration gain reflects a better policy or merely a cleaner tool contract, and reproduction becomes limited to approximate reimplementations [@Liu2025Mcpagentbench; @Li2025Dissonances].

Interface choices also determine how errors are surfaced and corrected. A permissive contract that silently coerces inputs can inflate success rates but hide brittleness, whereas a strict contract that fails loudly can make agents look weaker while producing more diagnosable trajectories. For synthesis, the important question is not just whether a tool was called, but whether the call was valid under a declared schema and whether the system had a principled recovery path when the interface rejected or mis-executed the request.
