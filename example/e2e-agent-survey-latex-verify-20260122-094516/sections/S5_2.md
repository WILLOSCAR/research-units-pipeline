Coordination turns an agent into a team. Once multiple agents interact, the “protocol” is no longer just an evaluation detail---it *is* the policy. As a result, turn-taking rules, message visibility, role specialization, and aggregation logic determine what information flows and what failures are possible. This makes multi-agent results particularly sensitive to interaction design, and it motivates protocol-aware synthesis rather than headline comparisons across disparate setups [@Wu2025Agents; @Silva2025Agents].

A baseline contrast is between loosely coupled coordination (agents independently propose and a router selects) and tightly coupled coordination (agents iteratively negotiate, critique, or verify). Controlled analyses of debate-style settings suggest that the benefits of multi-agent interaction can vary widely with task structure and evaluation choices, cautioning against treating “more agents” as a universally reliable lever [@Wu2025Agents; @Sun2025Agent]. In practice, coordination must be evaluated with the same care as planning: what constitutes agreement, when is communication allowed, and how are failures attributed to individual roles.

Training frameworks have begun to treat multi-turn interaction as a first-class object. SkyRL-Agent is presented as a framework for efficient, multi-turn, long-horizon agent training and evaluation, and results derived from it include a software engineering agent (SA-SWE-32B) trained from Qwen3-32B with a reported 24.4% Pass@1 baseline before reinforcement learning [@Cao2025Skyrl]. This line of work underscores an evaluation anchor for coordination: interaction policies change learning signals, so reported improvements should be paired with clear descriptions of turn structure, tool access, and reward definitions.

Applied settings provide another perspective on coordination: the goal is often not maximal accuracy on a single task, but throughput and reliability over many jobs. Continuum reports improved average job completion times on real-world agentic workloads such as SWE-Bench and BFCL with Llama-3.1 8B/70B, with improvements that scale as the number of turns increases [@Li2025Continuum]. In cybersecurity-style workloads, CRAKEN reports solving 25--30% more MITRE ATT&CK techniques than prior work via knowledge-based execution, illustrating how coordination and tool use can change practical coverage even when tasks are heterogeneous [@Shao2025Craken].

Coordination is also mediated by shared state, which links it back to memory design. MemTool evaluates different interaction “modes” across 13+ LLMs over 100 consecutive user interactions, tracking both task completion and tool removal ratios as a proxy for short-term memory efficiency under a fixed benchmark protocol [@Lumer2025Memtool]. This suggests a concrete comparison axis: whether coordination policies rely on shared persistent memory, per-agent private memories, or explicit message passing can change both cost and failure modes, and these choices should be reported alongside performance.

Two limitations shape how to read multi-agent evidence. First, attribution is hard: when a team succeeds, it can be unclear whether gains come from better reasoning, better division of labor, or simply more total compute and retries, so ablations that hold budget fixed are essential [@Wu2025Agents; @Li2025Continuum]. Second, many systems are evaluated under narrow interaction protocols; transfer to new coordination settings can be limited unless protocols and incentives are explicitly modeled and stress-tested, especially when tools and environments are adversarial or noisy [@Lichkovski2025Agent; @Silva2025Agents].

From an evaluation standpoint, the key confounder is budget. Coordination policies often increase the total amount of computation and the number of tool calls, so improvements should be compared under matched constraints whenever possible. This is visible in practice-oriented settings: job completion time gains that scale with turn count can reflect better coordination, but they can also reflect simply doing more work per task if budgets are not normalized [@Li2025Continuum; @Cao2025Skyrl]. Similarly, coverage improvements on heterogeneous cybersecurity workloads (e.g., solving 25--30% more ATT&CK techniques) are meaningful only when the interaction protocol and tool permissions are explicitly stated, because coordination can change the effective action space [@Shao2025Craken; @Silva2025Agents]. Where these details are missing, multi-agent claims are best read as evidence that interaction policies matter, not as definitive rankings of which coordination scheme is superior [@Wu2025Agents; @Sun2025Agent].

A further complication is that coordination protocols embed implicit assumptions about trust and information flow. Whether agents can see each other’s tool outputs, whether messages are summarized or raw, and whether roles are fixed or adaptive can all change the effective state space of the system. Studies that explicitly control interaction structure make it easier to attribute gains to coordination rather than to incidental information sharing; when these controls are absent, coordination evidence is limited and comparisons across papers become brittle [@Wu2025Agents; @Lichkovski2025Agent].

Coordination design also admits multiple “styles” that are easy to conflate. A decentralized team that gathers parallel proposals can be efficient, whereas a deliberative team that iteratively critiques and verifies can be slower but more robust on long-horizon tasks. Because these styles imply different communication costs and different error-correction mechanisms, evaluation protocols should state message budgets, visibility, and termination criteria to make multi-agent comparisons interpretable.
