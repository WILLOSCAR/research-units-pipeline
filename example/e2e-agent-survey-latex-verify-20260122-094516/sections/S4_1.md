Planning is where agents trade reliability for resources. A central tension is that deeper deliberation, explicit search, and verification can improve success on multi-step tasks, but they also increase latency and tool cost and can introduce their own failure modes (overthinking, inconsistent intermediate state, cascading retries). As a result, planning results are easiest to interpret when evaluation protocols make budgets and loop policies explicit rather than treating them as background settings [@Yao2022React; @Seo2025Simuhome].

Moreover, one family of approaches builds planning loops by prompting and structuring the model’s reasoning and action choices. ReAct interleaves reasoning traces with grounded actions, providing a simple but effective template for long-horizon interaction [@Yao2022React]. Hierarchical or tree-structured controllers extend this idea by imposing control flow, decomposing tasks, or exploring multiple candidates before committing to an action sequence, aiming to reduce early-commitment errors that are common in interactive environments [@Choi2025Reactree; @Khoee2025Gatelens]. These designs often improve robustness by changing *how* decisions are made rather than by changing the base model.

A contrasting family treats planning competence as something that can be trained or optimized directly. On a complex task-planning benchmark, Hu et al. report that a 1.5B parameter model trained with single-turn GRPO can achieve a 70% success rate for long-horizon planning tasks, outperforming larger baselines up to 14B parameters under that evaluation setup [@Hu2025Training]. Unlike prompt-structured planners, training-based approaches shift the main comparison axis toward data signals, reward definitions, and generalization: gains may reflect better policy learning, but they also risk being benchmark- or protocol-specific if the training distribution matches the evaluation too closely.

Safety-aware planning highlights that “good plans” are not always “safe plans”. SafeAgentBench is designed to evaluate embodied agents in interactive simulations under both explicit and implicit hazards, forcing planners to account for risk while pursuing task goals [@Yin2024Safeagentbench]. This changes what evaluation should measure: success under hazards is not only a function of planning depth, but also of how agents model constraints, detect unsafe states, and decide when to abort or ask for clarification.

Because planning loops can fail in rare but consequential ways, evaluation increasingly borrows from testing methodology. Zhou et al. report that seed test case generation can yield a 2--2.5x boost to the coverage of risk outcomes and tool-calling trajectories across diverse agent settings, suggesting that benchmark scores alone may miss important tail failures [@Zhou2025Siraj; @Ji2024Testing]. In this view, the evaluation protocol is part of the planner: what scenarios are sampled, what constitutes a failure, and how trajectories are logged can change which planning strategies appear reliable.

Two limitations temper strong conclusions in this space. First, planning comparisons are often under-specified with respect to budgets, retries, and verification policies, so results can be difficult to transfer across benchmarks even when task names match [@Seo2025Simuhome; @Yao2022React]. Second, as planners become more structured, it becomes unclear whether improvements come from better decision-making or from better tooling around the model (control flow, gating, testing harnesses); where ablations are missing, the safest interpretation is about the *direction* of trade-offs rather than about any universal ranking [@Choi2025Reactree; @Zhou2025Siraj].

Unified evaluations that hold the loop constant can also change what we learn about planning. Seo et al. evaluate 16 agents under a shared ReAct-style framework and report distinct capability and limitation profiles across models, suggesting that “planning quality” is not a single scalar but a bundle of behaviors (decomposition, recovery, verification) that surface differently under the same protocol [@Seo2025Simuhome; @Yao2022React]. This kind of setup helps interpret training-based gains as well: a 70% success rate on one long-horizon benchmark is informative only insofar as the budget, retries, and tool assumptions align with the target deployment regime [@Hu2025Training; @Yin2024Safeagentbench]. When those assumptions are mismatched, the right conclusion is often about which planning *strategy* is robust under which constraints, not about whether one model “can plan” in general.

Finally, planning benchmarks differ in what they reward: some score only end-task success, while others expose intermediate safety violations, dead-ends, or hazard encounters. This distinction can flip conclusions about which planners are “better”, because conservative policies may look weak under pure success metrics but strong under hazard-aware protocols that penalize unsafe trajectories [@Yin2024Safeagentbench; @Seo2025Simuhome]. In that sense, planning is not only about generating a plan but about defining what counts as an acceptable trajectory under the evaluation rules, and current evidence remains limited when those rules are not reported precisely [@Ji2024Testing; @Zhou2025Siraj].

A key contrast, then, is between planners that externalize structure in prompts and control flow and planners that internalize it through training. Prompt-structured planners can be adapted quickly to new tasks and constraints, whereas training-centric planners may achieve stronger in-distribution reliability under a fixed benchmark but risk locking in protocol-specific behaviors. Making this contrast visible requires reporting not only final success but also intermediate decision traces, budget usage, and the failure modes the planner is designed to avoid.
