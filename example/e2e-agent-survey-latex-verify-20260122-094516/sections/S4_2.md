Memory turns an agent from a single-trajectory solver into a stateful system, but it also introduces a new failure surface: retrieval can be wrong, stale, or adversarial, and the agent may not know which. The core tension is that richer memory can improve long-horizon coherence and tool use, whereas it can also amplify compounding errors and make evaluation less interpretable unless retrieval assumptions are made explicit [@Huang2025Retrieval; @Chen2025Grounded].

A practical distinction is between ephemeral working memory (scratchpads, short-term summaries) and persistent memory (indexes, long-term stores, artifact repositories). Ephemeral memory is cheap and easy to reset between episodes, but it can force repeated tool calls and re-derivation; persistent memory can reduce redundancy, yet it raises questions about what is written, when it is retrieved, and how leakage or contamination is prevented across tasks. These design choices are part of the protocol: changing write or retrieval policy can change both success and cost even when the planner is unchanged [@Yao2022React; @Yu2026Agentic].

In code-centric settings, summarization and representation choices can dominate whether retrieval is useful. Meta-RAG proposes using summaries to condense codebases by an average of 79.8% into a compact, structured natural-language representation, aiming to make retrieval more stable and context-efficient for downstream reasoning [@Tawosi2025Meta]. This kind of approach illustrates an evaluation anchor for memory systems: improvements should be tied to task families (e.g., code comprehension or modification) and to measurable properties of the memory (compression ratio, retrieval accuracy), not only to end-task success.

Tooling scaffolds also function as memory. Verma et al. evaluate a coding-agent setup on N=5 context-intensive SWE-bench Lite instances using an optimized scaffold with persistent bash state and a string-replacement editor, effectively changing what information is carried forward across tool calls and edits [@Verma2026Active]. Even when models are fixed, such scaffolding can alter the agentâ€™s effective memory and error recovery behavior, which complicates head-to-head comparisons unless the toolchain and state persistence policy are reported.

Memory increases risk surfaces because it is an input channel. MSB catalogs 12 agent-specific attack patterns, including retrieval injection and prompt injections embedded in tool descriptions, emphasizing that retrieval and tool outputs can be manipulated to steer the loop [@Zhang2025Security]. Defensive work like Progent reports reducing attack success rates to 0% on agent benchmarks (e.g., AgentDojo, ASB, AgentPoison) while preserving utility and speed, highlighting a contrast: the same memory mechanisms that increase capability can widen the adversarial surface unless guarded end-to-end [@Shi2025Progent; @Zhang2025Security].

Two limitations recur. First, many memory mechanisms are evaluated under narrow task distributions, so it remains unclear how robust they are to retrieval noise, distribution shift, or adversarial inputs outside the benchmark protocol [@Huang2025Retrieval; @Zhang2025Security]. Second, memory improvements are easy to confound with better prompting or tooling; without ablations that hold the scaffold fixed, gains should be treated as conditional on the full system design rather than as properties of a single memory module [@Verma2026Active; @Chen2025Grounded].

Therefore, for evaluation, this suggests separating at least three measurable questions: (i) retrieval quality (does the memory return the right evidence), (ii) grounding behavior (does the agent actually use retrieved evidence rather than hallucinating), and (iii) robustness (how retrieval behaves under noise and adversarial inputs). Work that reports compression ratios or structured representations (e.g., 79.8% codebase condensation) provides one handle on (i), but it does not by itself guarantee grounded decisions in an interactive loop [@Tawosi2025Meta; @Chen2025Grounded]. Meanwhile, security taxonomies that include retrieval injection make clear that robustness requires explicit threat models and end-to-end defenses, not just better embeddings [@Zhang2025Security; @Shi2025Progent]. Without protocols that measure these pieces separately, memory improvements can remain ambiguous and limited to the specific benchmark harness.

In addition, memory systems should be stress-tested under adversarial and worst-case conditions, not only under average-case workloads. Security benchmarks that explicitly include retrieval injection and tool-description injection illustrate that a memory store can become an attack surface that steers the loop, even when the base model is unchanged [@Zhang2025Security]. Defensive systems that claim strong robustness benefits are most convincing when paired with attack-aware evaluation protocols and when they report which parts of the memory and toolchain are trusted versus sanitized; otherwise, robustness can remain limited to the specific harness and threat model used in the paper [@Shi2025Progent; @Huang2025Retrieval].

Finally, memory systems should be compared as policies, not as components. A retrieval-heavy design can improve factuality and reduce repeated tool calls, whereas a memory-light design can be easier to reset and less vulnerable to contamination across episodes. The right choice depends on the protocol: whether memory persists across tasks, whether it is user-specific, and whether the evaluation penalizes privacy leakage or unsafe reuse of prior context.
