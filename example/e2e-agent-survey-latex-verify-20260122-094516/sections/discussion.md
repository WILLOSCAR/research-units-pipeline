## Discussion

A first cross-cutting lesson is that “agent capability” is routinely confounded with interface design. When benchmarks increase API realism---through noisy tools, partial schemas, or complex permissioning---failures shift from reasoning errors toward interface failures such as routing mistakes, schema mismatch, and brittle recovery logic [@Kim2026Beyond; @Liu2025Mcpagentbench; @Xi2026Toolgym]. This suggests a practical reporting norm: treat the tool catalog and its contracts as versioned artifacts, because otherwise improvements can reflect a cleaner interface rather than a better policy.

A second lesson is that cost and budget assumptions are not a nuisance variable but a primary axis of comparison. Cost-aware studies show that the same base model can traverse a steep success--cost frontier depending on how planners search, retry, and verify [@Liu2025Costbench; @Mudur2025Feabench]. For survey-level synthesis, this implies that performance claims should be interpreted as protocol-scoped trade-offs, and that benchmarks should report cost models and budgets with the same care as task definitions.

A third lesson concerns coordination. Multi-agent settings amplify protocol sensitivity because communication, role specialization, and aggregation choices create new degrees of freedom that change both outcomes and failure modes [@Wu2025Agents; @Chuang2025Debate; @Papadakis2025Atlas]. As a result, “multi-agent gains” are particularly hard to transfer across papers unless interaction protocols (turn structure, visibility, incentives) and evaluation constraints are fixed.

Finally, risk surfaces in agentic systems are inseparable from tool use. Security benchmarks and monitoring-oriented evaluations show that prompt/tool injection, tool chaining, and privacy leakage arise at the system boundary, and defenses must be evaluated end-to-end rather than as model-only guardrails [@Zhang2025Security; @Lee2025Bench; @Kutasov2025Shade; @Li2025Stac; @Das2025Beyond]. A survey lens that jointly tracks protocols and threat models can therefore serve as an “evaluation checklist”: if a paper omits explicit tool access, logging, and adversarial assumptions, its claims should be treated as narrower than their headline numbers suggest.
