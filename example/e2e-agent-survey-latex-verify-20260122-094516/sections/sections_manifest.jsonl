{"kind": "global", "id": "abstract", "title": "Abstract", "path": "sections/abstract.md", "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "7de03c3c820bd0b5171d0f227e1db9c9b8b1e0cb", "bytes": 1050, "citations": ["Liu2025Costbench", "Mohammadi2025Evaluation", "Schick2023Toolformer", "Shinn2023Reflexion", "Yao2022React", "Zhang2025Security"]}
{"kind": "global", "id": "discussion", "title": "Discussion", "path": "sections/discussion.md", "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "d2b38c7ddf81af99ff68f03592609b50317cfc7c", "bytes": 2276, "citations": ["Chuang2025Debate", "Das2025Beyond", "Kim2026Beyond", "Kutasov2025Shade", "Lee2025Bench", "Li2025Stac", "Liu2025Costbench", "Liu2025Mcpagentbench", "Mudur2025Feabench", "Papadakis2025Atlas", "Wu2025Agents", "Xi2026Toolgym", "Zhang2025Security"]}
{"kind": "global", "id": "conclusion", "title": "Conclusion", "path": "sections/conclusion.md", "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "544b79e6c7092025ce0a0ef408cc69872b83b7fe", "bytes": 1030, "citations": ["Kim2026Beyond", "Kutasov2025Shade", "Liu2025Costbench", "Mohammadi2025Evaluation", "Xi2026Toolgym", "Yao2022React", "Zhang2025Security"]}
{"kind": "h2_lead", "id": "3", "title": "Foundations & Interfaces", "section_id": "3", "section_title": "Foundations & Interfaces", "path": "sections/S3_lead.md", "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "3e17e238ed642994489b2b637bfc4186f4f37c9a", "bytes": 1141, "citations": ["Kim2026Beyond", "Mohammadi2025Evaluation", "Schick2023Toolformer", "Yao2022React", "Zhang2025Security"]}
{"kind": "h2_lead", "id": "4", "title": "Core Components (Planning + Memory)", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "path": "sections/S4_lead.md", "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "56c7220369ed88aec1ce758be0d0aceb45fd7487", "bytes": 977, "citations": ["Liu2025Costbench", "Mohammadi2025Evaluation", "Mudur2025Feabench", "Yao2023Tree"]}
{"kind": "h2_lead", "id": "5", "title": "Learning, Adaptation & Coordination", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "path": "sections/S5_lead.md", "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "3bc6e875b9cab43c4a5222963991d4c0eeafbcb7", "bytes": 955, "citations": ["Chuang2025Debate", "Mohammadi2025Evaluation", "Papadakis2025Atlas", "Shinn2023Reflexion", "Wu2025Agents"]}
{"kind": "h2_lead", "id": "6", "title": "Evaluation & Risks", "section_id": "6", "section_title": "Evaluation & Risks", "path": "sections/S6_lead.md", "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "448cd9f394810556d33280a3370db6693238a13c", "bytes": 1082, "citations": ["Das2025Beyond", "Kim2026Beyond", "Kutasov2025Shade", "Lee2025Bench", "Li2025Stac", "Liu2025Costbench", "Mohammadi2025Evaluation", "Zhang2025Security"]}
{"kind": "h2", "id": "1", "title": "Introduction", "section_id": "1", "section_title": "Introduction", "path": "sections/S1.md", "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "5cbcdbaa89dce4935214018b1552c839c1a59f7f", "bytes": 3951, "citations": ["Kim2026Beyond", "Kutasov2025Shade", "Lee2025Bench", "Liu2025Costbench", "Liu2025Mcpagentbench", "Mohammadi2025Evaluation", "Mudur2025Feabench", "Plaat2025Agentic", "Schick2023Toolformer", "Shinn2023Reflexion", "Van2025Survey", "Xi2026Toolgym", "Yao2022React", "Zhang2025Security"]}
{"kind": "h2", "id": "2", "title": "Related Work", "section_id": "2", "section_title": "Related Work", "path": "sections/S2.md", "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "0e9f1647c036adc1e2bbe8cbd7bf56f22fddb868", "bytes": 3765, "citations": ["Das2025Beyond", "Kim2026Beyond", "Kutasov2025Shade", "Lee2025Bench", "Li2025Stac", "Liu2025Costbench", "Liu2025Mcpagentbench", "Luo2025Large", "Mohammadi2025Evaluation", "Mudur2025Feabench", "Plaat2025Agentic", "Sarkar2025Survey", "Schick2023Toolformer", "Shinn2023Reflexion", "Tang2025Agent", "Van2025Survey", "Xi2026Toolgym", "Yao2022React", "Yao2023Tree", "Yin2024Safeagentbench", "Zhang2025Security"]}
{"kind": "h3", "id": "3.1", "title": "Agent loop and action spaces", "section_id": "3", "section_title": "Foundations & Interfaces", "path": "sections/S3_1.md", "allowed_bibkeys_mapped": ["Feng2025Group", "Fumero2025Cybersleuth", "Gasmi2025Bridging", "Ghose2025Orfs", "Kim2025Bridging", "Li2025Agentswift", "Lin2026Froav", "Nusrat2025Automated", "Shang2024Agentsquare", "Shen2024Small", "Song2026Envscaler", "Sun2025Agent", "Wu2025Meta", "Xi2026Toolgym", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving", "Zheng2025Towards"], "allowed_bibkeys_selected": ["Feng2025Group", "Fumero2025Cybersleuth", "Ghose2025Orfs", "Kim2025Bridging", "Li2025Agentswift", "Lin2026Froav", "Shang2024Agentsquare", "Shen2024Small", "Song2026Envscaler", "Sun2025Agent", "Xi2026Toolgym", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving", "Zheng2025Towards"], "evidence_ids": ["E-P0092-60cc0d458f", "E-P0016-9d9d60644a", "E-P0027-c92ed293ba", "E-P0199-1063eee7ce", "E-P0014-904ba35500", "E-P0078-38a26e4777", "E-P0119-c8c4670812", "E-P0125-4b027dfb27", "E-P0156-e3f7ba21be", "E-P0001-ca4a00b5cf", "E-P0032-895b04aa5c", "E-P0140-1d5f67b08e", "E-P0032-8c9597d805", "E-P0027-9640816b42", "E-P0132-027093d5f5", "E-P0140-ddd045953e", "E-P0187-048b8b704f", "E-P0091-f1a25e82c1"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0078", "evidence_id": "E-P0078-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["Zhao2025Achieving"], "paper_id": "P0199", "evidence_id": "E-P0199-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[0]"}, {"hook_type": "quant", "text": "We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design.", "citations": ["Fumero2025Cybersleuth"], "paper_id": "P0119", "evidence_id": "E-P0119-c8c4670812", "pointer": "papers/paper_notes.jsonl:paper_id=P0119#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0125", "evidence_id": "E-P0125-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0125#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate.", "citations": ["Zheng2025Towards"], "paper_id": "P0156", "evidence_id": "E-P0156-e3f7ba21be", "pointer": "papers/paper_notes.jsonl:paper_id=P0156#key_results[0]"}], "allowed_bibkeys_chapter": ["Bulusu2024Mathviz", "Chen2025Agentguard", "Cheng2025Your", "Cui2025Toward", "Dong2025Etom", "Du2024Anytool", "Feng2025Group", "Fu2024Imprompter", "Fumero2025Cybersleuth", "Gasmi2025Bridging", "Ghose2025Orfs", "Hao2026From", "Jia2025Autotool", "Kim2025Bridging", "Li2025Agentswift", "Li2025Dissonances", "Li2026Toolprmbench", "Lin2026Froav", "Liu2025Mcpagentbench", "Lumer2025Memtool", "Mohammadi2025Evaluation", "Nusrat2025Automated", "Shang2024Agentsquare", "Shen2024Small", "Song2026Envscaler", "Sun2025Agent", "Wu2025Meta", "Xi2026Toolgym", "Xuan2026Confidence", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving", "Zheng2025Towards"], "allowed_bibkeys_global": ["Yao2022React"], "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "96b6f7ff291729243fcac91db442b45c920252f2", "bytes": 5230, "citations": ["Fumero2025Cybersleuth", "Ghose2025Orfs", "Kim2025Bridging", "Li2025Agentswift", "Shang2024Agentsquare", "Shen2024Small", "Song2026Envscaler", "Sun2025Agent", "Xi2026Toolgym", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving"]}
{"kind": "h3", "id": "3.2", "title": "Tool interfaces and orchestration", "section_id": "3", "section_title": "Foundations & Interfaces", "path": "sections/S3_2.md", "allowed_bibkeys_mapped": ["Bulusu2024Mathviz", "Chen2025Agentguard", "Cheng2025Your", "Cui2025Toward", "Dong2025Etom", "Du2024Anytool", "Fu2024Imprompter", "Ghose2025Orfs", "Hao2026From", "Jia2025Autotool", "Li2025Dissonances", "Li2026Toolprmbench", "Liu2025Mcpagentbench", "Lumer2025Memtool", "Mohammadi2025Evaluation", "Shen2024Small", "Xuan2026Confidence", "Yao2022React"], "allowed_bibkeys_selected": ["Bulusu2024Mathviz", "Dong2025Etom", "Du2024Anytool", "Ghose2025Orfs", "Hao2026From", "Li2025Dissonances", "Li2026Toolprmbench", "Liu2025Mcpagentbench", "Lumer2025Memtool", "Mohammadi2025Evaluation", "Shen2024Small", "Xuan2026Confidence", "Yao2022React"], "evidence_ids": ["E-P0047-37f9ea924c", "E-P0017-192e78b614", "E-P0056-f7a14123f9", "E-P0054-fae121f81b", "E-P0080-d5c234444e", "E-P0058-35271418ac", "E-P0001-ca4a00b5cf", "E-P0017-55ce44af76", "E-P0017-6ee6d5b951", "E-P0027-9640816b42", "E-P0171-b9de76d922", "E-P0033-3e2edc05cd", "E-P0056-3a4792de2b", "E-P0056-67d5c4342c", "E-P0093-ed4427964c", "E-P0098-c2fdc5ad72", "E-P0033-9861661010", "E-P0140-1d5f67b08e"], "anchor_facts": [{"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "paper_id": "P0054", "evidence_id": "E-P0054-fae121f81b", "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "eval", "text": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "paper_id": "P0017", "evidence_id": "E-P0017-192e78b614", "pointer": "papers/paper_notes.jsonl:paper_id=P0017#method"}, {"hook_type": "eval", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "paper_id": "P0056", "evidence_id": "E-P0056-f7a14123f9", "pointer": "papers/paper_notes.jsonl:paper_id=P0056#limitations[1]"}, {"hook_type": "quant", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "paper_id": "P0080", "evidence_id": "E-P0080-d5c234444e", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "eval", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "paper_id": "P0027", "evidence_id": "E-P0027-9640816b42", "pointer": "papers/paper_notes.jsonl:paper_id=P0027#key_results[1]"}], "allowed_bibkeys_chapter": ["Bulusu2024Mathviz", "Chen2025Agentguard", "Cheng2025Your", "Cui2025Toward", "Dong2025Etom", "Du2024Anytool", "Feng2025Group", "Fu2024Imprompter", "Fumero2025Cybersleuth", "Gasmi2025Bridging", "Ghose2025Orfs", "Hao2026From", "Jia2025Autotool", "Kim2025Bridging", "Li2025Agentswift", "Li2025Dissonances", "Li2026Toolprmbench", "Lin2026Froav", "Liu2025Mcpagentbench", "Lumer2025Memtool", "Mohammadi2025Evaluation", "Nusrat2025Automated", "Shang2024Agentsquare", "Shen2024Small", "Song2026Envscaler", "Sun2025Agent", "Wu2025Meta", "Xi2026Toolgym", "Xuan2026Confidence", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving", "Zheng2025Towards"], "allowed_bibkeys_global": ["Yao2022React"], "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "c0f03a46621af189deaf2fa8f4e18d6a1cc428fb", "bytes": 5382, "citations": ["Dong2025Etom", "Du2024Anytool", "Hao2026From", "Li2025Dissonances", "Li2026Toolprmbench", "Liu2025Mcpagentbench", "Lumer2025Memtool", "Mohammadi2025Evaluation", "Xuan2026Confidence"]}
{"kind": "h3", "id": "4.1", "title": "Planning and reasoning loops", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "path": "sections/S4_1.md", "allowed_bibkeys_mapped": ["Choi2025Reactree", "Hatalis2025Review", "Hong2025Planning", "Hu2025Training", "Ji2024Testing", "Khoee2025Gatelens", "Kim2025Bridging", "Kiruluta2025Novel", "Li2024Personal", "Nusrat2025Automated", "Seo2025Simuhome", "Silva2025Agents", "Wang2025Automated", "Yao2022React", "Yin2024Safeagentbench", "Zhou2025Reasoning", "Zhou2025Siraj", "Zhu2025Where"], "allowed_bibkeys_selected": ["Choi2025Reactree", "Hu2025Training", "Ji2024Testing", "Khoee2025Gatelens", "Kim2025Bridging", "Nusrat2025Automated", "Seo2025Simuhome", "Silva2025Agents", "Wang2025Automated", "Yao2022React", "Yin2024Safeagentbench", "Zhou2025Reasoning", "Zhou2025Siraj"], "evidence_ids": ["E-P0024-771620f84f", "E-P0087-076695cd77", "E-P0151-e2e3b7fa97", "E-P0151-469a70bb44", "E-P0001-ca4a00b5cf", "E-P0064-0b753b9422", "E-P0087-c2e8e2bb7f", "E-P0124-8afa74a630", "E-P0208-32aec6c669", "E-P0151-7b36039ae4", "E-P0176-c0a98eb625", "E-P0043-baa622fa7f", "E-P0021-e38b4bdff3", "E-P0043-b35b53de13", "E-P0124-6664c6cecd", "E-P0146-4bcafdb221", "E-P0016-04c60086db", "E-P0209-71f2629f1b"], "anchor_facts": [{"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"hook_type": "quant", "text": "Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories.", "citations": ["Zhou2025Siraj"], "paper_id": "P0064", "evidence_id": "E-P0064-0b753b9422", "pointer": "papers/paper_notes.jsonl:paper_id=P0064#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["Hu2025Training"], "paper_id": "P0024", "evidence_id": "E-P0024-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"hook_type": "eval", "text": "To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards.", "citations": ["Yin2024Safeagentbench"], "paper_id": "P0087", "evidence_id": "E-P0087-076695cd77", "pointer": "papers/paper_notes.jsonl:paper_id=P0087#method"}, {"hook_type": "quant", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "citations": ["Seo2025Simuhome"], "paper_id": "P0151", "evidence_id": "E-P0151-e2e3b7fa97", "pointer": "papers/paper_notes.jsonl:paper_id=P0151#limitations[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "SafeAgentBench includes: (1) an executable, diverse, and high-quality dataset of 750 tasks, rigorously curated to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17", "citations": ["Yin2024Safeagentbench"], "paper_id": "P0087", "evidence_id": "E-P0087-c2e8e2bb7f", "pointer": "papers/paper_notes.jsonl:paper_id=P0087#key_results[0]"}, {"hook_type": "quant", "text": "Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation.", "citations": ["Khoee2025Gatelens"], "paper_id": "P0124", "evidence_id": "E-P0124-8afa74a630", "pointer": "papers/paper_notes.jsonl:paper_id=P0124#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation shows that CHECKMATE outperforms the state-of-the-art system (Claude Code) in penetration capability, improving benchmark success rates by over 20%.", "citations": ["Wang2025Automated"], "paper_id": "P0208", "evidence_id": "E-P0208-32aec6c669", "pointer": "papers/paper_notes.jsonl:paper_id=P0208#key_results[0]"}, {"hook_type": "quant", "text": "We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities.", "citations": ["Seo2025Simuhome"], "paper_id": "P0151", "evidence_id": "E-P0151-7b36039ae4", "pointer": "papers/paper_notes.jsonl:paper_id=P0151#key_results[0]"}], "allowed_bibkeys_chapter": ["Anokhin2024Arigraph", "Chen2025Grounded", "Choi2025Reactree", "Hatalis2025Review", "Hong2025Planning", "Hu2025Training", "Huang2025Retrieval", "Ji2024Testing", "Khoee2025Gatelens", "Kim2025Bridging", "Kiruluta2025Novel", "Li2024Personal", "Li2025Agentswift", "Lin2026Froav", "Nusrat2025Automated", "Seo2025Simuhome", "Shi2025Progent", "Silva2025Agents", "Tawosi2025Meta", "Verma2026Active", "Wang2025Automated", "Wu2025Meta", "Xu2025Agentic", "Yao2022React", "Ye2025Task", "Ye2025Taska", "Yin2024Safeagentbench", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large", "Zhang2025Security", "Zhou2025Reasoning", "Zhou2025Siraj", "Zhu2025Where"], "allowed_bibkeys_global": ["Yao2022React"], "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "c3fa9a900e47d11f850459af429ceb85d7ce7b2a", "bytes": 5304, "citations": ["Choi2025Reactree", "Hu2025Training", "Ji2024Testing", "Khoee2025Gatelens", "Seo2025Simuhome", "Yao2022React", "Yin2024Safeagentbench", "Zhou2025Siraj"]}
{"kind": "h3", "id": "4.2", "title": "Memory and retrieval (RAG)", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "path": "sections/S4_2.md", "allowed_bibkeys_mapped": ["Anokhin2024Arigraph", "Chen2025Grounded", "Huang2025Retrieval", "Li2025Agentswift", "Lin2026Froav", "Shi2025Progent", "Tawosi2025Meta", "Verma2026Active", "Wu2025Meta", "Xu2025Agentic", "Yao2022React", "Ye2025Task", "Ye2025Taska", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large", "Zhang2025Security", "Zhu2025Where"], "allowed_bibkeys_selected": ["Chen2025Grounded", "Huang2025Retrieval", "Li2025Agentswift", "Lin2026Froav", "Shi2025Progent", "Tawosi2025Meta", "Verma2026Active", "Yao2022React", "Ye2025Task", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large", "Zhang2025Security"], "evidence_ids": ["E-P0019-f36b515991", "E-P0055-8bcb673a7d", "E-P0158-c9781caf3b", "E-P0055-d6095e10e9", "E-P0060-68db58914f", "E-P0158-4af0cf3c02", "E-P0001-ca4a00b5cf", "E-P0014-904ba35500", "E-P0055-7a6ec4daed", "E-P0184-9abcf1bf8a", "E-P0025-52fea1d199", "E-P0135-897bcc2f50", "E-P0185-f0f0faaada", "E-P0187-048b8b704f", "E-P0187-db4213c234", "E-P0051-af945eb2fa", "E-P0067-53536132a8", "E-P0135-79064ef6b3"], "anchor_facts": [{"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0014", "evidence_id": "E-P0014-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "quant", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "paper_id": "P0019", "evidence_id": "E-P0019-f36b515991", "pointer": "papers/paper_notes.jsonl:paper_id=P0019#key_results[1]"}, {"hook_type": "quant", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "paper_id": "P0184", "evidence_id": "E-P0184-9abcf1bf8a", "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}, {"hook_type": "quant", "text": "SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability.", "citations": ["Huang2025Retrieval"], "paper_id": "P0158", "evidence_id": "E-P0158-c9781caf3b", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#limitations[1]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "quant", "text": "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "citations": ["Huang2025Retrieval"], "paper_id": "P0158", "evidence_id": "E-P0158-4af0cf3c02", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[1]"}], "allowed_bibkeys_chapter": ["Anokhin2024Arigraph", "Chen2025Grounded", "Choi2025Reactree", "Hatalis2025Review", "Hong2025Planning", "Hu2025Training", "Huang2025Retrieval", "Ji2024Testing", "Khoee2025Gatelens", "Kim2025Bridging", "Kiruluta2025Novel", "Li2024Personal", "Li2025Agentswift", "Lin2026Froav", "Nusrat2025Automated", "Seo2025Simuhome", "Shi2025Progent", "Silva2025Agents", "Tawosi2025Meta", "Verma2026Active", "Wang2025Automated", "Wu2025Meta", "Xu2025Agentic", "Yao2022React", "Ye2025Task", "Ye2025Taska", "Yin2024Safeagentbench", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large", "Zhang2025Security", "Zhou2025Reasoning", "Zhou2025Siraj", "Zhu2025Where"], "allowed_bibkeys_global": ["Yao2022React"], "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "02516f446ba12e975593278689dce534a0b2de7c", "bytes": 5159, "citations": ["Chen2025Grounded", "Huang2025Retrieval", "Shi2025Progent", "Tawosi2025Meta", "Verma2026Active", "Yao2022React", "Yu2026Agentic", "Zhang2025Security"]}
{"kind": "h3", "id": "5.1", "title": "Self-improvement and adaptation", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "path": "sections/S5_1.md", "allowed_bibkeys_mapped": ["Belle2025Agents", "Chen2025Grounded", "Du2024Anytool", "Li2026Autonomous", "Nitin2025Faultline", "Sarukkai2025Context", "Shao2025Towards", "Tennant2024Moral", "Van2025Survey", "Wu2025Evolver", "Xi2025Agentprm", "Xia2025Sand", "Yao2022React", "Zhang2026Evoroute", "Zheng2025Towards", "Zhou2024Archer", "Zhou2024Star", "Zhou2025Self"], "allowed_bibkeys_selected": ["Du2024Anytool", "Li2026Autonomous", "Nitin2025Faultline", "Sarukkai2025Context", "Shao2025Towards", "Xia2025Sand", "Yao2022React", "Zhang2026Evoroute", "Zheng2025Towards", "Zhou2024Star", "Zhou2025Self"], "evidence_ids": ["E-P0022-2e6956a116", "E-P0028-67ea29ce26", "E-P0080-4da9e4ae32", "E-P0092-60cc0d458f", "E-P0175-2d7736fc46", "E-P0028-9980bf7642", "E-P0156-e3f7ba21be", "E-P0001-ca4a00b5cf", "E-P0080-d5c234444e", "E-P0130-cfef96abfa", "E-P0130-e7c6cee652", "E-P0157-f65d57a126", "E-P0028-499402e2fb", "E-P0049-74188ef933", "E-P0049-91a368737e", "E-P0062-6273763a98", "E-P0022-17d0e7f9d9", "E-P0175-fe3f0d32b8"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0022", "evidence_id": "E-P0022-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0022#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0092", "evidence_id": "E-P0092-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0092#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "citations": ["Li2026Autonomous"], "paper_id": "P0028", "evidence_id": "E-P0028-67ea29ce26", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#method"}, {"hook_type": "eval", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "paper_id": "P0080", "evidence_id": "E-P0080-4da9e4ae32", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#limitations[1]"}, {"hook_type": "quant", "text": "Optimized datasets have achieved substantial improvements, with an average increase of 12% and notable gains in specific metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset.", "citations": ["Zhou2024Star"], "paper_id": "P0175", "evidence_id": "E-P0175-2d7736fc46", "pointer": "papers/paper_notes.jsonl:paper_id=P0175#key_results[0]"}, {"hook_type": "quant", "text": "Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential.", "citations": ["Li2026Autonomous"], "paper_id": "P0028", "evidence_id": "E-P0028-9980bf7642", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}, {"hook_type": "quant", "text": "Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate.", "citations": ["Zheng2025Towards"], "paper_id": "P0156", "evidence_id": "E-P0156-e3f7ba21be", "pointer": "papers/paper_notes.jsonl:paper_id=P0156#key_results[0]"}, {"hook_type": "quant", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "paper_id": "P0080", "evidence_id": "E-P0080-d5c234444e", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}, {"hook_type": "quant", "text": "On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\\textbf{2$\\times$ cost reduction}$ at iso-accuracy.", "citations": ["Sarukkai2025Context"], "paper_id": "P0130", "evidence_id": "E-P0130-cfef96abfa", "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[1]"}], "allowed_bibkeys_chapter": ["Belle2025Agents", "Cao2025Skyrl", "Chang2025Alas", "Chen2025Grounded", "Chuang2025Debate", "Cui2025Toward", "Du2024Anytool", "Hao2025Multi", "Li2025Continuum", "Li2025What", "Li2026Autonomous", "Lichkovski2025Agent", "Lumer2025Memtool", "Nitin2025Faultline", "Papadakis2025Atlas", "Sarkar2025Survey", "Sarukkai2025Context", "Shao2025Craken", "Shao2025Towards", "Silva2025Agents", "Sun2025Agent", "Tennant2024Moral", "Van2025Survey", "Wang2023Voyager", "Wu2025Agents", "Wu2025Evolver", "Xi2025Agentprm", "Xia2025Sand", "Yao2022React", "Yim2024Evaluating", "Zhang2026Evoroute", "Zhao2025Achieving", "Zheng2025Towards", "Zhou2024Archer", "Zhou2024Star", "Zhou2025Self"], "allowed_bibkeys_global": ["Yao2022React"], "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "92d9afd5b09f114cbc1f2936de3fa0c367bef071", "bytes": 5244, "citations": ["Du2024Anytool", "Li2026Autonomous", "Nitin2025Faultline", "Yao2022React", "Zhang2026Evoroute", "Zheng2025Towards", "Zhou2024Star", "Zhou2025Self"]}
{"kind": "h3", "id": "5.2", "title": "Multi-agent coordination", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "path": "sections/S5_2.md", "allowed_bibkeys_mapped": ["Cao2025Skyrl", "Chang2025Alas", "Chuang2025Debate", "Cui2025Toward", "Hao2025Multi", "Li2025Continuum", "Li2025What", "Lichkovski2025Agent", "Lumer2025Memtool", "Papadakis2025Atlas", "Sarkar2025Survey", "Shao2025Craken", "Silva2025Agents", "Sun2025Agent", "Wang2023Voyager", "Wu2025Agents", "Yim2024Evaluating", "Zhao2025Achieving"], "allowed_bibkeys_selected": ["Cao2025Skyrl", "Chang2025Alas", "Cui2025Toward", "Li2025Continuum", "Lichkovski2025Agent", "Lumer2025Memtool", "Shao2025Craken", "Silva2025Agents", "Sun2025Agent", "Wu2025Agents", "Zhao2025Achieving"], "evidence_ids": ["E-P0199-1063eee7ce", "E-P0023-32b2c8cf19", "E-P0045-a256400826", "E-P0042-9d48d99db0", "E-P0218-a5937728f7", "E-P0058-35271418ac", "E-P0023-3af1ce8090", "E-P0043-baa622fa7f", "E-P0114-e31a1bbba7", "E-P0132-027093d5f5", "E-P0155-15e523063d", "E-P0043-b35b53de13", "E-P0155-171b93237a", "E-P0197-2430cc2982", "E-P0045-4e494f02c0", "E-P0045-696cef029c", "E-P0023-5ed988eb67", "E-P0132-dda37dfb29"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0058", "evidence_id": "E-P0058-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution.", "citations": ["Shao2025Craken"], "paper_id": "P0042", "evidence_id": "E-P0042-9d48d99db0", "pointer": "papers/paper_notes.jsonl:paper_id=P0042#key_results[1]"}, {"hook_type": "quant", "text": "Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning.", "citations": ["Cao2025Skyrl"], "paper_id": "P0023", "evidence_id": "E-P0023-3af1ce8090", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["Zhao2025Achieving"], "paper_id": "P0199", "evidence_id": "E-P0199-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[0]"}, {"hook_type": "eval", "text": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation.", "citations": ["Cao2025Skyrl"], "paper_id": "P0023", "evidence_id": "E-P0023-32b2c8cf19", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#method"}, {"hook_type": "quant", "text": "Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase.", "citations": ["Li2025Continuum"], "paper_id": "P0218", "evidence_id": "E-P0218-a5937728f7", "pointer": "papers/paper_notes.jsonl:paper_id=P0218#key_results[0]"}, {"hook_type": "limitation", "text": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "paper_id": "P0043", "evidence_id": "E-P0043-baa622fa7f", "pointer": "papers/paper_notes.jsonl:paper_id=P0043#key_results[1]"}, {"hook_type": "eval", "text": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "paper_id": "P0114", "evidence_id": "E-P0114-e31a1bbba7", "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"hook_type": "eval", "text": "Evaluating these systems is challenging, as their rapid evolution outpaces traditional human evaluation.", "citations": ["Sun2025Agent"], "paper_id": "P0132", "evidence_id": "E-P0132-027093d5f5", "pointer": "papers/paper_notes.jsonl:paper_id=P0132#key_results[1]"}], "allowed_bibkeys_chapter": ["Belle2025Agents", "Cao2025Skyrl", "Chang2025Alas", "Chen2025Grounded", "Chuang2025Debate", "Cui2025Toward", "Du2024Anytool", "Hao2025Multi", "Li2025Continuum", "Li2025What", "Li2026Autonomous", "Lichkovski2025Agent", "Lumer2025Memtool", "Nitin2025Faultline", "Papadakis2025Atlas", "Sarkar2025Survey", "Sarukkai2025Context", "Shao2025Craken", "Shao2025Towards", "Silva2025Agents", "Sun2025Agent", "Tennant2024Moral", "Van2025Survey", "Wang2023Voyager", "Wu2025Agents", "Wu2025Evolver", "Xi2025Agentprm", "Xia2025Sand", "Yao2022React", "Yim2024Evaluating", "Zhang2026Evoroute", "Zhao2025Achieving", "Zheng2025Towards", "Zhou2024Archer", "Zhou2024Star", "Zhou2025Self"], "allowed_bibkeys_global": ["Yao2022React"], "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "7be9ea83eda9a684254171a17dd338b476c8763e", "bytes": 5298, "citations": ["Cao2025Skyrl", "Li2025Continuum", "Lichkovski2025Agent", "Lumer2025Memtool", "Shao2025Craken", "Silva2025Agents", "Sun2025Agent", "Wu2025Agents"]}
{"kind": "h3", "id": "6.1", "title": "Benchmarks and evaluation protocols", "section_id": "6", "section_title": "Evaluation & Risks", "path": "sections/S6_1.md", "allowed_bibkeys_mapped": ["Chen2025Towards", "Dagan2024Plancraft", "Das2025Beyond", "Fu2025Eval", "Guo2025Cryptobench", "Ji2025Taxonomy", "Kim2026Beyond", "Liang2026Large", "Liu2025Secure", "Liu2026Agents", "Ma2023Large", "Mohammadi2025Evaluation", "Schick2023Toolformer", "Seo2025Simuhome", "Van2025Survey", "Wang2025Agentspec", "Zhan2025Sentinel", "Zhang2025Buildbench"], "allowed_bibkeys_selected": ["Chen2025Towards", "Das2025Beyond", "Fu2025Eval", "Guo2025Cryptobench", "Ji2025Taxonomy", "Kim2026Beyond", "Liu2026Agents", "Ma2023Large", "Mohammadi2025Evaluation", "Seo2025Simuhome", "Wang2025Agentspec", "Zhan2025Sentinel"], "evidence_ids": ["E-P0047-37f9ea924c", "E-P0069-7bac399c03", "E-P0151-e2e3b7fa97", "E-P0182-ad2b2ab52b", "E-P0030-8b56718f74", "E-P0069-4fc221fdea", "E-P0113-6a0e70c48e", "E-P0151-469a70bb44", "E-P0107-481ecd5602", "E-P0107-8e4049bab9", "E-P0144-753416ce70", "E-P0029-79f88927fa", "E-P0144-2895472ae1", "E-P0151-7b36039ae4", "E-P0219-031aeedf1f", "E-P0063-6a9a38705f", "E-P0068-e2d4798c18", "E-P0113-19bc1ede8a"], "anchor_facts": [{"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "quant", "text": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies", "citations": ["Liu2026Agents"], "paper_id": "P0030", "evidence_id": "E-P0030-8b56718f74", "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}, {"hook_type": "quant", "text": "This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan.", "citations": ["Chen2025Towards"], "paper_id": "P0069", "evidence_id": "E-P0069-4fc221fdea", "pointer": "papers/paper_notes.jsonl:paper_id=P0069#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0047", "evidence_id": "E-P0047-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0047#key_results[0]"}, {"hook_type": "eval", "text": "Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.", "citations": ["Chen2025Towards"], "paper_id": "P0069", "evidence_id": "E-P0069-7bac399c03", "pointer": "papers/paper_notes.jsonl:paper_id=P0069#method"}, {"hook_type": "quant", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "citations": ["Seo2025Simuhome"], "paper_id": "P0151", "evidence_id": "E-P0151-e2e3b7fa97", "pointer": "papers/paper_notes.jsonl:paper_id=P0151#limitations[1]"}, {"hook_type": "quant", "text": "Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and", "citations": ["Ma2023Large"], "paper_id": "P0182", "evidence_id": "E-P0182-ad2b2ab52b", "pointer": "papers/paper_notes.jsonl:paper_id=P0182#key_results[0]"}, {"hook_type": "quant", "text": "Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specif", "citations": ["Liu2026Agents"], "paper_id": "P0030", "evidence_id": "E-P0030-8b56718f74", "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}, {"hook_type": "quant", "text": "In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations.", "citations": ["Das2025Beyond"], "paper_id": "P0113", "evidence_id": "E-P0113-6a0e70c48e", "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identify 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.", "citations": ["Wang2025Agentspec"], "paper_id": "P0107", "evidence_id": "E-P0107-481ecd5602", "pointer": "papers/paper_notes.jsonl:paper_id=P0107#key_results[1]"}, {"hook_type": "quant", "text": "Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs).", "citations": ["Wang2025Agentspec"], "paper_id": "P0107", "evidence_id": "E-P0107-8e4049bab9", "pointer": "papers/paper_notes.jsonl:paper_id=P0107#key_results[0]"}], "allowed_bibkeys_chapter": ["An2025Ipiguard", "Bonagiri2025Check", "Chen2025Towards", "Dagan2024Plancraft", "Das2025Beyond", "Fang2025Should", "Fu2025Eval", "Gasmi2025Bridging", "Guo2025Cryptobench", "Hadeliya2025When", "Ji2025Taxonomy", "Kamath2025Enforcing", "Kim2024When", "Kim2026Beyond", "Li2024Personal", "Liang2026Large", "Liu2025Secure", "Liu2026Agents", "Luo2025Agrail", "Ma2023Large", "Mohammadi2025Evaluation", "Schick2023Toolformer", "Seo2025Simuhome", "Sha2025Agent", "Shao2025Towards", "Shi2025Progent", "Van2025Survey", "Wang2023Survey", "Wang2025Adversarial", "Wang2025Agentspec", "Yuan2024Judge", "Zhan2025Sentinel", "Zhang2024Agent", "Zhang2025Buildbench", "Zhang2025Security"], "allowed_bibkeys_global": ["Yao2022React"], "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "f5a57ada2b7485464f96bf9001d32a67e1bd90b1", "bytes": 5223, "citations": ["Chen2025Towards", "Das2025Beyond", "Fu2025Eval", "Ji2025Taxonomy", "Kim2026Beyond", "Liu2026Agents", "Mohammadi2025Evaluation", "Seo2025Simuhome", "Wang2025Agentspec", "Zhan2025Sentinel"]}
{"kind": "h3", "id": "6.2", "title": "Safety, security, and governance", "section_id": "6", "section_title": "Evaluation & Risks", "path": "sections/S6_2.md", "allowed_bibkeys_mapped": ["An2025Ipiguard", "Bonagiri2025Check", "Fang2025Should", "Fu2025Eval", "Gasmi2025Bridging", "Hadeliya2025When", "Kamath2025Enforcing", "Kim2024When", "Li2024Personal", "Luo2025Agrail", "Sha2025Agent", "Shao2025Towards", "Shi2025Progent", "Wang2023Survey", "Wang2025Adversarial", "Yuan2024Judge", "Zhang2024Agent", "Zhang2025Security"], "allowed_bibkeys_selected": ["Bonagiri2025Check", "Fu2025Eval", "Gasmi2025Bridging", "Kamath2025Enforcing", "Kim2024When", "Shao2025Towards", "Shi2025Progent", "Wang2025Adversarial", "Yuan2024Judge", "Zhang2024Agent", "Zhang2025Security"], "evidence_ids": ["E-P0055-d6095e10e9", "E-P0055-8bcb673a7d", "E-P0181-03f4e0cdc0", "E-P0060-68db58914f", "E-P0144-753416ce70", "E-P0055-7a6ec4daed", "E-P0144-2895472ae1", "E-P0200-8512d7eecf", "E-P0200-ee6f97d5e5", "E-P0076-51b52c66a1", "E-P0076-e26328e18c", "E-P0174-c9f8ee0e0e", "E-P0013-7edb91824f", "E-P0157-f65d57a126", "E-P0181-37b9602feb", "E-P0120-2718dbc45e", "E-P0041-8a2c2e2291", "E-P0041-8e34a29629"], "anchor_facts": [{"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0060", "evidence_id": "E-P0060-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0060#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[1]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0055", "evidence_id": "E-P0055-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0144", "evidence_id": "E-P0144-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "paper_id": "P0144", "evidence_id": "E-P0144-2895472ae1", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}, {"hook_type": "quant", "text": "Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions.", "citations": ["Zhang2024Agent"], "paper_id": "P0076", "evidence_id": "E-P0076-51b52c66a1", "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[1]"}], "allowed_bibkeys_chapter": ["An2025Ipiguard", "Bonagiri2025Check", "Chen2025Towards", "Dagan2024Plancraft", "Das2025Beyond", "Fang2025Should", "Fu2025Eval", "Gasmi2025Bridging", "Guo2025Cryptobench", "Hadeliya2025When", "Ji2025Taxonomy", "Kamath2025Enforcing", "Kim2024When", "Kim2026Beyond", "Li2024Personal", "Liang2026Large", "Liu2025Secure", "Liu2026Agents", "Luo2025Agrail", "Ma2023Large", "Mohammadi2025Evaluation", "Schick2023Toolformer", "Seo2025Simuhome", "Sha2025Agent", "Shao2025Towards", "Shi2025Progent", "Van2025Survey", "Wang2023Survey", "Wang2025Adversarial", "Wang2025Agentspec", "Yuan2024Judge", "Zhan2025Sentinel", "Zhang2024Agent", "Zhang2025Buildbench", "Zhang2025Security"], "allowed_bibkeys_global": ["Yao2022React"], "generated_at": "2026-01-22T10:24:36", "exists": true, "sha1": "60621f9b5f8acbafbee5542b5700bf305e7fd382", "bytes": 5338, "citations": ["Bonagiri2025Check", "Fu2025Eval", "Gasmi2025Bridging", "Kamath2025Enforcing", "Kim2024When", "Shi2025Progent", "Wang2025Adversarial", "Yuan2024Judge", "Zhang2024Agent", "Zhang2025Security"]}
