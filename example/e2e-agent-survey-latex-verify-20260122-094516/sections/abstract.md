## Abstract

Large language model (LLM) agents embed an LLM in a closed-loop system that observes, decides, and acts through tools or environments. In this setting, interface contracts and evaluation protocols are part of the object of study: changing tool access, budgets, or logging policies can change what a reported success rate actually means. We survey recent agentic systems with an evidence-first lens, organizing the design space from interfaces and action spaces, through planning and memory, to adaptation and multi-agent coordination, and finally evaluation and risk. Across these lenses, we highlight protocol-sensitive comparisons (what transfers across benchmarks, and what does not) and summarize recurring failure modes that emerge at the system boundary. The result is a practical map for designing and evaluating agents, and a research agenda for making agent results more interpretable and reproducible [@Yao2022React; @Schick2023Toolformer; @Shinn2023Reflexion; @Liu2025Costbench; @Mohammadi2025Evaluation; @Zhang2025Security].
