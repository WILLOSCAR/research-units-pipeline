Evaluation is the binding constraint that makes the rest of the taxonomy meaningful: without matched protocols, it is unclear whether two agents differ in decision quality, tool access, budget, or interface cleanliness. Recent benchmarks emphasize this by varying API realism and budget assumptions, showing that protocol choices can dominate reported performance and that reporting conventions are part of reproducible progress [@Mohammadi2025Evaluation; @Kim2026Beyond; @Liu2025Costbench].

Those same protocols also define risk surfaces. Tool use introduces new attack channels and monitoring requirements, and security-focused benchmarks operationalize failures such as tool-chain jailbreaks, software-task exploitation, and privacy leakage as end-to-end system properties [@Zhang2025Security; @Li2025Stac; @Lee2025Bench; @Kutasov2025Shade; @Das2025Beyond]. The chapter therefore first clarifies what should be measured and reported for interpretable agent comparisons, then synthesizes how threat models and governance constraints change what it means to deploy agents safely.
