LLM agents are increasingly deployed as interactive systems that must decide what to do next, call tools, recover from failures, and complete multi-step tasks under latency and cost constraints. The resulting behaviors are not determined by the base model alone: the surrounding loop, the tool/environment boundary, and the evaluation protocol jointly shape what “capable” means in practice [@Plaat2025Agentic; @Van2025Survey; @Kim2026Beyond].

We adopt a systems definition that makes this dependence explicit. An LLM agent is an LLM embedded in a control loop that (i) maintains an internal state, (ii) selects actions from a defined action space (tool calls, code edits, web interactions, environment actions), and (iii) receives observations that condition subsequent decisions. This definition separates agents from adjacent settings such as one-off tool invocation or retrieval-only augmentation, while aligning with the dominant “reason + act” lineage in which reasoning is coupled to grounded actions [@Yao2022React; @Schick2023Toolformer; @Shinn2023Reflexion].

A recurring difficulty is that agent results are protocol-sensitive. Cost-aware studies show that planning depth, retries, and verification policies can move an agent along a sharp success--cost frontier, so headline success rates are hard to compare unless budgets are normalized [@Liu2025Costbench; @Mudur2025Feabench]. Similarly, evaluations that approximate realistic tool ecosystems highlight that API complexity, tool noise, and permission constraints can dominate outcomes, which makes “progress” inseparable from benchmark design and environment versioning [@Kim2026Beyond; @Xi2026Toolgym; @Liu2025Mcpagentbench].

This survey therefore uses two lenses as first-class citizens: interface contracts and evaluation/threat-model assumptions. We organize the design space from foundations and interfaces (agent-loop semantics, action spaces, and tool orchestration) to core components (planning loops and memory), then to adaptation and coordination (self-improvement and multi-agent interaction), and finally to evaluation and risk. The goal is not only to summarize mechanisms, but to make comparisons interpretable by pairing claims with the protocols and failure modes that delimit them [@Mohammadi2025Evaluation; @Kim2026Beyond; @Zhang2025Security].

Survey methodology (one-paragraph evidence policy): we queried arXiv from 2022 onward using agent-related keywords, collecting 809 candidate records and deduplicating to 800. From this pool, we curated a 220-paper core set to construct the taxonomy and the evidence base used throughout. Unless explicitly stated otherwise, our synthesis relies primarily on titles, abstracts, and arXiv metadata; when evaluation details (budgets, tool access, logging policies, threat models) are under-specified, we keep claims conservative and treat protocol descriptions as key evidence for comparison [@Mohammadi2025Evaluation; @Kim2026Beyond].

Our contributions are threefold. First, we provide a taxonomy that treats the agent loop and its interfaces as the unit of analysis, making explicit how action spaces and orchestration policies constrain reliability [@Yao2022React; @Schick2023Toolformer]. Second, we distill protocol-aware comparison axes across planning, memory, and adaptation, emphasizing how cost models and environment assumptions affect conclusions [@Liu2025Costbench; @Xi2026Toolgym]. Third, we integrate risk surfaces into the same evaluative frame, including tool-induced security failures and monitoring challenges that arise only in closed-loop deployments [@Zhang2025Security; @Lee2025Bench; @Kutasov2025Shade].

The remainder of the paper follows this lens order: interfaces and action spaces (Section 3), core components for long-horizon behavior (Section 4), adaptation and coordination (Section 5), and evaluation and risk (Section 6), followed by a cross-cutting discussion and conclusion.
