{"sub_id": "3.1", "title": "Agent loop and action spaces", "anchors": [{"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["@Li2025Agentswift"], "paper_id": "P0019", "evidence_id": "E-P0019-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0019#key_results[0]"}, {"hook_type": "quant", "text": "Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents.", "citations": ["@Xi2026Toolgym"], "paper_id": "P0059", "evidence_id": "E-P0059-8c9597d805", "pointer": "papers/paper_notes.jsonl:paper_id=P0059#key_results[1]"}, {"hook_type": "quant", "text": "Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness.", "citations": ["@Xi2026Toolgym"], "paper_id": "P0059", "evidence_id": "E-P0059-895b04aa5c", "pointer": "papers/paper_notes.jsonl:paper_id=P0059#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["@Feng2025Group"], "paper_id": "P0024", "evidence_id": "E-P0024-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["@Zhao2025Achieving"], "paper_id": "P0138", "evidence_id": "E-P0138-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0138#key_results[0]"}, {"hook_type": "eval", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["@Liu2025Mcpagentbench"], "paper_id": "P0090", "evidence_id": "E-P0090-f7a14123f9", "pointer": "papers/paper_notes.jsonl:paper_id=P0090#limitations[1]"}, {"hook_type": "quant", "text": "Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions.", "citations": ["@Qiu2025Locobench"], "paper_id": "P0182", "evidence_id": "E-P0182-7aa3167337", "pointer": "papers/paper_notes.jsonl:paper_id=P0182#key_results[1]"}, {"hook_type": "quant", "text": "We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design.", "citations": ["@Fumero2025Cybersleuth"], "paper_id": "P0156", "evidence_id": "E-P0156-c8c4670812", "pointer": "papers/paper_notes.jsonl:paper_id=P0156#key_results[0]"}, {"hook_type": "quant", "text": "Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces", "citations": ["@Qiu2025Locobench"], "paper_id": "P0182", "evidence_id": "E-P0182-fa8206acb2", "pointer": "papers/paper_notes.jsonl:paper_id=P0182#key_results[0]"}], "generated_at": "2026-01-18T01:21:06"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "anchors": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["@Lumer2025Memtool"], "paper_id": "P0029", "evidence_id": "E-P0029-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["@Li2025Dissonances"], "paper_id": "P0088", "evidence_id": "E-P0088-fae121f81b", "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"hook_type": "quant", "text": "MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control.", "citations": ["@Lumer2025Memtool"], "paper_id": "P0029", "evidence_id": "E-P0029-38dc800de9", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[1]"}, {"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["@Zhou2025Self"], "paper_id": "P0035", "evidence_id": "E-P0035-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0035#key_results[0]"}, {"hook_type": "quant", "text": "To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing tool", "citations": ["@Liu2025Toolscope"], "paper_id": "P0220", "evidence_id": "E-P0220-32b9aafe57", "pointer": "papers/paper_notes.jsonl:paper_id=P0220#method"}, {"hook_type": "quant", "text": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "citations": ["@Liu2025Toolscope"], "paper_id": "P0220", "evidence_id": "E-P0220-468a77ff1d", "pointer": "papers/paper_notes.jsonl:paper_id=P0220#key_results[0]"}, {"hook_type": "quant", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["@Du2024Anytool"], "paper_id": "P0108", "evidence_id": "E-P0108-d5c234444e", "pointer": "papers/paper_notes.jsonl:paper_id=P0108#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["@Mohammadi2025Evaluation"], "paper_id": "P0164", "evidence_id": "E-P0164-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0164#key_results[0]"}], "generated_at": "2026-01-18T01:21:06"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "anchors": [{"hook_type": "quant", "text": "Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and", "citations": ["@Liu2025Costbench"], "paper_id": "P0077", "evidence_id": "E-P0077-32d61c8fae", "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["@Hu2025Training"], "paper_id": "P0039", "evidence_id": "E-P0039-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0039#key_results[0]"}, {"hook_type": "quant", "text": "Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design", "citations": ["@Kim2025Bridging"], "paper_id": "P0011", "evidence_id": "E-P0011-04c60086db", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}, {"hook_type": "quant", "text": "Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries.", "citations": ["@Nakano2025Guided"], "paper_id": "P0084", "evidence_id": "E-P0084-99359acdd7", "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "citations": ["@Seo2025Simuhome"], "paper_id": "P0212", "evidence_id": "E-P0212-e2e3b7fa97", "pointer": "papers/paper_notes.jsonl:paper_id=P0212#limitations[1]"}, {"hook_type": "quant", "text": "Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design p", "citations": ["@Kim2025Bridging"], "paper_id": "P0011", "evidence_id": "E-P0011-04c60086db", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}, {"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap track", "citations": ["@Du2025Memr"], "paper_id": "P0184", "evidence_id": "E-P0184-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}, {"hook_type": "quant", "text": "It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters.", "citations": ["@Zhou2025Reasoning"], "paper_id": "P0034", "evidence_id": "E-P0034-c17bcfb7d4", "pointer": "papers/paper_notes.jsonl:paper_id=P0034#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["@Hu2025Training"], "paper_id": "P0039", "evidence_id": "E-P0039-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0039#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and p", "citations": ["@Liu2025Costbench"], "paper_id": "P0077", "evidence_id": "E-P0077-32d61c8fae", "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["@Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "generated_at": "2026-01-18T01:21:06"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "anchors": [{"hook_type": "quant", "text": "We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets.", "citations": ["@Wei2025Memory"], "paper_id": "P0080", "evidence_id": "E-P0080-06e45507a7", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}, {"hook_type": "quant", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["@Tawosi2025Meta"], "paper_id": "P0031", "evidence_id": "E-P0031-f36b515991", "pointer": "papers/paper_notes.jsonl:paper_id=P0031#key_results[1]"}, {"hook_type": "quant", "text": "Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,", "citations": ["@Ye2025Task"], "paper_id": "P0014", "evidence_id": "E-P0014-3bf4dab38c", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap track", "citations": ["@Du2025Memr"], "paper_id": "P0184", "evidence_id": "E-P0184-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}, {"hook_type": "limitation", "text": "In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution,", "citations": ["@Wei2025Memory"], "paper_id": "P0080", "evidence_id": "E-P0080-f1ec9700e7", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#limitations[1]"}, {"hook_type": "quant", "text": "Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend", "citations": ["@Du2025Memr"], "paper_id": "P0184", "evidence_id": "E-P0184-497b158080", "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[0]"}, {"hook_type": "quant", "text": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["@Abbineni2025Muallm"], "paper_id": "P0187", "evidence_id": "E-P0187-e294aeefb5", "pointer": "papers/paper_notes.jsonl:paper_id=P0187#key_results[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["@Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperf", "citations": ["@Ye2025Task"], "paper_id": "P0014", "evidence_id": "E-P0014-3bf4dab38c", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "quant", "text": "Meta-RAG scores 84.67 % and 53.0 % for file-level and function-level correct localization rates, respectively, achieving state-of-the-art performance.", "citations": ["@Tawosi2025Meta"], "paper_id": "P0031", "evidence_id": "E-P0031-e3ebb83eb2", "pointer": "papers/paper_notes.jsonl:paper_id=P0031#key_results[0]"}], "generated_at": "2026-01-18T01:21:06"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "anchors": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["@Zhou2025Self"], "paper_id": "P0035", "evidence_id": "E-P0035-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0035#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["@Zhang2026Evoroute"], "paper_id": "P0055", "evidence_id": "E-P0055-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "quant", "text": "This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively", "citations": ["@Wu2025Evolver"], "paper_id": "P0081", "evidence_id": "E-P0081-0c10233369", "pointer": "papers/paper_notes.jsonl:paper_id=P0081#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["@Zhao2025Achieving"], "paper_id": "P0138", "evidence_id": "E-P0138-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0138#key_results[0]"}, {"hook_type": "limitation", "text": "We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion.", "citations": ["@Van2025Survey"], "paper_id": "P0061", "evidence_id": "E-P0061-7acf4de689", "pointer": "papers/paper_notes.jsonl:paper_id=P0061#limitations[1]"}, {"hook_type": "quant", "text": "Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-l", "citations": ["@Zhao2025Achieving"], "paper_id": "P0138", "evidence_id": "E-P0138-5f246d0ca8", "pointer": "papers/paper_notes.jsonl:paper_id=P0138#key_results[1]"}, {"hook_type": "quant", "text": "This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively re", "citations": ["@Wu2025Evolver"], "paper_id": "P0081", "evidence_id": "E-P0081-0c10233369", "pointer": "papers/paper_notes.jsonl:paper_id=P0081#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["@Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "In controlled Catanatron experiments, HexMachina learns from scratch and evolves players that outperform the strongest human-crafted baseline (AlphaBeta), achieving a 54% win rate and surpassing prompt-driven and no-discovery baselines.", "citations": ["@Belle2025Agents"], "paper_id": "P0020", "evidence_id": "E-P0020-7929ef7a56", "pointer": "papers/paper_notes.jsonl:paper_id=P0020#key_results[0]"}, {"hook_type": "quant", "text": "Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).", "citations": ["@Zhou2024Archer"], "paper_id": "P0044", "evidence_id": "E-P0044-faa3d4c9ee", "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}], "generated_at": "2026-01-18T01:21:06"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "anchors": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["@Lumer2025Memtool"], "paper_id": "P0029", "evidence_id": "E-P0029-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]"}, {"hook_type": "quant", "text": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and", "citations": ["@Cao2025Skyrl"], "paper_id": "P0036", "evidence_id": "E-P0036-5ed988eb67", "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[1]"}, {"hook_type": "quant", "text": "DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions.", "citations": ["@Chuang2025Debate"], "paper_id": "P0157", "evidence_id": "E-P0157-3acffd03b4", "pointer": "papers/paper_notes.jsonl:paper_id=P0157#key_results[0]"}, {"hook_type": "quant", "text": "MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control.", "citations": ["@Lumer2025Memtool"], "paper_id": "P0029", "evidence_id": "E-P0029-38dc800de9", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[1]"}, {"hook_type": "quant", "text": "Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning.", "citations": ["@Cao2025Skyrl"], "paper_id": "P0036", "evidence_id": "E-P0036-3af1ce8090", "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]"}, {"hook_type": "eval", "text": "This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP).", "citations": ["@Sarkar2025Survey"], "paper_id": "P0037", "evidence_id": "E-P0037-2c281ce5fb", "pointer": "papers/paper_notes.jsonl:paper_id=P0037#method"}, {"hook_type": "quant", "text": "Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, ex", "citations": ["@Wang2023Voyager"], "paper_id": "P0005", "evidence_id": "E-P0005-506120a6cd", "pointer": "papers/paper_notes.jsonl:paper_id=P0005#key_results[1]"}, {"hook_type": "quant", "text": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve", "citations": ["@Cao2025Skyrl"], "paper_id": "P0036", "evidence_id": "E-P0036-5ed988eb67", "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[1]"}, {"hook_type": "quant", "text": "It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA.", "citations": ["@Wang2023Voyager"], "paper_id": "P0005", "evidence_id": "E-P0005-b2eceb5b30", "pointer": "papers/paper_notes.jsonl:paper_id=P0005#key_results[0]"}, {"hook_type": "quant", "text": "Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).", "citations": ["@Zhou2024Archer"], "paper_id": "P0044", "evidence_id": "E-P0044-faa3d4c9ee", "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}], "generated_at": "2026-01-18T01:21:06"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "anchors": [{"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["@Fu2025Eval"], "paper_id": "P0199", "evidence_id": "E-P0199-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[1]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["@Mohammadi2025Evaluation"], "paper_id": "P0164", "evidence_id": "E-P0164-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0164#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["@Shi2025Progent"], "paper_id": "P0032", "evidence_id": "E-P0032-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#method"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["@Mohammadi2025Evaluation"], "paper_id": "P0164", "evidence_id": "E-P0164-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0164#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["@Shang2024Agentsquare"], "paper_id": "P0043", "evidence_id": "E-P0043-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0043#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[1]"}, {"hook_type": "quant", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Ar", "citations": ["@Kale2025Reliable"], "paper_id": "P0203", "evidence_id": "E-P0203-e0345118bc", "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[0]"}, {"hook_type": "quant", "text": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["@Kim2026Beyond"], "paper_id": "P0054", "evidence_id": "E-P0054-79f88927fa", "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}], "generated_at": "2026-01-18T01:21:06"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "anchors": [{"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["@Shi2025Progent"], "paper_id": "P0032", "evidence_id": "E-P0032-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[1]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#method"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Ar", "citations": ["@Kale2025Reliable"], "paper_id": "P0203", "evidence_id": "E-P0203-e0345118bc", "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[0]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["@Fu2025Eval"], "paper_id": "P0199", "evidence_id": "E-P0199-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[1]"}, {"hook_type": "quant", "text": "A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset.", "citations": ["@Lee2025Bench"], "paper_id": "P0205", "evidence_id": "E-P0205-32438dad15", "pointer": "papers/paper_notes.jsonl:paper_id=P0205#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases.", "citations": ["@Li2025Stac"], "paper_id": "P0209", "evidence_id": "E-P0209-82fdf6d15e", "pointer": "papers/paper_notes.jsonl:paper_id=P0209#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["@Fu2025Eval"], "paper_id": "P0199", "evidence_id": "E-P0199-2895472ae1", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[0]"}], "generated_at": "2026-01-18T01:21:06"}
