# Claim–Evidence matrix

This artifact is bullets-only and is meant to make evidence explicit before writing.

Generated as a projection of `outline/evidence_drafts.jsonl` (evidence packs).

## 3.1 Agent loop and action spaces

- RQ: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?
- Claim: We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM).
  - Axes: mechanism / architecture; data / training setup; evaluation protocol (datasets / metrics / human); efficiency / compute; failure modes / limitations
  - Evidence levels: fulltext=0, abstract=18, title=0.
  - Evidence: `P0011` [@Kim2025Bridging] — We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0011#method)
  - Evidence: `P0138` [@Zhao2025Achieving] — However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0138#key_results[0])
  - Evidence: `P0090` [@Liu2025Mcpagentbench] — To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0090#limitations[1])
  - Evidence: `P0182` [@Qiu2025Locobench] — Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0182#key_results[1])
  - Evidence: `P0156` [@Fumero2025Cybersleuth] — We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0156#key_results[0])
  - Evidence: `P0182` [@Qiu2025Locobench] — Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0182#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 3.2 Tool interfaces and orchestration

- RQ: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?
- Claim: To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select o
  - Axes: tool interface (function calling, schemas, protocols); tool selection / routing policy; sandboxing / permissions / observability; mechanism / architecture; data / training setup
  - Evidence levels: fulltext=0, abstract=18, title=0.
  - Evidence: `P0220` [@Liu2025Toolscope] — To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing toolsets to fit within context limits without sacrificing accuracy. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0220#method)
  - Evidence: `P0035` [@Zhou2025Self] — Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0035#key_results[0])
  - Evidence: `P0028` [@Dong2025Bench] — MSC-Bench provides a diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using agents. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0028#limitations[1])
  - Evidence: `P0088` [@Li2025Dissonances] — Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - Evidence: `P0220` [@Liu2025Toolscope] — Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0220#key_results[0])
  - Evidence: `P0220` [@Liu2025Toolscope] — To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing toolsets to fit within context limits without sacrificing accuracy. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0220#key_results[1])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 4.1 Planning and reasoning loops

- RQ: Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?
- Claim: Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications.
  - Axes: control loop design (planner / executor, search); deliberation method (CoT / ToT / MCTS); action grounding (tool calls vs environment actions); mechanism / architecture; data / training setup
  - Evidence levels: fulltext=0, abstract=18, title=0.
  - Evidence: `P0039` [@Hu2025Training] — Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0039#method)
  - Evidence: `P0084` [@Nakano2025Guided] — Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and required 86.2\%, 118.7\%, and 205.9\% more model queries. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0084#key_results[0])
  - Evidence: `P0212` [@Seo2025Simuhome] — Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0212#limitations[1])
  - Evidence: `P0013` [@Hong2025Planning] — We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - Evidence: `P0011` [@Kim2025Bridging] — Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
  - Evidence: `P0184` [@Du2025Memr] — From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0184#key_results[1])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 4.2 Memory and retrieval (RAG)

- RQ: Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?
- Claim: In our study, we introduce AriGraph, a novel method wherein the agent constructs and updates a memory graph that integrates semantic and episodic memories while exploring the environment.
  - Axes: memory type (episodic / semantic / scratchpad); retrieval source + index (docs / web / logs); write / update / forgetting policy; mechanism / architecture; data / training setup
  - Evidence levels: fulltext=0, abstract=18, title=0.
  - Evidence: `P0045` [@Anokhin2024Arigraph] — In our study, we introduce AriGraph, a novel method wherein the agent constructs and updates a memory graph that integrates semantic and episodic memories while exploring the environment. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0045#method)
  - Evidence: `P0184` [@Du2025Memr] — From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0184#key_results[1])
  - Evidence: `P0080` [@Wei2025Memory] — In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0080#limitations[1])
  - Evidence: `P0031` [@Tawosi2025Meta] — Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\%, into a compact, structured, natural language representation. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0031#key_results[1])
  - Evidence: `P0184` [@Du2025Memr] — Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0184#key_results[0])
  - Evidence: `P0187` [@Abbineni2025Muallm] — To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0187#key_results[1])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 5.1 Self-improvement and adaptation

- RQ: Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?
- Claim: In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself.
  - Axes: training signal (SFT / preference / RL); data synthesis + evaluator / reward; generalization + regression control; mechanism / architecture; data / training setup
  - Evidence levels: fulltext=0, abstract=18, title=0.
  - Evidence: `P0035` [@Zhou2025Self] — In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0035#method)
  - Evidence: `P0138` [@Zhao2025Achieving] — However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0138#key_results[0])
  - Evidence: `P0061` [@Van2025Survey] — We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0061#limitations[1])
  - Evidence: `P0035` [@Zhou2025Self] — Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0035#key_results[0])
  - Evidence: `P0138` [@Zhao2025Achieving] — Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0138#key_results[1])
  - Evidence: `P0081` [@Wu2025Evolver] — This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively retrieves distilled principles to guide its decision-making, accumulating a diverse set of behavioral trajectories. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0081#key_results[0])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 5.2 Multi-agent coordination

- RQ: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?
- Claim: This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Con
  - Axes: communication protocol + role assignment; aggregation (vote / debate / referee); stability (collusion, mode collapse, incentives); mechanism / architecture; data / training setup
  - Evidence levels: fulltext=0, abstract=18, title=0.
  - Evidence: `P0037` [@Sarkar2025Survey] — This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP). (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0037#method)
  - Evidence: `P0005` [@Wang2023Voyager] — Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0005#key_results[1])
  - Evidence: `P0050` [@Shen2024Small] — While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0050#limitations[1])
  - Evidence: `P0029` [@Lumer2025Memtool] — Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
  - Evidence: `P0029` [@Lumer2025Memtool] — MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0029#key_results[1])
  - Evidence: `P0036` [@Cao2025Skyrl] — We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0036#key_results[1])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 6.1 Benchmarks and evaluation protocols

- RQ: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?
- Claim: We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invoc
  - Axes: tool interface (function calling, schemas, protocols); tool selection / routing policy; sandboxing / permissions / observability; task suites (web / code / embodied / tools); metrics (success, cost, reliability, safety)
  - Evidence levels: fulltext=0, abstract=18, title=0.
  - Evidence: `P0089` [@Zhang2025Security] — We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#method)
  - Evidence: `P0164` [@Mohammadi2025Evaluation] — This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0164#key_results[0])
  - Evidence: `P0032` [@Shi2025Progent] — Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0032#limitations[1])
  - Evidence: `P0043` [@Shang2024Agentsquare] — Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0043#key_results[0])
  - Evidence: `P0089` [@Zhang2025Security] — MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - Evidence: `P0089` [@Zhang2025Security] — We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[1])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.

## 6.2 Safety, security, and governance

- RQ: Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?
- Claim: We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invoc
  - Axes: threat model (prompt / tool injection, exfiltration); defense surface (policy, sandbox, monitoring); security evaluation protocol; mechanism / architecture; data / training setup
  - Evidence levels: fulltext=0, abstract=18, title=0.
  - Evidence: `P0089` [@Zhang2025Security] — We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#method)
  - Evidence: `P0089` [@Zhang2025Security] — MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - Evidence: `P0161` [@Lichkovski2025Agent] — We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0161#limitations[1])
  - Evidence: `P0203` [@Kale2025Reliable] — To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0203#key_results[0])
  - Evidence: `P0199` [@Fu2025Eval] — RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0199#key_results[1])
  - Evidence: `P0089` [@Zhang2025Security] — We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[1])
  - Caveat: Evidence is not full-text grounded for this subsection; treat claims as provisional and avoid strong generalizations.
