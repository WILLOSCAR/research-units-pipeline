# Evidence draft: 6.2 Safety, security, and governance

## Evidence snippets (with provenance)
- (E-P0089-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#method)
- (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). @Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- (E-P0161-a256400826) We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. @Lichkovski2025Agent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0161#limitations[1])
- (E-P0203-e0345118bc) To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. @Kale2025Reliable (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0203#key_results[0])
- (E-P0199-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. @Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0199#key_results[1])
- (E-P0089-7a6ec4daed) We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. @Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[1])
- (E-P0032-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. @Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
- (E-P0205-32438dad15) A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset. @Lee2025Bench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0205#key_results[0])
- (E-P0209-82fdf6d15e) Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases. @Li2025Stac (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0209#key_results[0])
- (E-P0199-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. @Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0199#key_results[0])

## Definitions / setup

- Setup: Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Safety, security, and governance'.. Axes: threat model (prompt / tool injection, exfiltration); defense surface (policy, sandbox, monitoring); security evaluation protocol; mechanism / architecture; data / training setup. @Liu2026Agents @Plaat2025Agentic @Shi2025Progent

## Claim candidates

- We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security
- MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP @Zhang2025Security
- We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. @Lichkovski2025Agent
- To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. @Kale2025Reliable
- RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. @Fu2025Eval

## Concrete comparisons

- Axis: threat model (prompt / tool injection, exfiltration); A: Agent frameworks / architectures: `P0056`, `P0006`, `P0032`; B: Safety / security / guardrails: `P0040`, `P0065`, `P0074`. @Shi2025Progent @Zhang2025Security
  - A highlight: (E-P0032-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. @Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
  - B highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0089-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#method)
- Axis: defense surface (policy, sandbox, monitoring); A: Agent frameworks / architectures: `P0056`, `P0006`, `P0032`; B: Safety / security / guardrails: `P0040`, `P0065`, `P0074`. @Shi2025Progent @Zhang2025Security
  - A highlight: (E-P0032-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. @Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
  - B highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0089-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#method)
- Axis: security evaluation protocol; A: Agent frameworks / architectures: `P0056`, `P0006`, `P0032`; B: Safety / security / guardrails: `P0040`, `P0065`, `P0074`. @Shi2025Progent @Zhang2025Security
  - A highlight: (E-P0032-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. @Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
  - B highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0089-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#method)
- Axis: mechanism / architecture; A: Agent frameworks / architectures: `P0056`, `P0006`, `P0032`; B: Safety / security / guardrails: `P0040`, `P0065`, `P0074`. @Shi2025Progent @Zhang2025Security
  - A highlight: (E-P0032-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. @Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
  - B highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0089-7a6ec4daed) We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[1])
- Axis: data / training setup; A: Agent frameworks / architectures: `P0056`, `P0006`, `P0032`; B: Safety / security / guardrails: `P0040`, `P0065`, `P0074`. @Shi2025Progent @Zhang2025Security
  - A highlight: (E-P0032-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. @Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
  - B highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0089-7a6ec4daed) We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[1])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: LLMs; ASB; AgentDojo; AgentPoison; RSP; GSI; RSV; FEVER; RSP-M; HotpotQA. @Liu2026Agents @Plaat2025Agentic @Shi2025Progent @Zhou2025Reasoning

## Failures / limitations

- Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. @Liu2026Agents
- We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society. @Plaat2025Agentic
- Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. @Shi2025Progent
- LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments. @Shi2025Progent

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- training data and supervision signal
- baseline choices and ablation evidence
