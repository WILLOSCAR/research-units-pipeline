# Evidence draft: 6.1 Benchmarks and evaluation protocols

## Evidence snippets (with provenance)
- (E-P0089-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#method)
- (E-P0164-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. @Mohammadi2025Evaluation (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0164#key_results[0])
- (E-P0032-6829c8b583) Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. @Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0032#limitations[1])
- (E-P0043-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. @Shang2024Agentsquare (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0043#key_results[0])
- (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). @Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- (E-P0089-7a6ec4daed) We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. @Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[1])
- (E-P0199-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. @Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0199#key_results[1])
- (E-P0203-e0345118bc) To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. @Kale2025Reliable (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0203#key_results[0])
- (E-P0054-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. @Kim2026Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0054#key_results[0])
- (E-P0032-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. @Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0032#key_results[0])

## Definitions / setup

- Setup: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Benchmarks and evaluation protocols'.. Axes: tool interface (function calling, schemas, protocols); tool selection / routing policy; sandboxing / permissions / observability; task suites (web / code / embodied / tools); metrics (success, cost, reliability, safety). @Li2026Autonomous @Kim2026Beyond @Zhu2025Where

## Claim candidates

- We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security
- This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation @Mohammadi2025Evaluation
- Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. @Shi2025Progent
- Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. @Shang2024Agentsquare
- MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP @Zhang2025Security

## Concrete comparisons

- Axis: tool interface (function calling, schemas, protocols); A: Agent frameworks / architectures: `P0052`, `P0054`, `P0015`; B: Evaluation / benchmark-focused works: `P0054`, `P0089`, `P0101`. @Zhang2025Security @Fu2025Eval
  - A highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - A highlight: (E-P0089-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#method)
  - B highlight: (E-P0199-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. @Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0199#key_results[1])
  - B highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0052`, `P0054`, `P0015`; B: Evaluation / benchmark-focused works: `P0054`, `P0089`, `P0101`. @Zhang2025Security @Fu2025Eval
  - A highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - A highlight: (E-P0089-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#method)
  - B highlight: (E-P0199-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. @Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0199#key_results[1])
  - B highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- Axis: sandboxing / permissions / observability; A: Agent frameworks / architectures: `P0052`, `P0054`, `P0015`; B: Evaluation / benchmark-focused works: `P0054`, `P0089`, `P0101`. @Zhang2025Security @Mohammadi2025Evaluation
  - A highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - A highlight: (E-P0089-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#method)
  - B highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0164-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and @Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0164#key_results[0])
- Axis: task suites (web / code / embodied / tools); A: Agent frameworks / architectures: `P0052`, `P0054`, `P0015`; B: Evaluation / benchmark-focused works: `P0054`, `P0089`, `P0101`. @Zhang2025Security @Fu2025Eval
  - A highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - A highlight: (E-P0089-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#method)
  - B highlight: (E-P0199-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. @Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0199#key_results[1])
  - B highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- Axis: metrics (success, cost, reliability, safety); A: Agent frameworks / architectures: `P0052`, `P0054`, `P0015`; B: Evaluation / benchmark-focused works: `P0054`, `P0089`, `P0101`. @Shi2025Progent @Zhang2025Security @Mohammadi2025Evaluation
  - A highlight: (E-P0032-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. @Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
  - A highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0164-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and @Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0164#key_results[0])
  - B highlight: (E-P0089-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed @Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: DeepSeek-V3; API; LLMs; WildAGTEval; ALFWorld; GAIA; AgentErrorTaxonomy; AgentErrorBench; WebShop; AgentDebug. @Li2026Autonomous @Kim2026Beyond @Zhu2025Where @Lidayan2025Abbel

## Failures / limitations

- Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures. @Li2026Autonomous
- Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. @Zhu2025Where
- First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. @Zhu2025Where
- Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. @Zhu2025Where

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- baseline choices and ablation evidence
