# Evidence draft: 3.2 Tool interfaces and orchestration

## Evidence snippets (with provenance)
- (E-P0220-32b9aafe57) To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing toolsets to fit within context limits without sacrificing accuracy. @Liu2025Toolscope (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0220#method)
- (E-P0035-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. @Zhou2025Self (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0035#key_results[0])
- (E-P0028-f8def223dc) MSC-Bench provides a diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using agents. @Dong2025Bench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0028#limitations[1])
- (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
- (E-P0220-468a77ff1d) Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. @Liu2025Toolscope (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0220#key_results[0])
- (E-P0220-cb30da23cb) To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing toolsets to fit within context limits without sacrificing accuracy. @Liu2025Toolscope (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0220#key_results[1])
- (E-P0029-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. @Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
- (E-P0029-38dc800de9) MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0029#key_results[1])
- (E-P0108-d5c234444e) Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. @Du2024Anytool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0108#key_results[0])
- (E-P0164-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. @Mohammadi2025Evaluation (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0164#key_results[0])

## Definitions / setup

- Setup: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Tool interfaces and orchestration'.. Axes: tool interface (function calling, schemas, protocols); tool selection / routing policy; sandboxing / permissions / observability; mechanism / architecture; data / training setup. @Xuan2026Confidence @Lumer2025Memtool @Zhou2025Self

## Claim candidates

- To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing toolsets to fit within context limits without sacrificing accuracy. @Liu2025Toolscope
- Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. @Zhou2025Self
- MSC-Bench provides a diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using agents. @Dong2025Bench
- Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances
- Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. @Liu2025Toolscope

## Concrete comparisons

- Axis: tool interface (function calling, schemas, protocols); A: Agent frameworks / architectures: `P0129`, `P0029`, `P0035`; B: Tool-use and function calling: `P0129`, `P0028`, `P0029`. @Lumer2025Memtool @Li2025Dissonances
  - A highlight: (E-P0029-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
  - A highlight: (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0029-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
  - B highlight: (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0129`, `P0029`, `P0035`; B: Tool-use and function calling: `P0129`, `P0028`, `P0029`. @Lumer2025Memtool @Li2025Dissonances
  - A highlight: (E-P0029-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
  - A highlight: (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0029-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
  - B highlight: (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
- Axis: sandboxing / permissions / observability; A: Agent frameworks / architectures: `P0129`, `P0029`, `P0035`; B: Tool-use and function calling: `P0129`, `P0028`, `P0029`. @Li2025Dissonances @Lumer2025Memtool
  - A highlight: (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - A highlight: (E-P0029-38dc800de9) MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[1])
  - B highlight: (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0029-38dc800de9) MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[1])
- Axis: mechanism / architecture; A: Agent frameworks / architectures: `P0129`, `P0029`, `P0035`; B: Tool-use and function calling: `P0129`, `P0028`, `P0029`. @Li2025Dissonances @Lumer2025Memtool
  - A highlight: (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - A highlight: (E-P0029-38dc800de9) MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[1])
  - B highlight: (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0029-38dc800de9) MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[1])
- Axis: data / training setup; A: Agent frameworks / architectures: `P0129`, `P0029`, `P0035`; B: Tool-use and function calling: `P0129`, `P0028`, `P0029`. @Zhou2025Self @Li2025Dissonances @Lumer2025Memtool
  - A highlight: (E-P0035-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. @Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0035#key_results[0])
  - A highlight: (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0088-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. @Li2025Dissonances (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0029-38dc800de9) MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[1])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: LLMs; MCP; MemTool; ScaleMCP; TauBench; TicToc; FMs; MatSci; ReAct; AutoTool. @Xuan2026Confidence @Lumer2025Memtool @Zhou2025Self @Cheng2025Your

## Failures / limitations

- Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. @Xuan2026Confidence
- The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. @Zhou2025Self
- However, a critical yet overlooked limitation of these agents is that they, by default, assume a stationary context, failing to account for the real-world time elapsed between messages. @Cheng2025Your
- This limitation hinders decisions about when to invoke tools, leading agents to either over-rely on stale context and skip needed tool calls, or under-rely on it and redundantly repeat tool calls. @Cheng2025Your

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- training data and supervision signal
- baseline choices and ablation evidence
