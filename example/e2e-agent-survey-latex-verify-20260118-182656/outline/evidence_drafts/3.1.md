# Evidence draft: 3.1 Agent loop and action spaces

## Evidence snippets (with provenance)
- (E-P0011-9d9d60644a) We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). @Kim2025Bridging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0011#method)
- (E-P0138-1063eee7ce) However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. @Zhao2025Achieving (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0138#key_results[0])
- (E-P0090-f7a14123f9) To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. @Liu2025Mcpagentbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0090#limitations[1])
- (E-P0182-7aa3167337) Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. @Qiu2025Locobench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0182#key_results[1])
- (E-P0156-c8c4670812) We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design. @Fumero2025Cybersleuth (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0156#key_results[0])
- (E-P0182-fa8206acb2) Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. @Qiu2025Locobench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0182#key_results[0])
- (E-P0019-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. @Li2025Agentswift (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0019#key_results[0])
- (E-P0024-4b027dfb27) We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. @Feng2025Group (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
- (E-P0059-895b04aa5c) Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. @Xi2026Toolgym (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0059#key_results[0])
- (E-P0059-8c9597d805) Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. @Xi2026Toolgym (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0059#key_results[1])

## Definitions / setup

- Setup: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Agent loop and action spaces'.. Axes: mechanism / architecture; data / training setup; evaluation protocol (datasets / metrics / human); efficiency / compute; failure modes / limitations. @Zhang2026Evoroute @Xi2026Toolgym @Song2026Envscaler

## Claim candidates

- We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). @Kim2025Bridging
- However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. @Zhao2025Achieving
- To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. @Liu2025Mcpagentbench
- Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. @Qiu2025Locobench
- We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design. @Fumero2025Cybersleuth

## Concrete comparisons

- Axis: mechanism / architecture; A: Agent frameworks / architectures: `P0055`, `P0059`, `P0125`; B: Planning / reasoning loops: `P0011`, `P0019`, `P0166`. @Li2025Agentswift @Xi2026Toolgym @Kim2025Bridging
  - A highlight: (E-P0019-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. @Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0019#key_results[0])
  - A highlight: (E-P0059-8c9597d805) Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. @Xi2026Toolgym (pointer: papers/paper_notes.jsonl:paper_id=P0059#key_results[1])
  - B highlight: (E-P0019-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. @Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0019#key_results[0])
  - B highlight: (E-P0011-9d9d60644a) We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). @Kim2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0011#method)
- Axis: data / training setup; A: Agent frameworks / architectures: `P0055`, `P0059`, `P0125`; B: Planning / reasoning loops: `P0011`, `P0019`, `P0166`. @Xi2026Toolgym @Feng2025Group @Li2025Agentswift @Kim2025Bridging
  - A highlight: (E-P0059-895b04aa5c) Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. @Xi2026Toolgym (pointer: papers/paper_notes.jsonl:paper_id=P0059#key_results[0])
  - A highlight: (E-P0024-4b027dfb27) We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. @Feng2025Group (pointer: papers/paper_notes.jsonl:paper_id=P0024#key_results[0])
  - B highlight: (E-P0019-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. @Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0019#key_results[0])
  - B highlight: (E-P0011-9d9d60644a) We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). @Kim2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0011#method)
- Axis: evaluation protocol (datasets / metrics / human); A: Agent frameworks / architectures: `P0055`, `P0059`, `P0125`; B: Planning / reasoning loops: `P0011`, `P0019`, `P0166`. @Li2025Agentswift @Xi2026Toolgym @Kim2025Bridging
  - A highlight: (E-P0019-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. @Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0019#key_results[0])
  - A highlight: (E-P0059-8c9597d805) Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. @Xi2026Toolgym (pointer: papers/paper_notes.jsonl:paper_id=P0059#key_results[1])
  - B highlight: (E-P0019-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. @Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0019#key_results[0])
  - B highlight: (E-P0011-9d9d60644a) We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). @Kim2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0011#method)
- Axis: efficiency / compute; A: Agent frameworks / architectures: `P0055`, `P0059`, `P0125`; B: Planning / reasoning loops: `P0011`, `P0019`, `P0166`. @Li2025Agentswift @Xi2026Toolgym @Kim2025Bridging
  - A highlight: (E-P0019-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. @Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0019#key_results[0])
  - A highlight: (E-P0059-8c9597d805) Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. @Xi2026Toolgym (pointer: papers/paper_notes.jsonl:paper_id=P0059#key_results[1])
  - B highlight: (E-P0019-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. @Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0019#key_results[0])
  - B highlight: (E-P0011-9d9d60644a) We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). @Kim2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0011#method)
- Axis: failure modes / limitations; A: Agent frameworks / architectures: `P0055`, `P0059`, `P0125`; B: Planning / reasoning loops: `P0011`, `P0019`, `P0166`. @Li2025Agentswift @Xi2026Toolgym @Kim2025Bridging
  - A highlight: (E-P0019-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. @Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0019#key_results[0])
  - A highlight: (E-P0059-8c9597d805) Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. @Xi2026Toolgym (pointer: papers/paper_notes.jsonl:paper_id=P0059#key_results[1])
  - B highlight: (E-P0019-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. @Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0019#key_results[0])
  - B highlight: (E-P0011-9d9d60644a) We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). @Kim2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0011#method)

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: LLMs; GAIA; EvoRoute; BrowseComp; DeepSeek-v3; LLM-simulated; SFT; RUC-NLPIR; EnvScaler; SkelBuilder. @Zhang2026Evoroute @Xi2026Toolgym @Song2026Envscaler @Kim2025Bridging

## Failures / limitations

- We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. @Zhang2026Evoroute
- Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge. @Li2025Agentswift
- MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. @Wu2025Meta
- We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi-agent extensions. @Wu2025Meta

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- failure modes / known limitations
- baseline choices and ablation evidence
