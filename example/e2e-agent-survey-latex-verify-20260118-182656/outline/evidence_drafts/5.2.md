# Evidence draft: 5.2 Multi-agent coordination

## Evidence snippets (with provenance)
- (E-P0037-2c281ce5fb) This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP). @Sarkar2025Survey (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0037#method)
- (E-P0005-506120a6cd) Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. @Wang2023Voyager (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0005#key_results[1])
- (E-P0050-c92ed293ba) While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. @Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0050#limitations[1])
- (E-P0029-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. @Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
- (E-P0029-38dc800de9) MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0029#key_results[1])
- (E-P0036-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. @Cao2025Skyrl (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0036#key_results[1])
- (E-P0157-3acffd03b4) DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. @Chuang2025Debate (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0157#key_results[0])
- (E-P0005-b2eceb5b30) It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. @Wang2023Voyager (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0005#key_results[0])
- (E-P0036-3af1ce8090) Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. @Cao2025Skyrl (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
- (E-P0044-faa3d4c9ee) Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on). @Zhou2024Archer (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0044#key_results[0])

## Definitions / setup

- Setup: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Multi-agent coordination'.. Axes: communication protocol + role assignment; aggregation (vote / debate / referee); stability (collusion, mode collapse, incentives); mechanism / architecture; data / training setup. @Silva2025Agents @Lumer2025Memtool @Cao2025Skyrl

## Claim candidates

- This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP). @Sarkar2025Survey
- Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. @Wang2023Voyager
- While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. @Shen2024Small
- Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. @Lumer2025Memtool
- MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool

## Concrete comparisons

- Axis: communication protocol + role assignment; A: Agent frameworks / architectures: `P0021`, `P0029`, `P0036`; B: Multi-agent coordination: `P0021`, `P0087`, `P0137`. @Lumer2025Memtool @Cao2025Skyrl @Chuang2025Debate
  - A highlight: (E-P0029-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
  - A highlight: (E-P0036-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and @Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[1])
  - B highlight: (E-P0157-3acffd03b4) DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. @Chuang2025Debate (pointer: papers/paper_notes.jsonl:paper_id=P0157#key_results[0])
- Axis: aggregation (vote / debate / referee); A: Agent frameworks / architectures: `P0021`, `P0029`, `P0036`; B: Multi-agent coordination: `P0021`, `P0087`, `P0137`. @Cao2025Skyrl @Lumer2025Memtool @Chuang2025Debate
  - A highlight: (E-P0036-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and @Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[1])
  - A highlight: (E-P0029-38dc800de9) MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[1])
  - B highlight: (E-P0157-3acffd03b4) DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. @Chuang2025Debate (pointer: papers/paper_notes.jsonl:paper_id=P0157#key_results[0])
- Axis: stability (collusion, mode collapse, incentives); A: Agent frameworks / architectures: `P0021`, `P0029`, `P0036`; B: Multi-agent coordination: `P0021`, `P0087`, `P0137`. @Cao2025Skyrl @Lumer2025Memtool @Chuang2025Debate
  - A highlight: (E-P0036-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and @Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[1])
  - A highlight: (E-P0029-38dc800de9) MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[1])
  - B highlight: (E-P0157-3acffd03b4) DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. @Chuang2025Debate (pointer: papers/paper_notes.jsonl:paper_id=P0157#key_results[0])
- Axis: mechanism / architecture; A: Agent frameworks / architectures: `P0021`, `P0029`, `P0036`; B: Multi-agent coordination: `P0021`, `P0087`, `P0137`. @Cao2025Skyrl @Lumer2025Memtool @Chuang2025Debate
  - A highlight: (E-P0036-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and @Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[1])
  - A highlight: (E-P0029-38dc800de9) MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. @Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0029#key_results[1])
  - B highlight: (E-P0157-3acffd03b4) DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. @Chuang2025Debate (pointer: papers/paper_notes.jsonl:paper_id=P0157#key_results[0])
- Axis: data / training setup; A: Agent frameworks / architectures: `P0021`, `P0029`, `P0036`; B: Multi-agent coordination: `P0021`, `P0087`, `P0137`. @Cao2025Skyrl @Chuang2025Debate
  - A highlight: (E-P0036-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and @Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[1])
  - A highlight: (E-P0036-3af1ce8090) Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. @Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - B highlight: (E-P0157-3acffd03b4) DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. @Chuang2025Debate (pointer: papers/paper_notes.jsonl:paper_id=P0157#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: LLMs; MCP; MemTool; ScaleMCP; SA-SWE-32B; AST-based; SWE-Bench; SWE; SkyRL-Agent; SkyRL-train. @Silva2025Agents @Lumer2025Memtool @Cao2025Skyrl @Sarkar2025Survey

## Failures / limitations

- This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. @Silva2025Agents
- The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems. @Sarkar2025Survey
- Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains. @Papadakis2025Atlas
- While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics. @Chuang2025Debate

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- training data and supervision signal
- baseline choices and ablation evidence
