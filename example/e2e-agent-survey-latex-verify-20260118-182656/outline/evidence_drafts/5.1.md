# Evidence draft: 5.1 Self-improvement and adaptation

## Evidence snippets (with provenance)
- (E-P0035-e872c22cb6) In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. @Zhou2025Self (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0035#method)
- (E-P0138-1063eee7ce) However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. @Zhao2025Achieving (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0138#key_results[0])
- (E-P0061-7acf4de689) We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. @Van2025Survey (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0061#limitations[1])
- (E-P0035-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. @Zhou2025Self (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0035#key_results[0])
- (E-P0138-5f246d0ca8) Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. @Zhao2025Achieving (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0138#key_results[1])
- (E-P0081-0c10233369) This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively retrieves distilled principles to guide its decision-making, accumulating a diverse set of behavioral trajectories. @Wu2025Evolver (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0081#key_results[0])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. @Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0020-7929ef7a56) In controlled Catanatron experiments, HexMachina learns from scratch and evolves players that outperform the strongest human-crafted baseline (AlphaBeta), achieving a 54% win rate and surpassing prompt-driven and no-discovery baselines. @Belle2025Agents (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0020#key_results[0])
- (E-P0044-faa3d4c9ee) Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on). @Zhou2024Archer (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0044#key_results[0])
- (E-P0055-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. @Zhang2026Evoroute (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0055#key_results[0])

## Definitions / setup

- Setup: Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Self-improvement and adaptation'.. Axes: training signal (SFT / preference / RL); data synthesis + evaluator / reward; generalization + regression control; mechanism / architecture; data / training setup. @Zhang2026Evoroute @Belle2025Agents @Zhou2025Self

## Claim candidates

- In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. @Zhou2025Self
- However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. @Zhao2025Achieving
- We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. @Van2025Survey
- Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. @Zhou2025Self
- Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. @Zhao2025Achieving

## Concrete comparisons

- Axis: training signal (SFT / preference / RL); A: Agent frameworks / architectures: `P0055`, `P0020`, `P0035`; B: Memory / retrieval augmentation: `P0080`, `P0198`. @Zhou2025Self
  - A highlight: (E-P0035-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. @Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0035#key_results[0])
  - A highlight: (E-P0035-e872c22cb6) In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. @Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0035#method)
- Axis: data synthesis + evaluator / reward; A: Agent frameworks / architectures: `P0055`, `P0020`, `P0035`; B: Memory / retrieval augmentation: `P0080`, `P0198`. @Zhou2025Self @Zhang2026Evoroute
  - A highlight: (E-P0035-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. @Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0035#key_results[0])
  - A highlight: (E-P0055-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. @Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
- Axis: generalization + regression control; A: Agent frameworks / architectures: `P0055`, `P0020`, `P0035`; B: Memory / retrieval augmentation: `P0080`, `P0198`. @Wu2025Evolver @Zhang2026Evoroute
  - A highlight: (E-P0081-0c10233369) This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively @Wu2025Evolver (pointer: papers/paper_notes.jsonl:paper_id=P0081#key_results[0])
  - A highlight: (E-P0055-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. @Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
- Axis: mechanism / architecture; A: Agent frameworks / architectures: `P0055`, `P0020`, `P0035`; B: Memory / retrieval augmentation: `P0080`, `P0198`. @Wu2025Evolver @Zhang2026Evoroute
  - A highlight: (E-P0081-0c10233369) This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively @Wu2025Evolver (pointer: papers/paper_notes.jsonl:paper_id=P0081#key_results[0])
  - A highlight: (E-P0055-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. @Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0055#key_results[0])
- Axis: data / training setup; A: Agent frameworks / architectures: `P0055`, `P0020`, `P0035`; B: Memory / retrieval augmentation: `P0080`, `P0198`. @Zhou2025Self
  - A highlight: (E-P0035-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. @Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0035#key_results[0])
  - A highlight: (E-P0035-e872c22cb6) In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. @Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0035#method)

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: LLMs; GAIA; EvoRoute; BrowseComp; ReAct; HexMachina; AlphaBeta; TauBench; FMs; MatSci. @Zhang2026Evoroute @Belle2025Agents @Zhou2025Self @Van2025Survey

## Failures / limitations

- We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. @Zhang2026Evoroute
- The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. @Zhou2025Self
- We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. @Van2025Survey
- In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. @Wei2025Memory

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- training data and supervision signal
- baseline choices and ablation evidence
