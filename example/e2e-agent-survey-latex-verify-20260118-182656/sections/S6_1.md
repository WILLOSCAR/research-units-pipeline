Benchmarks and evaluation protocols are the common language that makes agent claims comparable, yet they are also a major source of disagreement: the same agent can look strong under an unconstrained success metric and weak under a budgeted or safety-aware protocol. A central tension is that end-to-end benchmarks capture realistic failure compounding, whereas component-level evaluations make it easier to attribute improvements and to reproduce results [@Mohammadi2025Evaluation; @Fu2025Eval]. This motivates treating protocol design as part of the agent research contribution.

Benchmark suites for agents are increasingly diverse. Some focus on interactive task completion, others on tool-use correctness, and others on safety stress tests. AgentSquare (2024) exemplifies the “suite” approach by packaging multiple scenarios under a consistent harness, which can reduce the temptation to overfit to a single task type [@Shang2024Agentsquare]. However, suites still vary in tool availability and reset policies, so comparisons across suites remain limited unless protocols are standardized.

Reliability-focused work emphasizes that metrics must reflect what matters in deployment: not just whether an answer is correct, but whether it is obtained efficiently and safely under constraints. This leads to protocol elements such as cost budgets, latency caps, and repeated-trial stability checks, which can change which methods look preferable [@Kale2025Reliable; @Kim2026Beyond; @Li2026Autonomous]. Moreover, explicit protocol reporting enables reviewers to identify when gains come from harness differences rather than from method differences.

Security-aware evaluation further complicates the picture. Zhang et al. (2025) propose a taxonomy of 12 agent attacks, including categories such as prompt injection and manipulation routes, which provides a concrete scaffold for threat-model-aware benchmarking rather than ad-hoc red teaming [@Zhang2025Security]. Systems like Progent-style evaluations connect these threat models to agent use cases and show that safety conclusions depend on both interface exposure and monitoring, not just on model alignment [@Shi2025Progent].

Taken together, the evaluation layer should be understood as a set of design choices with their own failure modes: leakage, non-determinism, and missing cost models can all bias conclusions. Therefore, survey comparisons should prefer studies that report protocol details (budgets, tool access, scoring rules) and that include ablations separating model capability from harness effects [@Chen2025Towards; @Mohammadi2025Evaluation].

A key limitation is that many benchmarks remain partial proxies for deployment. They may omit adversarial environments, long-tail tool failures, or governance constraints, leading to optimistic performance estimates. This motivates two directions: (i) richer benchmark reporting standards (what was allowed and what was measured), and (ii) cross-benchmark calibration studies that quantify how conclusions change when protocols differ [@Fu2025Eval; @Kale2025Reliable].

A useful mental model is that an agent benchmark is a bundle of hidden assumptions: tool reliability, availability, budget, and what counts as evidence. When these are not reported, evaluation becomes a moving target, and seemingly “better” methods may simply be evaluated under easier conditions. This is why evaluation meta-work argues for reporting standards and for separating harness effects from model effects, especially when systems involve tool calls and multi-turn state [@Mohammadi2025Evaluation; @Fu2025Eval].

For security-aware evaluation, protocol specificity is even more critical. Attack taxonomies (e.g., 12 attack categories) provide coverage targets, but they only become meaningful when a benchmark specifies attacker capabilities, tool permissions, and monitoring hooks [@Zhang2025Security]. Therefore, the most actionable direction is to treat benchmark design as an engineering artifact: publish harnesses, specify randomness controls, and include cost models so that results can be reproduced and compared across labs [@Kim2026Beyond; @Kale2025Reliable; @Li2026Autonomous].

Calibration is arguably the missing layer. When two benchmarks claim to measure similar abilities but differ in tool availability, reset rules, and scoring, headline results cannot be compared directly. Meta-evaluation work suggests that a small number of controlled protocol swaps (budget, tool set, verification rule) can quantify how brittle a method is to harness changes and can prevent leaderboard overfitting from being mistaken for progress [@Mohammadi2025Evaluation; @Fu2025Eval]. In this view, publishing harnesses and reporting randomness controls are not just reproducibility hygiene; they are the mechanism by which agent papers make claims that survive across labs [@Kim2026Beyond; @Li2026Autonomous].

Another practical lesson is to report failure structure, not only success. For agents, a single score often conflates tool-call errors, planning failures, and safety violations. Protocol-aware evaluation encourages logging intermediate events (tool-call validity, verification outcomes, abort reasons) and reporting distributions across runs, which supports a more honest discussion of reliability and risk envelopes [@Kale2025Reliable; @Chen2025Towards]. For safety-aware suites, threat taxonomies become useful only when mapped to concrete attacker capabilities and monitoring hooks; otherwise “secure” can mean “not tested” [@Zhang2025Security; @Shi2025Progent].
