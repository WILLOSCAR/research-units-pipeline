Safety and security for tool-using agents are not separable from system design: the tool interface defines the attack surface, the logging surface, and the available mitigations. A central tension is that higher autonomy reduces human oversight costs, whereas it increases the risk of prompt injection, data exfiltration, and tool abuse under realistic deployment constraints [@Zhang2025Security; @Shi2025Progent]. This implies that surveys should treat threat models as part of the agent design space, not as an external appendix.

Threat models for agents go beyond content manipulation. When an agent delegates actions to tools, attackers can target the decision process (e.g., steering tool selection) or the state (e.g., poisoning memory) rather than only the final output. Progent-style evaluations emphasize agent use cases and benchmarked attack conditions (e.g., AgentDojo/ASB/AgentPoison-style setups), illustrating that vulnerability depends on interface exposure and tool permissions as much as on model behavior [@Shi2025Progent]. However, reported results can be hard to compare without standardized protocols.

Mitigations often take the form of checks, monitors, and constrained execution. Systems that emphasize systematic checking aim to catch tool misuse and unsafe actions before they execute, but they also introduce latency and can create new failure modes when checks are incomplete or mis-specified [@Bonagiri2025Check; @Lee2025Bench]. In contrast, lightweight guardrails may preserve usability but provide weaker guarantees, especially under adaptive attackers.

A governance-oriented view reframes safety as an engineering contract: what actions are allowed, how they are audited, and what rollback mechanisms exist. Reliability and evaluation work suggests that safety claims should be tied to measurable protocol conditions (budgets, tool access, logging), because otherwise “safe” can mean “safe under a different harness” [@Kale2025Reliable; @Fu2025Eval]. Therefore, practical deployments may require layered controls: sandboxing at the tool layer, provenance at the memory layer, and monitoring at the decision layer [@Lichkovski2025Agent].

Taken together, agent safety research benefits from integrating threat models with evaluation. Attack taxonomies provide coverage targets, while benchmarks provide measurable failure rates under controlled conditions. This suggests that future evaluation suites should explicitly parameterize adversarial strength, tool permissions, and the cost of verification, rather than treating these as hidden implementation details [@Zhang2025Security; @Shi2025Progent].

Moreover, governance constraints are often external to research prototypes: real organizations impose compliance, privacy, and audit requirements that benchmarks rarely model. Bridging this gap likely requires collaboration between research and deployment communities, and a shift toward reporting “safety envelopes” (what the system can guarantee under what assumptions) rather than one-size-fits-all safety claims [@Li2025Stac; @Kale2025Reliable].

A concrete governance view is to describe “what can go wrong” in the same language as “what can be done.” If the action space includes file I/O, web access, or code execution, then the governance layer must specify which actions are permitted, how they are audited, and what escalation paths exist when anomalies are detected [@Lichkovski2025Agent]. In contrast, purely content-level guardrails may miss tool-layer abuse, which is why safety work increasingly ties checks to execution events rather than only to generated text [@Bonagiri2025Check; @Lee2025Bench].

Finally, governance constraints need to be made measurable. Reliability framing suggests reporting “safety envelopes” tied to protocol parameters: budgets, permissions, monitoring latency, and false positive/negative rates of checks [@Kale2025Reliable; @Fu2025Eval]. This remains limited by benchmark realism—organizational compliance and privacy constraints are rarely modeled—but the direction is clear: treat governance as a system contract with testable conditions, and report those conditions as part of the evaluation protocol rather than as a narrative disclaimer [@Li2025Stac; @Zhang2025Security].

At the system boundary, permissioning and provenance are the concrete levers that connect abstract threat models to implementable mitigations. If a benchmark treats tool calls as always-successful and side-effect free, it cannot meaningfully measure prompt injection, exfiltration, or tool abuse; conversely, realistic permissions and failure modes force methods to specify how they detect anomalies and when they escalate or refuse actions [@Zhang2025Security; @Shi2025Progent]. This is also why checkers and monitors should be evaluated with their own operating characteristics (latency, false positives/negatives), since aggressive checking can shift risk rather than eliminate it [@Bonagiri2025Check; @Lee2025Bench].

Governance becomes actionable when expressed as a testable contract: what events are logged, who can inspect them, and what interventions exist (rollback, sandboxing, human review). Reliability-focused framing encourages reporting these contracts alongside the evaluation harness, because monitoring latency and audit policies can change the effective threat model even if the underlying agent policy is unchanged [@Kale2025Reliable; @Fu2025Eval]. Practical deployments also motivate least-privilege tool design and clear escalation paths, so that when an agent fails it fails safely; this engineering detail deserves to be part of survey comparisons rather than an afterthought [@Lichkovski2025Agent; @Li2025Stac].
