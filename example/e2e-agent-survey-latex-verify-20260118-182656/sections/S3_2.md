Tool interfaces are the “ground truth boundary” for LLM agents: they determine what actions can actually change the world, what observability the agent has, and what constraints (permissions, rate limits, and cost budgets) are enforced. A central tension is that richer tool APIs reduce friction for general problem solving, whereas tighter interface contracts make failures easier to detect and safer to contain under realistic evaluation protocols [@Du2024Anytool; @Liu2025Toolscope]. This motivates treating interface design as a first-class comparison axis rather than an implementation detail.

At the interface level, systems differ in whether they expose a small, curated function set or a large, open-ended tool catalog. AutoTool-style approaches emphasize automated tool selection and routing to reduce manual engineering, but this increases the need for reliable tool descriptions and robust fallback behavior when the selected tool fails [@Jia2025Autotool]. In contrast, AnyTool-like framing highlights generality—covering broad tool families—but pushes more responsibility onto orchestration policies to keep action sequences valid and interpretable [@Du2024Anytool].

Orchestration becomes especially visible when tools include memory substrates. MemTool reports evaluation across 13+ LLMs on the ScaleMCP benchmark, and it explicitly studies stability over 100 consecutive uses, which makes “interface drift” observable rather than anecdotal [@Lumer2025Memtool]. However, memory-as-a-tool also blurs the boundary between state and action: writing to memory is an action that can silently change future behavior, which complicates attribution when metrics improve or degrade [@Li2025Dissonances].

Because interface choices dominate failure modes, tool-use evaluation needs protocols that surface tool-specific errors (invalid parameters, permission violations, hallucinated tool names) and account for cost. Evaluation-oriented studies emphasize that many reported improvements are not comparable unless the protocol specifies tool availability, allowed retries, and what counts as a successful tool call versus a successful end-to-end task [@Mohammadi2025Evaluation; @Dong2025Bench]. In practice, a benchmark that scores only final success can hide systematic tool misuse, whereas a protocol that scores tool validity may penalize exploratory behavior that is acceptable in some deployments.

Self-improvement loops interact with tool interfaces in a subtle way. When agents learn to challenge or critique themselves, they often change not just the content of their reasoning but also their tool-use policies (when to call a tool, when to verify, and when to stop). Therefore, the same interface can yield different trade-offs depending on whether the system emphasizes conservative verification or aggressive exploration under a fixed budget [@Zhou2025Self; @Liu2025Toolscope]. In contrast, purely static tool policies may look strong on narrow benchmarks but remain brittle when tool distributions shift.

A key limitation is that the interface contract is rarely fully specified in papers: permissions, sandboxing, and failure recovery policies are often implicit. This makes it hard to transfer conclusions from one tool suite to another, even when the underlying model is similar. Future work should report interface-level conditions—tool schemas, constraints, and logging hooks—as part of the experimental protocol, so that orchestration decisions can be compared without relying on “demo feel” as evidence [@Mohammadi2025Evaluation; @Li2025Dissonances].

Moreover, “tool interface” includes more than the list of functions: it includes schemas, validation, error semantics, and permissioning. ToolScope-style analyses emphasize that agents can appear competent while silently relying on underspecified contracts (e.g., tools that never fail, or tools that return perfectly formatted outputs), which can collapse when moved to real deployments [@Liu2025Toolscope]. In contrast, evaluation harnesses that explicitly score tool-call validity and parameter correctness surface failure modes early, even if they make headline success rates look worse [@Dong2025Bench; @Mohammadi2025Evaluation].

From a systems standpoint, a practical orchestration policy should specify at least three things: selection (how a tool is chosen), execution (how errors are handled and retried), and verification (what evidence is required before an action is accepted). AutoTool-style routing helps on selection, but without strong verification it can amplify tool misuse; AnyTool-style breadth helps coverage, but it increases the burden on execution-time safeguards [@Jia2025Autotool; @Du2024Anytool]. MemTool’s repeated-use setting is a useful stress test here because it makes “policy drift” observable over long runs rather than only in single episodes [@Lumer2025Memtool; @Li2025Dissonances].

Finally, interface comparisons need observability, not just API lists. Without structured logging of tool arguments, tool outputs, and verification signals, interface failures collapse into “model errors” in post-hoc analysis. Tool-focused evaluations argue for reporting tool-call traces and schema validation outcomes as part of the benchmark artifact, because they reveal whether a method improves selection, execution robustness, or verification discipline [@Dong2025Bench; @Liu2025Toolscope]. Long-run stress tests are particularly informative here: they can expose interface drift, caching pathologies, and subtle permission mismatches that single-episode benchmarks miss [@Lumer2025Memtool; @Mohammadi2025Evaluation].
