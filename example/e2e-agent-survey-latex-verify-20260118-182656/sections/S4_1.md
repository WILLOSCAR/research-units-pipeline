Planning and reasoning loops are the control layer that turns an agent from a reactive tool caller into a system that can allocate deliberation, recover from errors, and trade off cost against success. A central tension is that deeper deliberation can improve long-horizon task completion, whereas stricter budgets and cost models penalize unnecessary reasoning and tool calls [@Yao2022React; @Liu2025Costbench]. This implies that planning methods should be compared under protocols that make budgets and stopping rules explicit.

One route is to improve planning via learning signals rather than only via prompting. For example, Hu et al. (2025) report that a 1.5B-parameter model trained with single-turn GRPO can outperform larger baselines up to 14B parameters on a complex task planning benchmark, with success rates reported around 70% for long-horizon planning [@Hu2025Training]. Moreover, this kind of result is only meaningful when the benchmark specifies the task distribution and metric definition; otherwise, “better planning” can collapse into “different evaluation.”

A second route is to refine deliberation and guidance at inference time. In security-oriented settings, Nakano et al. (2025) report that a self-guided reasoning tool completes only 13.5%, 16.5%, and 75.7% of subtasks in different configurations and requires 86.2%, 118.7%, and 205.9% more model queries, highlighting that planning quality and cost are often tightly coupled [@Nakano2025Guided]. However, the interpretation depends on protocol details: what counts as a subtask, whether retries are allowed, and how tools are exposed.

Beyond raw planning performance, loop design decisions affect what kinds of reasoning are even possible. CostBench (2025) explicitly studies cost-aware planning and reports that agents may fail to identify cost-optimal solutions in static settings, which makes “budget awareness” a measurable axis rather than an afterthought [@Liu2025Costbench]. In contrast, protocol-free evaluations can hide whether an agent is truly planning or simply sampling many candidates until something works.

Taken together, the most informative evidence comes from comparisons that hold protocols fixed while varying loop mechanisms. Under a unified ReAct-style framework, Seo et al. (2025) evaluate 16 agents and report distinct capability and limitation profiles across models, suggesting that “planning loop” is not a monolith but a family of control patterns whose behavior depends on the environment and metric [@Seo2025Simuhome; @Yao2022React]. Therefore, survey synthesis should emphasize which control-loop assumptions are shared (planner/executor separation, search strategy, verification) and which are artifacts of evaluation.

A key limitation is that planning loops open new attack and failure surfaces. Reasoning-style vulnerabilities and process-oriented attacks can manipulate how an agent allocates steps, which may induce premature errors even when content filters are strong [@Zhou2025Reasoning]. This motivates treating verification, logging, and bounded reasoning policies as part of planning design, not just as safety add-ons, especially when systems rely on memory retrieval or novelty-seeking mechanisms that can amplify spurious plans [@Du2025Memr; @Kiruluta2025Novel; @Kim2025Bridging].

Viewed through a protocol lens, planning methods differ in what they optimize and what they assume. Some loops assume that extra deliberation is essentially free, so they optimize for success rate; others explicitly trade success against cost or latency, which turns planning into a constrained optimization problem. CostBench makes this distinction concrete by pushing cost-aware planning into the benchmark definition, while training-based approaches report gains under their own task distributions and reward specifications [@Liu2025Costbench; @Hu2025Training]. In contrast, prompt-centric planning claims can be hard to interpret unless the paper reports the same constraints (tool access, retry limits, budget) alongside the metric.

For readers deciding “what to build,” the most useful comparison is not CoT vs ToT by name, but whether a system has (i) an explicit planner/executor split, (ii) a verification step that can halt bad plans early, and (iii) a budgeted control policy that prevents runaway deliberation. Simulation-style evaluations such as SimuHome help because they unify multiple agents under a shared ReAct-style loop and reveal how planning behavior changes with environment complexity [@Seo2025Simuhome; @Yao2022React]. However, because planning interacts with novelty-seeking and memory retrieval, conclusions can drift when the loop includes additional components; this is one reason to demand ablations and protocol reporting when papers claim general planning improvements [@Kiruluta2025Novel; @Du2025Memr; @Hong2025Planning].

One pragmatic synthesis is to separate “planner capacity” from “planner discipline.” Capacity asks whether the loop can search or decompose tasks effectively; discipline asks whether it can stop, verify, and stay within budget when search is tempting. Benchmarks that expose both success and resource traces (queries/tool calls/latency) make this distinction measurable and reduce the chance that a method wins by simply spending more [@Liu2025Costbench; @Nakano2025Guided]. For future surveys and evaluations, reporting a small set of standardized controls (budget, retry policy, and verification trigger) would make cross-paper planning claims far easier to compare than yet another taxonomy of prompting names [@Seo2025Simuhome; @Hu2025Training].
