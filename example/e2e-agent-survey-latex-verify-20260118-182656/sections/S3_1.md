A tool-using agent is defined as much by its loop boundary and action space as by its underlying model. A central tension is that natural-language actions make the loop flexible and easy to prototype, whereas more structured actions make behavior easier to audit and evaluate under a protocol with explicit budgets and stop conditions [@Yao2022React; @Kim2025Bridging]. This suggests that “agent capability” should be compared through interface-level decisions, not just through anecdotal task success.

To ground this loop view, many systems adopt variants of the state→decide→act→observe abstraction, but differ in what they treat as state (raw observations vs summarized memory) and what they treat as an action (free-form text vs constrained tool calls). In 2022, ReAct made this distinction visible by interleaving reasoning traces with explicit actions, which helped clarify where planning ends and execution begins [@Yao2022React]. Moreover, later systems often reinterpret “action space” as an interface contract: what actions are allowed, how errors are handled, and how retries are counted in evaluation [@Liu2025Mcpagentbench].

Action spaces also shape what benchmarks can measure. When actions are continuous or open-ended, evaluation tends to rely on end-to-end task completion metrics that blur failure causes; in contrast, discretized action spaces enable finer-grained metrics (e.g., subtask success, tool-call validity, and cost) but may under-represent real deployment variability. Tool-focused environments such as ToolGym (2026) make this trade-off explicit by constraining what actions are executable while still exposing long-horizon dependencies that stress planning [@Xi2026Toolgym].

Recent benchmark work highlights that even “agent loop” papers often embed evaluation assumptions. For example, AgentSwift reports evaluation across seven benchmarks spanning multiple domains, which helps separate loop design choices from narrow task artifacts [@Li2025Agentswift]. Similarly, LocoBench (2025) and MCPAgentBench (2025) emphasize protocol design—reset policies, task suite composition, and scoring—so that differences in action representations can be compared under a more stable metric definition [@Qiu2025Locobench; @Liu2025Mcpagentbench]. However, these benchmarks still differ in tool availability and cost modeling, so cross-benchmark comparisons remain limited.

Taken together, the most actionable comparisons focus on how loop boundary choices interact with evaluation constraints. Works that emphasize explicit routing or control decisions can look stronger under cost-aware metrics because they reduce wasted actions, whereas more free-form loops may appear stronger under unconstrained “success if eventually solved” scoring. Therefore, a protocol-aware synthesis should ask: what actions are allowed, what is penalized (latency, tool calls, tokens), and what verification is required before an episode is considered complete [@Zhao2025Achieving; @Zhang2026Evoroute].

A key limitation is that richer action spaces enlarge the error surface. In security-relevant settings, a mis-specified action representation can turn a recoverable planning error into an irreversible tool abuse event, and benchmark scores may not reflect the real risk trade-off. Evidence from domain-specific agent studies (e.g., cybersecurity investigation workflows) underscores that the same loop design can behave very differently when the environment is adversarial or partially observed, so future work should report threat-model assumptions alongside the action space definition [@Fumero2025Cybersleuth; @Feng2025Group].

One way to make these comparisons less hand-wavy is to treat the loop boundary as an experimental variable. If a benchmark exposes the same environment but changes what counts as an action (free-form steps vs validated tool calls), then improvements can be attributed more cleanly to planning or to interface choice. AgentSwif(t) explicitly leans on breadth—seven benchmarks across heterogeneous domains—which is useful precisely because it pressures the action space to be robust across different observation and feedback modalities [@Li2025Agentswift]. By contrast, specialized benches can go deeper on protocol detail: they can specify tool schemas, episode resets, and scoring for intermediate steps, but they risk overfitting conclusions to one style of loop [@Liu2025Mcpagentbench; @Qiu2025Locobench].

For practitioners, a lightweight checklist helps: (i) make the action space explicit (what is callable, with what parameters), (ii) log enough state to support post-mortems, and (iii) choose evaluation metrics that separate “did it solve the task” from “did it waste actions or take unsafe actions.” This is where controlled environments such as ToolGym are valuable: they let authors report success, cost, and error-recovery behavior under comparable protocols instead of relying on narrative demos [@Xi2026Toolgym; @Zhang2026Evoroute]. However, even with better harnesses, action-space design remains limited by mismatched assumptions about tool reliability and environment adversariality, which is why domain-specific studies remain important for stress testing loop definitions [@Fumero2025Cybersleuth; @Feng2025Group].

Concretely, surveys can normalize comparisons by asking each paper to spell out an “episode contract”: the allowed tool schemas, retry/error semantics, stop conditions, and what is logged for post-mortems. Benchmarks such as MCPAgentBench and ToolGym make these contracts more explicit, which is precisely why they are useful for isolating the impact of action representations and loop boundaries [@Liu2025Mcpagentbench; @Xi2026Toolgym]. With this contract view, classic reasoning–action splits like ReAct can be evaluated less as a prompting trick and more as a design choice about traceability and auditability under protocol constraints [@Yao2022React; @Kim2025Bridging].
