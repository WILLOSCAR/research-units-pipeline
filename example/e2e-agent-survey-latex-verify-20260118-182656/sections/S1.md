Agentic use of large language models has moved from a curiosity to a deployment pattern: systems increasingly combine an LLM with tools, memory, and execution scaffolding to solve tasks that are too long-horizon or too environment-dependent for one-shot generation. In this setting, the central question is no longer “can a model produce the right text,” but “can a looped system make and verify decisions under constraints” [@Yao2022React; @Schick2023Toolformer].

We use “agent” in the systems sense: a policy that observes state, chooses actions, and receives feedback through an environment or tool interface. This boundary is deliberately narrower than embodied robotics or classical RL; our focus is on tool-using LLM agents that operate via APIs, external resources, and structured tasks. As a result, concepts like action space, interface contract, and cost model become the right primitives for comparison, often more informative than model size alone [@Du2024Anytool; @Liu2025Toolscope].

Method note (evidence policy): this run is abstract-first, so we treat quantitative details as provisional unless a paper note records the full protocol. We still prefer protocol-aware comparisons—benchmarks, metrics, cost budgets, tool availability—because those are the conditions under which claims become reproducible and transferable [@Mohammadi2025Evaluation; @Fu2025Eval].

This survey contributes a paper-like map of the agent design space. We structure the literature around (i) loop boundaries and action spaces, (ii) tool interfaces and orchestration, (iii) planning and memory components, (iv) adaptation and multi-agent coordination, and (v) evaluation and risks. The goal is not to list papers, but to surface the tensions that determine engineering outcomes: reliability vs flexibility, deliberation vs cost, memory depth vs drift, and autonomy vs governance [@Zhang2025Security; @Kale2025Reliable].
