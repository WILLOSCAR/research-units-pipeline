The closest neighbors to tool-using LLM agents include work on tool use for language models, retrieval-augmented generation, planning/reasoning prompting, and evaluation of interactive systems. Tool use papers show that models can learn to call external functions and APIs, but they often abstract away the end-to-end loop details that dominate reliability in deployed agents [@Schick2023Toolformer; @Du2024Anytool].

Planning and reasoning methods provide the cognitive substrate for agents, yet conclusions depend heavily on protocol choices: budgets, tool access, reset policies, and what counts as success. Recent benchmark-driven studies illustrate that planning quality can look very different under cost-aware versus accuracy-only metrics, making evaluation design inseparable from algorithm design [@Liu2025Costbench; @Mohammadi2025Evaluation].

Memory and retrieval systems overlap with agents because many “agent failures” are state failures: missing context, stale summaries, or ungrounded assumptions. However, agent settings add additional constraints—tool calls, multi-step execution, and dynamic environments—that are not always captured by standard RAG evaluations [@Wei2025Memory; @Tao2026Membox].

A growing set of surveys and meta-analyses aim to systematize agent research. We build on these efforts but adopt an explicitly decision-oriented lens: rather than organizing by buzzwords, we organize by interface contracts, component roles (planning/memory/adaptation), and protocol-level evaluation anchors that make trade-offs comparable [@Van2025Survey; @Sarkar2025Survey].

Finally, safety and security work is increasingly central in agent deployments. Threat models such as prompt injection and tool abuse are not merely “misuse cases”; they reflect systematic vulnerabilities created by open-ended tool interfaces and insufficient provenance tracking. Accordingly, we treat risk and governance as part of the core design space, not as an afterthought [@Zhang2025Security; @Shi2025Progent].
