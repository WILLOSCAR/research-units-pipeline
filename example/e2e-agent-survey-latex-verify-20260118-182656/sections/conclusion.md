## Conclusion

Evidence from recent tool-using agent work suggests that “agent performance” is not a single axis: interface contracts, planning/memory components, and adaptation/coordination schemes interact with evaluation protocols in ways that can reverse conclusions across benchmarks. As a result, survey taxonomies should be decision-oriented: they should help a reader predict which design choices change reliability, cost, and risk under a stated protocol.

Looking forward, the most valuable work may be less about inventing new loop variants and more about standardizing what it means to compare them: shared task suites, explicit budgets and tool access policies, and threat-model-aware evaluations that reflect real deployment constraints [@Mohammadi2025Evaluation; @Zhang2025Security].
