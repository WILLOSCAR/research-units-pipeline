Multi-agent systems extend the agent loop by distributing cognition across roles—planning, execution, verification, or critique—rather than forcing a single policy to do everything. A central tension is that specialization can reduce individual brittleness, whereas coordination introduces overhead, communication failures, and the risk of correlated errors [@Sarkar2025Survey; @Chuang2025Debate]. This implies that coordination protocols should be evaluated as first-class design choices.

A practical coordination axis is whether agents communicate in free-form language or through more structured messages. Free-form debate can surface contradictions and improve coverage, but it can also devolve into persuasion rather than verification if incentives are misaligned [@Chuang2025Debate]. In contrast, more structured collaboration protocols can be easier to audit, but they may limit the diversity of hypotheses explored.

Representative systems illustrate different points in this trade-off space. Voyager (2023) emphasizes long-horizon behavior through iterative skill acquisition and environment interaction, which naturally benefits from role separation across planning and execution [@Wang2023Voyager]. Smaller-agent studies (e.g., 2024) suggest that coordination quality is not purely a function of model scale; protocol design and task decomposition matter at least as much as raw capability [@Shen2024Small]. Moreover, coordination often changes evaluation: what counts as success for a team, and how credit is assigned.

Evaluation of multi-agent systems is still maturing. Work on evaluating multi-agent reasoning and collaboration highlights that aggregation rules (majority vote, arbitration, or hierarchical control) can change outcomes even when the underlying models are fixed, which makes “team performance” sensitive to protocol details [@Yim2024Evaluating; @Zahedifar2025Agent]. Therefore, multi-agent benchmarks should specify communication bandwidth, turn limits, and stopping criteria, not just final task correctness.

Taken together, coordination mechanisms are best compared by the failure modes they prevent or introduce. Debate-style systems can catch some factual errors, but they may still share blind spots if all agents rely on the same tools or retrieval sources; conversely, tool diversity can increase robustness but also increases orchestration complexity [@Sarkar2025Survey; @Lumer2025Memtool]. This suggests that practical systems may need mixed protocols: structured checks for safety-critical steps and freer collaboration for exploration.

A key limitation is that coordination can create new incentive problems and attack surfaces (collusion, sybil behaviors, and adversarial messaging). RL-based coordination policies may mitigate some issues, but they also require careful evaluation under distribution shift and adversarial conditions [@Cao2025Skyrl; @Zhou2024Archer]. Future work should report coordination assumptions explicitly and include stress tests where communication is noisy, tools are unreliable, or some agents are compromised.

Coordination overhead is not a footnote; it is a measurable part of system behavior. Communication bandwidth, turn limits, and shared tool access can dominate both cost and accuracy, which is why coordination protocols should be reported alongside task metrics. Shared memory or shared tool substrates can help agents stay aligned, but they also increase coupling and can make correlated errors more likely if all agents depend on the same retrieved evidence [@Lumer2025Memtool; @Sarkar2025Survey].

From an evaluation standpoint, the field still lacks “calibrated” multi-agent benchmarks: many papers report team-level success without specifying how disagreement is resolved or how failures are attributed. Work that explicitly evaluates collaboration and aggregation highlights that arbitration rules and verification roles can flip outcomes even under identical underlying models, which suggests that coordination should be tested under controlled protocol variations [@Yim2024Evaluating; @Zahedifar2025Agent]. This remains limited by the lack of standardized settings for adversarial messaging, noisy communication, and partial tool failures, so results should be read as conditional rather than universal [@Cao2025Skyrl; @Chuang2025Debate].

From a design perspective, many practical patterns can be expressed as separation of concerns: one agent proposes, another critiques, and a third arbitrates under a fixed policy. This makes coordination closer to an executable protocol than a vague “teamwork” metaphor, and it naturally suggests ablations: swap the arbiter policy, restrict communication, or remove shared memory to see which component drives gains [@Yim2024Evaluating; @Zahedifar2025Agent]. A remaining open question is how these protocols scale with tool-use: as agents share tool access and memory, correlated errors become more likely, so coordination should be paired with diversity mechanisms (independent retrieval, heterogeneous tools) and explicit budgets on communication to avoid winning by simply “talking more” [@Lumer2025Memtool; @Sarkar2025Survey].

As a result, survey comparisons should treat communication policy (who speaks, how often, with what evidence) as an evaluation variable rather than a narrative detail. Protocol choices like turn limits, arbitration rules, and messaging constraints often matter as much as model choice once systems scale beyond two roles [@Yim2024Evaluating; @Cao2025Skyrl].
