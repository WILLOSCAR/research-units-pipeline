{"kind": "global", "id": "abstract", "title": "Abstract", "path": "sections/abstract.md", "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "724f58be0fa9cd7f73fb0fd99acab0b4bbd6e486", "bytes": 1029, "citations": ["Mohammadi2025Evaluation", "Schick2023Toolformer", "Yao2022React", "Zhang2025Security"]}
{"kind": "global", "id": "discussion", "title": "Discussion", "path": "sections/discussion.md", "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "40852367c35f6b0ebf3a574715e16275567e6ef1", "bytes": 1575, "citations": ["Du2024Anytool", "Fu2025Eval", "Liu2025Costbench", "Mohammadi2025Evaluation", "Shi2025Progent", "Zhang2025Security"]}
{"kind": "global", "id": "conclusion", "title": "Conclusion", "path": "sections/conclusion.md", "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "ef7268e6f3720b6602c2d872d40675549a883358", "bytes": 803, "citations": ["Mohammadi2025Evaluation", "Zhang2025Security"]}
{"kind": "h2_lead", "id": "3", "title": "Foundations & Interfaces", "section_id": "3", "section_title": "Foundations & Interfaces", "path": "sections/S3_lead.md", "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "2e137dabd42373292741e27a54aba76156b0b111", "bytes": 833, "citations": ["Du2024Anytool", "Liu2025Mcpagentbench", "Liu2025Toolscope", "Yao2022React"]}
{"kind": "h2_lead", "id": "4", "title": "Core Components (Planning + Memory)", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "path": "sections/S4_lead.md", "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "f088c59c53cf09d8e74bd993216eb9fca500e9e7", "bytes": 698, "citations": ["Liu2025Costbench", "Tao2026Membox", "Wei2025Memory", "Yao2022React"]}
{"kind": "h2_lead", "id": "5", "title": "Learning, Adaptation & Coordination", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "path": "sections/S5_lead.md", "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "f5c3cb9f83bfb1e175826077cfd0d81245811e2b", "bytes": 813, "citations": ["Chuang2025Debate", "Sarkar2025Survey", "Zhou2024Archer", "Zhou2025Self"]}
{"kind": "h2_lead", "id": "6", "title": "Evaluation & Risks", "section_id": "6", "section_title": "Evaluation & Risks", "path": "sections/S6_lead.md", "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "83fe07220be764f6e058e2fd849d64703f5a9c09", "bytes": 656, "citations": ["Fu2025Eval", "Kale2025Reliable", "Mohammadi2025Evaluation", "Zhang2025Security"]}
{"kind": "h2", "id": "1", "title": "Introduction", "section_id": "1", "section_title": "Introduction", "path": "sections/S1.md", "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "4b583ca6fb630146788bb0dc5d357a542cfa9648", "bytes": 1930, "citations": ["Du2024Anytool", "Fu2025Eval", "Kale2025Reliable", "Liu2025Toolscope", "Mohammadi2025Evaluation", "Schick2023Toolformer", "Yao2022React", "Zhang2025Security"]}
{"kind": "h2", "id": "2", "title": "Related Work", "section_id": "2", "section_title": "Related Work", "path": "sections/S2.md", "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "eeeebb1de20e834e35b147753133e1a9d7359d7e", "bytes": 2026, "citations": ["Du2024Anytool", "Liu2025Costbench", "Mohammadi2025Evaluation", "Sarkar2025Survey", "Schick2023Toolformer", "Shi2025Progent", "Tao2026Membox", "Van2025Survey", "Wei2025Memory", "Zhang2025Security"]}
{"kind": "h3", "id": "3.1", "title": "Agent loop and action spaces", "section_id": "3", "section_title": "Foundations & Interfaces", "path": "sections/S3_1.md", "allowed_bibkeys_mapped": ["Feng2025Group", "Fumero2025Cybersleuth", "Gasmi2025Bridging", "Ghose2025Orfs", "Kim2025Bridging", "Li2025Agentswift", "Li2025From", "Liu2025Mcpagentbench", "Luo2025Large", "Qiu2025Locobench", "Song2026Envscaler", "Wu2025Meta", "Xi2026Toolgym", "Xu2025Exemplar", "Yao2022React", "You2025Datawiseagent", "Zhang2026Evoroute", "Zhao2025Achieving"], "allowed_bibkeys_selected": ["Feng2025Group", "Fumero2025Cybersleuth", "Kim2025Bridging", "Li2025Agentswift", "Liu2025Mcpagentbench", "Qiu2025Locobench", "Xi2026Toolgym", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving"], "evidence_ids": ["E-P0011-9d9d60644a", "E-P0138-1063eee7ce", "E-P0090-f7a14123f9", "E-P0182-7aa3167337", "E-P0156-c8c4670812", "E-P0182-fa8206acb2", "E-P0019-904ba35500", "E-P0024-4b027dfb27", "E-P0059-895b04aa5c", "E-P0059-8c9597d805", "E-P0001-ca4a00b5cf", "E-P0055-60cc0d458f"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["@Li2025Agentswift"], "paper_id": "P0019", "evidence_id": "E-P0019-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0019#key_results[0]"}, {"hook_type": "quant", "text": "Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents.", "citations": ["@Xi2026Toolgym"], "paper_id": "P0059", "evidence_id": "E-P0059-8c9597d805", "pointer": "papers/paper_notes.jsonl:paper_id=P0059#key_results[1]"}, {"hook_type": "quant", "text": "Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness.", "citations": ["@Xi2026Toolgym"], "paper_id": "P0059", "evidence_id": "E-P0059-895b04aa5c", "pointer": "papers/paper_notes.jsonl:paper_id=P0059#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["@Feng2025Group"], "paper_id": "P0024", "evidence_id": "E-P0024-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0024#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["@Zhao2025Achieving"], "paper_id": "P0138", "evidence_id": "E-P0138-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0138#key_results[0]"}, {"hook_type": "eval", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["@Liu2025Mcpagentbench"], "paper_id": "P0090", "evidence_id": "E-P0090-f7a14123f9", "pointer": "papers/paper_notes.jsonl:paper_id=P0090#limitations[1]"}, {"hook_type": "quant", "text": "Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions.", "citations": ["@Qiu2025Locobench"], "paper_id": "P0182", "evidence_id": "E-P0182-7aa3167337", "pointer": "papers/paper_notes.jsonl:paper_id=P0182#key_results[1]"}, {"hook_type": "quant", "text": "We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design.", "citations": ["@Fumero2025Cybersleuth"], "paper_id": "P0156", "evidence_id": "E-P0156-c8c4670812", "pointer": "papers/paper_notes.jsonl:paper_id=P0156#key_results[0]"}, {"hook_type": "quant", "text": "Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces", "citations": ["@Qiu2025Locobench"], "paper_id": "P0182", "evidence_id": "E-P0182-fa8206acb2", "pointer": "papers/paper_notes.jsonl:paper_id=P0182#key_results[0]"}], "allowed_bibkeys_chapter": ["Cheng2025Your", "Dong2025Bench", "Du2024Anytool", "Feng2025Group", "Fumero2025Cybersleuth", "Gasmi2025Bridging", "Ghose2025Orfs", "Jia2025Autotool", "Kim2025Bridging", "Li2024Personal", "Li2024Stride", "Li2025Agentswift", "Li2025Dissonances", "Li2025From", "Liu2025Mcpagentbench", "Liu2025Toolscope", "Lumer2025Memtool", "Luo2025Large", "Mohammadi2025Evaluation", "Qiu2025Locobench", "Shen2024Small", "Song2026Envscaler", "Van2025Survey", "Wu2025Meta", "Xi2026Toolgym", "Xian2025Measuring", "Xu2025Exemplar", "Xuan2026Confidence", "Yao2022React", "You2025Datawiseagent", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2025Self"], "allowed_bibkeys_global": ["Liu2023Reason", "Yao2022React"], "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "182c2b61049d237fc6d0bc330e45b67f2ebf7d99", "bytes": 5989, "citations": ["Feng2025Group", "Fumero2025Cybersleuth", "Kim2025Bridging", "Li2025Agentswift", "Liu2025Mcpagentbench", "Qiu2025Locobench", "Xi2026Toolgym", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving"]}
{"kind": "h3", "id": "3.2", "title": "Tool interfaces and orchestration", "section_id": "3", "section_title": "Foundations & Interfaces", "path": "sections/S3_2.md", "allowed_bibkeys_mapped": ["Cheng2025Your", "Dong2025Bench", "Du2024Anytool", "Ghose2025Orfs", "Jia2025Autotool", "Li2024Personal", "Li2024Stride", "Li2025Dissonances", "Liu2025Mcpagentbench", "Liu2025Toolscope", "Lumer2025Memtool", "Mohammadi2025Evaluation", "Shen2024Small", "Van2025Survey", "Xian2025Measuring", "Xuan2026Confidence", "Yao2022React", "Zhou2025Self"], "allowed_bibkeys_selected": ["Dong2025Bench", "Du2024Anytool", "Jia2025Autotool", "Li2025Dissonances", "Liu2025Toolscope", "Lumer2025Memtool", "Mohammadi2025Evaluation", "Zhou2025Self"], "evidence_ids": ["E-P0220-32b9aafe57", "E-P0035-2e6956a116", "E-P0028-f8def223dc", "E-P0088-fae121f81b", "E-P0220-468a77ff1d", "E-P0220-cb30da23cb", "E-P0029-35271418ac", "E-P0029-38dc800de9", "E-P0108-d5c234444e", "E-P0164-37f9ea924c", "E-P0070-d3344c79dc", "E-P0108-e046416ff1"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["@Lumer2025Memtool"], "paper_id": "P0029", "evidence_id": "E-P0029-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["@Li2025Dissonances"], "paper_id": "P0088", "evidence_id": "E-P0088-fae121f81b", "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"hook_type": "quant", "text": "MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control.", "citations": ["@Lumer2025Memtool"], "paper_id": "P0029", "evidence_id": "E-P0029-38dc800de9", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[1]"}, {"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["@Zhou2025Self"], "paper_id": "P0035", "evidence_id": "E-P0035-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0035#key_results[0]"}, {"hook_type": "quant", "text": "To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing tool", "citations": ["@Liu2025Toolscope"], "paper_id": "P0220", "evidence_id": "E-P0220-32b9aafe57", "pointer": "papers/paper_notes.jsonl:paper_id=P0220#method"}, {"hook_type": "quant", "text": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "citations": ["@Liu2025Toolscope"], "paper_id": "P0220", "evidence_id": "E-P0220-468a77ff1d", "pointer": "papers/paper_notes.jsonl:paper_id=P0220#key_results[0]"}, {"hook_type": "quant", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["@Du2024Anytool"], "paper_id": "P0108", "evidence_id": "E-P0108-d5c234444e", "pointer": "papers/paper_notes.jsonl:paper_id=P0108#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["@Mohammadi2025Evaluation"], "paper_id": "P0164", "evidence_id": "E-P0164-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0164#key_results[0]"}], "allowed_bibkeys_chapter": ["Cheng2025Your", "Dong2025Bench", "Du2024Anytool", "Feng2025Group", "Fumero2025Cybersleuth", "Gasmi2025Bridging", "Ghose2025Orfs", "Jia2025Autotool", "Kim2025Bridging", "Li2024Personal", "Li2024Stride", "Li2025Agentswift", "Li2025Dissonances", "Li2025From", "Liu2025Mcpagentbench", "Liu2025Toolscope", "Lumer2025Memtool", "Luo2025Large", "Mohammadi2025Evaluation", "Qiu2025Locobench", "Shen2024Small", "Song2026Envscaler", "Van2025Survey", "Wu2025Meta", "Xi2026Toolgym", "Xian2025Measuring", "Xu2025Exemplar", "Xuan2026Confidence", "Yao2022React", "You2025Datawiseagent", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2025Self"], "allowed_bibkeys_global": ["Liu2023Reason", "Yao2022React"], "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "537f1fd6857d4e97ccbc391f4d01fb24f00ef708", "bytes": 5657, "citations": ["Dong2025Bench", "Du2024Anytool", "Jia2025Autotool", "Li2025Dissonances", "Liu2025Toolscope", "Lumer2025Memtool", "Mohammadi2025Evaluation", "Zhou2025Self"]}
{"kind": "h3", "id": "4.1", "title": "Planning and reasoning loops", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "path": "sections/S4_1.md", "allowed_bibkeys_mapped": ["Dagan2024Plancraft", "Du2025Memr", "Hatalis2025Review", "Hong2025Planning", "Hu2025Evaluating", "Hu2025Training", "Huang2025Surgical", "Kim2025Bridging", "Kiruluta2025Novel", "Li2025Draft", "Liu2025Costbench", "Lu2025Pilotrl", "Nakano2025Guided", "Seo2025Simuhome", "Silva2025Agents", "Yang2025Coarse", "Yao2022React", "Zhou2025Reasoning"], "allowed_bibkeys_selected": ["Du2025Memr", "Hong2025Planning", "Hu2025Training", "Kim2025Bridging", "Kiruluta2025Novel", "Liu2025Costbench", "Nakano2025Guided", "Seo2025Simuhome", "Yao2022React", "Zhou2025Reasoning"], "evidence_ids": ["E-P0039-f4d80ca542", "E-P0084-99359acdd7", "E-P0212-e2e3b7fa97", "E-P0013-1ad09376fe", "E-P0011-04c60086db", "E-P0184-1b99de5317", "E-P0034-c17bcfb7d4", "E-P0039-771620f84f", "E-P0077-32d61c8fae", "E-P0001-ca4a00b5cf", "E-P0060-a666552417", "E-P0060-c7423180cf"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and", "citations": ["@Liu2025Costbench"], "paper_id": "P0077", "evidence_id": "E-P0077-32d61c8fae", "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["@Hu2025Training"], "paper_id": "P0039", "evidence_id": "E-P0039-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0039#key_results[0]"}, {"hook_type": "quant", "text": "Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design", "citations": ["@Kim2025Bridging"], "paper_id": "P0011", "evidence_id": "E-P0011-04c60086db", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}, {"hook_type": "quant", "text": "Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries.", "citations": ["@Nakano2025Guided"], "paper_id": "P0084", "evidence_id": "E-P0084-99359acdd7", "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models.", "citations": ["@Seo2025Simuhome"], "paper_id": "P0212", "evidence_id": "E-P0212-e2e3b7fa97", "pointer": "papers/paper_notes.jsonl:paper_id=P0212#limitations[1]"}, {"hook_type": "quant", "text": "Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design p", "citations": ["@Kim2025Bridging"], "paper_id": "P0011", "evidence_id": "E-P0011-04c60086db", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}, {"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap track", "citations": ["@Du2025Memr"], "paper_id": "P0184", "evidence_id": "E-P0184-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}, {"hook_type": "quant", "text": "It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters.", "citations": ["@Zhou2025Reasoning"], "paper_id": "P0034", "evidence_id": "E-P0034-c17bcfb7d4", "pointer": "papers/paper_notes.jsonl:paper_id=P0034#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["@Hu2025Training"], "paper_id": "P0039", "evidence_id": "E-P0039-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0039#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and p", "citations": ["@Liu2025Costbench"], "paper_id": "P0077", "evidence_id": "E-P0077-32d61c8fae", "pointer": "papers/paper_notes.jsonl:paper_id=P0077#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["@Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "allowed_bibkeys_chapter": ["Abbineni2025Muallm", "Anokhin2024Arigraph", "Dagan2024Plancraft", "Du2025Memr", "Hatalis2025Review", "Hong2025Planning", "Hu2025Evaluating", "Hu2025Training", "Huang2025Surgical", "Kim2025Bridging", "Kiruluta2025Novel", "Li2025Draft", "Li2025Graphcodeagent", "Liu2023Reason", "Liu2025Costbench", "Lu2025Pilotrl", "Nakano2025Guided", "Seo2025Simuhome", "Silva2025Agents", "Tao2026Membox", "Tawosi2025Meta", "Wei2025Memory", "Wu2025Meta", "Xia2025From", "Xu2025Agentic", "Yang2025Coarse", "Yao2022React", "Ye2025Task", "Ye2025Taska", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large", "Zhou2025Reasoning"], "allowed_bibkeys_global": ["Liu2023Reason", "Yao2022React"], "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "01bcb6b88bd16fd1ddaecc9f91b14b57cb445972", "bytes": 5649, "citations": ["Du2025Memr", "Hong2025Planning", "Hu2025Training", "Kim2025Bridging", "Kiruluta2025Novel", "Liu2025Costbench", "Nakano2025Guided", "Seo2025Simuhome", "Yao2022React", "Zhou2025Reasoning"]}
{"kind": "h3", "id": "4.2", "title": "Memory and retrieval (RAG)", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "path": "sections/S4_2.md", "allowed_bibkeys_mapped": ["Abbineni2025Muallm", "Anokhin2024Arigraph", "Du2025Memr", "Hu2025Evaluating", "Li2025Graphcodeagent", "Liu2023Reason", "Tao2026Membox", "Tawosi2025Meta", "Wei2025Memory", "Wu2025Meta", "Xia2025From", "Xu2025Agentic", "Yao2022React", "Ye2025Task", "Ye2025Taska", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large"], "allowed_bibkeys_selected": ["Abbineni2025Muallm", "Anokhin2024Arigraph", "Du2025Memr", "Tao2026Membox", "Tawosi2025Meta", "Wei2025Memory", "Yao2022React", "Ye2025Task"], "evidence_ids": ["E-P0045-43287d5458", "E-P0184-1b99de5317", "E-P0080-f1ec9700e7", "E-P0031-f36b515991", "E-P0184-497b158080", "E-P0187-e294aeefb5", "E-P0080-06e45507a7", "E-P0001-ca4a00b5cf", "E-P0014-3bf4dab38c", "E-P0031-e3ebb83eb2", "E-P0128-d8feedefd0", "E-P0187-4d40260657"], "anchor_facts": [{"hook_type": "quant", "text": "We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets.", "citations": ["@Wei2025Memory"], "paper_id": "P0080", "evidence_id": "E-P0080-06e45507a7", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#key_results[0]"}, {"hook_type": "quant", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["@Tawosi2025Meta"], "paper_id": "P0031", "evidence_id": "E-P0031-f36b515991", "pointer": "papers/paper_notes.jsonl:paper_id=P0031#key_results[1]"}, {"hook_type": "quant", "text": "Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,", "citations": ["@Ye2025Task"], "paper_id": "P0014", "evidence_id": "E-P0014-3bf4dab38c", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "quant", "text": "From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap track", "citations": ["@Du2025Memr"], "paper_id": "P0184", "evidence_id": "E-P0184-1b99de5317", "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[1]"}, {"hook_type": "limitation", "text": "In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution,", "citations": ["@Wei2025Memory"], "paper_id": "P0080", "evidence_id": "E-P0080-f1ec9700e7", "pointer": "papers/paper_notes.jsonl:paper_id=P0080#limitations[1]"}, {"hook_type": "quant", "text": "Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend", "citations": ["@Du2025Memr"], "paper_id": "P0184", "evidence_id": "E-P0184-497b158080", "pointer": "papers/paper_notes.jsonl:paper_id=P0184#key_results[0]"}, {"hook_type": "quant", "text": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "citations": ["@Abbineni2025Muallm"], "paper_id": "P0187", "evidence_id": "E-P0187-e294aeefb5", "pointer": "papers/paper_notes.jsonl:paper_id=P0187#key_results[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["@Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperf", "citations": ["@Ye2025Task"], "paper_id": "P0014", "evidence_id": "E-P0014-3bf4dab38c", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}, {"hook_type": "quant", "text": "Meta-RAG scores 84.67 % and 53.0 % for file-level and function-level correct localization rates, respectively, achieving state-of-the-art performance.", "citations": ["@Tawosi2025Meta"], "paper_id": "P0031", "evidence_id": "E-P0031-e3ebb83eb2", "pointer": "papers/paper_notes.jsonl:paper_id=P0031#key_results[0]"}], "allowed_bibkeys_chapter": ["Abbineni2025Muallm", "Anokhin2024Arigraph", "Dagan2024Plancraft", "Du2025Memr", "Hatalis2025Review", "Hong2025Planning", "Hu2025Evaluating", "Hu2025Training", "Huang2025Surgical", "Kim2025Bridging", "Kiruluta2025Novel", "Li2025Draft", "Li2025Graphcodeagent", "Liu2023Reason", "Liu2025Costbench", "Lu2025Pilotrl", "Nakano2025Guided", "Seo2025Simuhome", "Silva2025Agents", "Tao2026Membox", "Tawosi2025Meta", "Wei2025Memory", "Wu2025Meta", "Xia2025From", "Xu2025Agentic", "Yang2025Coarse", "Yao2022React", "Ye2025Task", "Ye2025Taska", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large", "Zhou2025Reasoning"], "allowed_bibkeys_global": ["Liu2023Reason", "Yao2022React"], "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "a8b08d8c0ba058e7181cde2f655b73fc218c69e5", "bytes": 5365, "citations": ["Abbineni2025Muallm", "Anokhin2024Arigraph", "Du2025Memr", "Tao2026Membox", "Tawosi2025Meta", "Wei2025Memory", "Yao2022React", "Ye2025Task"]}
{"kind": "h3", "id": "5.1", "title": "Self-improvement and adaptation", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "path": "sections/S5_1.md", "allowed_bibkeys_mapped": ["Belle2025Agents", "Chen2025Grounded", "Du2024Anytool", "He2025Enabling", "Nitin2025Faultline", "Sarukkai2025Context", "Schneider2025Learning", "Van2025Survey", "Wang2025Ragen", "Wang2025Swarms", "Wei2025Memory", "Wu2025Evolver", "Xia2025Sand", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2024Archer", "Zhou2025Self"], "allowed_bibkeys_selected": ["Belle2025Agents", "Nitin2025Faultline", "Van2025Survey", "Wei2025Memory", "Wu2025Evolver", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2024Archer", "Zhou2025Self"], "evidence_ids": ["E-P0035-e872c22cb6", "E-P0138-1063eee7ce", "E-P0061-7acf4de689", "E-P0035-2e6956a116", "E-P0138-5f246d0ca8", "E-P0081-0c10233369", "E-P0001-ca4a00b5cf", "E-P0020-7929ef7a56", "E-P0044-faa3d4c9ee", "E-P0055-60cc0d458f", "E-P0080-06e45507a7", "E-P0082-74188ef933"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["@Zhou2025Self"], "paper_id": "P0035", "evidence_id": "E-P0035-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0035#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["@Zhang2026Evoroute"], "paper_id": "P0055", "evidence_id": "E-P0055-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}, {"hook_type": "quant", "text": "This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively", "citations": ["@Wu2025Evolver"], "paper_id": "P0081", "evidence_id": "E-P0081-0c10233369", "pointer": "papers/paper_notes.jsonl:paper_id=P0081#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["@Zhao2025Achieving"], "paper_id": "P0138", "evidence_id": "E-P0138-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0138#key_results[0]"}, {"hook_type": "limitation", "text": "We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion.", "citations": ["@Van2025Survey"], "paper_id": "P0061", "evidence_id": "E-P0061-7acf4de689", "pointer": "papers/paper_notes.jsonl:paper_id=P0061#limitations[1]"}, {"hook_type": "quant", "text": "Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-l", "citations": ["@Zhao2025Achieving"], "paper_id": "P0138", "evidence_id": "E-P0138-5f246d0ca8", "pointer": "papers/paper_notes.jsonl:paper_id=P0138#key_results[1]"}, {"hook_type": "quant", "text": "This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively re", "citations": ["@Wu2025Evolver"], "paper_id": "P0081", "evidence_id": "E-P0081-0c10233369", "pointer": "papers/paper_notes.jsonl:paper_id=P0081#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["@Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "In controlled Catanatron experiments, HexMachina learns from scratch and evolves players that outperform the strongest human-crafted baseline (AlphaBeta), achieving a 54% win rate and surpassing prompt-driven and no-discovery baselines.", "citations": ["@Belle2025Agents"], "paper_id": "P0020", "evidence_id": "E-P0020-7929ef7a56", "pointer": "papers/paper_notes.jsonl:paper_id=P0020#key_results[0]"}, {"hook_type": "quant", "text": "Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).", "citations": ["@Zhou2024Archer"], "paper_id": "P0044", "evidence_id": "E-P0044-faa3d4c9ee", "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}], "allowed_bibkeys_chapter": ["Belle2025Agents", "Cao2025Skyrl", "Chang2025Alas", "Chen2025Grounded", "Chuang2025Debate", "Du2024Anytool", "Hao2025Multi", "Hassouna2024Agent", "He2025Enabling", "Li2025Draft", "Liu2025Aligning", "Lumer2025Memtool", "Nitin2025Faultline", "Papadakis2025Atlas", "Sarkar2025Survey", "Sarukkai2025Context", "Schneider2025Learning", "Shen2024Small", "Silva2025Agents", "Van2025Survey", "Wang2023Voyager", "Wang2025Ragen", "Wang2025Swarms", "Wei2025Memory", "Wu2025Agents", "Wu2025Evolver", "Xia2025Sand", "Yao2022React", "Yim2024Evaluating", "Zahedifar2025Agent", "Zhang2025Cognitive", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2024Archer", "Zhou2025Self"], "allowed_bibkeys_global": ["Liu2023Reason", "Yao2022React"], "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "3efdfabafc9c84035ad86d791b289f48c7da37f9", "bytes": 5683, "citations": ["Belle2025Agents", "Nitin2025Faultline", "Van2025Survey", "Wei2025Memory", "Wu2025Evolver", "Yao2022React", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2024Archer", "Zhou2025Self"]}
{"kind": "h3", "id": "5.2", "title": "Multi-agent coordination", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "path": "sections/S5_2.md", "allowed_bibkeys_mapped": ["Cao2025Skyrl", "Chang2025Alas", "Chuang2025Debate", "Hao2025Multi", "Hassouna2024Agent", "Li2025Draft", "Liu2025Aligning", "Lumer2025Memtool", "Papadakis2025Atlas", "Sarkar2025Survey", "Shen2024Small", "Silva2025Agents", "Wang2023Voyager", "Wu2025Agents", "Yim2024Evaluating", "Zahedifar2025Agent", "Zhang2025Cognitive", "Zhou2024Archer"], "allowed_bibkeys_selected": ["Cao2025Skyrl", "Chuang2025Debate", "Lumer2025Memtool", "Sarkar2025Survey", "Shen2024Small", "Wang2023Voyager", "Yim2024Evaluating", "Zahedifar2025Agent", "Zhou2024Archer"], "evidence_ids": ["E-P0037-2c281ce5fb", "E-P0005-506120a6cd", "E-P0050-c92ed293ba", "E-P0029-35271418ac", "E-P0029-38dc800de9", "E-P0036-5ed988eb67", "E-P0157-3acffd03b4", "E-P0005-b2eceb5b30", "E-P0036-3af1ce8090", "E-P0044-faa3d4c9ee", "E-P0087-ebcb5ebf6c", "E-P0046-95dc2415e7"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["@Lumer2025Memtool"], "paper_id": "P0029", "evidence_id": "E-P0029-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]"}, {"hook_type": "quant", "text": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and", "citations": ["@Cao2025Skyrl"], "paper_id": "P0036", "evidence_id": "E-P0036-5ed988eb67", "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[1]"}, {"hook_type": "quant", "text": "DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions.", "citations": ["@Chuang2025Debate"], "paper_id": "P0157", "evidence_id": "E-P0157-3acffd03b4", "pointer": "papers/paper_notes.jsonl:paper_id=P0157#key_results[0]"}, {"hook_type": "quant", "text": "MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control.", "citations": ["@Lumer2025Memtool"], "paper_id": "P0029", "evidence_id": "E-P0029-38dc800de9", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[1]"}, {"hook_type": "quant", "text": "Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning.", "citations": ["@Cao2025Skyrl"], "paper_id": "P0036", "evidence_id": "E-P0036-3af1ce8090", "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]"}, {"hook_type": "eval", "text": "This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP).", "citations": ["@Sarkar2025Survey"], "paper_id": "P0037", "evidence_id": "E-P0037-2c281ce5fb", "pointer": "papers/paper_notes.jsonl:paper_id=P0037#method"}, {"hook_type": "quant", "text": "Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, ex", "citations": ["@Wang2023Voyager"], "paper_id": "P0005", "evidence_id": "E-P0005-506120a6cd", "pointer": "papers/paper_notes.jsonl:paper_id=P0005#key_results[1]"}, {"hook_type": "quant", "text": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve", "citations": ["@Cao2025Skyrl"], "paper_id": "P0036", "evidence_id": "E-P0036-5ed988eb67", "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[1]"}, {"hook_type": "quant", "text": "It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA.", "citations": ["@Wang2023Voyager"], "paper_id": "P0005", "evidence_id": "E-P0005-b2eceb5b30", "pointer": "papers/paper_notes.jsonl:paper_id=P0005#key_results[0]"}, {"hook_type": "quant", "text": "Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).", "citations": ["@Zhou2024Archer"], "paper_id": "P0044", "evidence_id": "E-P0044-faa3d4c9ee", "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[0]"}], "allowed_bibkeys_chapter": ["Belle2025Agents", "Cao2025Skyrl", "Chang2025Alas", "Chen2025Grounded", "Chuang2025Debate", "Du2024Anytool", "Hao2025Multi", "Hassouna2024Agent", "He2025Enabling", "Li2025Draft", "Liu2025Aligning", "Lumer2025Memtool", "Nitin2025Faultline", "Papadakis2025Atlas", "Sarkar2025Survey", "Sarukkai2025Context", "Schneider2025Learning", "Shen2024Small", "Silva2025Agents", "Van2025Survey", "Wang2023Voyager", "Wang2025Ragen", "Wang2025Swarms", "Wei2025Memory", "Wu2025Agents", "Wu2025Evolver", "Xia2025Sand", "Yao2022React", "Yim2024Evaluating", "Zahedifar2025Agent", "Zhang2025Cognitive", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2024Archer", "Zhou2025Self"], "allowed_bibkeys_global": ["Liu2023Reason", "Yao2022React"], "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "00c750793aab39a8f0b5de37abb2658d4b4682a9", "bytes": 5544, "citations": ["Cao2025Skyrl", "Chuang2025Debate", "Lumer2025Memtool", "Sarkar2025Survey", "Shen2024Small", "Wang2023Voyager", "Yim2024Evaluating", "Zahedifar2025Agent", "Zhou2024Archer"]}
{"kind": "h3", "id": "6.1", "title": "Benchmarks and evaluation protocols", "section_id": "6", "section_title": "Evaluation & Risks", "path": "sections/S6_1.md", "allowed_bibkeys_mapped": ["Chen2025Towards", "Dagan2024Plancraft", "Fu2025Eval", "He2025Plan", "Ji2025Taxonomy", "Kale2025Reliable", "Kim2026Beyond", "Li2024Personal", "Li2026Autonomous", "Lidayan2025Abbel", "Liu2023Reason", "Mohammadi2025Evaluation", "Shang2024Agentsquare", "Shao2025Craken", "Shi2025Progent", "Zhan2025Sentinel", "Zhang2025Security", "Zhu2025Where"], "allowed_bibkeys_selected": ["Chen2025Towards", "Fu2025Eval", "Kale2025Reliable", "Kim2026Beyond", "Li2026Autonomous", "Mohammadi2025Evaluation", "Shang2024Agentsquare", "Shi2025Progent", "Zhang2025Security"], "evidence_ids": ["E-P0089-8bcb673a7d", "E-P0164-37f9ea924c", "E-P0032-6829c8b583", "E-P0043-38a26e4777", "E-P0089-d6095e10e9", "E-P0089-7a6ec4daed", "E-P0199-753416ce70", "E-P0203-e0345118bc", "E-P0054-79f88927fa", "E-P0032-68db58914f", "E-P0052-9980bf7642", "E-P0102-4fc221fdea"], "anchor_facts": [{"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["@Fu2025Eval"], "paper_id": "P0199", "evidence_id": "E-P0199-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[1]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["@Mohammadi2025Evaluation"], "paper_id": "P0164", "evidence_id": "E-P0164-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0164#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["@Shi2025Progent"], "paper_id": "P0032", "evidence_id": "E-P0032-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#method"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["@Mohammadi2025Evaluation"], "paper_id": "P0164", "evidence_id": "E-P0164-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0164#key_results[0]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["@Shang2024Agentsquare"], "paper_id": "P0043", "evidence_id": "E-P0043-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0043#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[1]"}, {"hook_type": "quant", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Ar", "citations": ["@Kale2025Reliable"], "paper_id": "P0203", "evidence_id": "E-P0203-e0345118bc", "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[0]"}, {"hook_type": "quant", "text": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["@Kim2026Beyond"], "paper_id": "P0054", "evidence_id": "E-P0054-79f88927fa", "pointer": "papers/paper_notes.jsonl:paper_id=P0054#key_results[0]"}], "allowed_bibkeys_chapter": ["An2025Ipiguard", "Bonagiri2025Check", "Chen2025Towards", "Dagan2024Plancraft", "Fu2025Eval", "Gasmi2025Bridging", "Hadeliya2025When", "He2025Plan", "Ji2025Taxonomy", "Kale2025Reliable", "Kim2026Beyond", "Lee2025Bench", "Li2024Personal", "Li2025Stac", "Li2026Autonomous", "Lichkovski2025Agent", "Lidayan2025Abbel", "Liu2023Reason", "Liu2026Agents", "Luo2025Agrail", "Mohammadi2025Evaluation", "Plaat2025Agentic", "Rosario2025Architecting", "Sha2025Agent", "Shang2024Agentsquare", "Shao2025Craken", "Shi2025Progent", "Zhan2025Sentinel", "Zhang2025Security", "Zhou2025Reasoning", "Zhu2025Where"], "allowed_bibkeys_global": ["Liu2023Reason", "Yao2022React"], "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "57f57620d9bcceea7a05b895017d1fe59df7afac", "bytes": 5582, "citations": ["Chen2025Towards", "Fu2025Eval", "Kale2025Reliable", "Kim2026Beyond", "Li2026Autonomous", "Mohammadi2025Evaluation", "Shang2024Agentsquare", "Shi2025Progent", "Zhang2025Security"]}
{"kind": "h3", "id": "6.2", "title": "Safety, security, and governance", "section_id": "6", "section_title": "Evaluation & Risks", "path": "sections/S6_2.md", "allowed_bibkeys_mapped": ["An2025Ipiguard", "Bonagiri2025Check", "Fu2025Eval", "Gasmi2025Bridging", "Hadeliya2025When", "Kale2025Reliable", "Lee2025Bench", "Li2025Stac", "Lichkovski2025Agent", "Liu2023Reason", "Liu2026Agents", "Luo2025Agrail", "Plaat2025Agentic", "Rosario2025Architecting", "Sha2025Agent", "Shi2025Progent", "Zhang2025Security", "Zhou2025Reasoning"], "allowed_bibkeys_selected": ["Bonagiri2025Check", "Fu2025Eval", "Kale2025Reliable", "Lee2025Bench", "Li2025Stac", "Lichkovski2025Agent", "Shi2025Progent", "Zhang2025Security"], "evidence_ids": ["E-P0089-8bcb673a7d", "E-P0089-d6095e10e9", "E-P0161-a256400826", "E-P0203-e0345118bc", "E-P0199-753416ce70", "E-P0089-7a6ec4daed", "E-P0032-68db58914f", "E-P0205-32438dad15", "E-P0209-82fdf6d15e", "E-P0199-2895472ae1", "E-P0205-0a269c6135", "E-P0075-7edb91824f"], "anchor_facts": [{"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["@Shi2025Progent"], "paper_id": "P0032", "evidence_id": "E-P0032-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[1]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#method"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["@Zhang2025Security"], "paper_id": "P0089", "evidence_id": "E-P0089-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}, {"hook_type": "quant", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Ar", "citations": ["@Kale2025Reliable"], "paper_id": "P0203", "evidence_id": "E-P0203-e0345118bc", "pointer": "papers/paper_notes.jsonl:paper_id=P0203#key_results[0]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["@Fu2025Eval"], "paper_id": "P0199", "evidence_id": "E-P0199-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[1]"}, {"hook_type": "quant", "text": "A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset.", "citations": ["@Lee2025Bench"], "paper_id": "P0205", "evidence_id": "E-P0205-32438dad15", "pointer": "papers/paper_notes.jsonl:paper_id=P0205#key_results[0]"}, {"hook_type": "quant", "text": "Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases.", "citations": ["@Li2025Stac"], "paper_id": "P0209", "evidence_id": "E-P0209-82fdf6d15e", "pointer": "papers/paper_notes.jsonl:paper_id=P0209#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["@Fu2025Eval"], "paper_id": "P0199", "evidence_id": "E-P0199-2895472ae1", "pointer": "papers/paper_notes.jsonl:paper_id=P0199#key_results[0]"}], "allowed_bibkeys_chapter": ["An2025Ipiguard", "Bonagiri2025Check", "Chen2025Towards", "Dagan2024Plancraft", "Fu2025Eval", "Gasmi2025Bridging", "Hadeliya2025When", "He2025Plan", "Ji2025Taxonomy", "Kale2025Reliable", "Kim2026Beyond", "Lee2025Bench", "Li2024Personal", "Li2025Stac", "Li2026Autonomous", "Lichkovski2025Agent", "Lidayan2025Abbel", "Liu2023Reason", "Liu2026Agents", "Luo2025Agrail", "Mohammadi2025Evaluation", "Plaat2025Agentic", "Rosario2025Architecting", "Sha2025Agent", "Shang2024Agentsquare", "Shao2025Craken", "Shi2025Progent", "Zhan2025Sentinel", "Zhang2025Security", "Zhou2025Reasoning", "Zhu2025Where"], "allowed_bibkeys_global": ["Liu2023Reason", "Yao2022React"], "generated_at": "2026-01-18T03:48:20", "exists": true, "sha1": "63ffbe2d00cb0ab1cfa156f49aadc76ff98f1a81", "bytes": 5725, "citations": ["Bonagiri2025Check", "Fu2025Eval", "Kale2025Reliable", "Lee2025Bench", "Li2025Stac", "Lichkovski2025Agent", "Shi2025Progent", "Zhang2025Security"]}
