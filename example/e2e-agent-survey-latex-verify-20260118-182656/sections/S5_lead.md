Adaptation mechanisms and multi-agent coordination are often introduced as ways to reduce brittleness: agents can critique and improve themselves over time, or multiple agents can divide labor and cross-check outputs. However, these mechanisms also introduce new failure modes—reward hacking, instability, and coordination overhead—that only become visible under long-horizon evaluation protocols [@Zhou2025Self; @Sarkar2025Survey].

This chapter frames the landscape through two questions: how agents change with experience (reflection, optimization, or learned controllers), and how they coordinate (roles, protocols, aggregation). In both cases, the most informative comparisons connect mechanism choices back to measurable outcomes under a stated benchmark or metric [@Zhou2024Archer; @Chuang2025Debate].
