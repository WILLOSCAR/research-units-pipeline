Memory and retrieval determine what an agent treats as state over time, which is why many “agent failures” are memory failures: missing context, stale summaries, or ungrounded assumptions. A central tension is that richer memory improves grounding in multi-turn settings, whereas aggressive compression and retrieval heuristics can introduce drift that is hard to detect under coarse evaluation metrics [@Wei2025Memory; @Yao2022React]. This suggests that memory mechanisms should be evaluated with protocols that stress long-horizon consistency rather than only single-turn accuracy.

A practical way to organize memory work is by the unit of storage and the unit of retrieval. Some systems store free-form text summaries; others store structured traces, graphs, or tool call records that can be queried more deterministically. Memory-box style systems make the “state representation” explicit by treating memory modules as composable components that can be swapped and benchmarked across tasks [@Tao2026Membox]. In contrast, more implicit memory can look simpler to implement but becomes difficult to audit when failures occur.

Structured memories also enable different evaluation lenses. AriGraph (2024) uses a graph-based representation that can support targeted retrieval and relational reasoning, which can change the failure modes compared to purely textual retrieval [@Anokhin2024Arigraph]. Moreover, task-specific memory policies often hinge on what the environment exposes (tool logs, web pages, code execution traces), which is why “memory” in agents is not identical to classic RAG in static QA settings [@Ye2025Task].

Recent comparative studies make the protocol issue concrete. Wei et al. (2025) report implementing over ten representative memory modules and evaluating them across 10 diverse multi-turn, goal-oriented tasks, which helps isolate what memory contributes under a shared metric definition [@Wei2025Memory]. However, even such studies can be sensitive to evaluation details: how success is scored, how long context is truncated, and whether external tools are available.

In contrast to hand-designed memory modules, meta-level approaches emphasize learning or adapting retrieval policies. These routes can reduce manual engineering, but they also risk overfitting to benchmark artifacts or amplifying spurious correlations when memory content is noisy [@Tawosi2025Meta; @Abbineni2025Muallm]. Therefore, when comparing memory systems, it is useful to separate (i) what is stored, (ii) how it is retrieved, and (iii) how retrieval decisions are verified.

A key limitation is that memory introduces new security and robustness concerns: poisoning, prompt injection via stored content, and subtle state drift that only manifests many steps later. Without explicit provenance and verification hooks, a memory system can accumulate errors that look like “planning mistakes” downstream. This motivates evaluation protocols that include stress tests for long-horizon consistency and adversarial memory content, rather than treating memory as a purely helpful add-on [@Du2025Memr; @Ye2025Task].

A complementary viewpoint is to treat retrieval as an autonomous subsystem with its own objectives and failure modes. MemR$^3$ frames memory retrieval as an agent-like process, which highlights that retrieval is not only about finding relevant text but also about deciding what evidence is trustworthy and when to refresh state [@Du2025Memr]. In contrast, graph-based representations such as AriGraph make state relational and queryable, which can reduce some hallucination modes but introduces its own brittleness when the graph construction is noisy or incomplete [@Anokhin2024Arigraph].

In practice, memory evaluation should go beyond average task success and measure stability: does the agent’s state drift across repeated runs, does retrieval amplify spurious correlations, and does summarization silently erase constraints needed for safe tool use. Comparative evaluations across 10 multi-turn tasks and many memory modules are helpful because they expose which improvements are consistent and which are benchmark-specific [@Wei2025Memory]. Still, a limitation is that long-horizon memory failures often look like planning failures; future protocols should include explicit memory stress tests (poisoned entries, contradictory evidence, and partial observability) so that memory and planning contributions can be disentangled [@Tao2026Membox; @Ye2025Task].

A useful reporting standard is to make memory operations first-class in the trace: what was retrieved, why it was retrieved (query), and how it changed the next action. Without such traces, it is hard to tell whether improved success comes from better state modeling or from accidental leakage (e.g., storing answers in memory) under a lax protocol. Modular frameworks like MemBox make it easier to swap storage/retrieval policies under identical tasks, while graph-based representations make state relational and queryable; both directions strengthen the case for treating memory as an auditable subsystem rather than a “black box context” [@Tao2026Membox; @Anokhin2024Arigraph]. Even in abstract-first evidence, surveys can insist on this traceability contract: memory should be inspectable and verifiable, not merely available [@Du2025Memr; @Wei2025Memory].
