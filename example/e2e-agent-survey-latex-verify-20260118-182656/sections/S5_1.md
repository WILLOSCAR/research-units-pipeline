Self-improvement mechanisms aim to make agents less brittle by letting them adapt their behavior based on feedback, critique, or learned optimization signals. A central tension is that adaptation can improve robustness on multi-turn tasks, whereas it can also induce drift and reward hacking that is invisible under short-horizon metrics [@Zhou2025Self; @Van2025Survey]. This motivates evaluating adaptation as a protocol-aware system change, not as a cosmetic prompt tweak.

One common pattern is “self-challenging” or critique loops that modify the agent’s plan or tool-use policy after failures. Zhou et al. (2025) evaluate a self-challenging framework on two multi-turn tool-use agent benchmarks (M3ToolEval and TauBench), emphasizing that improvements should be measured under fixed task suites and scoring rules rather than via cherry-picked examples [@Zhou2025Self]. Moreover, such loops often change when the agent decides to verify or to stop, which can alter cost and failure modes under a budgeted protocol.

Another pattern is to learn controllers that adapt actions based on environmental feedback. RL-style approaches for agents can formalize adaptation objectives, but they inherit classical pitfalls: proxy objectives, instability, and sensitivity to evaluation distribution shift. Archer-style training (2024) illustrates this route by explicitly optimizing agent behavior rather than relying purely on prompt engineering, which can improve repeatability when protocols are well specified [@Zhou2024Archer; @Zhao2025Achieving]. In contrast, purely prompt-based adaptation may appear strong in demos but be hard to reproduce.

Adaptation also interacts with the rest of the agent stack. Systems that evolve or route behaviors over time can shift which planning and memory components are actually used, which makes ablations essential for interpretation [@Wu2025Evolver; @Zhang2026Evoroute]. Therefore, a survey synthesis should treat “adaptation” as a coordination problem across components: what changes, what remains fixed, and what evidence supports the claimed improvement.

Taken together, adaptation gains are most convincing when they come with protocol-level anchors: benchmark names, metrics, and budget conditions. It is also helpful when papers link adaptation back to a clear failure analysis (what went wrong before, what changes after), because otherwise improvements can be indistinguishable from overfitting to the evaluation harness [@Zhou2025Self; @Nitin2025Faultline]. This suggests that adaptation research should report both performance and stability metrics across repeated runs.

A key limitation is that self-improvement can amplify errors if the feedback loop is mis-specified. Even when adaptation improves average performance, it may increase tail risk (rare catastrophic failures) or degrade safety properties if verification is relaxed. This motivates coupling adaptation with memory and monitoring: tracking what changed, under what evidence, and how the agent’s policy can be rolled back when anomalies are detected [@Wei2025Memory; @Belle2025Agents; @Yao2022React].

Beyond critique loops, “adaptation” increasingly includes explicit policy evolution: systems that route between behaviors or evolve prompts/programs based on observed failures. Evolver-style approaches make this a first-class mechanism rather than an ad-hoc retry, but they also raise the bar for evaluation: the protocol must specify what feedback is available, how many iterations are allowed, and how overfitting is detected [@Wu2025Evolver]. Likewise, route-selection or evolution frameworks can change which tools are used and which failure modes appear, so it is essential that papers report not just final success but also how the policy changed over time [@Zhang2026Evoroute; @Zhao2025Achieving].

For safety and reliability, adaptation needs guardrails. A practical mitigation is to treat adaptations as versioned changes with explicit rollback and to couple improvements with verification policies that prevent “improving” by relaxing checks. This is especially important when adaptation interacts with memory: a self-improving agent can learn to exploit a memory shortcut that passes a benchmark but fails under distribution shift [@Wei2025Memory; @Nitin2025Faultline]. Therefore, even in abstract-first evidence, a cautious survey should frame adaptation claims as conditional on protocol details and stability evidence, not as unconditional progress [@Belle2025Agents; @Zhou2025Self].

Methodologically, adaptation papers would benefit from borrowing evaluation idioms from software changes: report a baseline, a delta (what changed), and a regression envelope (what got worse). If a self-challenging loop increases success, it is also important to report whether it increases tool-call count, latency, or failure severity under the same budget, and whether gains persist across benchmark variants rather than only the training-like distribution [@Zhou2025Self; @Nitin2025Faultline]. Evolutionary or routing-based adaptation can be framed as a controlled search over policies; without clear limits on iterations and explicit overfitting checks, it becomes hard to separate genuine robustness from benchmark-specific tuning [@Wu2025Evolver; @Zhang2026Evoroute].

Even lightweight audits help: track which tools and verification steps are used before and after adaptation, and report whether “improvements” come from better decisions or from relaxed checks. This makes adaptation results easier to reuse across settings and reduces the chance that protocol drift is mistaken for capability gains [@Zhou2025Self; @Zhao2025Achieving].
