# LLM agents survey (tool-use, planning, memory, multi-agent)

## Abstract

Tool-using LLM agents are best understood as closed-loop systems: they maintain an internal state, decide on actions, interact with tools or environments, and incorporate observations back into subsequent decisions. In practice, seemingly small choices about the loop boundary and action representation can dominate reliability, cost, and safety outcomes, which is why agent papers increasingly read like systems papers rather than pure model papers [@Yao2022React; @Schick2023Toolformer].

This survey organizes the recent agent literature around four concrete levers: (i) interfaces and action spaces, (ii) planning and memory as reusable components, (iii) adaptation and multi-agent coordination, and (iv) evaluation and risk. Throughout, we emphasize interface contracts and evaluation protocols as the “execution layer” that makes claims comparable across works, and we highlight where benchmarks and threat models remain misaligned with deployment realities [@Mohammadi2025Evaluation; @Zhang2025Security].

## Introduction

Agentic use of large language models has moved from a curiosity to a deployment pattern: systems increasingly combine an LLM with tools, memory, and execution scaffolding to solve tasks that are too long-horizon or too environment-dependent for one-shot generation. In this setting, the central question is no longer “can a model produce the right text,” but “can a looped system make and verify decisions under constraints” [@Yao2022React; @Schick2023Toolformer].

We use “agent” in the systems sense: a policy that observes state, chooses actions, and receives feedback through an environment or tool interface. This boundary is deliberately narrower than embodied robotics or classical RL; our focus is on tool-using LLM agents that operate via APIs, external resources, and structured tasks. As a result, concepts like action space, interface contract, and cost model become the right primitives for comparison, often more informative than model size alone [@Du2024Anytool; @Liu2025Toolscope].

Method note (evidence policy): this run is abstract-first, so we treat quantitative details as provisional unless a paper note records the full protocol. We still prefer protocol-aware comparisons—benchmarks, metrics, cost budgets, tool availability—because those are the conditions under which claims become reproducible and transferable [@Mohammadi2025Evaluation; @Fu2025Eval].

This survey contributes a paper-like map of the agent design space. We structure the literature around (i) loop boundaries and action spaces, (ii) tool interfaces and orchestration, (iii) planning and memory components, (iv) adaptation and multi-agent coordination, and (v) evaluation and risks. The goal is not to list papers, but to surface the tensions that determine engineering outcomes: reliability vs flexibility, deliberation vs cost, memory depth vs drift, and autonomy vs governance [@Zhang2025Security; @Kale2025Reliable].

## Related Work

The closest neighbors to tool-using LLM agents include work on tool use for language models, retrieval-augmented generation, planning/reasoning prompting, and evaluation of interactive systems. Tool use papers show that models can learn to call external functions and APIs, but they often abstract away the end-to-end loop details that dominate reliability in deployed agents [@Schick2023Toolformer; @Du2024Anytool].

Planning and reasoning methods provide the cognitive substrate for agents, yet conclusions depend heavily on protocol choices: budgets, tool access, reset policies, and what counts as success. Recent benchmark-driven studies illustrate that planning quality can look very different under cost-aware versus accuracy-only metrics, making evaluation design inseparable from algorithm design [@Liu2025Costbench; @Mohammadi2025Evaluation].

Memory and retrieval systems overlap with agents because many “agent failures” are state failures: missing context, stale summaries, or ungrounded assumptions. However, agent settings add additional constraints—tool calls, multi-step execution, and dynamic environments—that are not always captured by standard RAG evaluations [@Wei2025Memory; @Tao2026Membox].

A growing set of surveys and meta-analyses aim to systematize agent research. We build on these efforts but adopt an explicitly decision-oriented lens: rather than organizing by buzzwords, we organize by interface contracts, component roles (planning/memory/adaptation), and protocol-level evaluation anchors that make trade-offs comparable [@Van2025Survey; @Sarkar2025Survey].

Finally, safety and security work is increasingly central in agent deployments. Threat models such as prompt injection and tool abuse are not merely “misuse cases”; they reflect systematic vulnerabilities created by open-ended tool interfaces and insufficient provenance tracking. Accordingly, we treat risk and governance as part of the core design space, not as an afterthought [@Zhang2025Security; @Shi2025Progent].

## Foundations & Interfaces

Foundations and interfaces set the execution contract for everything that follows: what counts as state, what counts as an action, and what feedback closes the loop. A practical tension is that more expressive action spaces make it easier to solve open-ended tasks, but they also make it harder to verify behavior and to enforce safety boundaries under realistic budgets [@Yao2022React; @Du2024Anytool].

We therefore separate the “agent loop” choice (how decisions and observations are represented) from the “tool interface” choice (how actions are grounded in APIs and orchestrators). The two are intertwined: orchestration policies can effectively redefine the action space, and benchmarks often implicitly assume one loop structure, which can bias conclusions about what works [@Liu2025Mcpagentbench; @Liu2025Toolscope].

### Agent loop and action spaces

A tool-using agent is defined as much by its loop boundary and action space as by its underlying model. A central tension is that natural-language actions make the loop flexible and easy to prototype, whereas more structured actions make behavior easier to audit and evaluate under a protocol with explicit budgets and stop conditions [@Yao2022React; @Kim2025Bridging]. This suggests that “agent capability” should be compared through interface-level decisions, not just through anecdotal task success.

At the system level, evaluation is realized in a range of implementations (e.g., Li et al. [@Li2025From]; Ghose et al. [@Ghose2025Orfs]; Song et al. [@Song2026Envscaler]; Wu et al. [@Wu2025Meta]; You et al. [@You2025Datawiseagent]; and Xu et al. [@Xu2025Exemplar]).

To ground this loop view, many systems adopt variants of the state→decide→act→observe abstraction, but differ in what they treat as state (raw observations vs summarized memory) and what they treat as an action (free-form text vs constrained tool calls). In 2022, ReAct made this distinction visible by interleaving reasoning traces with explicit actions, which helped clarify where planning ends and execution begins [@Yao2022React]. Moreover, later systems often reinterpret “action space” as an interface contract: what actions are allowed, how errors are handled, and how retries are counted in evaluation [@Liu2025Mcpagentbench].

Action spaces also shape what benchmarks can measure. When actions are continuous or open-ended, evaluation tends to rely on end-to-end task completion metrics that blur failure causes; in contrast, discretized action spaces enable finer-grained metrics (e.g., subtask success, tool-call validity, and cost) but may under-represent real deployment variability. Tool-focused environments such as ToolGym (2026) make this trade-off explicit by constraining what actions are executable while still exposing long-horizon dependencies that stress planning [@Xi2026Toolgym].

Recent benchmark work highlights that even “agent loop” papers often embed evaluation assumptions. For example, AgentSwift reports evaluation across seven benchmarks spanning multiple domains, which helps separate loop design choices from narrow task artifacts [@Li2025Agentswift]. Similarly, LocoBench (2025) and MCPAgentBench (2025) emphasize protocol design—reset policies, task suite composition, and scoring—so that differences in action representations can be compared under a more stable metric definition [@Qiu2025Locobench; @Liu2025Mcpagentbench]. However, these benchmarks still differ in tool availability and cost modeling, so cross-benchmark comparisons remain limited.

Taken together, the most actionable comparisons focus on how loop boundary choices interact with evaluation constraints. Works that emphasize explicit routing or control decisions can look stronger under cost-aware metrics because they reduce wasted actions, whereas more free-form loops may appear stronger under unconstrained “success if eventually solved” scoring. Therefore, a protocol-aware synthesis should ask: what actions are allowed, what is penalized (latency, tool calls, tokens), and what verification is required before an episode is considered complete [@Zhao2025Achieving; @Zhang2026Evoroute].

A key limitation is that richer action spaces enlarge the error surface. In security-relevant settings, a mis-specified action representation can turn a recoverable planning error into an irreversible tool abuse event, and benchmark scores may not reflect the real risk trade-off. Evidence from domain-specific agent studies (e.g., cybersecurity investigation workflows) underscores that the same loop design can behave very differently when the environment is adversarial or partially observed, so future work should report threat-model assumptions alongside the action space definition [@Fumero2025Cybersleuth; @Feng2025Group].

One way to make these comparisons less hand-wavy is to treat the loop boundary as an experimental variable. If a benchmark exposes the same environment but changes what counts as an action (free-form steps vs validated tool calls), then improvements can be attributed more cleanly to planning or to interface choice. AgentSwif(t) explicitly leans on breadth—seven benchmarks across heterogeneous domains—which is useful precisely because it pressures the action space to be robust across different observation and feedback modalities [@Li2025Agentswift]. By contrast, specialized benches can go deeper on protocol detail: they can specify tool schemas, episode resets, and scoring for intermediate steps, but they risk overfitting conclusions to one style of loop [@Liu2025Mcpagentbench; @Qiu2025Locobench].

For practitioners, a lightweight checklist helps: (i) make the action space explicit (what is callable, with what parameters), (ii) log enough state to support post-mortems, and (iii) choose evaluation metrics that separate “did it solve the task” from “did it waste actions or take unsafe actions.” This is where controlled environments such as ToolGym are valuable: they let authors report success, cost, and error-recovery behavior under comparable protocols instead of relying on narrative demos [@Xi2026Toolgym; @Zhang2026Evoroute]. However, even with better harnesses, action-space design remains limited by mismatched assumptions about tool reliability and environment adversariality, which is why domain-specific studies remain important for stress testing loop definitions [@Fumero2025Cybersleuth; @Feng2025Group].

Concretely, surveys can normalize comparisons by asking each paper to spell out an “episode contract”: the allowed tool schemas, retry/error semantics, stop conditions, and what is logged for post-mortems. Benchmarks such as MCPAgentBench and ToolGym make these contracts more explicit, which is precisely why they are useful for isolating the impact of action representations and loop boundaries [@Liu2025Mcpagentbench; @Xi2026Toolgym]. With this contract view, classic reasoning–action splits like ReAct can be evaluated less as a prompting trick and more as a design choice about traceability and auditability under protocol constraints [@Yao2022React; @Kim2025Bridging].

After Agent loop and action spaces, Tool interfaces and orchestration makes the bridge explicit via function calling, tool schema, routing; tool interface (function calling, schemas, protocols), tool selection / routing policy, setting up a cleaner A-vs-B comparison.

### Tool interfaces and orchestration

Tool interfaces are the “ground truth boundary” for LLM agents: they determine what actions can actually change the world, what observability the agent has, and what constraints (permissions, rate limits, and cost budgets) are enforced. A central tension is that richer tool APIs reduce friction for general problem solving, whereas tighter interface contracts make failures easier to detect and safer to contain under realistic evaluation protocols [@Du2024Anytool; @Liu2025Toolscope]. This motivates treating interface design as a first-class comparison axis rather than an implementation detail.

Concrete implementations of tool interfaces appear in Cheng et al. [@Cheng2025Your]; Xuan et al. [@Xuan2026Confidence]; Li et al. [@Li2024Personal]; Li et al. [@Li2024Stride]; and Xian et al. [@Xian2025Measuring].

At the interface level, systems differ in whether they expose a small, curated function set or a large, open-ended tool catalog. AutoTool-style approaches emphasize automated tool selection and routing to reduce manual engineering, but this increases the need for reliable tool descriptions and robust fallback behavior when the selected tool fails [@Jia2025Autotool]. In contrast, AnyTool-like framing highlights generality—covering broad tool families—but pushes more responsibility onto orchestration policies to keep action sequences valid and interpretable [@Du2024Anytool].

Orchestration becomes especially visible when tools include memory substrates. MemTool reports evaluation across 13+ LLMs on the ScaleMCP benchmark, and it explicitly studies stability over 100 consecutive uses, which makes “interface drift” observable rather than anecdotal [@Lumer2025Memtool]. However, memory-as-a-tool also blurs the boundary between state and action: writing to memory is an action that can silently change future behavior, which complicates attribution when metrics improve or degrade [@Li2025Dissonances].

Because interface choices dominate failure modes, tool-use evaluation needs protocols that surface tool-specific errors (invalid parameters, permission violations, hallucinated tool names) and account for cost. Evaluation-oriented studies emphasize that many reported improvements are not comparable unless the protocol specifies tool availability, allowed retries, and what counts as a successful tool call versus a successful end-to-end task [@Mohammadi2025Evaluation; @Dong2025Bench]. In practice, a benchmark that scores only final success can hide systematic tool misuse, whereas a protocol that scores tool validity may penalize exploratory behavior that is acceptable in some deployments.

Self-improvement loops interact with tool interfaces in a subtle way. When agents learn to challenge or critique themselves, they often change not just the content of their reasoning but also their tool-use policies (when to call a tool, when to verify, and when to stop). Therefore, the same interface can yield different trade-offs depending on whether the system emphasizes conservative verification or aggressive exploration under a fixed budget [@Zhou2025Self; @Liu2025Toolscope]. In contrast, purely static tool policies may look strong on narrow benchmarks but remain brittle when tool distributions shift.

A key limitation is that the interface contract is rarely fully specified in papers: permissions, sandboxing, and failure recovery policies are often implicit. This makes it hard to transfer conclusions from one tool suite to another, even when the underlying model is similar. Future work should report interface-level conditions—tool schemas, constraints, and logging hooks—as part of the experimental protocol, so that orchestration decisions can be compared without relying on “demo feel” as evidence [@Mohammadi2025Evaluation; @Li2025Dissonances].

Moreover, “tool interface” includes more than the list of functions: it includes schemas, validation, error semantics, and permissioning. ToolScope-style analyses emphasize that agents can appear competent while silently relying on underspecified contracts (e.g., tools that never fail, or tools that return perfectly formatted outputs), which can collapse when moved to real deployments [@Liu2025Toolscope]. In contrast, evaluation harnesses that explicitly score tool-call validity and parameter correctness surface failure modes early, even if they make headline success rates look worse [@Dong2025Bench; @Mohammadi2025Evaluation].

From a systems standpoint, a practical orchestration policy should specify at least three things: selection (how a tool is chosen), execution (how errors are handled and retried), and verification (what evidence is required before an action is accepted). AutoTool-style routing helps on selection, but without strong verification it can amplify tool misuse; AnyTool-style breadth helps coverage, but it increases the burden on execution-time safeguards [@Jia2025Autotool; @Du2024Anytool]. MemTool’s repeated-use setting is a useful stress test here because it makes “policy drift” observable over long runs rather than only in single episodes [@Lumer2025Memtool; @Li2025Dissonances].

Finally, interface comparisons need observability, not just API lists. Without structured logging of tool arguments, tool outputs, and verification signals, interface failures collapse into “model errors” in post-hoc analysis. Tool-focused evaluations argue for reporting tool-call traces and schema validation outcomes as part of the benchmark artifact, because they reveal whether a method improves selection, execution robustness, or verification discipline [@Dong2025Bench; @Liu2025Toolscope]. Long-run stress tests are particularly informative here: they can expose interface drift, caching pathologies, and subtle permission mismatches that single-episode benchmarks miss [@Lumer2025Memtool; @Mohammadi2025Evaluation].

## Core Components (Planning + Memory)

Planning and memory are the two reusable components that most directly determine whether an agent can act coherently over long horizons. Planning governs how the system allocates deliberation and chooses actions; memory governs what the system treats as state and how it stays grounded when context is long or noisy [@Yao2022React; @Wei2025Memory].

The key comparison axes in this chapter are protocol-level: cost budgets, tool availability, and evaluation metrics. Without holding these fixed, it is easy to conflate “better planning” with “more expensive planning,” or to attribute gains to memory when they are actually due to different task setups [@Liu2025Costbench; @Tao2026Membox].

### Planning and reasoning loops

Planning and reasoning loops are the control layer that turns an agent from a reactive tool caller into a system that can allocate deliberation, recover from errors, and trade off cost against success. A central tension is that deeper deliberation can improve long-horizon task completion, whereas stricter budgets and cost models penalize unnecessary reasoning and tool calls [@Yao2022React; @Liu2025Costbench]. This implies that planning methods should be compared under protocols that make budgets and stopping rules explicit.

Concrete system realizations of planning/control loop are described in Silva et al. [@Silva2025Agents]; Hatalis et al. [@Hatalis2025Review]; Lu et al. [@Lu2025Pilotrl]; Huang et al. [@Huang2025Surgical]; Hu et al. [@Hu2025Evaluating]; and Yang et al. [@Yang2025Coarse].

One route is to improve planning via learning signals rather than only via prompting. For example, Hu et al. (2025) report that a 1.5B-parameter model trained with single-turn GRPO can outperform larger baselines up to 14B parameters on a complex task planning benchmark, with success rates reported around 70% for long-horizon planning [@Hu2025Training]. Moreover, this kind of result is only meaningful when the benchmark specifies the task distribution and metric definition; otherwise, “better planning” can collapse into “different evaluation.”

A second route is to refine deliberation and guidance at inference time. In security-oriented settings, Nakano et al. (2025) report that a self-guided reasoning tool completes only 13.5%, 16.5%, and 75.7% of subtasks in different configurations and requires 86.2%, 118.7%, and 205.9% more model queries, highlighting that planning quality and cost are often tightly coupled [@Nakano2025Guided]. However, the interpretation depends on protocol details: what counts as a subtask, whether retries are allowed, and how tools are exposed.

Beyond raw planning performance, loop design decisions affect what kinds of reasoning are even possible. CostBench (2025) explicitly studies cost-aware planning and reports that agents may fail to identify cost-optimal solutions in static settings, which makes “budget awareness” a measurable axis rather than an afterthought [@Liu2025Costbench]. In contrast, protocol-free evaluations can hide whether an agent is truly planning or simply sampling many candidates until something works.

Taken together, the most informative evidence comes from comparisons that hold protocols fixed while varying loop mechanisms. Under a unified ReAct-style framework, Seo et al. (2025) evaluate 16 agents and report distinct capability and limitation profiles across models, suggesting that “planning loop” is not a monolith but a family of control patterns whose behavior depends on the environment and metric [@Seo2025Simuhome; @Yao2022React]. Therefore, survey synthesis should emphasize which control-loop assumptions are shared (planner/executor separation, search strategy, verification) and which are artifacts of evaluation.

A key limitation is that planning loops open new attack and failure surfaces. Reasoning-style vulnerabilities and process-oriented attacks can manipulate how an agent allocates steps, which may induce premature errors even when content filters are strong [@Zhou2025Reasoning]. This motivates treating verification, logging, and bounded reasoning policies as part of planning design, not just as safety add-ons, especially when systems rely on memory retrieval or novelty-seeking mechanisms that can amplify spurious plans [@Du2025Memr; @Kiruluta2025Novel; @Kim2025Bridging].

Viewed through a protocol lens, planning methods differ in what they optimize and what they assume. Some loops assume that extra deliberation is essentially free, so they optimize for success rate; others explicitly trade success against cost or latency, which turns planning into a constrained optimization problem. CostBench makes this distinction concrete by pushing cost-aware planning into the benchmark definition, while training-based approaches report gains under their own task distributions and reward specifications [@Liu2025Costbench; @Hu2025Training]. In contrast, prompt-centric planning claims can be hard to interpret unless the paper reports the same constraints (tool access, retry limits, budget) alongside the metric.

For readers deciding “what to build,” the most useful comparison is not CoT vs ToT by name, but whether a system has (i) an explicit planner/executor split, (ii) a verification step that can halt bad plans early, and (iii) a budgeted control policy that prevents runaway deliberation. Simulation-style evaluations such as SimuHome help because they unify multiple agents under a shared ReAct-style loop and reveal how planning behavior changes with environment complexity [@Seo2025Simuhome; @Yao2022React]. However, because planning interacts with novelty-seeking and memory retrieval, conclusions can drift when the loop includes additional components; this is one reason to demand ablations and protocol reporting when papers claim general planning improvements [@Kiruluta2025Novel; @Du2025Memr; @Hong2025Planning].

One pragmatic synthesis is to separate “planner capacity” from “planner discipline.” Capacity asks whether the loop can search or decompose tasks effectively; discipline asks whether it can stop, verify, and stay within budget when search is tempting. Benchmarks that expose both success and resource traces (queries/tool calls/latency) make this distinction measurable and reduce the chance that a method wins by simply spending more [@Liu2025Costbench; @Nakano2025Guided]. For future surveys and evaluations, reporting a small set of standardized controls (budget, retry policy, and verification trigger) would make cross-paper planning claims far easier to compare than yet another taxonomy of prompting names [@Seo2025Simuhome; @Hu2025Training].

Memory and retrieval (RAG) follows naturally by turning Planning and reasoning loops's framing into retrieval, index, write policy; memory type (episodic / semantic / scratchpad), retrieval source + index (docs / web / logs)-anchored evaluation questions.

### Memory and retrieval (RAG)

Memory and retrieval determine what an agent treats as state over time, which is why many “agent failures” are memory failures: missing context, stale summaries, or ungrounded assumptions. A central tension is that richer memory improves grounding in multi-turn settings, whereas aggressive compression and retrieval heuristics can introduce drift that is hard to detect under coarse evaluation metrics [@Wei2025Memory; @Yao2022React]. This suggests that memory mechanisms should be evaluated with protocols that stress long-horizon consistency rather than only single-turn accuracy.

At the system level, memory/retrieval is realized in a range of implementations (e.g., Yu et al. [@Yu2026Agentic]; Xia et al. [@Xia2025From]; Zhang et al. [@Zhang2025Large]; and Ye et al. [@Ye2025Taska]).

A practical way to organize memory work is by the unit of storage and the unit of retrieval. Some systems store free-form text summaries; others store structured traces, graphs, or tool call records that can be queried more deterministically. Memory-box style systems make the “state representation” explicit by treating memory modules as composable components that can be swapped and benchmarked across tasks [@Tao2026Membox]. In contrast, more implicit memory can look simpler to implement but becomes difficult to audit when failures occur.

Structured memories also enable different evaluation lenses. AriGraph (2024) uses a graph-based representation that can support targeted retrieval and relational reasoning, which can change the failure modes compared to purely textual retrieval [@Anokhin2024Arigraph]. Moreover, task-specific memory policies often hinge on what the environment exposes (tool logs, web pages, code execution traces), which is why “memory” in agents is not identical to classic RAG in static QA settings [@Ye2025Task].

Recent comparative studies make the protocol issue concrete. Wei et al. (2025) report implementing over ten representative memory modules and evaluating them across 10 diverse multi-turn, goal-oriented tasks, which helps isolate what memory contributes under a shared metric definition [@Wei2025Memory]. However, even such studies can be sensitive to evaluation details: how success is scored, how long context is truncated, and whether external tools are available.

In contrast to hand-designed memory modules, meta-level approaches emphasize learning or adapting retrieval policies. These routes can reduce manual engineering, but they also risk overfitting to benchmark artifacts or amplifying spurious correlations when memory content is noisy [@Tawosi2025Meta; @Abbineni2025Muallm]. Therefore, when comparing memory systems, it is useful to separate (i) what is stored, (ii) how it is retrieved, and (iii) how retrieval decisions are verified.

A key limitation is that memory introduces new security and robustness concerns: poisoning, prompt injection via stored content, and subtle state drift that only manifests many steps later. Without explicit provenance and verification hooks, a memory system can accumulate errors that look like “planning mistakes” downstream. This motivates evaluation protocols that include stress tests for long-horizon consistency and adversarial memory content, rather than treating memory as a purely helpful add-on [@Du2025Memr; @Ye2025Task].

A complementary viewpoint is to treat retrieval as an autonomous subsystem with its own objectives and failure modes. MemR$^3$ frames memory retrieval as an agent-like process, which highlights that retrieval is not only about finding relevant text but also about deciding what evidence is trustworthy and when to refresh state [@Du2025Memr]. In contrast, graph-based representations such as AriGraph make state relational and queryable, which can reduce some hallucination modes but introduces its own brittleness when the graph construction is noisy or incomplete [@Anokhin2024Arigraph].

In practice, memory evaluation should go beyond average task success and measure stability: does the agent’s state drift across repeated runs, does retrieval amplify spurious correlations, and does summarization silently erase constraints needed for safe tool use. Comparative evaluations across 10 multi-turn tasks and many memory modules are helpful because they expose which improvements are consistent and which are benchmark-specific [@Wei2025Memory]. Still, a limitation is that long-horizon memory failures often look like planning failures; future protocols should include explicit memory stress tests (poisoned entries, contradictory evidence, and partial observability) so that memory and planning contributions can be disentangled [@Tao2026Membox; @Ye2025Task].

A useful reporting standard is to make memory operations first-class in the trace: what was retrieved, why it was retrieved (query), and how it changed the next action. Without such traces, it is hard to tell whether improved success comes from better state modeling or from accidental leakage (e.g., storing answers in memory) under a lax protocol. Modular frameworks like MemBox make it easier to swap storage/retrieval policies under identical tasks, while graph-based representations make state relational and queryable; both directions strengthen the case for treating memory as an auditable subsystem rather than a “black box context” [@Tao2026Membox; @Anokhin2024Arigraph]. Even in abstract-first evidence, surveys can insist on this traceability contract: memory should be inspectable and verifiable, not merely available [@Du2025Memr; @Wei2025Memory].

## Learning, Adaptation & Coordination

Adaptation mechanisms and multi-agent coordination are often introduced as ways to reduce brittleness: agents can critique and improve themselves over time, or multiple agents can divide labor and cross-check outputs. However, these mechanisms also introduce new failure modes—reward hacking, instability, and coordination overhead—that only become visible under long-horizon evaluation protocols [@Zhou2025Self; @Sarkar2025Survey].

This chapter frames the landscape through two questions: how agents change with experience (reflection, optimization, or learned controllers), and how they coordinate (roles, protocols, aggregation). In both cases, the most informative comparisons connect mechanism choices back to measurable outcomes under a stated benchmark or metric [@Zhou2024Archer; @Chuang2025Debate].

### Self-improvement and adaptation

Self-improvement mechanisms aim to make agents less brittle by letting them adapt their behavior based on feedback, critique, or learned optimization signals. A central tension is that adaptation can improve robustness on multi-turn tasks, whereas it can also induce drift and reward hacking that is invisible under short-horizon metrics [@Zhou2025Self; @Van2025Survey]. This motivates evaluating adaptation as a protocol-aware system change, not as a cosmetic prompt tweak.

Concrete system realizations of learning/feedback are described in Xia et al. [@Xia2025Sand]; Sarukkai et al. [@Sarukkai2025Context]; Chen et al. [@Chen2025Grounded]; He et al. [@He2025Enabling]; Schneider et al. [@Schneider2025Learning]; and Wang et al. [@Wang2025Ragen].

One common pattern is “self-challenging” or critique loops that modify the agent’s plan or tool-use policy after failures. Zhou et al. (2025) evaluate a self-challenging framework on two multi-turn tool-use agent benchmarks (M3ToolEval and TauBench), emphasizing that improvements should be measured under fixed task suites and scoring rules rather than via cherry-picked examples [@Zhou2025Self]. Moreover, such loops often change when the agent decides to verify or to stop, which can alter cost and failure modes under a budgeted protocol.

Another pattern is to learn controllers that adapt actions based on environmental feedback. RL-style approaches for agents can formalize adaptation objectives, but they inherit classical pitfalls: proxy objectives, instability, and sensitivity to evaluation distribution shift. Archer-style training (2024) illustrates this route by explicitly optimizing agent behavior rather than relying purely on prompt engineering, which can improve repeatability when protocols are well specified [@Zhou2024Archer; @Zhao2025Achieving]. In contrast, purely prompt-based adaptation may appear strong in demos but be hard to reproduce.

Adaptation also interacts with the rest of the agent stack. Systems that evolve or route behaviors over time can shift which planning and memory components are actually used, which makes ablations essential for interpretation [@Wu2025Evolver; @Zhang2026Evoroute]. Therefore, a survey synthesis should treat “adaptation” as a coordination problem across components: what changes, what remains fixed, and what evidence supports the claimed improvement.

Taken together, adaptation gains are most convincing when they come with protocol-level anchors: benchmark names, metrics, and budget conditions. It is also helpful when papers link adaptation back to a clear failure analysis (what went wrong before, what changes after), because otherwise improvements can be indistinguishable from overfitting to the evaluation harness [@Zhou2025Self; @Nitin2025Faultline]. This suggests that adaptation research should report both performance and stability metrics across repeated runs.

A key limitation is that self-improvement can amplify errors if the feedback loop is mis-specified. Even when adaptation improves average performance, it may increase tail risk (rare catastrophic failures) or degrade safety properties if verification is relaxed. This motivates coupling adaptation with memory and monitoring: tracking what changed, under what evidence, and how the agent’s policy can be rolled back when anomalies are detected [@Wei2025Memory; @Belle2025Agents; @Yao2022React].

Beyond critique loops, “adaptation” increasingly includes explicit policy evolution: systems that route between behaviors or evolve prompts/programs based on observed failures. Evolver-style approaches make this a first-class mechanism rather than an ad-hoc retry, but they also raise the bar for evaluation: the protocol must specify what feedback is available, how many iterations are allowed, and how overfitting is detected [@Wu2025Evolver]. Likewise, route-selection or evolution frameworks can change which tools are used and which failure modes appear, so it is essential that papers report not just final success but also how the policy changed over time [@Zhang2026Evoroute; @Zhao2025Achieving].

For safety and reliability, adaptation needs guardrails. A practical mitigation is to treat adaptations as versioned changes with explicit rollback and to couple improvements with verification policies that prevent “improving” by relaxing checks. This is especially important when adaptation interacts with memory: a self-improving agent can learn to exploit a memory shortcut that passes a benchmark but fails under distribution shift [@Wei2025Memory; @Nitin2025Faultline]. Therefore, even in abstract-first evidence, a cautious survey should frame adaptation claims as conditional on protocol details and stability evidence, not as unconditional progress [@Belle2025Agents; @Zhou2025Self].

Methodologically, adaptation papers would benefit from borrowing evaluation idioms from software changes: report a baseline, a delta (what changed), and a regression envelope (what got worse). If a self-challenging loop increases success, it is also important to report whether it increases tool-call count, latency, or failure severity under the same budget, and whether gains persist across benchmark variants rather than only the training-like distribution [@Zhou2025Self; @Nitin2025Faultline]. Evolutionary or routing-based adaptation can be framed as a controlled search over policies; without clear limits on iterations and explicit overfitting checks, it becomes hard to separate genuine robustness from benchmark-specific tuning [@Wu2025Evolver; @Zhang2026Evoroute].

Even lightweight audits help: track which tools and verification steps are used before and after adaptation, and report whether “improvements” come from better decisions or from relaxed checks. This makes adaptation results easier to reuse across settings and reduces the chance that protocol drift is mistaken for capability gains [@Zhou2025Self; @Zhao2025Achieving].

Multi-agent coordination follows naturally by turning Self-improvement and adaptation's framing into roles, communication, debate; communication protocol + role assignment, aggregation (vote / debate / referee)-anchored evaluation questions.

### Multi-agent coordination

Multi-agent systems extend the agent loop by distributing cognition across roles—planning, execution, verification, or critique—rather than forcing a single policy to do everything. A central tension is that specialization can reduce individual brittleness, whereas coordination introduces overhead, communication failures, and the risk of correlated errors [@Sarkar2025Survey; @Chuang2025Debate]. This implies that coordination protocols should be evaluated as first-class design choices.

In practice, the literature operationalizes coordination across multiple implementations (e.g., Papadakis et al. [@Papadakis2025Atlas]; Wu et al. [@Wu2025Agents]; Chang et al. [@Chang2025Alas]; Zhang et al. [@Zhang2025Cognitive]; Hassouna et al. [@Hassouna2024Agent]; and Li et al. [@Li2025Draft]).

A practical coordination axis is whether agents communicate in free-form language or through more structured messages. Free-form debate can surface contradictions and improve coverage, but it can also devolve into persuasion rather than verification if incentives are misaligned [@Chuang2025Debate]. In contrast, more structured collaboration protocols can be easier to audit, but they may limit the diversity of hypotheses explored.

Representative systems illustrate different points in this trade-off space. Voyager (2023) emphasizes long-horizon behavior through iterative skill acquisition and environment interaction, which naturally benefits from role separation across planning and execution [@Wang2023Voyager]. Smaller-agent studies (e.g., 2024) suggest that coordination quality is not purely a function of model scale; protocol design and task decomposition matter at least as much as raw capability [@Shen2024Small]. Moreover, coordination often changes evaluation: what counts as success for a team, and how credit is assigned.

Evaluation of multi-agent systems is still maturing. Work on evaluating multi-agent reasoning and collaboration highlights that aggregation rules (majority vote, arbitration, or hierarchical control) can change outcomes even when the underlying models are fixed, which makes “team performance” sensitive to protocol details [@Yim2024Evaluating; @Zahedifar2025Agent]. Therefore, multi-agent benchmarks should specify communication bandwidth, turn limits, and stopping criteria, not just final task correctness.

Taken together, coordination mechanisms are best compared by the failure modes they prevent or introduce. Debate-style systems can catch some factual errors, but they may still share blind spots if all agents rely on the same tools or retrieval sources; conversely, tool diversity can increase robustness but also increases orchestration complexity [@Sarkar2025Survey; @Lumer2025Memtool]. This suggests that practical systems may need mixed protocols: structured checks for safety-critical steps and freer collaboration for exploration.

A key limitation is that coordination can create new incentive problems and attack surfaces (collusion, sybil behaviors, and adversarial messaging). RL-based coordination policies may mitigate some issues, but they also require careful evaluation under distribution shift and adversarial conditions [@Cao2025Skyrl; @Zhou2024Archer]. Future work should report coordination assumptions explicitly and include stress tests where communication is noisy, tools are unreliable, or some agents are compromised.

Coordination overhead is not a footnote; it is a measurable part of system behavior. Communication bandwidth, turn limits, and shared tool access can dominate both cost and accuracy, which is why coordination protocols should be reported alongside task metrics. Shared memory or shared tool substrates can help agents stay aligned, but they also increase coupling and can make correlated errors more likely if all agents depend on the same retrieved evidence [@Lumer2025Memtool; @Sarkar2025Survey].

From an evaluation standpoint, the field still lacks “calibrated” multi-agent benchmarks: many papers report team-level success without specifying how disagreement is resolved or how failures are attributed. Work that explicitly evaluates collaboration and aggregation highlights that arbitration rules and verification roles can flip outcomes even under identical underlying models, which suggests that coordination should be tested under controlled protocol variations [@Yim2024Evaluating; @Zahedifar2025Agent]. This remains limited by the lack of standardized settings for adversarial messaging, noisy communication, and partial tool failures, so results should be read as conditional rather than universal [@Cao2025Skyrl; @Chuang2025Debate].

From a design perspective, many practical patterns can be expressed as separation of concerns: one agent proposes, another critiques, and a third arbitrates under a fixed policy. This makes coordination closer to an executable protocol than a vague “teamwork” metaphor, and it naturally suggests ablations: swap the arbiter policy, restrict communication, or remove shared memory to see which component drives gains [@Yim2024Evaluating; @Zahedifar2025Agent]. A remaining open question is how these protocols scale with tool-use: as agents share tool access and memory, correlated errors become more likely, so coordination should be paired with diversity mechanisms (independent retrieval, heterogeneous tools) and explicit budgets on communication to avoid winning by simply “talking more” [@Lumer2025Memtool; @Sarkar2025Survey].

As a result, survey comparisons should treat communication policy (who speaks, how often, with what evidence) as an evaluation variable rather than a narrative detail. Protocol choices like turn limits, arbitration rules, and messaging constraints often matter as much as model choice once systems scale beyond two roles [@Yim2024Evaluating; @Cao2025Skyrl].

## Evaluation & Risks

Evaluation and risk are the shared constraints that determine whether an agent design is meaningful beyond a demo. Benchmarks define what is measured and what is ignored; security and governance define what cannot be ignored in deployment, even if it is inconvenient to measure [@Mohammadi2025Evaluation; @Zhang2025Security].

We focus on protocol-aware evaluation: task suites, cost models, and reproducibility controls, alongside threat models such as prompt injection, data exfiltration, and tool abuse. The goal is to make the evaluation layer explicit enough that conclusions can transfer across systems and settings [@Fu2025Eval; @Kale2025Reliable].

### Benchmarks and evaluation protocols

Benchmarks and evaluation protocols are the common language that makes agent claims comparable, yet they are also a major source of disagreement: the same agent can look strong under an unconstrained success metric and weak under a budgeted or safety-aware protocol. A central tension is that end-to-end benchmarks capture realistic failure compounding, whereas component-level evaluations make it easier to attribute improvements and to reproduce results [@Mohammadi2025Evaluation; @Fu2025Eval]. This motivates treating protocol design as part of the agent research contribution.

Concretely, prior work instantiates tool interfaces in several systems (e.g., Ji et al. [@Ji2025Taxonomy]; Zhan et al. [@Zhan2025Sentinel]; Dagan et al. [@Dagan2024Plancraft]; Zhu et al. [@Zhu2025Where]; and Lidayan et al. [@Lidayan2025Abbel]).

Benchmark suites for agents are increasingly diverse. Some focus on interactive task completion, others on tool-use correctness, and others on safety stress tests. AgentSquare (2024) exemplifies the “suite” approach by packaging multiple scenarios under a consistent harness, which can reduce the temptation to overfit to a single task type [@Shang2024Agentsquare]. However, suites still vary in tool availability and reset policies, so comparisons across suites remain limited unless protocols are standardized.

Reliability-focused work emphasizes that metrics must reflect what matters in deployment: not just whether an answer is correct, but whether it is obtained efficiently and safely under constraints. This leads to protocol elements such as cost budgets, latency caps, and repeated-trial stability checks, which can change which methods look preferable [@Kale2025Reliable; @Kim2026Beyond; @Li2026Autonomous]. Moreover, explicit protocol reporting enables reviewers to identify when gains come from harness differences rather than from method differences.

Security-aware evaluation further complicates the picture. Zhang et al. (2025) propose a taxonomy of 12 agent attacks, including categories such as prompt injection and manipulation routes, which provides a concrete scaffold for threat-model-aware benchmarking rather than ad-hoc red teaming [@Zhang2025Security]. Systems like Progent-style evaluations connect these threat models to agent use cases and show that safety conclusions depend on both interface exposure and monitoring, not just on model alignment [@Shi2025Progent].

Taken together, the evaluation layer should be understood as a set of design choices with their own failure modes: leakage, non-determinism, and missing cost models can all bias conclusions. Therefore, survey comparisons should prefer studies that report protocol details (budgets, tool access, scoring rules) and that include ablations separating model capability from harness effects [@Chen2025Towards; @Mohammadi2025Evaluation].

A key limitation is that many benchmarks remain partial proxies for deployment. They may omit adversarial environments, long-tail tool failures, or governance constraints, leading to optimistic performance estimates. This motivates two directions: (i) richer benchmark reporting standards (what was allowed and what was measured), and (ii) cross-benchmark calibration studies that quantify how conclusions change when protocols differ [@Fu2025Eval; @Kale2025Reliable].

A useful mental model is that an agent benchmark is a bundle of hidden assumptions: tool reliability, availability, budget, and what counts as evidence. When these are not reported, evaluation becomes a moving target, and seemingly “better” methods may simply be evaluated under easier conditions. This is why evaluation meta-work argues for reporting standards and for separating harness effects from model effects, especially when systems involve tool calls and multi-turn state [@Mohammadi2025Evaluation; @Fu2025Eval].

For security-aware evaluation, protocol specificity is even more critical. Attack taxonomies (e.g., 12 attack categories) provide coverage targets, but they only become meaningful when a benchmark specifies attacker capabilities, tool permissions, and monitoring hooks [@Zhang2025Security]. Therefore, the most actionable direction is to treat benchmark design as an engineering artifact: publish harnesses, specify randomness controls, and include cost models so that results can be reproduced and compared across labs [@Kim2026Beyond; @Kale2025Reliable; @Li2026Autonomous].

Calibration is arguably the missing layer. When two benchmarks claim to measure similar abilities but differ in tool availability, reset rules, and scoring, headline results cannot be compared directly. Meta-evaluation work suggests that a small number of controlled protocol swaps (budget, tool set, verification rule) can quantify how brittle a method is to harness changes and can prevent leaderboard overfitting from being mistaken for progress [@Mohammadi2025Evaluation; @Fu2025Eval]. In this view, publishing harnesses and reporting randomness controls are not just reproducibility hygiene; they are the mechanism by which agent papers make claims that survive across labs [@Kim2026Beyond; @Li2026Autonomous].

Another practical lesson is to report failure structure, not only success. For agents, a single score often conflates tool-call errors, planning failures, and safety violations. Protocol-aware evaluation encourages logging intermediate events (tool-call validity, verification outcomes, abort reasons) and reporting distributions across runs, which supports a more honest discussion of reliability and risk envelopes [@Kale2025Reliable; @Chen2025Towards]. For safety-aware suites, threat taxonomies become useful only when mapped to concrete attacker capabilities and monitoring hooks; otherwise “secure” can mean “not tested” [@Zhang2025Security; @Shi2025Progent].

Rather than restarting, Safety, security, and governance carries forward the thread from Benchmarks and evaluation protocols and stresses it through threat model, prompt/tool injection, monitoring; threat model (prompt / tool injection, exfiltration), defense surface (policy, sandbox, monitoring).

### Safety, security, and governance

Safety and security for tool-using agents are not separable from system design: the tool interface defines the attack surface, the logging surface, and the available mitigations. A central tension is that higher autonomy reduces human oversight costs, whereas it increases the risk of prompt injection, data exfiltration, and tool abuse under realistic deployment constraints [@Zhang2025Security; @Shi2025Progent]. This implies that surveys should treat threat models as part of the agent design space, not as an external appendix.

Concrete implementations of security appear in Gasmi et al. [@Gasmi2025Bridging]; Hadeliya et al. [@Hadeliya2025When]; Luo et al. [@Luo2025Agrail]; Sha et al. [@Sha2025Agent]; An et al. [@An2025Ipiguard]; and Liu et al. [@Liu2026Agents].

Threat models for agents go beyond content manipulation. When an agent delegates actions to tools, attackers can target the decision process (e.g., steering tool selection) or the state (e.g., poisoning memory) rather than only the final output. Progent-style evaluations emphasize agent use cases and benchmarked attack conditions (e.g., AgentDojo/ASB/AgentPoison-style setups), illustrating that vulnerability depends on interface exposure and tool permissions as much as on model behavior [@Shi2025Progent]. However, reported results can be hard to compare without standardized protocols.

Mitigations often take the form of checks, monitors, and constrained execution. Systems that emphasize systematic checking aim to catch tool misuse and unsafe actions before they execute, but they also introduce latency and can create new failure modes when checks are incomplete or mis-specified [@Bonagiri2025Check; @Lee2025Bench]. In contrast, lightweight guardrails may preserve usability but provide weaker guarantees, especially under adaptive attackers.

A governance-oriented view reframes safety as an engineering contract: what actions are allowed, how they are audited, and what rollback mechanisms exist. Reliability and evaluation work suggests that safety claims should be tied to measurable protocol conditions (budgets, tool access, logging), because otherwise “safe” can mean “safe under a different harness” [@Kale2025Reliable; @Fu2025Eval]. Therefore, practical deployments may require layered controls: sandboxing at the tool layer, provenance at the memory layer, and monitoring at the decision layer [@Lichkovski2025Agent].

Taken together, agent safety research benefits from integrating threat models with evaluation. Attack taxonomies provide coverage targets, while benchmarks provide measurable failure rates under controlled conditions. This suggests that future evaluation suites should explicitly parameterize adversarial strength, tool permissions, and the cost of verification, rather than treating these as hidden implementation details [@Zhang2025Security; @Shi2025Progent].

Moreover, governance constraints are often external to research prototypes: real organizations impose compliance, privacy, and audit requirements that benchmarks rarely model. Bridging this gap likely requires collaboration between research and deployment communities, and a shift toward reporting “safety envelopes” (what the system can guarantee under what assumptions) rather than one-size-fits-all safety claims [@Li2025Stac; @Kale2025Reliable].

A concrete governance view is to describe “what can go wrong” in the same language as “what can be done.” If the action space includes file I/O, web access, or code execution, then the governance layer must specify which actions are permitted, how they are audited, and what escalation paths exist when anomalies are detected [@Lichkovski2025Agent]. In contrast, purely content-level guardrails may miss tool-layer abuse, which is why safety work increasingly ties checks to execution events rather than only to generated text [@Bonagiri2025Check; @Lee2025Bench].

Finally, governance constraints need to be made measurable. Reliability framing suggests reporting “safety envelopes” tied to protocol parameters: budgets, permissions, monitoring latency, and false positive/negative rates of checks [@Kale2025Reliable; @Fu2025Eval]. This remains limited by benchmark realism—organizational compliance and privacy constraints are rarely modeled—but the direction is clear: treat governance as a system contract with testable conditions, and report those conditions as part of the evaluation protocol rather than as a narrative disclaimer [@Li2025Stac; @Zhang2025Security].

At the system boundary, permissioning and provenance are the concrete levers that connect abstract threat models to implementable mitigations. If a benchmark treats tool calls as always-successful and side-effect free, it cannot meaningfully measure prompt injection, exfiltration, or tool abuse; conversely, realistic permissions and failure modes force methods to specify how they detect anomalies and when they escalate or refuse actions [@Zhang2025Security; @Shi2025Progent]. This is also why checkers and monitors should be evaluated with their own operating characteristics (latency, false positives/negatives), since aggressive checking can shift risk rather than eliminate it [@Bonagiri2025Check; @Lee2025Bench].

Governance becomes actionable when expressed as a testable contract: what events are logged, who can inspect them, and what interventions exist (rollback, sandboxing, human review). Reliability-focused framing encourages reporting these contracts alongside the evaluation harness, because monitoring latency and audit policies can change the effective threat model even if the underlying agent policy is unchanged [@Kale2025Reliable; @Fu2025Eval]. Practical deployments also motivate least-privilege tool design and clear escalation paths, so that when an agent fails it fails safely; this engineering detail deserves to be part of survey comparisons rather than an afterthought [@Lichkovski2025Agent; @Li2025Stac].

## Discussion

A recurring pattern across agent systems is that progress often comes from tightening contracts rather than adding new prompts. When the tool boundary is explicit (what can be called, with what permissions, and under what cost model), it becomes easier to reason about failure recovery and to compare planning or memory strategies under consistent protocols [@Du2024Anytool; @Liu2025Costbench]. In contrast, when the boundary is implicit, many improvements become hard to attribute: gains may come from better prompting, better tool routing, or simply different evaluation settings.

A second theme is that evaluation is still the bottleneck. Benchmarks are proliferating, but protocol details (cost budgets, tool availability, reset policies, and contamination controls) often vary enough that “higher score” does not translate into a clear engineering decision [@Mohammadi2025Evaluation; @Fu2025Eval]. This gap is especially visible for long-horizon tasks, where small differences in memory policy or error handling compound over time.

Finally, safety and governance are not add-ons; they are entangled with interface design. Prompt injection and tool abuse are easiest to study when systems expose a clear action space and logging surface, but these same interfaces also enlarge the attack surface if permissions and provenance are under-specified [@Zhang2025Security; @Shi2025Progent]. A practical implication is that agent research should report threat models and monitoring hooks as first-class experimental conditions, not as post-hoc disclaimers.

## Conclusion

Evidence from recent tool-using agent work suggests that “agent performance” is not a single axis: interface contracts, planning/memory components, and adaptation/coordination schemes interact with evaluation protocols in ways that can reverse conclusions across benchmarks. As a result, survey taxonomies should be decision-oriented: they should help a reader predict which design choices change reliability, cost, and risk under a stated protocol.

Looking forward, the most valuable work may be less about inventing new loop variants and more about standardizing what it means to compare them: shared task suites, explicit budgets and tool access policies, and threat-model-aware evaluations that reflect real deployment constraints [@Mohammadi2025Evaluation; @Zhang2025Security].
